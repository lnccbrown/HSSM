{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introducing a custom Reinforcement Learning - Sequential Sampling Model (RLSSM) into HSSM**\n",
    "\n",
    "This tutorial demonstrates how to incorporate custom Reinforcement Learning Sequential Sampling Models (RLSSM) into the HSSM framework by modifying the `rldm.py` file. The tutorial walks through the key steps needed to define, implement, and integrate your custom RLSSM likelihood functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "# Import HSSM and simulator package\n",
    "import hssm\n",
    "from hssm.utils import decorate_atomic_simulator\n",
    "from hssm.likelihoods.rldm import make_rldm_logp_op\n",
    "from hssm.distribution_utils.dist import make_hssm_rv\n",
    "from ssms.basic_simulators.simulator import simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for the plots\n",
    "plt.style.use('seaborn-v0_8-dark-palette')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section I: Introduce the likelihood function for your custom model in HSSM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define Your Custom RLSSM Model Configuration\n",
    "\n",
    "**Location**: Add to the `rlssm_model_config_list` dictionary\n",
    "\n",
    "**Purpose**: Define the meta-data and parameters of your custom model in a configuration dictionary.\n",
    "\n",
    "**Details**:\n",
    "- Create a new entry in the `rlssm_model_config_list` dictionary with a unique model name\n",
    "- Specify the following required fields:\n",
    "  - `name`: Name your custom rlssm model\n",
    "  - `description`: Optional description of the model\n",
    "  - `n_params`: Number of model parameters \n",
    "  - `n_extra_fields`: Number of extra_fields columns passed to the likelihood function (e.g., trial, feedback)\n",
    "  - `list_params`: List of parameter names in the order they'll be passed to the likelihood function\n",
    "  - `extra_fields`: List of extra_fields columns\n",
    "  - `decision_model`: Type of the likelihood for the decision process model (typically \"LAN\" - likelihood approximation networks)\n",
    "  - `LAN`: Specific LAN model to use (e.g., \"angle\")\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "\"my_custom_rlssm\": {\n",
    "    \"name\": \"my_custom_rlssm\", \n",
    "    \"description\": \"Custom RLSSM with special features\", \n",
    "    \"n_params\": 8, \n",
    "    \"n_extra_fields\": 3, \n",
    "    \"list_params\": [\"param1\", \"param2\", \"param3\", ...], \n",
    "    \"extra_fields\": [\"extra_fields1\", \"extra_fields2\", \"extra_fields3\", ...], \n",
    "    \"decision_model\": \"LAN\", \n",
    "    \"LAN\": \"angle\", \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Specify Which Model Configuration to Use\n",
    "\n",
    "\n",
    "\n",
    "**Location**: Update the `MODEL_NAME` variable\n",
    "\n",
    "\n",
    "\n",
    "**Purpose**: Inform the HSSM package which model configuration to use from your defined list.\n",
    "\n",
    "\n",
    "\n",
    "**Details**:\n",
    "\n",
    "- Set `MODEL_NAME` to match one of the keys in your `rlssm_model_config_list` dictionary\n",
    "\n",
    "- The system will automatically load the corresponding configuration\n",
    "\n",
    "- This makes it easy to switch between different model variants\n",
    "\n",
    "\n",
    "\n",
    "**Example**:\n",
    "\n",
    "```python\n",
    "\n",
    "MODEL_NAME = \"my_custom_rlssm\"Â  # Must match a key in rlssm_model_config_list\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Your Custom Likelihood Function\n",
    "\n",
    "**Location**: Create a new function following the naming pattern `{model_name}_logp_inner_func`\n",
    "\n",
    "**Purpose**: Define the core computational logic for your RLSSM model. See the existing implementation in `rldm.py` for template details.\n",
    "\n",
    "**Details**:\n",
    "- **Function signature**: Must follow the pattern:\n",
    "  ```python\n",
    "  def my_custom_rlssm_logp_inner_func(\n",
    "      subj,\n",
    "      ntrials_subj,\n",
    "      data,\n",
    "      *model_params,    # Your specific parameters\n",
    "      *extra_fields,    # Additional data fields\n",
    "  ):\n",
    "  ```\n",
    "- **Input requirements**:\n",
    "  - `subj`: Subject index\n",
    "  - `ntrials_subj`: Number of trials per subject\n",
    "  - `data`: RT and response data matrix\n",
    "  - Parameters must match the order specified in `list_params`\n",
    "  - Extra fields must match those specified in `extra_fields`\n",
    "\n",
    "- **Output requirements**:\n",
    "  - Return a 1D array of log likelihoods for each trial\n",
    "  - Must be differentiable for gradient-based sampling/optimization\n",
    "\n",
    "- **Implementation notes**:\n",
    "  - Use JAX operations for automatic differentiation\n",
    "  - Handle parameter slicing for individual subjects using `dynamic_slice`\n",
    "  - Implement your specific RL update rules and decision mechanisms\n",
    "  - Structure the LAN matrix according to your decision model requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Update the Vectorized Function Reference\n",
    "\n",
    "**Location**: Modify the `rldm_logp_inner_func_vmapped` assignment\n",
    "\n",
    "**Purpose**: Enable parallel computation across multiple subjects.\n",
    "\n",
    "**Details**:\n",
    "- Update the function reference to point to your custom likelihood function\n",
    "- The vectorization pattern remains the same - only the first argument (subject index) gets vectorized\n",
    "- Ensure the `total_params` count includes all parameters plus extra fields plus data columns\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "rldm_logp_inner_func_vmapped = jax.vmap(\n",
    "    my_custom_rlssm_logp_inner_func,  # Update this to your function name\n",
    "    in_axes=[0] + [None] * total_params,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Update Parameter Unpacking in the Main Likelihood Function\n",
    "\n",
    "**Location**: Modify the `logp` function within `make_logp_func`\n",
    "\n",
    "**Purpose**: Ensure parameters are correctly extracted and passed to your custom function.\n",
    "\n",
    "**Details**:\n",
    "- **Parameter extraction**: Adjust the indexing to match your model's parameter structure:\n",
    "  ```python\n",
    "  # Extract extra fields (adjust indices based on your model)\n",
    "  participant_id = dist_params[n_params]      # Usually after all model params\n",
    "  trial = dist_params[n_params + 1]\n",
    "  feedback = dist_params[n_params + 2]\n",
    "  # ... additional extra fields as needed\n",
    "  \n",
    "  # Extract model parameters\n",
    "  param1, param2, ..., paramN = dist_params[:MODEL_CONFIG[\"n_params\"]]\n",
    "  ```\n",
    "\n",
    "- **Function call**: Update the `vec_logp` call to pass parameters in the correct order:\n",
    "  ```python\n",
    "  return vec_logp(\n",
    "      subj,\n",
    "      n_trials,\n",
    "      data,\n",
    "      param1,    # Your specific parameters\n",
    "      param2,\n",
    "      # ...\n",
    "      paramN,\n",
    "      trial,     # Extra fields\n",
    "      feedback,\n",
    "      # ...\n",
    "  )\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Verify VJP Function Compatibility\n",
    "\n",
    "**Location**: Check the `vjp_logp` function within `make_vjp_logp_func`\n",
    "\n",
    "**Purpose**: Ensure gradient computation works correctly with your parameter structure.\n",
    "\n",
    "**Details**:\n",
    "- The VJP (Vector-Jacobian Product) function should automatically work with your custom model\n",
    "- The slicing `[1:MODEL_CONFIG[\"n_params\"] + 1]` excludes the data and extra fields from gradient computation\n",
    "- No changes typically needed unless you have special gradient requirements\n",
    "\n",
    "**Notes**:\n",
    "- VJP is used for automatic differentiation during MCMC sampling\n",
    "- The function computes gradients with respect to model parameters only\n",
    "- Extra fields (trial, feedback, etc.) are not differentiated\n",
    "- If your model has unusual parameter dependencies, you may need custom gradient handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section II: Using the custom model with HSSM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the demo dataset\n",
    "This data file contains (synthetic) data from a simulated 2-armed bandit task. We examine the dataset -- it contains the typical columns that are expected from a canonical instrumental learning task. `participant_id` identifies the subject id, `trial` identifies the sequence of trials within the subject data, `response` and `rt` are the data columns recorded for each trial, `feedback` column shows the reward obtained on a given trial and `correct` records whether the response was correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic RLSSM dataset containing both behavioral data and ground truth parameters\n",
    "savefile = np.load(\"../../tests/fixtures/rldm_data.npy\", allow_pickle=True).item()\n",
    "dataset = savefile['data']\n",
    "\n",
    "# Rename trial column to match HSSM conventions\n",
    "dataset.rename(columns={'trial': 'trial_id'}, inplace=True)\n",
    "\n",
    "# Examine the dataset structure\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data structure and extract dataset configuration \n",
    "dataset, n_participants, n_trials = hssm.check_data_for_rl(dataset)\n",
    "\n",
    "print(f\"Number of participants: {n_participants}\")\n",
    "print(f\"Number of trials: {n_trials}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct HSSM-compatible PyMC distribution from a simulator and JAX likelihood callable\n",
    "\n",
    "We now construct a custom model that is compatible with HSSM and PyMC. Note that HSSM internally constructs a PyMC object (which is used for sampling) based on the user-specified HSSM model. In other words, we are peeling the abstration layers conveniently afforded by HSSM to directly use the core machinery of HSSM. This advanced [HSSM tutorial](https://lnccbrown.github.io/HSSM/tutorials/jax_callable_contribution_onnx_example/) explains how to use HSSM when starting from the very basics of a model -- a simulator and a JAX likelihood callable. \n",
    "\n",
    "The simulator function is used for generating samples from the model (for posterior predictives, etc.) and the likelihood callable is employed for sampling/inference. This preview tutorial exposes the key flexibility of the HSSM for use in fitting RLSSM models. Therefore, the subsequent tutorial will focus only on the sampling/inference aspect. We create a dummy simulator function to bypass the need for defining the actual simulator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define a pytensor RandomVariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for the RLSSM model (RL + decision model parameters)\n",
    "list_params = ['rl.alpha', 'rl.alpha_neg', 'scaler', 'a', 'z', 't', 'theta']\n",
    "\n",
    "# Create a dummy simulator for generating synthetic data (used for posterior predictives)\n",
    "# This bypasses the need for a full RLSSM simulator implementation\n",
    "def create_dummy_simulator():\n",
    "    \"\"\"Create a dummy simulator function for RLSSM model.\"\"\"\n",
    "    def sim_wrapper(simulator_fun, theta, model, n_samples, random_state, **kwargs):\n",
    "        # Generate random RT and choice data as placeholders\n",
    "        sim_rt = np.random.uniform(0.2, 0.6, n_samples)\n",
    "        sim_ch = np.random.randint(0, 2, n_samples)\n",
    "        \n",
    "        return np.column_stack([sim_rt, sim_ch])\n",
    "\n",
    "    # Wrap the simulator function with required metadata\n",
    "    wrapped_simulator = partial(sim_wrapper, simulator_fun=simulator, model=\"custom\", n_samples=1)\n",
    "\n",
    "    # Decorate the simulator to make it compatible with HSSM\n",
    "    return decorate_atomic_simulator(model_name=\"custom\", choices=[0, 1], obs_dim=2)(wrapped_simulator)\n",
    "\n",
    "# Create the simulator and RandomVariable\n",
    "decorated_simulator = create_dummy_simulator()\n",
    "\n",
    "# Create a PyTensor RandomVariable using `make_hssm_rv` for use in the PyMC model\n",
    "CustomRV = make_hssm_rv(\n",
    "    simulator_fun=decorated_simulator, list_params=list_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define a likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pytensor Op for the likelihood function.\n",
    "# The `make_rldm_logp_op` function is a utility that wraps the base JAX likelihood function into a HSSM/PyMC-compatible callable.\n",
    "\n",
    "logp_jax_op = make_rldm_logp_op(\n",
    "    n_participants=n_participants,\n",
    "    n_trials=n_trials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the likelihood function\n",
    "\n",
    "def extract_data_columns(dataset):\n",
    "    \"\"\"Extract required data columns from dataset.\"\"\"\n",
    "    return {\n",
    "        'participant_id': dataset[\"participant_id\"].values,\n",
    "        'trial': dataset[\"trial_id\"].values,\n",
    "        'response': dataset[\"response\"].values,\n",
    "        'feedback': dataset[\"feedback\"].values,\n",
    "        'rt': dataset[\"rt\"].values\n",
    "    }\n",
    "\n",
    "def create_test_parameters(n_trials):\n",
    "    \"\"\"Create dummy parameters for testing the likelihood function.\"\"\"\n",
    "    return {\n",
    "        'rl_alpha': np.ones(n_trials) * 0.60,\n",
    "        'rl_alpha_neg': np.ones(n_trials) * 0.60,  \n",
    "        'scaler': np.ones(n_trials) * 3.2,\n",
    "        'a': np.ones(n_trials) * 1.2,\n",
    "        'z': np.ones(n_trials) * 0.1,\n",
    "        't': np.ones(n_trials) * 0.1,\n",
    "        'theta': np.ones(n_trials) * 0.1\n",
    "}\n",
    "\n",
    "# Extract data and create test parameters\n",
    "data_columns = extract_data_columns(dataset)\n",
    "num_subj = len(np.unique(data_columns['participant_id']))\n",
    "n_trials_total = num_subj * 200\n",
    "\n",
    "test_params = create_test_parameters(n_trials_total)\n",
    "\n",
    "# Evaluate the likelihood function\n",
    "test_logp_out = logp_jax_op(\n",
    "    np.column_stack((data_columns['rt'], data_columns['response'])),\n",
    "    test_params['rl_alpha'],\n",
    "    test_params['rl_alpha_neg'],\n",
    "    test_params['scaler'],\n",
    "    test_params['a'], \n",
    "    test_params['z'],\n",
    "    test_params['t'],\n",
    "    test_params['theta'],\n",
    "    data_columns['participant_id'],\n",
    "    data_columns['trial'],\n",
    "    data_columns['feedback'],\n",
    ")\n",
    "\n",
    "LL = test_logp_out.eval()\n",
    "print(f\"Log likelihood: {np.sum(LL):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define a model config and HSSM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the model config\n",
    "\n",
    "# Configure the HSSM model \n",
    "model_config = hssm.ModelConfig(\n",
    "    response=[\"rt\", \"response\"],        # Dependent variables (RT and choice)\n",
    "    list_params=                        # List of model parameters\n",
    "        ['rl.alpha', 'rl.alpha_neg', 'scaler', 'a', 'z', 't', 'theta'],            \n",
    "    choices=[0, 1],                     # Possible choice options\n",
    "    default_priors={},                  # Use custom priors (defined below)\n",
    "    bounds=dict(                        # Parameter bounds for optimization\n",
    "        rl_alpha=(0.01, 1),             # Learning rate bounds\n",
    "        rl_alpha_neg=(0.01, 1),         # Negative learning rate bounds\n",
    "        scaler=(1, 4),                  # Scaler bounds\n",
    "        a=(0.3, 2.5),                   # Boundary separation bounds\n",
    "        z=(0.1, 0.9),                   # Bias bounds\n",
    "        t=(0.1, 2.0),                   # Non-decision time bounds\n",
    "        theta=(0.0, 1.2)                # Collapse rate bounds\n",
    "        ),\n",
    "    rv=CustomRV,                        # Custom RandomVariable that we created earlier\n",
    "    extra_fields=[                      # Additional data columns to be passed to the likelihood function as extra_fields\n",
    "        \"participant_id\", \n",
    "        \"trial_id\", \n",
    "        \"feedback\"],  \n",
    "    backend=\"jax\"                       # Use JAX for computation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hierarchical HSSM model with custom likelihood function\n",
    "hssm_model = hssm.HSSM(\n",
    "    data=dataset,                        # Input dataset\n",
    "    model_config=model_config,           # Model configuration\n",
    "    p_outlier=0,                         # No outlier modeling\n",
    "    lapse=None,                          # No lapse rate modeling\n",
    "    loglik=logp_jax_op,                  # Custom RLDM likelihood function\n",
    "    loglik_kind=\"approx_differentiable\", # Use approximate gradients\n",
    "    noncentered=True,                    # Use non-centered parameterization\n",
    "    process_initvals=False,              # Skip initial value processing in HSSM\n",
    "    include=[\n",
    "        # Define hierarchical priors: group-level intercepts + subject-level random effects\n",
    "        hssm.Param(\"rl.alpha\", \n",
    "                formula=\"rl_alpha ~ 1 + (1|participant_id)\", \n",
    "                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.01, upper=1, mu=0.3)}),\n",
    "        hssm.Param(\"rl.alpha_neg\", \n",
    "                formula=\"rl_alpha_neg ~ 1 + (1|participant_id)\", \n",
    "                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.01, upper=1, mu=0.3)}),\n",
    "        hssm.Param(\"scaler\", \n",
    "                formula=\"scaler ~ 1 + (1|participant_id)\", \n",
    "                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=1, upper=4, mu=1.5)}),\n",
    "        hssm.Param(\"a\", \n",
    "                formula=\"a ~ 1 + (1|participant_id)\", \n",
    "                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.3, upper=2.5, mu=1.0)}),\n",
    "        hssm.Param(\"z\", \n",
    "                formula=\"z ~ 1 + (1|participant_id)\", \n",
    "                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.1, upper=0.9, mu=0.2)}),\n",
    "        hssm.Param(\"t\", \n",
    "                formula=\"t ~ 1 + (1|participant_id)\", \n",
    "                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.01, upper=2, mu=0.2, initval=0.1)}),\n",
    "        hssm.Param(\"theta\", \n",
    "                formula=\"theta ~ 1 + (1|participant_id)\", \n",
    "                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.00, upper=1.2, mu=0.3)}),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample using NUTS MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MCMC sampling using NUTS sampler with JAX backend\n",
    "# Note: Using small number of samples for demonstration (increase for real analysis)\n",
    "idata_mcmc = hssm_model.sample(\n",
    "    sampler='nuts_numpyro',  # JAX-based NUTS sampler for efficiency\n",
    "    chains=1,                # Number of parallel chains\n",
    "    draws=1000,                # Number of posterior samples\n",
    "    tune=1000,                 # Number of tuning/warmup samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata_mcmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess the model fits\n",
    "\n",
    "We examine the quality of fits by comparing the recovered parameters with the ground-truth data generating parameters of the simulated dataset. \n",
    "We examine the quality of fits both at group-level as well as subject-level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining group-level posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter names for analysis\n",
    "\n",
    "list_group_mean_params = [\n",
    "    \"rl.alpha_Intercept\",\n",
    "    \"rl.alpha_neg_Intercept\",\n",
    "    \"scaler_Intercept\",\n",
    "    \"a_Intercept\",\n",
    "    \"z_Intercept\",\n",
    "    \"t_Intercept\",\n",
    "    \"theta_Intercept\",\n",
    "]\n",
    "\n",
    "list_group_sd_params = [\n",
    "    \"rl.alpha_1|participant_id_sigma\",\n",
    "    \"rl.alpha_neg_1|participant_id_sigma\",\n",
    "    \"scaler_1|participant_id_sigma\",\n",
    "    \"a_1|participant_id_sigma\", \n",
    "    \"z_1|participant_id_sigma\",\n",
    "    \"t_1|participant_id_sigma\",\n",
    "    \"theta_1|participant_id_sigma\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping from HSSM model parameter names to ground truth values.\n",
    "def create_ground_truth_mapping(savefile):\n",
    "\n",
    "    return {\n",
    "    \"rl.alpha_Intercept\": savefile['params_true_group']['rl_alpha_mean'],\n",
    "    \"scaler_Intercept\": savefile['params_true_group']['scaler_mean'],\n",
    "    \"a_Intercept\": savefile['params_true_group']['a_mean'],\n",
    "    \"z_Intercept\": savefile['params_true_group']['z_mean'],\n",
    "    \"t_Intercept\": savefile['params_true_group']['t_mean'],\n",
    "    \"theta_Intercept\": savefile['params_true_group']['theta_mean'],\n",
    "}\n",
    "\n",
    "ground_truth_params = create_ground_truth_mapping(savefile)\n",
    "print(\"Ground truth group means:\\n\")\n",
    "for param, value in ground_truth_params.items():\n",
    "    print(f\"{param}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior distributions and MCMC traces for group-level parameters\n",
    "# Vertical lines show ground truth values for parameter recovery assessment\n",
    "az.plot_trace(idata_mcmc, var_names=list_group_mean_params,\n",
    "              lines=[(key_, {}, ground_truth_params[key_]) for key_ in ground_truth_params])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
