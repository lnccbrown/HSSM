{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSSM Advanced Tutorial\n",
    "\n",
    "<center> <img src=\"./images/HSSM_logo.png\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the HSSM Advanced Tutorial!\n",
    "\n",
    "In this short tutorial we will explore some of the features of HSSM that are geared towards users who want to do one of two things:\n",
    "\n",
    "1. Generally work with custom model that are not provided by HSSM out of the box (please consider contributing directly to the HSSM ecosystem eventually :))\n",
    "2. Build custom PyMC models around observation models build via HSSM low-level utilities (to e.g. break out of the hierarchical regression corsett imposed via the [Bambi](https://bambinos.github.io/bambi/) interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Instructions\n",
    "\n",
    "If you would like to run this tutorial on Google colab, please click this [link](https://githubtocolab.com/lnccbrown/HSSM/blob/carney-workshop-scientific-workflow/carney_workshop/hssm_advanced_1.ipynb). \n",
    "\n",
    "Once you are *in the colab*:\n",
    "\n",
    "1. Follow the **installation instructions below**  (uncomment the respective code)\n",
    "2.  **restart your runtime**. \n",
    "\n",
    "**NOTE**:\n",
    "\n",
    "You may want to *switch your runtime* to have a GPU or TPU. To do so, go to *Runtime* > *Change runtime type* and select the desired hardware accelerator.\n",
    "Note that if you switch your runtime you have to follow the installation instructions again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running this on Colab, please uncomment the next line\n",
    "# !pip install hssm\n",
    "# !pip install onnxruntime\n",
    "# !pip install \"zeus-mcmc>=2.5.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data Files\n",
    "# !wget -P  data/carney_workshop_2025_data/ https://raw.githubusercontent.com/lnccbrown/HSSM/carney-workshop-scientific-workflow/carney_workshop/data/carney_workshop_2025_data/race_3_no_bias_lan_batch.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import hssm\n",
    "import arviz as az\n",
    "import onnx\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Blackbox Likelihood directly from an `onnx` file\n",
    "\n",
    "\n",
    "<center> <img src=\"./images/onnx_logo.png\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HSSM allows us to work natively with with neural networks that are encoded as `.onnx` files.\n",
    "We will see later that we provide even more sophisticated machinery, but here we first focus on loading an `.onnx` file into an `onnx runtime` and then wrapping that runtime into a `blackbox likelihood`, the most flexible type of likelihood we can construct.\n",
    "\n",
    "The downside of this approach is that we loose access to gradients, the upside is that we are allowed to do literally whatever we want inside this likelihood function. See also here for a more generic [PyMC Tutorial](https://www.pymc.io/projects/examples/en/latest/howto/blackbox_external_likelihood_numpy.html) if you want to learn about how `blackbox` likelihoods work in [PyMC](https://www.pymc.io/welcome.html) directly.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Network Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Networks\n",
    "from pathlib import Path\n",
    "network_path = Path(\"data\", \"carney_workshop_2025_data\", \"race_3_no_bias_lan_batch.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting and Testing the onnx runtime\n",
    "\n",
    "\n",
    "Below we load this `onnx` model. Think of it as an alternative format for saving and loading neural networks, that can serve as a translation layer between the basic neural network fromeworks such as [Pytorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/) and [JAX](https://docs.jax.dev/en/latest/quickstart.html), but can also used stand-alone package for fast computation with neural networks. \n",
    "\n",
    "We exploit the latter in this section, however HSSM internally focuses on the \"translation-layer\" concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load onnx model\n",
    "onnx_model = onnx.load(network_path)\n",
    "input_name = onnx_model.graph.input[0].name\n",
    "ort_session = ort.InferenceSession(onnx_model.SerializeToString())\n",
    "\n",
    "# Test inference speed\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    ort_session.run(\n",
    "        None, {input_name: np.random.uniform(size=(1000, 8)).astype(np.float32)}\n",
    "    )\n",
    "end = time.time()\n",
    "print(f\"Time taken: {(end - start) / 100} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_blackbox_race_model(data, v0, v1, v2, a, t, z):\n",
    "    \"\"\"Calculate log-likelihood for a 3-choice race model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Array of shape (n_trials, 2) containing response times in first column\n",
    "        and choices (0, 1, or 2) in second column\n",
    "    v0 : float\n",
    "        Drift rate for accumulator 0\n",
    "    v1 : float\n",
    "        Drift rate for accumulator 1\n",
    "    v2 : float\n",
    "        Drift rate for accumulator 2\n",
    "    a : float\n",
    "        Decision threshold/boundary\n",
    "    t : float\n",
    "        Non-decision time\n",
    "    z : float\n",
    "        Starting point bias\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Array of log-likelihood values for each trial\n",
    "    \"\"\"\n",
    "    data_nrows = data.shape[0]\n",
    "    data = np.vstack(\n",
    "        [np.full(data_nrows, param_) for param_ in [v0, v1, v2, a, t, z]]\n",
    "        + [data[:, 0], data[:, 1]]\n",
    "    ).T.astype(np.float32)\n",
    "    return ort_session.run(None, {input_name: data})[0].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Underlying Model\n",
    "\n",
    "\n",
    "This example focuses on a `Race` model with three choice options. See the picture below for an illustration:\n",
    "\n",
    "<center> <img src=\"./images/Race_3.png\"> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "v0 = 1.0\n",
    "v1 = 0.5\n",
    "v2 = 0.25\n",
    "a = 1.5\n",
    "t = 0.3\n",
    "z = 0.5\n",
    "\n",
    "# simulate some data from the model\n",
    "obs_race3 = hssm.simulate_data(\n",
    "    theta=dict(v0=v0, v1=v1, v2=v2, a=a, t=t, z=z), model=\"race_no_bias_3\", size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that outputs are reasonable\n",
    "colors = [\"black\", \"green\", \"blue\"]\n",
    "for choice in [0, 1, 2]:\n",
    "    rts = np.linspace(0, 20, 1000)\n",
    "    choices = np.repeat(choice, 1000)\n",
    "\n",
    "    data = np.vstack([rts, choices]).T\n",
    "    out = my_blackbox_race_model(data, v0, v1, v2, a, t, z)\n",
    "\n",
    "    plt.plot(rts, np.exp(out), \n",
    "             label=f\"response: {choice}\",\n",
    "             color=colors[choice])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct HSSM Model\n",
    "\n",
    "We can now construct our HSSM, using our custom likelihood very easily. We just need to supply a few extra arguments, so that HSSM has sufficient information to instantiate a complete model.\n",
    "\n",
    "- Our likelihood is a \"blackbox\" function, so we set `log_lik_kind=\"blackbox\"`\n",
    "- We need to pass the core parameters and legal parameter bounds via the `model_config` argument\n",
    "- We need to tell HSSM what legal responses are for the given model via the `choices` argument\n",
    "\n",
    "The above arguments need to be set for any custom model.\n",
    "\n",
    "Apart from that we set `z=0.5`, which is a choice informed by our experience fitting this 'Race' model, and we set `p_outlier=0` because we are dealing with synthetic data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hssm.HSSM(\n",
    "    data=obs_race3,\n",
    "    model=\"race_no_bias_3\",  # some name for the model\n",
    "    model_config={\n",
    "        \"list_params\": [\"v0\", \"v1\", \"v2\", \"a\", \"z\", \"t\"],\n",
    "        \"bounds\": {\n",
    "            \"v0\": (0.0, 2.5),\n",
    "            \"v1\": (0.0, 2.5),\n",
    "            \"v2\": (0.0, 2.5),\n",
    "            \"a\": (1.0, 3.0),\n",
    "            \"z\": (0.0, 0.9),\n",
    "            \"t\": (0.001, 2),\n",
    "        },\n",
    "    },  # minimal specification of model parameters and parameter bounds\n",
    "    loglik_kind=\"blackbox\",  # use the blackbox loglik\n",
    "    loglik=my_blackbox_race_model,\n",
    "    choices=[0, 1, 2],  # list the legal choice options\n",
    "    z=0.5,\n",
    "    p_outlier=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sample(draws=500,\n",
    "             tune=200,\n",
    "             discard_tuned_samples=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "az.plot_trace(model.traces, var_names=[\"~v0_mean\"])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_pair(model.traces,\n",
    "             var_names=[\"~v0_mean\"],\n",
    "             kind = \"kde\",\n",
    "             marginals = True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voil√†, we can do our standard analysis with this HSSM model based on a custom specifciation of our *Sequential Sampling Model*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It gets better\n",
    "\n",
    "\n",
    "The *blackbox* idea is very general and allows us to test an unlimited amount of models. However, in some sense it is like fighting bare handed. We can do a lot better if we construct out likelihood (insofar it is a priori differentiable) so that we can make use of gradients.\n",
    "\n",
    "Remember that the `.onnx` file is just a representation for a neural network... this Network is actually a [LAN](https://elifesciences.org/articles/65074), and hence, by definition differentiable with respect to it's inputs (the parameters of the model).\n",
    "\n",
    "\n",
    "<center> <img src=\"./images/basic_lan_pic.png\" height=300 width=600> </center>\n",
    "\n",
    "HSSM is smart enough to be able to construct a valid wrapper around this so that we can use the network with gradients! Let's see how this works below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model = hssm.HSSM(\n",
    "    data=obs_race3,\n",
    "    model=\"race_no_bias_3\",  # some name for the model\n",
    "    model_config={\n",
    "        \"list_params\": [\"v0\", \"v1\", \"v2\", \"a\", \"z\", \"t\"],\n",
    "        \"bounds\": {\n",
    "            \"v0\": (0.0, 2.5),\n",
    "            \"v1\": (0.0, 2.5),\n",
    "            \"v2\": (0.0, 2.5),\n",
    "            \"a\": (1.0, 3.0),\n",
    "            \"z\": (0.0, 0.9),\n",
    "            \"t\": (0.001, 2),\n",
    "        },\n",
    "        \"backend\": \"jax\", # can choose \"jax\" or \"pytensor\" here\n",
    "    },  # minimal specification of model parameters and parameter bounds\n",
    "    loglik_kind=\"approx_differentiable\",  # use the blackbox loglik\n",
    "    loglik=network_path,\n",
    "    choices=[0, 1, 2],  # list the legal choice options\n",
    "    z=0.5,\n",
    "    p_outlier=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model_idata = better_model.sample(draws=500,\n",
    "                                         tune=200,\n",
    "                                         discard_tuned_samples=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we are back to our beloved [NUTS](https://arxiv.org/abs/1111.4246) (gradient based) sampler here! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(better_model_idata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Samplers are trash, I know something better!\n",
    "### Help yourself!\n",
    "\n",
    "If you don't like the samplers HSSM or even PyMC can provide out of the box, we have a remedy.\n",
    "You can just compile the likelihood, and use it as a simple function downstream, however you like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp_fun = better_model.compile_logp()  # msynth.pymc_model.compile_logp()\n",
    "print(logp_fun(better_model.initial_point(transformed=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing the compiled likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time\n",
    "import time\n",
    "\n",
    "my_start_point = better_model.initial_point(transformed=False)\n",
    "start_time = time.time()\n",
    "for i in range(1000):\n",
    "    logp_fun(my_start_point)\n",
    "print((time.time() - start_time) / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a simple wrapper and sample via third party library\n",
    "\n",
    "<center> <img src=\"./images/zeus_sampler.png\" height=300 width=600> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mylogp(theta: list[float]) -> float:\n",
    "    \"\"\"Wrap function for compiled log probability function to work with zeus sampler.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "        theta: List of model parameters [v, a, z, t] where:\n",
    "            v: Drift rate\n",
    "            a: Boundary separation\n",
    "            z: Starting point\n",
    "            t: Non-decision time\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        float: Log probability value for the given parameters\n",
    "    \"\"\"\n",
    "    v0, v1, v2, a, t = theta\n",
    "    return logp_fun({\"v0\": v0, \"v1\": v1, \"v2\": v2, \"a\": a, \"t\": t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zeus\n",
    "\n",
    "start = np.random.uniform(low=-0.2,\n",
    "                          high=0.2,\n",
    "                          size=(10, 5)) + np.tile([0.5, 1.5, 0.5, 1.5, 0.3], (10, 1)\n",
    ")\n",
    "sampler = zeus.EnsembleSampler(10, 5, mylogp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.run_mcmc(start, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 1.5 * 5))\n",
    "for n in range(5):\n",
    "    plt.subplot2grid((5, 1), (n, 0))\n",
    "    plt.plot(sampler.get_chain()[:, :, n], alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More flexibility: We can use PyMC directly!\n",
    "\n",
    "Now for a different degree of freedom we provide. Turns out HSSM has a few low level utilities that come in very handy for working directly with PyMC.\n",
    "\n",
    "<center> <img src=\"./images/pymc_logo.png\" height=300 width=600> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Example: Importing pre-defined random variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDM models (the Wiener First-Passage Time distribution)\n",
    "from hssm.distribution_utils import make_distribution\n",
    "from hssm.likelihoods import DDM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate\n",
    "param_dict_pymc = dict(v=0.5,\n",
    "                       a=1.5,\n",
    "                       z=0.5,\n",
    "                       t=0.5,\n",
    "                       theta=0.0)\n",
    "dataset_pymc = hssm.simulate_data(model=\"ddm\", theta=param_dict_pymc, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pymc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First PyMC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "\n",
    "with pm.Model() as ddm_pymc:\n",
    "    # Data\n",
    "    obs_pt = pm.Data(\"obs\", dataset_pymc[[\"rt\", \"response\"]].values)\n",
    "\n",
    "    # Priors\n",
    "    v = pm.Uniform(\"v\", \n",
    "                   lower=-10.0,\n",
    "                   upper=10.0)\n",
    "    a = pm.HalfNormal(\"a\",\n",
    "                      sigma=2.0)\n",
    "    z = pm.Uniform(\"z\",\n",
    "                   lower=0.01,\n",
    "                   upper=0.99)\n",
    "    t = pm.Uniform(\"t\",\n",
    "                   lower=0.0,\n",
    "                   upper=0.6)\n",
    "    \n",
    "    # Likelihood\n",
    "    ddm = DDM(\n",
    "        \"DDM\", v=v, a=a, z=z, t=t, observed=dataset_pymc[[\"rt\", \"response\"]].values\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(model=ddm_pymc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ddm_pymc:\n",
    "    # Sample Posterior\n",
    "    ddm_pymc_trace = pm.sample()\n",
    "    # Get Posterior Predictive Samples\n",
    "    pm.sample_posterior_predictive(ddm_pymc_trace,\n",
    "                                   extend_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(\n",
    "    ddm_pymc_trace,\n",
    "    lines=[(key_, {}, param_dict_pymc[key_]) \\\n",
    "           for key_ in param_dict_pymc],\n",
    ")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(ddm_pymc_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Example (1): Construct your own random variable\n",
    "\n",
    "Under the hood, a lot of the heavy lifting done by HSSM boils down to the creating of valid Random Variables that are compatible with PyMC specifications. \n",
    "\n",
    "In broad strokes: In probabilist programming, a minimal specification for a random variable that let's us sample all the quantities of interest for a standard Bayesian Analysis are:\n",
    "\n",
    "- A likelihood function\n",
    "- A valid simulator\n",
    "\n",
    "If we have both of these, we can do everything. We can sample from the *posterior*, from the *prior predictive* and from the *posterior predictive*,which will cover essentially 99.9% of our needs.\n",
    "\n",
    "Below, we use HSSM low level functionality to construct such a random variable in three simple steps. Once constructed we use it in a PyMC model natively! \n",
    "\n",
    "**Note**:\n",
    "\n",
    "Truth be told, we can run MCMC without ever supplying a valid simulator and other tutorials showcase that scenario. However in such cases you are on your own once you want to sample from *posterior predictives* and/or *prior predictives*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hssm.distribution_utils.dist import (\n",
    "    make_distribution,\n",
    "    make_hssm_rv,\n",
    "    make_likelihood_callable,\n",
    ")\n",
    "\n",
    "# Step 1: Define a pytensor RandomVariable\n",
    "CustomRV = make_hssm_rv(\n",
    "    simulator_fun=\"race_no_bias_3\",\n",
    "    list_params=[\"v0\", \"v1\", \"v2\", \"a\", \"z\", \"t\"]\n",
    ")\n",
    "\n",
    "# # Step 2: Define a likelihood function\n",
    "logp_jax_op = make_likelihood_callable(\n",
    "    loglik=network_path,\n",
    "    loglik_kind=\"approx_differentiable\",\n",
    "    backend=\"jax\",\n",
    "    params_is_reg=[False, False, False, False, False, False],\n",
    "    params_only=False,\n",
    ")\n",
    "\n",
    "# Step 2: Define a distribution\n",
    "CustomDistribution = make_distribution(\n",
    "    rv=CustomRV,\n",
    "    loglik=logp_jax_op,\n",
    "    list_params=[\"v0\", \"v1\", \"v2\", \"a\", \"z\", \"t\"],\n",
    "    bounds=dict(v0=(-3, 3), v1=(-3, 3), v2=(-3, 3), a=(0.5, 3.0), z=(0.1, 0.9), t=(0, 2.0)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "\n",
    "with pm.Model() as race3_pymc:\n",
    "    # Data\n",
    "    obs_pt = pm.Data(\"obs\", obs_race3[[\"rt\", \"response\"]].values)\n",
    "\n",
    "    # Priors\n",
    "    v0 = pm.Uniform(\"v0\", \n",
    "                   lower=-3.0,\n",
    "                   upper=3.0,\n",
    "                   initval=-0.0)\n",
    "    v1 = pm.Uniform(\"v1\", \n",
    "                   lower=-3.0,\n",
    "                   upper=3.0,\n",
    "                   initval=0.0)\n",
    "    v2 = pm.Uniform(\"v2\", \n",
    "                   lower=-3.0,\n",
    "                   upper=3.0,\n",
    "                   initval=0.0)\n",
    "    a = pm.HalfNormal(\"a\",\n",
    "                      sigma=2.0,\n",
    "                      initval=1.0)\n",
    "    z = 0.5 # We fix this parameter because it is extremely hard to jointly identify both z and a\n",
    "    t = pm.Uniform(\"t\",\n",
    "                   lower=0.0,\n",
    "                   upper=2.0,\n",
    "                   initval=0.1)\n",
    "    \n",
    "    # Likelihood\n",
    "    race3_obs = CustomDistribution(\n",
    "        \"Race3\", v0=v0, v1=v1, v2=v2, a=a, z=z, t=t, observed=obs_pt\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race3_pymc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(model=race3_pymc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with race3_pymc:\n",
    "    # Sample Posterior\n",
    "    race3_pymc_trace = pm.sample(nuts_sampler = \"numpyro\",\n",
    "                                 tune = 500,\n",
    "                                 draws = 500)\n",
    "    # Get Posterior Predictive Samples\n",
    "    pm.sample_posterior_predictive(race3_pymc_trace,\n",
    "                                   extend_inferencedata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Example (2): Build PyMC model that is harder to construct via HSSM directly\n",
    "\n",
    "So far we illustrated various way of constructing random variables via HSSM utilities. The PyMC models we finally built however, \n",
    "weren't worth the manual labour... we could have let our HSSM high-level interface taken over the job.\n",
    "\n",
    "Below we will construct a PyMC model whose properties are a bit harder (impossible) to replicate directly via HSSM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Data\n",
    "\n",
    "Notice in the data simulation below: We are applying a regresion to all of `v0`, `v1`, `v2` and `a` and de facto the `beta` parameter is shared amongst all four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Regression Setup\n",
    "beta = 0.5\n",
    "difficulty = np.random.uniform(low = -1, high = 1, size = 1000)\n",
    "\n",
    "# Set parameters\n",
    "v0 = 1.0 - beta * difficulty\n",
    "v1 = 0.5 - beta * difficulty\n",
    "v2 = 0.25 - beta * difficulty\n",
    "a = 1.5 + beta * difficulty\n",
    "t = 0.3\n",
    "z = 0.5\n",
    "\n",
    "# simulate some data from the model\n",
    "obs_race3_reg = hssm.simulate_data(\n",
    "    theta=dict(v0=v0, v1=v1, v2=v2, a=a, t=t, z=z), model=\"race_no_bias_3\", size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_race3_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hssm.distribution_utils.dist import (\n",
    "    make_distribution,\n",
    "    make_hssm_rv,\n",
    "    make_likelihood_callable,\n",
    ")\n",
    "\n",
    "# Step 1: Define a pytensor RandomVariable\n",
    "CustomRV2 = make_hssm_rv(\n",
    "    simulator_fun=\"race_no_bias_3\",\n",
    "    list_params=[\"v0\", \"v1\", \"v2\", \"a\", \"z\", \"t\"]\n",
    ")\n",
    "\n",
    "# # Step 2: Define a likelihood function\n",
    "logp_jax_op2 = make_likelihood_callable(\n",
    "    loglik=network_path,\n",
    "    loglik_kind=\"approx_differentiable\",\n",
    "    backend=\"jax\",\n",
    "    params_is_reg=[True, True, True, True, False, False],\n",
    "    params_only=False,\n",
    ")\n",
    "\n",
    "# Step 2: Define a distribution\n",
    "CustomDistribution2 = make_distribution(\n",
    "    rv=CustomRV2,\n",
    "    loglik=logp_jax_op2,\n",
    "    list_params=[\"v0\", \"v1\", \"v2\", \"a\", \"z\", \"t\"],\n",
    "    bounds=dict(v0=(-3, 3), v1=(-3, 3), v2=(-3, 3), a=(0.5, 3.0), z=(0.1, 0.9), t=(0, 2.0)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "\n",
    "There was a subtle change to the previous version. We changes the `params_is_reg` argument from all `False` to `[True, True, True, True, False, False]`. \n",
    "\n",
    "Why do we do that? Because we want to construct a model that takes in `v0`, `v1`, `v2` and `a` via regressions. The likelihood therefore needs to accept these parameters as vectors, because they can vary trial-by-trial now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the PyMC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "\n",
    "with pm.Model() as race3_pymc2:\n",
    "    # Data\n",
    "    difficulty_pt = pm.Data(\"difficulty\", difficulty)\n",
    "    obs_pt = pm.Data(\"obs\", obs_race3_reg[[\"rt\", \"response\"]].values)\n",
    "\n",
    "    # Priors\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=1)\n",
    "    v0 = pm.Uniform(\"v0\", \n",
    "                   lower=-3.0,\n",
    "                   upper=3.0,\n",
    "                   initval=-0.0)\n",
    "    v1 = pm.Uniform(\"v1\", \n",
    "                   lower=-3.0,\n",
    "                   upper=3.0,\n",
    "                   initval=0.0)\n",
    "    v2 = pm.Uniform(\"v2\", \n",
    "                   lower=-3.0,\n",
    "                   upper=3.0,\n",
    "                   initval=0.0)\n",
    "    a = pm.HalfNormal(\"a\",\n",
    "                      sigma=2.0,\n",
    "                      initval=1.0)\n",
    "    t = pm.Uniform(\"t\",\n",
    "                   lower=0.0,\n",
    "                   upper=2.0,\n",
    "                   initval=0.1)\n",
    "    \n",
    "    z = 0.5 # We fix this parameter because it is extremely hard to jointly identify both z and a\n",
    "    \n",
    "\n",
    "    # Compute Regressions\n",
    "    reg_a = pm.Deterministic(\"reg_a\", a + beta * difficulty_pt)\n",
    "    reg_v0 = pm.Deterministic(\"reg_v0\", v0 - beta * difficulty_pt)\n",
    "    reg_v1 = pm.Deterministic(\"reg_v1\", v1 - beta * difficulty_pt)\n",
    "    reg_v2 = pm.Deterministic(\"reg_v2\", v2 - beta * difficulty_pt)\n",
    "\n",
    "    # Likelihood\n",
    "    race3_obs = CustomDistribution2(\n",
    "        \"Race3\", v0=reg_v0, v1=reg_v1, v2=reg_v2, a=reg_a, z=z, t=t, observed= obs_pt\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "\n",
    "Notice the construction of Deterministics. Take a look at the model graph below, the rectangular plate should seem familiar from basic HSSM graphs!\n",
    "PyMC is quite flexible on that front. We can use a lot of basic math naively and it will take care of obvious broadcasting and allow us to differentiate through!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(model=race3_pymc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with race3_pymc2:\n",
    "    # Sample Posterior\n",
    "    race3_pymc_trace2 = pm.sample(nuts_sampler = \"numpyro\",\n",
    "                                  tune = 500,\n",
    "                                  draws = 500)\n",
    "    # Get Posterior Predictive Samples\n",
    "    pm.sample_posterior_predictive(race3_pymc_trace2,\n",
    "                                   extend_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(race3_pymc_trace2, var_names = [\"~reg\"], filter_vars = \"like\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End:\n",
    "\n",
    "We have seen how to use introduce custom models into HSSM workflows, how to do posterior sampling with HSSM models via third party libraries and how to use the low-level functionality of HSSM to construct random variables that can be used via PyMC!\n",
    "\n",
    "This tutorial ends here, but please feel invited to take occasion and check out more advanced topics from the HSSM docs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointers to more advanced Topics\n",
    "\n",
    "We are scratching only the surface of what cann be done with [HSSM](https://github.com/lnccbrown/HSSM/), let alone the broader eco-system supporting [simulation based inference (SBI)](https://simulation-based-inference.org/).\n",
    "\n",
    "Check out our simulator package, [ssm-simulators](https://github.com/lnccbrown/ssm-simulators) as well as our our little neural network library for training [LANs](https://elifesciences.org/articles/65074), [lanfactory](https://github.com/lnccbrown/LANfactory). \n",
    "\n",
    "Exciting work is being done (more on this in the next tutorial) on connecting to other packages in the wider eco-system, such as [BayesFlow](https://bayesflow.org/main/index.html) as well as the [sbi](https://sbi-dev.github.io/sbi/v0.24.0/) package.\n",
    "\n",
    "Here is a taste of advanced topics with links to corresponding tutorials:\n",
    "\n",
    "- [Variational Inference with HSSM](https://lnccbrown.github.io/HSSM/tutorials/variational_inference/)\n",
    "- [Build PyMC models with HSSM random variables](https://lnccbrown.github.io/HSSM/tutorials/pymc/)\n",
    "- [Connect compiled models to third party MCMC libraries](https://lnccbrown.github.io/HSSM/tutorials/compile_logp/)\n",
    "- [Construct custom models from simulators and contributed likelihoods](https://lnccbrown.github.io/HSSM/tutorials/jax_callable_contribution_onnx_example/)\n",
    "- [Using link functions to transform parameters](https://lnccbrown.github.io/HSSM/api/link/#hssm.Link)\n",
    "\n",
    "you will find this and a lot more information in the [official documentation](https://lnccbrown.github.io/HSSM/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
