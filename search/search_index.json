{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>HSSM (Hierarchical Sequential Sampling Modeling) is a modern Python toolbox that provides state-of-the-art likelihood approximation methods within the Python Bayesian ecosystem. It facilitates hierarchical model building and inference via fast and robust MCMC samplers. User-friendly, extensible, and flexible, HSSM can rigorously estimate the impact of neural and other trial-by-trial covariates through parameter-wise mixed-effects models for a large variety of cognitive process models.</p> <p>HSSM is a BRAINSTORM project in collaboration with the Center for Computation and Visualization (CCV) and the Center for Computational Brain Science within the Carney Institute at Brown University.</p>"},{"location":"#citation","title":"Citation","text":"<p>Fengler, A., Xu, Y., Bera, K., Omar, A., Frank, M.J. (in preparation). HSSM: A generalized toolbox for hierarchical bayesian estimation of computational models in cognitive neuroscience.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Allows approximate hierarchical Bayesian inference via various likelihood   approximators.</li> <li>Estimate impact of neural and other trial-by-trial covariates via native   hierarchical mixed-regression support.</li> <li>Extensible for users to add novel models with corresponding likelihoods.</li> <li>Built on PyMC with support from the Python Bayesian ecosystem at large.</li> <li>Incorporates Bambi's intuitive <code>lmer</code>-like regression parameter specification   for within- and between-subject effects.</li> <li>Native ArviZ support for plotting and other convenience functions to aid the   Bayesian workflow.</li> <li>Utilizes the ONNX format for translation of differentiable likelihood   approximators across backends.</li> </ul>"},{"location":"#example","title":"Example","text":"<p>Here is a simple example of how to use HSSM:</p> <pre><code>import hssm\n\n# Load a package-supplied dataset\ncav_data = hssm.load_data(\"cavanagh_theta\")\n\n# Define a basic hierarchical model with trial-level covariates\nmodel = hssm.HSSM(\n    model=\"ddm\",\n    data=cav_data,\n    include=[\n        {\n            \"name\": \"v\",\n            \"prior\": {\n                \"Intercept\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 1.0},\n                \"theta\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 1.0},\n            },\n            \"formula\": \"v ~ theta + (1|participant_id)\",\n            \"link\": \"identity\",\n        },\n    ],\n)\n\n# Sample from the posterior for this model\nmodel.sample()\n</code></pre> <p>To quickly get started with HSSM, please follow this tutorial. For a deeper dive into HSSM, please follow our main tutorial.</p>"},{"location":"#installation","title":"Installation","text":"<p>HSSM can be directly installed into your conda environment on Linux and MacOS. Installing HSSM on windows takes only one more simple step. We have a more detailed installation guide for users with more specific setups.</p> <p>Important Update: From HSSM 0.2.2, the official recommended way to install HSSM is through conda.</p>"},{"location":"#install-hssm-on-linux-and-macos-cpu-only","title":"Install HSSM on Linux and MacOS (CPU only)","text":"<p>Use the following command to install HSSM into your virtual environment:</p> <pre><code>conda install -c conda-forge hssm\n</code></pre>"},{"location":"#install-hssm-on-linux-and-macos-with-gpu-support","title":"Install HSSM on Linux and MacOS (with GPU Support)","text":"<p>If you need to sample with GPU, please install JAX with GPU support before installing HSSM:</p> <pre><code>conda install jaxlib=*=*cuda* jax cuda-nvcc -c conda-forge -c nvidia\nconda install -c conda-forge hssm\n</code></pre>"},{"location":"#install-hssm-on-windows-cpu-only","title":"Install HSSM on Windows (CPU only)","text":"<p>On Windows, we need to install pymc through conda, and then HSSM through pip:</p> <pre><code>conda install -c conda-forge pymc\npip install hssm\n</code></pre>"},{"location":"#install-hssm-on-windows-with-gpu-support","title":"Install HSSM on Windows (with GPU support)","text":"<p>You simply need to install JAX with GPU support before getting HSSM:</p> <pre><code>conda install -c conda-forge pymc\npip install hssm[cuda12]\n</code></pre>"},{"location":"#support-for-apple-silicon-amd-and-other-gpus","title":"Support for Apple Silicon, AMD, and other GPUs","text":"<p>JAX also has support other GPUs. Please follow the Official JAX installation guide to install the correct version of JAX before installing HSSM.</p>"},{"location":"#advanced-installation","title":"Advanced Installation","text":""},{"location":"#install-hssm-directly-with-pip","title":"Install HSSM directly with Pip","text":"<p>HSSM is also available through PyPI. You can directly install it with pip into any virtual environment via:</p> <pre><code>pip install hssm\n</code></pre> <p>Note</p> <p>While this installation is much simpler, you might encounter this warning message <code>WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.</code> Please refer to our advanced installation guide for more details.</p>"},{"location":"#install-the-dev-version-of-hssm","title":"Install the dev version of HSSM","text":"<p>You can install the dev version of <code>hssm</code> directly from this repo:</p> <pre><code>pip install git+https://github.com/lnccbrown/HSSM.git\n</code></pre>"},{"location":"#install-hssm-on-google-colab","title":"Install HSSM on Google Colab","text":"<p>Google Colab comes with PyMC and JAX pre-configured. That holds true even if you are using the GPU and TPU backend, so you simply need to install HSSM via pip on Colab regardless of the backend you are using:</p> <pre><code>!pip install hssm\n</code></pre>"},{"location":"#troubleshooting","title":"Troubleshooting","text":"<p>Note</p> <p>Possible solutions to any issues with installations with hssm can be located here. Also feel free to start a new discussion thread if you don't find answers there. We recommend installing HSSM into a new conda environment with Python 3.10 or 3.11 to prevent any problems with dependencies during the installation process. Please note that hssm is only tested for python 3.10, 3.11. As of HSSM v0.2.0, support for Python 3.9 is dropped. Use unsupported python versions with caution.</p>"},{"location":"#license","title":"License","text":"<p>HSSM is licensed under Copyright 2023, Brown University, Providence, RI</p>"},{"location":"#support","title":"Support","text":"<p>For questions, please feel free to open a discussion.</p> <p>For bug reports and feature requests, please feel free to open an issue using the corresponding template.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>If you want to contribute to this project, please follow our contribution guidelines.</p>"},{"location":"CONTRIBUTING/","title":"Contributing to HSSM","text":"<p>We invite contributions to the HSSM project from interested individuals or groups. To ensure that your contributions align with the conventions of the HSSM project and can be integrated as efficiently as possible, we provide the following guidelines.</p>"},{"location":"CONTRIBUTING/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Ways to Contribute</li> <li>Opening Issues</li> <li>Pull Request Step-by-Step</li> </ol>"},{"location":"CONTRIBUTING/#ways-to-contribute","title":"Ways to Contribute","text":"<p>There are three main ways you can contribute to HSSM (listed in decreasing order of scope):</p> <ol> <li>Expanding the existing codebase with new features or improvements to existing ones. Use the \"Feature Request\" label.</li> <li>Contributing to or improving the project's documentation and examples (located in <code>hssm/examples</code>). Use the \"Documentation\" label.</li> <li>Rectifying issues with the existing codebase. These can range from minor software bugs to more significant design problems. Use the \"Bug\" label.</li> </ol>"},{"location":"CONTRIBUTING/#opening-issues","title":"Opening Issues","text":"<p>If you find a bug or encounter any type of problem while using HSSM, please let us know by filing an issue on the GitHub Issue Tracker rather than via social media or direct emails to the developers.</p> <p>Please make sure your issue isn't already being addressed by other issues or pull requests. You can use the GitHub search tool to search for keywords in the project issue tracker. Please use appropriate labels for an issue.</p>"},{"location":"CONTRIBUTING/#pull-request-step-by-step","title":"Pull Request Step-by-Step","text":"<p>The preferred workflow for contributing to HSSM is to fork the GitHub repository, clone it to your local machine, and develop on a feature branch.</p>"},{"location":"CONTRIBUTING/#steps","title":"Steps","text":"<ol> <li> <p>Fork the project repository by clicking on the \u2018Fork\u2019 button near the top right of the main repository page. This creates a copy of the code under your GitHub user account.</p> </li> <li> <p>Clone your fork of the HSSM repo** from your GitHub account to your local disk.    <pre><code>git clone https://github.com/&lt;your GitHub handle&gt;/lnccbrown/hssm.git\n</code></pre></p> </li> <li> <p>Navigate to your <code>hssm</code> directory and add the base repository as a remote. This sets up a directive to propose your local changes to the <code>hssm</code> repository.    <pre><code>cd hssm\ngit remote add upstream https://github.com/lnccbrown/hssm.git\n</code></pre></p> </li> <li> <p>Create a feature branch to hold your changes:    <pre><code>git checkout -b my-feature\n</code></pre></p> </li> </ol> <p>[!WARNING] Routinely making changes in the main branch of a repository should be avoided. Always create a new feature branch before making any changes and make your changes in the feature branch.</p> <ol> <li>Add the new feature/changes on your feature branch. When finished, commit your changes:    <pre><code>git add &lt;modified_files&gt;\ngit commit -m \"commit message here\"\n</code></pre></li> </ol> <p>After committing, it is a good idea to sync with the base repository in case there have been any changes:    <pre><code>git fetch upstream\ngit rebase upstream/main\n</code></pre></p> <p>[!Note] If your changes require libraries not included in <code>hssm</code>, you'll need to use <code>uv</code> to update the dependency files. Please visit the official <code>uv</code> documentation and follow the installation instructions.</p> <p>After installing <code>uv</code>, you can add the new libraries (dependencies) to <code>pyproject.toml</code> by running: <pre><code>uv add &lt;package-name&gt;\n</code></pre> Replace <code>&lt;package-name&gt;</code> with the name of the library you need to add. This command will update the <code>pyproject.toml</code> file and install the new dependency. It will also add changes to the <code>uv.lock</code> file.</p> <p>Remember to commit the newly changed files. <pre><code>git add pyproject.toml\ngit commit -m \"Add &lt;package-name&gt; dependency\"\n</code></pre></p> <ol> <li> <p>Push the changes to your GitHub account with:    <pre><code>git push -u origin my-feature\n</code></pre></p> </li> <li> <p>Create a Pull Request:</p> </li> <li>Go to the GitHub web page of your fork of the HSSM repo.</li> <li>Click the \u2018Pull request\u2019 button to send your changes to the project\u2019s maintainers for review.</li> </ol> <p>This guide is adapted from the ArviZ contribution guide</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#0211","title":"0.2.11","text":"<p>This version include the following:</p> <ol> <li>Simplification of simulator logic inside HSSM random variables (see the <code>ssm-simulators</code> <code>0.11.3</code> release as well)</li> <li>Plotting functions now allow <code>prior_predictive</code> plots wherever suitable.</li> <li>A new tutorial on using bayeux for sampling</li> <li>Consolidated <code>plotting</code> tutorial</li> <li>New tutorial on how to use the <code>do-operator</code> from PyMC to control forward simulations</li> </ol>"},{"location":"changelog/#028","title":"0.2.8","text":"<p>This version of HSSM incorporated the following changes:</p> <ol> <li>Addition of tutorials for RLSSM paradigms.</li> <li>Restructure <code>rldm.py</code> to make it more template-based to allow easy introduction of new models.</li> </ol>"},{"location":"changelog/#027","title":"0.2.7","text":"<p>This version of HSSM incorporated the following changes:</p> <ol> <li>Bugfix in <code>mkdocs.yml</code>. No tutorial notebooks should be actively executed when re-building docs.</li> </ol>"},{"location":"changelog/#026","title":"0.2.6","text":"<p>This version of HSSM incorporated the following changes:</p> <ol> <li>Overhaul of our development infrastructure to depend on <code>uv</code></li> <li>Addition of various new tutorials (highlight: how to fit RL+SSM models)</li> <li>New <code>rldm</code> likelihoods</li> <li>Major internal refactor (e.g. <code>DataValidator</code> class for pre-processing)</li> </ol>"},{"location":"changelog/#025","title":"0.2.5","text":"<p>This version of HSSM incorporated the following changes:</p> <ol> <li>We added a new tutorial on how to use custom likelihood functions with HSSM.</li> <li>Added convenience function <code>compile_logp()</code> to return a compiled log-likelihood function to be used freely downstream.</li> <li>Some improvements to internals to allow for (1.)</li> <li>Major improvments to plotting with <code>model_cartoon_plots()</code></li> <li>Refactoring and minor improvements to pre-existing plots</li> <li>Added functionality to save and load models with <code>save_model()</code> and <code>load_model()</code>.</li> </ol>"},{"location":"changelog/#024","title":"0.2.4","text":"<p>This version of HSSM incorporated the following changes:</p> <ol> <li>We updated HSSM to be compatible with the major API changes in <code>bambi</code> v0.14.0.</li> <li>We fixed various graphing issues in <code>pymc</code> 5.16.0+, thanks to the API changes in <code>bambi</code>.</li> <li>We added variational inference via native <code>pymc</code>.</li> <li>We can now use <code>float64</code> inference.</li> <li>We fixed some minor bugs in providing initial values.</li> <li>We added a model.dic() convenience function.</li> <li>We added a model.restore_traces() convenience function.</li> <li>Other minor bug fixes.</li> </ol>"},{"location":"changelog/#023","title":"0.2.3","text":"<p>This is a maintenance release of HSSM, mainly to add a version constraint on <code>bambi</code> in light of the many breaking changes that version <code>0.1.4</code> introduces. This version also improved compatibility with <code>PyMC&gt;=5.15</code> and incorporated minor bug fixes:</p> <ol> <li>We incorporated a temporary fix to graphing which broke after <code>PyMC&gt;=5.15</code>.</li> <li>We deprecated <code>ndim</code> and <code>ndim_supp</code> definition in <code>SSMRandomVariable</code> in <code>PyMC&gt;-5.16</code>.</li> <li>We fixed a bug that prevents new traces from being returned if <code>model.sample()</code> is called again.</li> </ol>"},{"location":"changelog/#022","title":"0.2.2","text":"<p>HSSM is now on Conda! We now recommend installing HSSM through <code>conda install -c conda-forge hssm</code>. For advanced users, we also support installing the GPU version of JAX through <code>pip install hssm[cuda12]</code>.</p> <p>This version incorporates various bug fixes:</p> <ol> <li>We fixed a major bug that causes divergences for models using <code>approx_differentiable</code> and <code>blackbox</code> likelihoods. We are still looking into the issues of divergence with <code>analytical</code> likelihoods.</li> <li>We made the model creation process more robust, fixing errors when categorical variables are used with group identifiers.</li> <li>We updated the codebase according to the deprecations in higher versions of JAX.</li> <li>We implemented a temporary fix to an issue that might cause the kernel to die due to OOM.</li> </ol>"},{"location":"changelog/#021","title":"0.2.1","text":"<p>We added a few new features in 0.2.1:</p> <ol> <li>We have finished updating the HSSM code base to support go-nogo data and deadline. We will provide documentation once the networks are added to our huggingface repo.</li> <li>We updated <code>hssm.distribution_utils</code> to streamline the creation of <code>pm.Distribution</code>s.</li> <li>We now support response variables other than <code>rt</code> and <code>response</code>. They can be specified through <code>model_config</code> via the new <code>response</code> field.</li> <li>We have fixed some of the issues with convergence when using <code>log-logit</code> link functions and/or safe priors.</li> </ol> <p>Other minor updates</p> <ul> <li>Fixed an incompatible shape error during posterior predictive sampling when <code>p_outlier</code> is estimated as a parameter.</li> <li>Updated documentation for using <code>make_distribution</code> with PyMC.</li> </ul> <p>Bug fixes:</p> <ul> <li>Fixed default list of parameters for <code>ddm_full</code> model and the bounds for <code>ddm_sdv</code> model.</li> </ul>"},{"location":"changelog/#020","title":"0.2.0","text":"<p>This is a major version update! Many changes have taken place in this version:</p>"},{"location":"changelog/#breaking-changes","title":"Breaking changes","text":"<p>When <code>hierarchical</code> argument of <code>hssm.HSSM</code> is set to <code>True</code>, HSSM will look into the <code>data</code> provided for the <code>participant_id</code> field. If it does not exist, an error will be thrown.</p>"},{"location":"changelog/#new-features","title":"New features","text":"<ul> <li> <p>Added <code>link_settings</code> and <code>prior_settings</code> arguments to <code>hssm.HSSM</code>, which allows HSSM   to use intelligent default priors and link functions for complex hierarchical models.</p> </li> <li> <p>Added an <code>hssm.plotting</code> submodule with <code>plot_predictive()</code> and   <code>plot_quantile_probability</code> for creating posterior predictive plots and quantile   probability plots.</p> </li> <li> <p>Added an <code>extra_fields</code> argument to <code>hssm.HSSM</code> to pass additional data to the   likelihood function computation.</p> </li> <li> <p>Limited <code>PyMC</code>, <code>pytensor</code>, <code>numpy</code>, and <code>jax</code> dependency versions for compatibility.</p> </li> </ul>"},{"location":"changelog/#01x","title":"0.1.x","text":""},{"location":"changelog/#015","title":"0.1.5","text":"<p>We fixed the errors in v0.1.4. Sorry for the convenience! If you have accidentally downloaded v0.1.4, please make sure that you update hssm to the current version.</p> <ul> <li>We made Cython dependencies of this package available via pypi. We have also built   wheels for (almost) all platforms so there is no need to build these Cython   dependencies.</li> </ul>"},{"location":"changelog/#014","title":"0.1.4","text":"<ul> <li>Added support of <code>blackbox</code> likelihoods for <code>ddm</code> and <code>ddm_sdv</code> models.</li> <li>Added support for <code>full_ddm</code> models via <code>blackbox</code> likelihoods.</li> <li>Added the ability to use <code>hssm.Param</code> and <code>hssm.Prior</code> to specify model parameters.</li> <li>Added support for non-parameter fields to be involved in the computation of likelihoods.</li> <li>Major refactor of the code to improve readability and maintainability.</li> <li>Fixed a bug in model.sample_posterior_predictive().</li> </ul>"},{"location":"changelog/#013","title":"0.1.3","text":"<ul> <li>Added the ability to specify <code>inf</code>s in bounds.</li> <li>Fixed an issue where <code>nuts_numpyro</code> sampler fails with regression and lapse distribution.</li> <li>Defaults to <code>nuts_numpyro</code> sampler with <code>approx_differentiable</code> likelihoods and <code>jax</code> backend.</li> <li>Added a <code>hssm.show_defaults()</code> convenience function to print out default configs.</li> <li>Added default <code>blackbox</code> likelihoods for <code>ddm</code> and <code>ddm_sdv</code> models.</li> <li>Various under-the-hood documentation improvements.</li> </ul>"},{"location":"changelog/#012","title":"0.1.2","text":"<ul> <li>Improved numerical stability of <code>analytical</code> likelihoods.</li> <li>Added the ability to handle lapse distributions.</li> <li>Added the ability to perform prior predictive sampling.</li> <li>Improved model information output - now default priors provided by <code>bambi</code> is also printed.</li> <li>Added a <code>hierarchical</code> switch which turns all parameters into hierarchical   when <code>participant_id</code> is provided in data.</li> <li> <p>Parameters are now named more consistently (group-specific terms are now aliased correctly).</p> </li> <li> <p>Fixed a bug where information about which parameter is regression is incorrectly passed.</p> </li> <li>Added links to Colab to try out hssm in Google Colab.</li> </ul>"},{"location":"changelog/#011","title":"0.1.1","text":"<ul> <li>Handle <code>float</code> types in <code>PyTensor</code> and <code>JAX</code> more consistently and explicitly.</li> <li>Updated model output format to include likelihood kinds and display bounds more consistently.</li> <li>Support for <code>inf</code>s in bounds.</li> <li>Convenient method for simulating data with <code>ssm_simulators</code>.</li> <li>More test coverage.</li> <li>CI workflows for publishing package to PyPI.</li> <li>Enhancement to documentations.</li> </ul>"},{"location":"credits/","title":"Credits","text":"<p>We would like to extend our gratitude to the following individuals for their valuable contributions to the development of the HSSM package:</p> <ul> <li>Bambi - A special thanks to the Bambi project for providing inspiration, guidance, and support throughout the development process. Tom\u00e1s Capretto, a key contributor to Bambi, provided invaluable assistance in the development of the HSSM package.</li> <li>The PyMC team - Their advanced probabilistic programming framework has provided a solid foundation for us to build upon.</li> <li>The PyMC Discourse Community - We're grateful for this active and supportive community. Their insightful discussions and problem-solving have been an invaluable resource throughout the development process.</li> <li>Center for Computation &amp; Visualization, Brown University - We owe a debt of gratitude to this institution for their meticulous PR reviews and engaging discussions, which have significantly enhanced the quality and integrity of our package.</li> </ul> <p>Those contributions have greatly enhanced the functionality and quality of the HSSM.</p>"},{"location":"license/","title":"License","text":"<pre><code>Copyright 2023, Brown University, Providence, RI.\n\n                        All Rights Reserved\n\nPermission to use, copy, modify, and distribute this software and\nits documentation for any purpose other than its incorporation into a\ncommercial product or service is hereby granted without fee, provided\nthat the above copyright notice appear in all copies and that both\nthat copyright notice and this permission notice appear in supporting\ndocumentation, and that the name of Brown University not be used in\nadvertising or publicity pertaining to distribution of the software\nwithout specific, written prior permission.\n\nBROWN UNIVERSITY DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,\nINCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY\nPARTICULAR PURPOSE.  IN NO EVENT SHALL BROWN UNIVERSITY BE LIABLE FOR\nANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\nWHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\nACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\nOR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n</code></pre>"},{"location":"local_development/","title":"Setting up a local development environment","text":"<p>To contribute to HSSM, the first step is to clone the HSSM repo locally and set up a local development environment. This guide will walk you through setting up the local dev environment on different platforms.</p>"},{"location":"local_development/#step-1-install-uv","title":"Step 1. Install uv","text":"<p>HSSM is managed by uv. uv is an extremely fast Python package and project manager written in Rust that is gaining tremendous popularity in the Python world. If you have not already installed it, you need to install uv following its official installation guide. You can verify your uv installation by the following command.</p> <pre><code>uv --version\n</code></pre>"},{"location":"local_development/#step-2-clone-the-hssm-repo-and-set-up-the-virtual-environment","title":"Step 2. Clone the HSSM repo and set up the virtual environment","text":""},{"location":"local_development/#clone-the-hssm-repo","title":"Clone the HSSM repo","text":"<p>You can clone the HSSM repo with <code>git</code>:</p> <pre><code>git clone https://github.com/lnccbrown/HSSM.git\ncd HSSM\n</code></pre>"},{"location":"local_development/#set-up-the-virtual-environment-and-install-dependencies","title":"Set up the virtual environment and install dependencies","text":"<p>uv will handle the creation of the virtual environment with all dev and test dependencies with the following command:</p> <pre><code>uv sync --group dev --group test\n</code></pre> <p>This tells uv to install not only the HSSM dependencies but also the <code>dev</code> and <code>test</code> dependency groups defined in <code>pyproject.toml</code>.</p>"},{"location":"local_development/#optional-set-up-jax-for-gpu","title":"Optional: Set up JAX for GPU","text":"<p>If you need JAX to support GPU, you can install the GPU version of JAX through:</p> <pre><code>uv sync --all-groups --all-extras\n</code></pre> <p>or</p> <pre><code>uv sync --group dev --group test --extra cuda12\n</code></pre> <p>Please ensure that you have a GPU that supports CUDA 12 for this installation.</p>"},{"location":"local_development/#step-3-set-up-a-linear-algebra-package","title":"Step 3. Set up a linear algebra package","text":"<p>Different from installation through <code>conda</code>, which is what we recommend for HSSM users, when HSSM is installed through <code>uv</code> in a local development environment, <code>pytensor</code> does not usually know how to find the linear algebra acceleration library on your system, and when that happens, you will typically get slow inference speed using the default <code>pymc</code> NUTS sampler with the following warning:</p> <pre><code>WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n</code></pre> <p>If this happens, it means that we need to tell <code>pytensor</code> where the linear algebra library is installed on your system. There are many ways to do this, but the most convenient way is to create a <code>.pytensorrc</code> file in your home directory with the following content:</p> <pre><code>[blas]\nldflags=...\n</code></pre> <p>What <code>...</code> actually is depends on your operating system and hardware architecture. Here's the recommendation for certain setups:</p>"},{"location":"local_development/#macos-with-arm-chips-m-series-processors","title":"MacOS with ARM chips (M-series processors)","text":"<p>MacOS with ARM chips comes with accelerate framework, and you just need to tell your <code>pytensor</code> to use it:</p> <pre><code>[blas]\nldflags=-framework Accelerate\n</code></pre>"},{"location":"local_development/#intel-macs-linux-and-windows-wsl-on-intel-processors","title":"Intel Macs, Linux and Windows (WSL) on Intel processors","text":"<p>Intel has the oneAPI Math Kernel Library (oneMKL) that you can use as your acceleration library. You need to install the oneMKL library and then point your <code>.pytensorrc</code> file to it:</p> <pre><code>[blas]\nldflags=-Lwherever/mkl/is -lmkl_core -lmkl_rt -lpthread -lm\n</code></pre> <p>Just replace <code>wherever/mkl/is</code> with the absolute path of MKL installed on your system.</p> <p>Note</p> <p>If you wish to develop HSSM on Oscar, Brown University's HPC cluster, MKL is already available through a module. You can follow this instruction on how to write the <code>.pytensorrc</code> file.</p>"},{"location":"local_development/#other-systems","title":"Other systems","text":"<p>On other systems, we recommend installing <code>openblas</code> or <code>lapack</code>. You can follow the instructions on the openblas website or the lapack website to install one of these libraries.</p> <p>If you installed <code>openblas</code>, then your <code>.pytensorrc</code> file should look like this:</p> <pre><code>[blas]\nldflags=-L/path/to/openblas/lib -lopenblas\n</code></pre> <p>If you installed <code>openblas</code>, then your <code>.pytensorrc</code> file should look like this:</p> <pre><code>[blas]\nldflags=-L/path/to/lapack/lib -lblas\n</code></pre>"},{"location":"local_development/#follow-up","title":"Follow-up","text":"<p>Once the above steps are done, you should be able to run a <code>pytensor</code> based sampler without any warnings.</p> <p>We recommend that you also configure your IDE to use the Python interpreter in the virtual environment created by <code>uv</code>. Typically, this virtual environment is in the <code>.venv</code> directory under the <code>HSSM</code> directory. If you use Visual Studio Code, it will automatically detect this virtual environment and ask if you want to use the Python interpreter in that virtual environment as the interpreter.</p>"},{"location":"local_development/#run-test-suite","title":"Run test suite","text":"<p>We recommend that you run the test suite with <code>uv run</code> command:</p> <pre><code>uv run pytest\n</code></pre>"},{"location":"api/check_data_for_rl/","title":"hssm.check_data_for_rl","text":""},{"location":"api/check_data_for_rl/#hssm.check_data_for_rl","title":"hssm.check_data_for_rl","text":"<pre><code>check_data_for_rl(\n    data: DataFrame,\n    participant_id_col: str = \"participant_id\",\n    trial_id_col: str = \"trial_id\",\n) -&gt; tuple[pd.DataFrame, int, int]\n</code></pre> <p>Check if the data is suitable for Reinforcement Learning (RL) models.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DataFrame</code>)           \u2013            <p>The data to check.</p> </li> <li> <code>participant_id_col</code>               (<code>str</code>, default:                   <code>'participant_id'</code> )           \u2013            <p>The name of the column containing participant IDs.</p> </li> <li> <code>trial_id_col</code>               (<code>str</code>, default:                   <code>'trial_id'</code> )           \u2013            <p>The name of the column containing trial IDs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[DataFrame, int, int]</code>           \u2013            <p>A tuple containing the cleaned data, number of participants, and number of trials.</p> </li> </ul>"},{"location":"api/defaults/","title":"Module: Default Configurations for HSSM Models","text":"<p>This module provides default configurations for various models used in the Hierarchical Sequential Sampling Models (HSSM) class.</p>"},{"location":"api/defaults/#model-configurations","title":"Model Configurations:","text":"<p>The module includes a dictionary, <code>default_model_config</code>, that provides default configurations for a variety of models, including:</p> <ul> <li><code>ddm</code></li> <li><code>ddm_sdv</code></li> <li><code>full_ddm</code></li> <li><code>angle</code></li> <li><code>levy</code></li> <li><code>ornstein</code></li> <li><code>weibull</code></li> <li><code>race_no_bias_angle_4</code></li> <li><code>ddm_seq2_no_bias</code></li> </ul>"},{"location":"api/defaults/#configuration-parameters","title":"Configuration parameters","text":"<p>Each model configuration is specified by several parameters, which include:</p> <ul> <li><code>loglik</code></li> <li><code>list_params</code></li> <li><code>default_priors</code></li> <li><code>backend</code></li> <li><code>bounds</code></li> </ul>"},{"location":"api/defaults/#default-configurations","title":"Default Configurations","text":"<p>For each model, a dictionary is defined containing configurations for each <code>LoglikKind</code>. Each configuration includes:</p> <ul> <li><code>loglik</code>: the log-likelihood function or filename</li> <li><code>bounds</code>: the bounds for the model parameters</li> <li><code>default_priors</code>: the default priors for the model parameters</li> <li><code>backend</code>: (optional) the backend for approximating the likelihood</li> </ul>"},{"location":"api/defaults/#model-ddm","title":"Model: DDM","text":""},{"location":"api/defaults/#analytical","title":"Analytical","text":"<ul> <li>Log-likelihood kind: Analytical</li> <li>Log-likelihood: log_pdf</li> <li>Parameters: v, a, z, t</li> <li>Bounds:</li> <li>z: (0.0, 1.0)</li> <li>Default priors:</li> <li>v: Uniform (-10.0, 10.0)</li> <li>a: HalfNormal with sigma 2.0</li> <li>t: Uniform (0.0, 0.5) with initial value 0.1</li> </ul>"},{"location":"api/defaults/#approx-differentiable","title":"Approx Differentiable","text":"<ul> <li>Log-likelihood kind: Approx Differentiable</li> <li>Log-likelihood: ddm.onnx</li> <li>Backend: jax</li> <li>Parameters: v, a, z, t</li> <li>Bounds:</li> <li>v: (-3.0, 3.0)</li> <li>a: (0.3, 2.5)</li> <li>z: (0.1, 0.9)</li> <li>t: (0.0, 2.0)</li> </ul>"},{"location":"api/defaults/#model-ddm_sdv","title":"Model: DDM_SDV","text":""},{"location":"api/defaults/#analytical_1","title":"Analytical","text":"<ul> <li>Log-likelihood kind: Analytical</li> <li>Log-likelihood: log_pdf_sv</li> <li>Parameters: v, sv, a, z, t</li> <li>Bounds:</li> <li>z: (0.0, 1.0)</li> <li>Default priors:</li> <li>v: Uniform (-10.0, 10.0)</li> <li>sv: HalfNormal with sigma 2.0</li> <li>a: HalfNormal with sigma 2.0</li> <li>t: Uniform (0.0, 5.0) with initial value 0.1</li> </ul>"},{"location":"api/defaults/#approx-differentiable_1","title":"Approx Differentiable","text":"<ul> <li>Log-likelihood kind: Approx Differentiable</li> <li>Log-likelihood: ddm_sv.onnx</li> <li>Backend: jax</li> <li>Parameters: v, sv, a, z, t</li> <li>Bounds:</li> <li>v: (-3.0, 3.0)</li> <li>sv: (0.0, 1.0)</li> <li>a: (0.3, 2.5)</li> <li>z: (0.1, 0.9)</li> <li>t: (0.0, 2.0)</li> </ul>"},{"location":"api/defaults/#model-ornstein","title":"Model: Ornstein","text":"<ul> <li>Log-likelihood kind: Approx Differentiable</li> <li>Log-likelihood: ornstein.onnx</li> <li>Backend: jax</li> <li>Parameters: v, a, z, g, t</li> <li>Bounds:</li> <li>v: (-2.0, 2.0)</li> <li>a: (0.3, 3.0)</li> <li>z: (0.1, 0.9)</li> <li>g: (-1.0, 1.0)</li> <li>t: (1e-3, 2.0)</li> </ul>"},{"location":"api/defaults/#model-weibull","title":"Model: Weibull","text":"<ul> <li>Log-likelihood kind: Approx Differentiable</li> <li>Log-likelihood: weibull.onnx</li> <li>Backend: jax</li> <li>Parameters: v, a, z, t, alpha, beta</li> <li>Bounds:</li> <li>v: (-2.5, 2.5)</li> <li>a: (0.3, 2.5)</li> <li>z: (0.2, 0.8)</li> <li>t: (1e-3, 2.0)</li> <li>alpha: (0.31, 4.99)</li> <li>beta: (0.31, 6.99)</li> </ul>"},{"location":"api/defaults/#model-race_no_bias_angle_4","title":"Model: Race_no_bias_angle_4","text":"<ul> <li>Log-likelihood kind: Approx Differentiable</li> <li>Log-likelihood: race_no_bias_angle_4.onnx</li> <li>Backend: jax</li> <li>Parameters: v0, v1, v2, v3, a, z, ndt, theta</li> <li>Bounds:</li> <li>v0: (0.0, 2.5)</li> <li>v1: (0.0, 2.5)</li> <li>v2: (0.0, 2.5)</li> <li>v3: (0.0, 2.5)</li> <li>a: (1.0, 3.0)</li> <li>z: (0.0, 0.9)</li> <li>ndt: (0.0, 2.0)</li> <li>theta: (-0.1, 1.45)</li> </ul>"},{"location":"api/defaults/#model-ddm_seq2_no_bias","title":"Model: DDM_seq2_no_bias","text":"<ul> <li>Log-likelihood kind: Approx Differentiable</li> <li>Log-likelihood: ddm_seq2_no_bias.onnx</li> <li>Backend: jax</li> <li>Parameters: vh, vl1, vl2, a, t</li> <li>Bounds:</li> <li>vh: (-4.0, 4.0)</li> <li>vl1: (-4.0, 4.0)</li> <li>vl2: (-4.0, 4.0)</li> <li>a: (0.3, 2.5)</li> <li>t: (0.0, 2.0)</li> </ul>"},{"location":"api/defaults/#wfpt-and-wfpt_sdv-classes","title":"WFPT and WFPT_SDV Classes","text":"<p>The WFPT and WFPT_SDV classes are created using the make_distribution function. They represent the Drift Diffusion Model (<code>ddm</code>) and Drift Diffusion Model with inter-trial variability in drift (<code>ddm_sdv</code>) respectively. They use the log-likelihood functions and parameter lists from the default configurations and parameters.</p>"},{"location":"api/distribution_utils/","title":"hssm.distribution_utils","text":"<p>The <code>hssm.distribution_utils</code> contains useful functions for building <code>pm.Distribution</code> classes. Other than the <code>download_hf</code> function that downloads ONNX models shared on our huggingface model repository, you will generally not have to use these functions. For advanced users who want to build their own PyMC models, they can use these functions to create <code>pm.Distribution</code> and <code>RandomVariable</code> classes that they desire.</p>"},{"location":"api/distribution_utils/#hssm.distribution_utils","title":"hssm.distribution_utils","text":"<p>Utility functions for dynamically building pm.Distributions.</p> <p>Functions:</p> <ul> <li> <code>download_hf</code>             \u2013              <p>Download a file from a HuggingFace repository.</p> </li> <li> <code>load_onnx_model</code>             \u2013              <p>Load an ONNX model from a local file or from HuggingFace.</p> </li> <li> <code>make_distribution</code>             \u2013              <p>Make a <code>pymc.Distribution</code>.</p> </li> <li> <code>make_family</code>             \u2013              <p>Build a family in bambi.</p> </li> <li> <code>make_likelihood_callable</code>             \u2013              <p>Make a callable for the likelihood function.</p> </li> <li> <code>make_missing_data_callable</code>             \u2013              <p>Make a secondary network for the likelihood function.</p> </li> <li> <code>make_blackbox_op</code>             \u2013              <p>Wrap an arbitrary function in a pytensor Op.</p> </li> <li> <code>assemble_callables</code>             \u2013              <p>Assemble the likelihood callables into a single callable.</p> </li> </ul>"},{"location":"api/distribution_utils/#hssm.distribution_utils.download_hf","title":"hssm.distribution_utils.download_hf","text":"<pre><code>download_hf(path: str)\n</code></pre> <p>Download a file from a HuggingFace repository.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path of the file to download in the repository.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The local path where the file is downloaded.</p> </li> </ul> Notes <p>The repository is specified by the REPO_ID constant, which should be a valid HuggingFace.co repository ID. The file is downloaded using the HuggingFace Hub's  hf_hub_download function.</p>"},{"location":"api/distribution_utils/#hssm.distribution_utils.load_onnx_model","title":"hssm.distribution_utils.load_onnx_model","text":"<pre><code>load_onnx_model(model: str | PathLike | ModelProto) -&gt; onnx.ModelProto\n</code></pre> <p>Load an ONNX model from a local file or from HuggingFace.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str | PathLike | ModelProto</code>)           \u2013            <p>The model can be a path to a local ONNX file, a HuggingFace file, or a loaded ONNX ModelProto object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ModelProto</code>           \u2013            <p>The loaded ONNX model.</p> </li> </ul>"},{"location":"api/distribution_utils/#hssm.distribution_utils.make_distribution","title":"hssm.distribution_utils.make_distribution","text":"<pre><code>make_distribution(\n    rv: str | Type[RandomVariable] | RandomVariable | Callable[..., Any],\n    loglik: LogLikeFunc | Op,\n    list_params: list[str],\n    bounds: dict | None = None,\n    lapse: Prior | None = None,\n    extra_fields: list[ndarray] | None = None,\n) -&gt; Type[pm.Distribution]\n</code></pre> <p>Make a <code>pymc.Distribution</code>.</p> <p>Constructs a <code>pymc.Distribution</code> from a log-likelihood function and a RandomVariable op.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>           \u2013            <p>The name of the model.</p> </li> <li> <code>choices</code>           \u2013            <p>A list of integers indicating the choices.</p> </li> <li> <code>rv</code>               (<code>str | Type[RandomVariable] | RandomVariable | Callable[..., Any]</code>)           \u2013            <p>A RandomVariable Op (a class, not an instance) or a string indicating the model. If a string, a RandomVariable class will be created automatically with its <code>rng_fn</code> class method generated using the simulator identified with this string from the <code>ssm_simulators</code> package. If the string is not one of the supported models in the <code>ssm_simulators</code> package, a warning will be raised, and any attempt to sample from the RandomVariable will result in an error.</p> </li> <li> <code>loglik</code>               (<code>LogLikeFunc | Op</code>)           \u2013            <p>A loglikelihood function. It can be any Callable in Python.</p> </li> <li> <code>list_params</code>               (<code>list[str]</code>)           \u2013            <p>A list of parameters that the log-likelihood accepts. The order of the parameters in the list will determine the order in which the parameters are passed to the log-likelihood function.</p> </li> <li> <code>bounds</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary with parameters as keys (a string) and its boundaries as values. Example: {\"parameter\": (lower_boundary, upper_boundary)}.</p> </li> <li> <code>lapse</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>A bmb.Prior object representing the lapse distribution.</p> </li> <li> <code>extra_fields</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>An optional list of arrays that are stored in the class created and will be used in likelihood calculation. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Type[Distribution]</code>           \u2013            <p>A pymc.Distribution that uses the log-likelihood function.</p> </li> </ul>"},{"location":"api/distribution_utils/#hssm.distribution_utils.make_family","title":"hssm.distribution_utils.make_family","text":"<pre><code>make_family(\n    dist: Type[Distribution],\n    list_params: list[str],\n    link: str | dict[str, Link],\n    parent: str = \"v\",\n    likelihood_name: str = \"SSM Likelihood\",\n    family_name=\"SSM Family\",\n) -&gt; bmb.Family\n</code></pre> <p>Build a family in bambi.</p> <p>Parameters:</p> <ul> <li> <code>dist</code>               (<code>Type[Distribution]</code>)           \u2013            <p>A pm.Distribution class (not an instance).</p> </li> <li> <code>list_params</code>               (<code>list[str]</code>)           \u2013            <p>A list of parameters for the likelihood function.</p> </li> <li> <code>link</code>               (<code>str | dict[str, Link]</code>)           \u2013            <p>A link function or a dictionary of parameter: link functions.</p> </li> <li> <code>parent</code>               (<code>str</code>, default:                   <code>'v'</code> )           \u2013            <p>The parent parameter of the likelihood function. Defaults to v.</p> </li> <li> <code>likelihood_name</code>               (<code>str</code>, default:                   <code>'SSM Likelihood'</code> )           \u2013            <p>the name of the likelihood function. Defaults to \"SSM Likelihood\".</p> </li> <li> <code>family_name</code>           \u2013            <p>the name of the family. Defaults to \"SSM Family\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Family</code>           \u2013            <p>An instance of a bambi family.</p> </li> </ul>"},{"location":"api/distribution_utils/#hssm.distribution_utils.make_likelihood_callable","title":"hssm.distribution_utils.make_likelihood_callable","text":"<pre><code>make_likelihood_callable(\n    loglik: Op | Callable[..., Any] | PathLike | str,\n    loglik_kind: Literal[\"analytical\", \"approx_differentiable\", \"blackbox\"],\n    backend: Literal[\"pytensor\", \"jax\", \"other\"] | None,\n    params_is_reg: list[bool] | None = None,\n    params_only: bool | None = None,\n) -&gt; pytensor.graph.Op | Callable\n</code></pre> <p>Make a callable for the likelihood function.</p> <p>This function is intended to be general to support different kinds of likelihood functions.</p> <p>Parameters:</p> <ul> <li> <code>loglik</code>               (<code>Op | Callable[..., Any] | PathLike | str</code>)           \u2013            <p>The log-likelihood function. It can be a string, a path to an ONNX model, a pytensor Op, or a python function.</p> </li> <li> <code>loglik_kind</code>               (<code>Literal['analytical', 'approx_differentiable', 'blackbox']</code>)           \u2013            <p>The kind of the log-likelihood for the model. This parameter controls how the likelihood function is wrapped.</p> </li> <li> <code>backend</code>               (<code>Optional</code>)           \u2013            <p>The backend to use for the log-likelihood function.</p> </li> <li> <code>params_is_reg</code>               (<code>Optional</code>, default:                   <code>None</code> )           \u2013            <p>A list of boolean values indicating whether the parameters are regression parameters. Defaults to None.</p> </li> <li> <code>params_only</code>               (<code>Optional</code>, default:                   <code>None</code> )           \u2013            <p>Whether the missing data likelihood is takes its first argument as the data. Defaults to None.</p> </li> </ul>"},{"location":"api/distribution_utils/#hssm.distribution_utils.make_missing_data_callable","title":"hssm.distribution_utils.make_missing_data_callable","text":"<pre><code>make_missing_data_callable(\n    loglik: Op | Callable | PathLike | str,\n    backend: Literal[\"pytensor\", \"jax\", \"other\"] | None = \"jax\",\n    params_is_reg: list[bool] | None = None,\n    params_only: bool | None = None,\n) -&gt; pytensor.graph.Op | Callable\n</code></pre> <p>Make a secondary network for the likelihood function.</p> <p>Please refer to the documentation of <code>make_likelihood_callable</code> for more.</p>"},{"location":"api/distribution_utils/#hssm.distribution_utils.make_blackbox_op","title":"hssm.distribution_utils.make_blackbox_op","text":"<pre><code>make_blackbox_op(logp: Callable) -&gt; Op\n</code></pre> <p>Wrap an arbitrary function in a pytensor Op.</p> <p>Parameters:</p> <ul> <li> <code>logp</code>               (<code>Callable</code>)           \u2013            <p>A python function that represents the log-likelihood function. The function needs to have signature of logp(data, *dist_params) where <code>data</code> is a two-column numpy array and <code>dist_params</code>represents all parameters passed to the function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Op</code>           \u2013            <p>An pytensor op that wraps the log-likelihood function.</p> </li> </ul>"},{"location":"api/distribution_utils/#hssm.distribution_utils.assemble_callables","title":"hssm.distribution_utils.assemble_callables","text":"<pre><code>assemble_callables(\n    callable: Op | Callable,\n    missing_data_callable: Op | Callable,\n    params_only: bool | None,\n    has_deadline: bool,\n) -&gt; Callable\n</code></pre> <p>Assemble the likelihood callables into a single callable.</p> <p>Parameters:</p> <ul> <li> <code>callable</code>               (<code>Op | Callable</code>)           \u2013            <p>The callable for the likelihood function.</p> </li> <li> <code>missing_data_callable</code>               (<code>Op | Callable</code>)           \u2013            <p>The callable for the secondary network for the likelihood function.</p> </li> <li> <code>params_only</code>               (<code>bool | None</code>)           \u2013            <p>Whether the missing data likelihood is takes its first argument as the data.</p> </li> <li> <code>has_deadline</code>               (<code>bool</code>)           \u2013            <p>Whether the model has a deadline.</p> </li> </ul>"},{"location":"api/hssm/","title":"hssm.HSSM","text":"<p>Use <code>hssm.HSSM</code> class to construct an HSSM model.</p>"},{"location":"api/hssm/#hssm.HSSM","title":"hssm.HSSM","text":"<pre><code>HSSM(\n    data: DataFrame,\n    model: SupportedModels | str = \"ddm\",\n    choices: list[int] | None = None,\n    include: list[dict[str, Any] | UserParam] | None = None,\n    model_config: ModelConfig | dict | None = None,\n    loglik: str | PathLike | Callable | Op | type[Distribution] | None = None,\n    loglik_kind: LoglikKind | None = None,\n    p_outlier: float | dict | Prior | None = 0.05,\n    lapse: dict | Prior | None = bmb.Prior(\"Uniform\", lower=0.0, upper=20.0),\n    global_formula: str | None = None,\n    link_settings: Literal[\"log_logit\"] | None = None,\n    prior_settings: Literal[\"safe\"] | None = \"safe\",\n    extra_namespace: dict[str, Any] | None = None,\n    missing_data: bool | float = False,\n    deadline: bool | str = False,\n    loglik_missing_data: str | PathLike | Callable | Op | None = None,\n    process_initvals: bool = True,\n    initval_jitter: float = INITVAL_JITTER_SETTINGS[\"jitter_epsilon\"],\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DataValidator</code></p> <p>The basic Hierarchical Sequential Sampling Model (HSSM) class.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DataFrame</code>)           \u2013            <p>A pandas DataFrame with the minimum requirements of containing the data with the columns \"rt\" and \"response\".</p> </li> <li> <code>model</code>               (<code>SupportedModels | str</code>, default:                   <code>'ddm'</code> )           \u2013            <p>The name of the model to use. Currently supported models are \"ddm\", \"ddm_sdv\", \"full_ddm\", \"angle\", \"levy\", \"ornstein\", \"weibull\", \"race_no_bias_angle_4\", \"ddm_seq2_no_bias\". If any other string is passed, the model will be considered custom, in which case all <code>model_config</code>, <code>loglik</code>, and <code>loglik_kind</code> have to be provided by the user.</p> </li> <li> <code>choices</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>When an <code>int</code>, the number of choices that the participants can make. If <code>2</code>, the choices are [-1, 1] by default. If anything greater than <code>2</code>, the choices are [0, 1, ..., n_choices - 1] by default. If a <code>list</code> is provided, it should be the list of choices that the participants can make. Defaults to <code>2</code>. If any value other than the choices provided is found in the \"response\" column of the data, an error will be raised.</p> </li> <li> <code>include</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>A list of dictionaries specifying parameter specifications to include in the model. If left unspecified, defaults will be used for all parameter specifications. Defaults to None.</p> </li> <li> <code>model_config</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary containing the model configuration information. If None is provided, defaults will be used if there are any. Defaults to None. Fields for this <code>dict</code> are usually:</p> <ul> <li><code>\"list_params\"</code>: a list of parameters indicating the parameters of the model.     The order in which the parameters are specified in this list is important.     Values for each parameter will be passed to the likelihood function in this     order.</li> <li><code>\"backend\"</code>: Only used when <code>loglik_kind</code> is <code>approx_differentiable</code> and     an onnx file is supplied for the likelihood approximation network (LAN).     Valid values are <code>\"jax\"</code> or <code>\"pytensor\"</code>. It determines whether the LAN in     ONNX should be converted to <code>\"jax\"</code> or <code>\"pytensor\"</code>. If not provided,     <code>jax</code> will be used for maximum performance.</li> <li><code>\"default_priors\"</code>: A <code>dict</code> indicating the default priors for each parameter.</li> <li><code>\"bounds\"</code>: A <code>dict</code> indicating the boundaries for each parameter. In the case     of LAN, these bounds are training boundaries.</li> <li><code>\"rv\"</code>: Optional. Can be a <code>RandomVariable</code> class containing the user's own     <code>rng_fn</code> function for sampling from the distribution that the user is     supplying. If not supplied, HSSM will automatically generate a     <code>RandomVariable</code> using the simulator identified by <code>model</code> from the     <code>ssm_simulators</code> package. If <code>model</code> is not supported in <code>ssm_simulators</code>,     a warning will be raised letting the user know that sampling from the     <code>RandomVariable</code> will result in errors.</li> <li><code>\"extra_fields\"</code>: Optional. A list of strings indicating the additional     columns in <code>data</code> that will be passed to the likelihood function for     calculation. This is helpful if the likelihood function depends on data     other than the observed data and the parameter values.</li> </ul> </li> <li> <code>loglik</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>A likelihood function. Defaults to None. Requirements are:</p> <ol> <li>if <code>loglik_kind</code> is <code>\"analytical\"</code> or <code>\"blackbox\"</code>, a pm.Distribution, a    pytensor Op, or a Python callable can be used. Signatures are:<ul> <li><code>pm.Distribution</code>: needs to have parameters specified exactly as listed in <code>list_params</code></li> <li><code>pytensor.graph.Op</code> and <code>Callable</code>: needs to accept the parameters specified exactly as listed in <code>list_params</code></li> </ul> </li> <li>If <code>loglik_kind</code> is <code>\"approx_differentiable\"</code>, then in addition to the     specifications above, a <code>str</code> or <code>Pathlike</code> can also be used to specify a     path to an <code>onnx</code> file. If a <code>str</code> is provided, HSSM will first look locally     for an <code>onnx</code> file. If that is not successful, HSSM will try to download     that <code>onnx</code> file from Hugging Face hub.</li> <li>It can also be <code>None</code>, in which case a default likelihood function will be     used</li> </ol> </li> <li> <code>loglik_kind</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>A string that specifies the kind of log-likelihood function specified with <code>loglik</code>. Defaults to <code>None</code>. Can be one of the following:</p> <ul> <li><code>\"analytical\"</code>: an analytical (approximation) likelihood function. It is     differentiable and can be used with samplers that requires differentiation.</li> <li><code>\"approx_differentiable\"</code>: a likelihood approximation network (LAN) likelihood     function. It is differentiable and can be used with samplers that requires     differentiation.</li> <li><code>\"blackbox\"</code>: a black box likelihood function. It is typically NOT     differentiable.</li> <li><code>None</code>, in which a default will be used. For <code>ddm</code> type of models, the default     will be <code>analytical</code>. For other models supported, it will be     <code>approx_differentiable</code>. If the model is a custom one, a ValueError     will be raised.</li> </ul> </li> <li> <code>p_outlier</code>               (<code>optional</code>, default:                   <code>0.05</code> )           \u2013            <p>The fixed lapse probability or the prior distribution of the lapse probability. Defaults to a fixed value of 0.05. When <code>None</code>, the lapse probability will not be included in estimation.</p> </li> <li> <code>lapse</code>               (<code>optional</code>, default:                   <code>Prior('Uniform', lower=0.0, upper=20.0)</code> )           \u2013            <p>The lapse distribution. This argument is required only if <code>p_outlier</code> is not <code>None</code>. Defaults to Uniform(0.0, 10.0).</p> </li> <li> <code>global_formula</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>A string that specifies a regressions formula which will be used for all model parameters. If you specify parameter-wise regressions in addition, these will override the global regression for the respective parameter.</p> </li> <li> <code>link_settings</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>An optional string literal that indicates the link functions to use for each parameter. Helpful for hierarchical models where sampling might get stuck/ very slow. Can be one of the following:</p> <ul> <li><code>\"log_logit\"</code>: applies log link functions to positive parameters and generalized logit link functions to parameters that have explicit bounds.</li> <li><code>None</code>: unless otherwise specified, the <code>\"identity\"</code> link functions will be used. The default value is <code>None</code>.</li> </ul> </li> <li> <code>prior_settings</code>               (<code>optional</code>, default:                   <code>'safe'</code> )           \u2013            <p>An optional string literal that indicates the prior distributions to use for each parameter. Helpful for hierarchical models where sampling might get stuck/ very slow. Can be one of the following:</p> <ul> <li><code>\"safe\"</code>: HSSM will scan all parameters in the model and apply safe priors to all parameters that do not have explicit bounds.</li> <li>None: HSSM will use bambi to provide default priors for all parameters. Not recommended when you are using hierarchical models. The default value is <code>\"safe\"</code>.</li> </ul> </li> <li> <code>extra_namespace</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Additional user supplied variables with transformations or data to include in the environment where the formula is evaluated. Defaults to <code>None</code>.</p> </li> <li> <code>missing_data</code>               (<code>optional</code>, default:                   <code>False</code> )           \u2013            <p>Specifies whether the model should handle missing data. Can be a <code>bool</code> or a <code>float</code>. If <code>False</code>, and if the <code>rt</code> column contains in the data -999.0, the model will drop these rows and produce a warning. If <code>True</code>, the model will treat code -999.0 as missing data. If a <code>float</code> is provided, the model will treat this value as the missing data value. Defaults to <code>False</code>.</p> </li> <li> <code>deadline</code>               (<code>optional</code>, default:                   <code>False</code> )           \u2013            <p>Specifies whether the model should handle deadline data. Can be a <code>bool</code> or a <code>str</code>. If <code>False</code>, the model will not do nothing even if a deadline column is provided. If <code>True</code>, the model will treat the <code>deadline</code> column as deadline data. If a <code>str</code> is provided, the model will treat this value as the name of the deadline column. Defaults to <code>False</code>.</p> </li> <li> <code>loglik_missing_data</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>A likelihood function for missing data. Please see the <code>loglik</code> parameter to see how to specify the likelihood function this parameter. If nothing is provided, a default likelihood function will be used. This parameter is required only if either <code>missing_data</code> or <code>deadline</code> is not <code>False</code>. Defaults to <code>None</code>.</p> </li> <li> <code>process_initvals</code>               (<code>optional</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, the model will process the initial values. Defaults to <code>True</code>.</p> </li> <li> <code>initval_jitter</code>               (<code>optional</code>, default:                   <code>INITVAL_JITTER_SETTINGS['jitter_epsilon']</code> )           \u2013            <p>The jitter value for the initial values. Defaults to <code>0.01</code>.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to the <code>bmb.Model</code> object.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>sample</code>             \u2013              <p>Perform sampling using the <code>fit</code> method via bambi.Model.</p> </li> <li> <code>sample_posterior_predictive</code>             \u2013              <p>Perform posterior predictive sampling from the HSSM model.</p> </li> <li> <code>sample_prior_predictive</code>             \u2013              <p>Generate samples from the prior predictive distribution.</p> </li> <li> <code>vi</code>             \u2013              <p>Perform Variational Inference.</p> </li> <li> <code>find_MAP</code>             \u2013              <p>Perform Maximum A Posteriori estimation.</p> </li> <li> <code>log_likelihood</code>             \u2013              <p>Compute the log likelihood of the model.</p> </li> <li> <code>summary</code>             \u2013              <p>Produce a summary table with ArviZ but with additional convenience features.</p> </li> <li> <code>plot_trace</code>             \u2013              <p>Generate trace plot with ArviZ but with additional convenience features.</p> </li> <li> <code>graph</code>             \u2013              <p>Produce a graphviz Digraph from a built HSSM model.</p> </li> <li> <code>plot_predictive</code>             \u2013              <p>Produce a posterior predictive plot.</p> </li> <li> <code>plot_quantile_probability</code>             \u2013              <p>Produce a quantile probability plot.</p> </li> <li> <code>restore_traces</code>             \u2013              <p>Restore traces from an InferenceData object or a .netcdf file.</p> </li> <li> <code>initial_point</code>             \u2013              <p>Compute the initial point of the model.</p> </li> </ul>"},{"location":"api/hssm/#hssm.HSSM.traces","title":"hssm.HSSM.traces  <code>property</code>","text":"<pre><code>traces: InferenceData | Approximation\n</code></pre> <p>Return the trace of the model after sampling.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the model has not been sampled yet.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>InferenceData | Approximation</code>           \u2013            <p>The trace of the model after the last call to <code>sample()</code>.</p> </li> </ul>"},{"location":"api/hssm/#hssm.HSSM.pymc_model","title":"hssm.HSSM.pymc_model  <code>property</code>","text":"<pre><code>pymc_model: Model\n</code></pre> <p>Provide access to the PyMC model.</p> <p>Returns:</p> <ul> <li> <code>Model</code>           \u2013            <p>The PyMC model built by bambi</p> </li> </ul>"},{"location":"api/hssm/#hssm.HSSM.sample","title":"hssm.HSSM.sample","text":"<pre><code>sample(\n    sampler: Literal[\"mcmc\", \"nuts_numpyro\", \"nuts_blackjax\", \"laplace\", \"vi\"]\n    | None = None,\n    init: str | None = None,\n    initvals: str | dict | None = None,\n    include_response_params: bool = False,\n    **kwargs,\n) -&gt; az.InferenceData | pm.Approximation\n</code></pre> <p>Perform sampling using the <code>fit</code> method via bambi.Model.</p> <p>Parameters:</p> <ul> <li> <code>sampler</code>               (<code>Literal['mcmc', 'nuts_numpyro', 'nuts_blackjax', 'laplace', 'vi'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The sampler to use. Can be one of \"mcmc\", \"nuts_numpyro\", \"nuts_blackjax\", \"laplace\", or \"vi\". If using <code>blackbox</code> likelihoods, this cannot be \"nuts_numpyro\" or \"nuts_blackjax\". By default it is None, and sampler will automatically be chosen: when the model uses the <code>approx_differentiable</code> likelihood, and <code>jax</code> backend, \"nuts_numpyro\" will be used. Otherwise, \"mcmc\" (the default PyMC NUTS sampler) will be used.</p> </li> <li> <code>init</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Initialization method to use for the sampler. If any of the NUTS samplers is used, defaults to <code>\"adapt_diag\"</code>. Otherwise, defaults to <code>\"auto\"</code>.</p> </li> <li> <code>initvals</code>               (<code>str | dict | None</code>, default:                   <code>None</code> )           \u2013            <p>Pass initial values to the sampler. This can be a dictionary of initial values for parameters of the model, or a string \"map\" to use initialization at the MAP estimate. If \"map\" is used, the MAP estimate will be computed if not already attached to the base class from prior call to 'find_MAP`.</p> </li> <li> <code>include_response_params</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Include parameters of the response distribution in the output. These usually take more space than other parameters as there's one of them per observation. Defaults to False.</p> </li> <li> <code>kwargs</code>           \u2013            <p>Other arguments passed to bmb.Model.fit(). Please see [here] (https://bambinos.github.io/bambi/api_reference.html#bambi.models.Model.fit) for full documentation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>InferenceData | Approximation</code>           \u2013            <p>A reference to the <code>model.traces</code> object, which stores the traces of the last call to <code>model.sample()</code>. <code>model.traces</code> is an ArviZ <code>InferenceData</code> instance if <code>sampler</code> is <code>\"mcmc\"</code> (default), <code>\"nuts_numpyro\"</code>, <code>\"nuts_blackjax\"</code> or \"<code>laplace\"</code>, or an <code>Approximation</code> object if <code>\"vi\"</code>.</p> </li> </ul>"},{"location":"api/hssm/#hssm.HSSM.sample_posterior_predictive","title":"hssm.HSSM.sample_posterior_predictive","text":"<pre><code>sample_posterior_predictive(\n    idata: InferenceData | None = None,\n    data: DataFrame | None = None,\n    inplace: bool = True,\n    include_group_specific: bool = True,\n    kind: Literal[\"response\", \"response_params\"] = \"response\",\n    draws: int | float | list[int] | ndarray | None = None,\n    safe_mode: bool = True,\n) -&gt; az.InferenceData | None\n</code></pre> <p>Perform posterior predictive sampling from the HSSM model.</p> <p>Parameters:</p> <ul> <li> <code>idata</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>The <code>InferenceData</code> object returned by <code>HSSM.sample()</code>. If not provided, the <code>InferenceData</code> from the last time <code>sample()</code> is called will be used.</p> </li> <li> <code>data</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>An optional data frame with values for the predictors that are used to obtain out-of-sample predictions. If omitted, the original dataset is used.</p> </li> <li> <code>inplace</code>               (<code>optional</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code> will modify idata in-place and append a <code>posterior_predictive</code> group to <code>idata</code>. Otherwise, it will return a copy of idata with the predictions added, by default True.</p> </li> <li> <code>include_group_specific</code>               (<code>optional</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code> will make predictions including the group specific effects. Otherwise, predictions are made with common effects only (i.e. group- specific are set to zero), by default True.</p> </li> <li> <code>kind</code>               (<code>Literal['response', 'response_params']</code>, default:                   <code>'response'</code> )           \u2013            <p>Indicates the type of prediction required. Can be <code>\"response_params\"</code> or <code>\"response\"</code>. The first returns draws from the posterior distribution of the likelihood parameters, while the latter returns the draws from the posterior predictive distribution (i.e. the posterior probability distribution for a new observation) in addition to the posterior distribution. Defaults to \"response_params\".</p> </li> <li> <code>draws</code>               (<code>int | float | list[int] | ndarray | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of samples to draw from the posterior predictive distribution from each chain. When it's an integer &gt;= 1, the number of samples to be extracted from the <code>draw</code> dimension. If this integer is larger than the number of posterior samples in each chain, all posterior samples will be used in posterior predictive sampling. When a float between 0 and 1, the proportion of samples from the draw dimension from each chain to be used in posterior predictive sampling.. If this proportion is very small, at least one sample will be used. When None, all posterior samples will be used. Defaults to None.</p> </li> <li> <code>safe_mode</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, the function will split the draws into chunks of 10 to avoid memory issues. Defaults to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the model has not been sampled yet and idata is not provided.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>InferenceData | None</code>           \u2013            <p>InferenceData or None</p> </li> </ul>"},{"location":"api/hssm/#hssm.HSSM.sample_prior_predictive","title":"hssm.HSSM.sample_prior_predictive","text":"<pre><code>sample_prior_predictive(\n    draws: int = 500,\n    var_names: str | list[str] | None = None,\n    omit_offsets: bool = True,\n    random_seed: Generator | None = None,\n) -&gt; az.InferenceData\n</code></pre> <p>Generate samples from the prior predictive distribution.</p> <p>Parameters:</p> <ul> <li> <code>draws</code>               (<code>int</code>, default:                   <code>500</code> )           \u2013            <p>Number of draws to sample from the prior predictive distribution. Defaults to 500.</p> </li> <li> <code>var_names</code>               (<code>str | list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of names of variables for which to compute the prior predictive distribution. Defaults to <code>None</code> which means both observed and unobserved RVs.</p> </li> <li> <code>omit_offsets</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to omit offset terms. Defaults to <code>True</code>.</p> </li> <li> <code>random_seed</code>               (<code>Generator | None</code>, default:                   <code>None</code> )           \u2013            <p>Seed for the random number generator.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>InferenceData</code>           \u2013            <p><code>InferenceData</code> object with the groups <code>prior</code>, <code>prior_predictive</code> and <code>observed_data</code>.</p> </li> </ul>"},{"location":"api/hssm/#hssm.HSSM.vi","title":"hssm.HSSM.vi","text":"<pre><code>vi(\n    method: str = \"advi\",\n    niter: int = 10000,\n    draws: int = 1000,\n    return_idata: bool = True,\n    ignore_mcmc_start_point_defaults=False,\n    **vi_kwargs,\n) -&gt; pm.Approximation | az.InferenceData\n</code></pre> <p>Perform Variational Inference.</p> <p>Parameters:</p> <ul> <li> <code>niter</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The number of iterations to run the VI algorithm. Defaults to 3000.</p> </li> <li> <code>method</code>               (<code>str</code>, default:                   <code>'advi'</code> )           \u2013            <p>The method to use for VI. Can be one of \"advi\" or \"fullrank_advi\", \"svgd\", \"asvgd\".Defaults to \"advi\".</p> </li> <li> <code>draws</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>The number of samples to draw from the posterior distribution. Defaults to 1000.</p> </li> <li> <code>return_idata</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, returns an InferenceData object. Otherwise, returns the approximation object directly. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>    pm.Approximation or az.InferenceData: The mean field approximation object.</code>           \u2013            </li> </ul>"},{"location":"api/hssm/#hssm.HSSM.find_MAP","title":"hssm.HSSM.find_MAP","text":"<pre><code>find_MAP(**kwargs)\n</code></pre> <p>Perform Maximum A Posteriori estimation.</p> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>A dictionary containing the MAP estimates of the model parameters.</p> </li> </ul>"},{"location":"api/hssm/#hssm.HSSM.log_likelihood","title":"hssm.HSSM.log_likelihood","text":"<pre><code>log_likelihood(\n    idata: InferenceData | None = None,\n    data: DataFrame | None = None,\n    inplace: bool = True,\n    keep_likelihood_params: bool = False,\n) -&gt; az.InferenceData | None\n</code></pre> <p>Compute the log likelihood of the model.</p> <p>Parameters:</p> <ul> <li> <code>idata</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>The <code>InferenceData</code> object returned by <code>HSSM.sample()</code>. If not provided,</p> </li> <li> <code>data</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>A pandas DataFrame with values for the predictors that are used to obtain out-of-sample predictions. If omitted, the original dataset is used.</p> </li> <li> <code>inplace</code>               (<code>optional</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code> will modify idata in-place and append a <code>log_likelihood</code> group to <code>idata</code>. Otherwise, it will return a copy of idata with the predictions added, by default True.</p> </li> <li> <code>keep_likelihood_params</code>               (<code>optional</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, the trial wise likelihood parameters that are computed on route to getting the log likelihood are kept in the <code>idata</code> object. Defaults to False. See also the method <code>add_likelihood_parameters_to_idata</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>InferenceData | None</code>           \u2013            <p>InferenceData or None</p> </li> </ul>"},{"location":"api/hssm/#hssm.HSSM.summary","title":"hssm.HSSM.summary","text":"<pre><code>summary(\n    data: InferenceData | None = None,\n    include_deterministic: bool = False,\n    **kwargs,\n) -&gt; pd.DataFrame | xr.Dataset\n</code></pre> <p>Produce a summary table with ArviZ but with additional convenience features.</p> <p>This is a simple wrapper for the az.summary() function. By default, it filters out the deterministic values from the plot. Please see the [arviz documentation] (https://arviz-devs.github.io/arviz/api/generated/arviz.summary.html) for additional parameters that can be specified.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>InferenceData | None</code>, default:                   <code>None</code> )           \u2013            <p>An ArviZ InferenceData object. If None, the traces stored in the model will be used.</p> </li> <li> <code>include_deterministic</code>               (<code>optional</code>, default:                   <code>False</code> )           \u2013            <p>Whether to include deterministic variables in the plot. Defaults to False. Note that if include_deterministic is set to False and and <code>var_names</code> is provided, the <code>var_names</code> provided will be modified to also exclude the deterministic values. If this is not desirable, set <code>include_deterministic</code> to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame | Dataset</code>           \u2013            <p>A pandas DataFrame or xarray Dataset containing the summary statistics.</p> </li> </ul>"},{"location":"api/hssm/#hssm.HSSM.plot_trace","title":"hssm.HSSM.plot_trace","text":"<pre><code>plot_trace(\n    data: InferenceData | None = None,\n    include_deterministic: bool = False,\n    tight_layout: bool = True,\n    **kwargs,\n) -&gt; None\n</code></pre> <p>Generate trace plot with ArviZ but with additional convenience features.</p> <p>This is a simple wrapper for the az.plot_trace() function. By default, it filters out the deterministic values from the plot. Please see the [arviz documentation] (https://arviz-devs.github.io/arviz/api/generated/arviz.plot_trace.html) for additional parameters that can be specified.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>An ArviZ InferenceData object. If None, the traces stored in the model will be used.</p> </li> <li> <code>include_deterministic</code>               (<code>optional</code>, default:                   <code>False</code> )           \u2013            <p>Whether to include deterministic variables in the plot. Defaults to False. Note that if include deterministic is set to False and and <code>var_names</code> is provided, the <code>var_names</code> provided will be modified to also exclude the deterministic values. If this is not desirable, set <code>include deterministic</code> to True.</p> </li> <li> <code>tight_layout</code>               (<code>optional</code>, default:                   <code>True</code> )           \u2013            <p>Whether to call plt.tight_layout() after plotting. Defaults to True.</p> </li> </ul>"},{"location":"api/hssm/#hssm.HSSM.graph","title":"hssm.HSSM.graph","text":"<pre><code>graph(formatting='plain', name=None, figsize=None, dpi=300, fmt='png')\n</code></pre> <p>Produce a graphviz Digraph from a built HSSM model.</p> <p>Requires graphviz, which may be installed most easily with <code>conda install -c conda-forge python-graphviz</code>. Alternatively, you may install the <code>graphviz</code> binaries yourself, and then <code>pip install graphviz</code> to get the python bindings. See http://graphviz.readthedocs.io/en/stable/manual.html for more information.</p> <p>Parameters:</p> <ul> <li> <code>formatting</code>           \u2013            <p>One of <code>\"plain\"</code> or <code>\"plain_with_params\"</code>. Defaults to <code>\"plain\"</code>.</p> </li> <li> <code>name</code>           \u2013            <p>Name of the figure to save. Defaults to <code>None</code>, no figure is saved.</p> </li> <li> <code>figsize</code>           \u2013            <p>Maximum width and height of figure in inches. Defaults to <code>None</code>, the figure size is set automatically. If defined and the drawing is larger than the given size, the drawing is uniformly scaled down so that it fits within the given size.  Only works if <code>name</code> is not <code>None</code>.</p> </li> <li> <code>dpi</code>           \u2013            <p>Point per inch of the figure to save. Defaults to 300. Only works if <code>name</code> is not <code>None</code>.</p> </li> <li> <code>fmt</code>           \u2013            <p>Format of the figure to save. Defaults to <code>\"png\"</code>. Only works if <code>name</code> is not <code>None</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Graph</code>           \u2013            <p>The graph</p> </li> </ul>"},{"location":"api/hssm/#hssm.HSSM.plot_predictive","title":"hssm.HSSM.plot_predictive","text":"<pre><code>plot_predictive(**kwargs) -&gt; mpl.axes.Axes | sns.FacetGrid\n</code></pre> <p>Produce a posterior predictive plot.</p> <p>Equivalent to calling <code>hssm.plotting.plot_predictive()</code> with the model. Please see that function for full documentation.</p> <p>Returns:</p> <ul> <li> <code>Axes | FacetGrid</code>           \u2013            <p>The matplotlib axis or seaborn FacetGrid object containing the plot.</p> </li> </ul>"},{"location":"api/hssm/#hssm.HSSM.plot_quantile_probability","title":"hssm.HSSM.plot_quantile_probability","text":"<pre><code>plot_quantile_probability(**kwargs) -&gt; mpl.axes.Axes | sns.FacetGrid\n</code></pre> <p>Produce a quantile probability plot.</p> <p>Equivalent to calling <code>hssm.plotting.plot_quantile_probability()</code> with the model. Please see that function for full documentation.</p> <p>Returns:</p> <ul> <li> <code>Axes | FacetGrid</code>           \u2013            <p>The matplotlib axis or seaborn FacetGrid object containing the plot.</p> </li> </ul>"},{"location":"api/hssm/#hssm.HSSM.restore_traces","title":"hssm.HSSM.restore_traces","text":"<pre><code>restore_traces(traces: InferenceData | Approximation | str | PathLike) -&gt; None\n</code></pre> <p>Restore traces from an InferenceData object or a .netcdf file.</p> <p>Parameters:</p> <ul> <li> <code>traces</code>               (<code>InferenceData | Approximation | str | PathLike</code>)           \u2013            <p>An InferenceData object or a path to a file containing the traces.</p> </li> </ul>"},{"location":"api/hssm/#hssm.HSSM.initial_point","title":"hssm.HSSM.initial_point","text":"<pre><code>initial_point(transformed: bool = False) -&gt; dict[str, np.ndarray]\n</code></pre> <p>Compute the initial point of the model.</p> <p>This is a slightly altered version of pm.initial_point.initial_point().</p> <p>Parameters:</p> <ul> <li> <code>transformed</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, return the initial point in transformed space.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>A dictionary containing the initial point of the model parameters.</p> </li> </ul>"},{"location":"api/likelihoods/","title":"hssm.likelihoods","text":"<p>The <code>hssm.likelihoods</code> submodule exports a few likelihood functions. These functions are already used in the model building process for some supported models, such as <code>ddm</code>, <code>ddm_sdv</code>, and <code>full_ddm</code>, so you typically would not have to deal with them. However, they can be helpful if you want to use them to build a model yourself in PyMC. Please checkout the this tutorial for more details.</p>"},{"location":"api/likelihoods/#hssm.likelihoods","title":"hssm.likelihoods","text":"<p>Likelihood functions and distributions that use them.</p> <p>Modules:</p> <ul> <li> <code>analytical</code>           \u2013            <p>pytensor implementation of the Wiener First Passage Time Distribution.</p> </li> <li> <code>blackbox</code>           \u2013            <p>Black box likelihoods written in Cython for \"ddm\" and \"ddm_sdv\" models.</p> </li> <li> <code>rldm</code>           \u2013            <p>The log-likelihood function for the RLDM model.</p> </li> <li> <code>rldm_optimized</code>           \u2013            <p>The log-likelihood function for the RLDM model.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>logp_ddm</code>             \u2013              <p>Compute analytical likelihood for the DDM model with <code>sv</code>.</p> </li> <li> <code>logp_ddm_bbox</code>             \u2013              <p>Compute blackbox log-likelihoods for ddm models.</p> </li> <li> <code>logp_ddm_sdv</code>             \u2013              <p>Compute the log-likelihood of the drift diffusion model f(t|v,a,z).</p> </li> <li> <code>logp_ddm_sdv_bbox</code>             \u2013              <p>Compute blackbox log-likelihoods for ddm_sdv models.</p> </li> <li> <code>logp_full_ddm</code>             \u2013              <p>Compute blackbox log-likelihoods for full_ddm models.</p> </li> </ul>"},{"location":"api/likelihoods/#hssm.likelihoods.logp_ddm","title":"hssm.likelihoods.logp_ddm","text":"<pre><code>logp_ddm(\n    data: ndarray,\n    v: float,\n    a: float,\n    z: float,\n    t: float,\n    err: float = 1e-15,\n    k_terms: int = 20,\n    epsilon: float = 1e-15,\n) -&gt; np.ndarray\n</code></pre> <p>Compute analytical likelihood for the DDM model with <code>sv</code>.</p> <p>Computes the log-likelihood of the drift diffusion model f(t|v,a,z) using the method and implementation of Navarro &amp; Fuss, 2009.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>data: 2-column numpy array of (response time, response)</p> </li> <li> <code>v</code>               (<code>float</code>)           \u2013            <p>Mean drift rate. (-inf, inf).</p> </li> <li> <code>a</code>               (<code>float</code>)           \u2013            <p>Value of decision upper bound. (0, inf).</p> </li> <li> <code>z</code>               (<code>float</code>)           \u2013            <p>Normalized decision starting point. (0, 1).</p> </li> <li> <code>t</code>               (<code>float</code>)           \u2013            <p>Non-decision time [0, inf).</p> </li> <li> <code>err</code>               (<code>float</code>, default:                   <code>1e-15</code> )           \u2013            <p>Error bound.</p> </li> <li> <code>k_terms</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>number of terms to use to approximate the PDF.</p> </li> <li> <code>epsilon</code>               (<code>float</code>, default:                   <code>1e-15</code> )           \u2013            <p>A small positive number to prevent division by zero or taking the log of zero.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The analytical likelihoods for DDM.</p> </li> </ul>"},{"location":"api/likelihoods/#hssm.likelihoods.logp_ddm_bbox","title":"hssm.likelihoods.logp_ddm_bbox","text":"<pre><code>logp_ddm_bbox(data: ndarray, v, a, z, t) -&gt; np.ndarray\n</code></pre> <p>Compute blackbox log-likelihoods for ddm models.</p>"},{"location":"api/likelihoods/#hssm.likelihoods.logp_ddm_sdv","title":"hssm.likelihoods.logp_ddm_sdv","text":"<pre><code>logp_ddm_sdv(\n    data: ndarray,\n    v: float,\n    a: float,\n    z: float,\n    t: float,\n    sv: float,\n    err: float = 1e-15,\n    k_terms: int = 20,\n    epsilon: float = 1e-15,\n) -&gt; np.ndarray\n</code></pre> <p>Compute the log-likelihood of the drift diffusion model f(t|v,a,z).</p> <p>Using the method and implementation of Navarro &amp; Fuss, 2009.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>2-column numpy array of (response time, response)</p> </li> <li> <code>v</code>               (<code>float</code>)           \u2013            <p>Mean drift rate. (-inf, inf).</p> </li> <li> <code>a</code>               (<code>float</code>)           \u2013            <p>Value of decision upper bound. (0, inf).</p> </li> <li> <code>z</code>               (<code>float</code>)           \u2013            <p>Normalized decision starting point. (0, 1).</p> </li> <li> <code>t</code>               (<code>float</code>)           \u2013            <p>Non-decision time [0, inf).</p> </li> <li> <code>sv</code>               (<code>float</code>)           \u2013            <p>Standard deviation of the drift rate [0, inf).</p> </li> <li> <code>err</code>               (<code>float</code>, default:                   <code>1e-15</code> )           \u2013            <p>Error bound.</p> </li> <li> <code>k_terms</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>number of terms to use to approximate the PDF.</p> </li> <li> <code>epsilon</code>               (<code>float</code>, default:                   <code>1e-15</code> )           \u2013            <p>A small positive number to prevent division by zero or taking the log of zero.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The log likelihood of the drift diffusion model with the standard deviation of sv.</p> </li> </ul>"},{"location":"api/likelihoods/#hssm.likelihoods.logp_ddm_sdv_bbox","title":"hssm.likelihoods.logp_ddm_sdv_bbox","text":"<pre><code>logp_ddm_sdv_bbox(data: ndarray, v, a, z, t, sv) -&gt; np.ndarray\n</code></pre> <p>Compute blackbox log-likelihoods for ddm_sdv models.</p>"},{"location":"api/likelihoods/#hssm.likelihoods.logp_full_ddm","title":"hssm.likelihoods.logp_full_ddm","text":"<pre><code>logp_full_ddm(data: ndarray, v, a, z, t, sz, sv, st)\n</code></pre> <p>Compute blackbox log-likelihoods for full_ddm models.</p>"},{"location":"api/link/","title":"hssm.Link","text":""},{"location":"api/link/#hssm.Link","title":"hssm.Link","text":"<pre><code>Link(\n    name,\n    link=None,\n    linkinv=None,\n    linkinv_backend=None,\n    bounds: tuple[float, float] | None = None,\n)\n</code></pre> <p>               Bases: <code>Link</code></p> <p>Representation of a generalized link function.</p> <p>This object contains two main functions. One is the link function itself, the function that maps values in the response scale to the linear predictor, and the other is the inverse of the link function, that maps values of the linear predictor to the response scale.</p> <p>The great majority of users will never interact with this class unless they want to create a custom <code>Family</code> with a custom <code>Link</code>. This is automatically handled for all the built-in families.</p> <p>Parameters:</p> <ul> <li> <code>name</code>           \u2013            <p>The name of the link function. If it is a known name, it's not necessary to pass any other arguments because functions are already defined internally. If not known, all of `link<code>,</code>linkinv<code>and</code>linkinv_backend`` must be specified.</p> </li> <li> <code>link</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>A function that maps the response to the linear predictor. Known as the :math:<code>g</code> function in GLM jargon. Does not need to be specified when <code>name</code> is a known name.</p> </li> <li> <code>linkinv</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>A function that maps the linear predictor to the response. Known as the :math:<code>g^{-1}</code> function in GLM jargon. Does not need to be specified when <code>name</code> is a known name.</p> </li> <li> <code>linkinv_backend</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Same than <code>linkinv</code> but must be something that works with PyMC backend (i.e. it must work with PyTensor tensors). Does not need to be specified when <code>name</code> is a known name.</p> </li> <li> <code>bounds</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Bounds of the response scale. Only needed when <code>name</code> is <code>gen_logit</code>.</p> </li> </ul>"},{"location":"api/load_data/","title":"hssm.load_data","text":""},{"location":"api/load_data/#hssm.load_data","title":"hssm.load_data","text":"<pre><code>load_data(dataset: Optional[str] = None) -&gt; pd.DataFrame | str\n</code></pre> <p>Load a dataset as a pandas DataFrame.</p> <p>If a valid dataset name is provided, this function will return the corresponding DataFrame. Otherwise, it lists the available datasets.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Name of the dataset to load. If not provided, a list of available datasets is returned.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the provided dataset name does not match any of the available datasets.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame or str</code>           \u2013            <p>Loaded dataset as a DataFrame if a valid dataset name was provided, otherwise a string listing the available datasets.</p> </li> </ul>"},{"location":"api/model_config/","title":"hssm.ModelConfig","text":""},{"location":"api/model_config/#hssm.ModelConfig","title":"hssm.ModelConfig  <code>dataclass</code>","text":"<pre><code>ModelConfig(\n    response: list[str] | None = None,\n    list_params: list[str] | None = None,\n    choices: list[int] | None = None,\n    default_priors: dict[str, ParamSpec] = dict(),\n    bounds: dict[str, tuple[float, float]] = dict(),\n    backend: Literal[\"jax\", \"pytensor\"] | None = None,\n    rv: RandomVariable | None = None,\n    extra_fields: list[str] | None = None,\n)\n</code></pre> <p>Representation for model_config provided by the user.</p>"},{"location":"api/param/","title":"hssm.Param","text":""},{"location":"api/param/#hssm.Param","title":"hssm.Param  <code>dataclass</code>","text":"<pre><code>Param(\n    name: str | None = None,\n    prior: float | ndarray | dict[str, Any] | Prior | None = None,\n    formula: str | None = None,\n    link: str | Link | None = None,\n    bounds: tuple[float, float] | None = None,\n)\n</code></pre> <p>Represent the user-provided specifications for the main HSSM class.</p> <p>Also provides convenience functions that can be used by the HSSM class to parse arguments.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>The name of the parameter. This can be omitted if the Param is specified as kwargs in the HSSM class.</p> </li> <li> <code>prior</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>If a formula is not specified (the non-regression case), this parameter expects a float value if the parameter is fixed or a dictionary that can be parsed by Bambi as a prior specification or a Bambi Prior object. If a formula is specified (the regression case), this parameter expects a dictionary of param:prior, where param is the name of the response variable specified in formula, and prior is specified as above. If left unspecified, default priors created by Bambi will be used.</p> </li> <li> <code>formula</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>The regression formula if the parameter depends on other variables.</p> </li> <li> <code>link</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>The link function for the regression. It is either a string that specifies a built-in link function in Bambi, or a Bambi Link object. If a regression is specified and link is not specified, \"identity\" will be used by default.</p> </li> <li> <code>bounds</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>If provided, the prior will be created with boundary checks. If this parameter is specified as a regression, boundary checks will be skipped at this point.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>from_dict</code>             \u2013              <p>Create a Param object from a dictionary.</p> </li> <li> <code>from_kwargs</code>             \u2013              <p>Create a Param object from keyword arguments.</p> </li> <li> <code>to_dict</code>             \u2013              <p>Convert the UserParam object to a dictionary with shallow copy.</p> </li> </ul>"},{"location":"api/param/#hssm.Param.is_regression","title":"hssm.Param.is_regression  <code>property</code>","text":"<pre><code>is_regression: bool\n</code></pre> <p>Check if the parameter is a regression parameter.</p>"},{"location":"api/param/#hssm.Param.is_simple","title":"hssm.Param.is_simple  <code>property</code>","text":"<pre><code>is_simple: bool\n</code></pre> <p>Check if the parameter is a simple parameter.</p>"},{"location":"api/param/#hssm.Param.from_dict","title":"hssm.Param.from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(param_dict: dict[str, Any]) -&gt; UserParam\n</code></pre> <p>Create a Param object from a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>param_dict</code>               (<code>dict[str, Any]</code>)           \u2013            <p>A dictionary with the keys \"name\", \"prior\", \"formula\", \"link\", and \"bounds\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>UserParam</code>           \u2013            <p>A Param object with the specified parameters.</p> </li> </ul>"},{"location":"api/param/#hssm.Param.from_kwargs","title":"hssm.Param.from_kwargs  <code>classmethod</code>","text":"<pre><code>from_kwargs(\n    name: str, param: Union[float, ndarray, dict[str, Any], Prior, UserParam]\n) -&gt; UserParam\n</code></pre> <p>Create a Param object from keyword arguments.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the parameter.</p> </li> <li> <code>param</code>               (<code>Union[float, ndarray, dict[str, Any], Prior, UserParam]</code>)           \u2013            <p>The prior specification for the parameter.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>UserParam</code>           \u2013            <p>A Param object with the specified parameters.</p> </li> </ul>"},{"location":"api/param/#hssm.Param.to_dict","title":"hssm.Param.to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert the UserParam object to a dictionary with shallow copy.</p>"},{"location":"api/plotting/","title":"hssm.plotting","text":"<p>The <code>hssm.plotting</code> module provide functionalities to create HSSM-specific plots such as the posterior predictive plots. Please checkout the plotting tutorial for more examples on how to use these functions. Note that each plotting function has an equivalent in the <code>hssm.HSSM</code> class, so you can call these functions from a built model without having to import these functions.</p>"},{"location":"api/plotting/#hssm.plotting","title":"hssm.plotting","text":"<p>Plotting functionalities for HSSM.</p> <p>Modules:</p> <ul> <li> <code>model_cartoon</code>           \u2013            <p>Plotting functionalities for HSSM.</p> </li> <li> <code>predictive</code>           \u2013            <p>Plotting functionalities for HSSM.</p> </li> <li> <code>quantile_probability</code>           \u2013            <p>Code for producing quantile probability plots.</p> </li> <li> <code>utils</code>           \u2013            <p>Plotting utilities for HSSM.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>plot_model_cartoon</code>             \u2013              <p>Plot the posterior predictive distribution against the observed data.</p> </li> <li> <code>plot_predictive</code>             \u2013              <p>Plot the posterior predictive distribution against the observed data.</p> </li> <li> <code>plot_quantile_probability</code>             \u2013              <p>Plot the quantile probabilities against the observed data.</p> </li> </ul>"},{"location":"api/plotting/#hssm.plotting.plot_model_cartoon","title":"hssm.plotting.plot_model_cartoon","text":"<pre><code>plot_model_cartoon(\n    model,\n    idata: InferenceData | None = None,\n    data: DataFrame | None = None,\n    predictive_group: Literal[\n        \"posterior_predictive\", \"prior_predictive\"\n    ] = \"posterior_predictive\",\n    plot_data: bool = True,\n    n_samples: int | float | None = 20,\n    n_samples_prior: int = 500,\n    row: str | None = None,\n    col: str | None = None,\n    col_wrap: int | None = None,\n    groups: str | Iterable[str] | None = None,\n    groups_order: Iterable[str] | dict[str, Iterable[str]] | None = None,\n    bins: int | ndarray | str | None = 50,\n    step: bool = False,\n    plot_predictive_mean: bool = True,\n    plot_predictive_samples: bool = False,\n    colors: str | list[str] | None = None,\n    linestyles: str | list[str] | tuple[str] | dict[str, str] = \"-\",\n    linewidths: float | list[float] | tuple[float] | dict[str, float] = 1.25,\n    title: str | None = \"Posterior Predictive Distribution\",\n    xlabel: str | None = \"Response Time\",\n    ylabel: str | None = \"\",\n    grid_kwargs: dict | None = None,\n    **kwargs,\n) -&gt; Axes | sns.FacetGrid | list[sns.FacetGrid]\n</code></pre> <p>Plot the posterior predictive distribution against the observed data.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>HSSM</code>)           \u2013            <p>A fitted HSSM model.</p> </li> <li> <code>idata</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>The InferenceData object with posterior samples. If not provided, will use the traces object stored inside the model. If posterior predictive samples are not present in this object, will generate posterior predictive samples using the this InferenceData object and the original data.</p> </li> <li> <code>data</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>The observed data.</p> <ul> <li>If <code>data</code> is provided and the idata object does not contain a <code>\"posterior_predictive\"</code> group, will generate posterior predictive samples using covariate provided in this object. If the group does exist, it is assumed that the posterior predictive samples are generated with the covariates provided in this DataFrame.</li> <li>If <code>data</code> is not provided (i.e., <code>data=None</code>), the behavior depends on whether \"plot_data\" is true or not. If <code>plot_data=True</code>, the plotting function will use the data stored in the <code>model</code> object and proceed as the case above. If <code>plot_data=False</code>, if posterior predictive samples are not present in the <code>idata</code> object, the plotting function will generate posterior predictive samples using the data stored in the <code>model</code> object. If posterior predictive samples exist in the <code>idata</code> object, these samples will be used for plotting, but a ValueError will be thrown if any of <code>col</code> or <code>row</code> is not None.</li> </ul> </li> <li> <code>predictive_group</code>               (<code>optional</code>, default:                   <code>'posterior_predictive'</code> )           \u2013            <p>The type of predictive distribution to plot, by default \"posterior_predictive\". Can be \"posterior_predictive\" or \"prior_predictive\".</p> </li> <li> <code>plot_data</code>               (<code>optional</code>, default:                   <code>True</code> )           \u2013            <p>Whether to plot the observed data, by default True.</p> </li> <li> <code>n_samples</code>               (<code>optional</code>, default:                   <code>20</code> )           \u2013            <p>When idata is provided, the number or proportion of predictive samples randomly drawn to be used from each chain for plotting. When idata is not provided, the number or proportion of posterior/prior samples to be used to generate predictive samples. The number or proportion are defined as follows:</p> <ul> <li>When an integer &gt;= 1, the number of samples to be extracted from the draw   dimension.</li> <li>When a float between 0 and 1, the proportion of samples to be extracted from   the draw dimension.</li> <li>When None, all samples are extracted.</li> </ul> </li> <li> <code>n_samples_prior</code>               (<code>int</code>, default:                   <code>500</code> )           \u2013            <p>When predictive_group is \"prior_predictive\", the number or proportion of prior samples to be used to generate predictive samples. The number or proportion are defined as follows: - When an integer &gt;= 1, the number of samples to be drawn from the prior and   respectively from the prior predictive.</p> </li> <li> <code>row</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Variables that define subsets of the data, which will be drawn on the row dimension of the facets in the grid. When both row and col are None, one single plot will be produced, by default None.</p> </li> <li> <code>col</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Variables that define subsets of the data, which will be drawn on the column dimension of the facets in the grid. When both row and col are None, one single plot will be produced, by default None.</p> </li> <li> <code>col_wrap</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>\u201cWrap\u201d the column variable at this width, so that the column facets span multiple rows. Incompatible with a row facet., by default None.</p> </li> <li> <code>groups</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Additional dimensions along which to plot different groups. This is useful when there are 3 or more dimensions of covariates to plot against, by default None.</p> </li> <li> <code>groups_order</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>The order to plot the groups, by default None, in which case the order is the order in which the groups appear in the data. Only when <code>groups</code> is a string, this can be an iterable of strings. Otherwise, this is a dictionary mapping the dimension name to the order of the groups in that dimension.</p> </li> <li> <code>bins</code>               (<code>optional</code>, default:                   <code>50</code> )           \u2013            <p>Specification of hist bins, by default 100. There are three options: - A string describing the binning strategy (passed to <code>np.histogram_bin_edges</code>). - A list-like defining the bin edges. - An integer defining the number of bins to be used.</p> </li> <li> <code>step</code>               (<code>optional</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot the distributions as a step function or a smooth density plot, by default False.</p> </li> <li> <code>colors</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Colors to use for the different levels of the hue variable. When a <code>str</code>, the color of posterior predictives, in which case an error will be thrown if <code>plot_data</code> is <code>True</code>. When a length-2 iterable, indicates the colors in the order of posterior predictives and observed data. The values must be interpretable by matplotlib. When None, use default color palette, by default None.</p> </li> <li> <code>linestyles</code>               (<code>optional</code>, default:                   <code>'-'</code> )           \u2013            <p>Linestyles to use for the different levels of the hue variable. When a <code>str</code>, the linestyle of both distributions. When a length-2 iterable, indicates the linestyles in the order of posterior predictives and observed data. The values must be interpretable by matplotlib. When None, use solid lines, by default \"-\". When dictionary, the keys must be 'predicted' and/or 'observed', and the values must be interpretable by matplotlib.</p> </li> <li> <code>linewidths</code>               (<code>optional</code>, default:                   <code>1.25</code> )           \u2013            <p>Linewidths to use for the different levels of the hue variable. When a <code>float</code>, the linewidth of both distributions. When a length-2 iterable, indicates the linewidths in the order of posterior predictives and observed data, by default 1.25.</p> </li> <li> <code>title</code>               (<code>optional</code>, default:                   <code>'Posterior Predictive Distribution'</code> )           \u2013            <p>The title of the plot, by default \"Posterior Predictive Distribution\". Ignored when <code>groups</code> is provided.</p> </li> <li> <code>xlabel</code>               (<code>optional</code>, default:                   <code>'Response Time'</code> )           \u2013            <p>The label for the x-axis, by default \"Response Time\".</p> </li> <li> <code>ylabel</code>               (<code>optional</code>, default:                   <code>''</code> )           \u2013            <p>The label for the y-axis, by default \"Density\".</p> </li> <li> <code>grid_kwargs</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Additional keyword arguments are passed to the [<code>sns.FacetGrid</code> constructor] (https://seaborn.pydata.org/generated/seaborn.FacetGrid.html#seaborn.FacetGrid.init) when any of row or col is provided. When producing a single plot, these arguments are ignored.</p> </li> <li> <code>kwargs</code>               (<code>optional</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments passed to ax.plot() functions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Axes | FacetGrid</code>           \u2013            <p>The matplotlib <code>axis</code> or seaborn <code>FacetGrid</code> object containing the plot.</p> </li> </ul>"},{"location":"api/plotting/#hssm.plotting.plot_predictive","title":"hssm.plotting.plot_predictive","text":"<pre><code>plot_predictive(\n    model,\n    idata: InferenceData | None = None,\n    data: DataFrame | None = None,\n    predictive_group: Literal[\n        \"posterior_predictive\", \"prior_predictive\"\n    ] = \"posterior_predictive\",\n    plot_data: bool = True,\n    n_samples: int | float | None = 20,\n    row: str | None = None,\n    col: str | None = None,\n    col_wrap: int | None = None,\n    groups: str | Iterable[str] | None = None,\n    groups_order: Iterable[str] | dict[str, Iterable[str]] | None = None,\n    bins: int | ndarray | str | None = 50,\n    x_range: tuple[float, float] | None = None,\n    step: bool = False,\n    hdi: float | str | tuple[float, float] | None = None,\n    colors: str | list[str] | None = None,\n    linestyles: str | list[str] | tuple[str] | dict[str, str] = \"-\",\n    linewidths: float | list[float] | tuple[float] | dict[str, float] = 1.25,\n    title: str | None = \"Posterior Predictive Distribution\",\n    xlabel: str | None = \"Response Time\",\n    ylabel: str | None = \"Density\",\n    grid_kwargs: dict | None = None,\n    **kwargs,\n) -&gt; Axes | sns.FacetGrid | list[sns.FacetGrid]\n</code></pre> <p>Plot the posterior predictive distribution against the observed data.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>HSSM</code>)           \u2013            <p>A fitted HSSM model.</p> </li> <li> <code>idata</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>The InferenceData object with posterior samples. If not provided, will use the traces object stored inside the model. If posterior predictive samples are not present in this object, will generate posterior predictive samples using the this InferenceData object and the original data.</p> </li> <li> <code>data</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>The observed data.</p> <ul> <li>If <code>data</code> is provided and the idata object does not contain a <code>\"posterior_predictive\"</code> group, will generate posterior predictive samples using covariate provided in this object. If the group does exist, it is assumed that the posterior predictive samples are generated with the covariates provided in this DataFrame.</li> <li>If <code>data</code> is not provided (i.e., <code>data=None</code>), the behavior depends on whether \"plot_data\" is true or not. If <code>plot_data=True</code>, the plotting function will use the data stored in the <code>model</code> object and proceed as the case above. If <code>plot_data=False</code>, if posterior predictive samples are not present in the <code>idata</code> object, the plotting function will generate posterior predictive samples using the data stored in the <code>model</code> object. If posterior predictive samples exist in the <code>idata</code> object, these samples will be used for plotting, but a ValueError will be thrown if any of <code>col</code> or <code>row</code> is not None.</li> </ul> </li> <li> <code>predictive_group</code>               (<code>optional</code>, default:                   <code>'posterior_predictive'</code> )           \u2013            <p>The type of predictive distribution to plot, by default \"posterior_predictive\". Can be \"posterior_predictive\" or \"prior_predictive\".</p> </li> <li> <code>plot_data</code>               (<code>optional</code>, default:                   <code>True</code> )           \u2013            <p>Whether to plot the observed data, by default True.</p> </li> <li> <code>n_samples</code>               (<code>optional</code>, default:                   <code>20</code> )           \u2013            <p>When idata is provided, the number or proportion of posterior predictive samples randomly drawn to be used from each chain for plotting. When idata is not provided, the number or proportion of posterior samples to be used to generate posterior predictive samples. The number or proportion are defined as follows:</p> <ul> <li>When an integer &gt;= 1, the number of samples to be extracted from the draw   dimension.</li> <li>When a float between 0 and 1, the proportion of samples to be extracted from   the draw dimension.</li> <li>When None, all samples are extracted.</li> </ul> </li> <li> <code>row</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Variables that define subsets of the data, which will be drawn on the row dimension of the facets in the grid. When both row and col are None, one single plot will be produced, by default None.</p> </li> <li> <code>col</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Variables that define subsets of the data, which will be drawn on the column dimension of the facets in the grid. When both row and col are None, one single plot will be produced, by default None.</p> </li> <li> <code>col_wrap</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>\u201cWrap\u201d the column variable at this width, so that the column facets span multiple rows. Incompatible with a row facet., by default None.</p> </li> <li> <code>groups</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Additional dimensions along which to plot different groups. This is useful when there are 3 or more dimensions of covariates to plot against, by default None.</p> </li> <li> <code>groups_order</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>The order to plot the groups, by default None, in which case the order is the order in which the groups appear in the data. Only when <code>groups</code> is a string, this can be an iterable of strings. Otherwise, this is a dictionary mapping the dimension name to the order of the groups in that dimension.</p> </li> <li> <code>bins</code>               (<code>optional</code>, default:                   <code>50</code> )           \u2013            <p>Specification of hist bins, by default 100. There are three options: - A string describing the binning strategy (passed to <code>np.histogram_bin_edges</code>). - A list-like defining the bin edges. - An integer defining the number of bins to be used.</p> </li> <li> <code>x_range</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>The lower and upper range of the bins. Lower and upper outliers are ignored. If not provided, range is simply the minimum and the maximum of the data, by default None.</p> </li> <li> <code>step</code>               (<code>optional</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot the distributions as a step function or a smooth density plot, by default False.</p> </li> <li> <code>hdi</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>A two-tuple of floats indicating the hdi to plot, by default None. The values in the tuple should be between 0 and 1, indicating the percentiles used to compute the interval. For example, (0.05, 0.95) will compute the 90% interval. There should be at least 50 posterior predictive samples for each chain for this to work properly. A warning message will be displayed if there are fewer than 50 posterior samples. If None, no interval is plotted. If a float, the interval the computed interval will be ((1 - hdi) / 2, 1 - (1 - hdi) / 2). If a string, the format needs to be e.g. '10%'.</p> </li> <li> <code>colors</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Colors to use for the different levels of the hue variable. When a <code>str</code>, the color of posterior predictives, in which case an error will be thrown if <code>plot_data</code> is <code>True</code>. When a length-2 iterable, indicates the colors in the order of posterior predictives and observed data. The values must be interpretable by matplotlib. When None, use default color palette, by default None.</p> </li> <li> <code>linestyles</code>               (<code>optional</code>, default:                   <code>'-'</code> )           \u2013            <p>Linestyles to use for the different levels of the hue variable. When a <code>str</code>, the linestyle of both distributions. When a length-2 iterable, indicates the linestyles in the order of posterior predictives and observed data. The values must be interpretable by matplotlib. When None, use solid lines, by default \"-\". When dictionary, the keys must be 'predicted' and/or 'observed', and the values must be interpretable by matplotlib.</p> </li> <li> <code>linewidths</code>               (<code>optional</code>, default:                   <code>1.25</code> )           \u2013            <p>Linewidths to use for the different levels of the hue variable. When a <code>float</code>, the linewidth of both distributions. When a length-2 iterable, indicates the linewidths in the order of posterior predictives and observed data, by default 1.25.</p> </li> <li> <code>title</code>               (<code>optional</code>, default:                   <code>'Posterior Predictive Distribution'</code> )           \u2013            <p>The title of the plot, by default \"Posterior Predictive Distribution\". Ignored when <code>groups</code> is provided.</p> </li> <li> <code>xlabel</code>               (<code>optional</code>, default:                   <code>'Response Time'</code> )           \u2013            <p>The label for the x-axis, by default \"Response Time\".</p> </li> <li> <code>ylabel</code>               (<code>optional</code>, default:                   <code>'Density'</code> )           \u2013            <p>The label for the y-axis, by default \"Density\".</p> </li> <li> <code>grid_kwargs</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Additional keyword arguments are passed to the [<code>sns.FacetGrid</code> constructor] (https://seaborn.pydata.org/generated/seaborn.FacetGrid.html#seaborn.FacetGrid.init) when any of row or col is provided. When producing a single plot, these arguments are ignored.</p> </li> <li> <code>kwargs</code>               (<code>optional</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments passed to ax.plot() functions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Axes | FacetGrid</code>           \u2013            <p>The matplotlib <code>axis</code> or seaborn <code>FacetGrid</code> object containing the plot.</p> </li> </ul>"},{"location":"api/plotting/#hssm.plotting.plot_quantile_probability","title":"hssm.plotting.plot_quantile_probability","text":"<pre><code>plot_quantile_probability(\n    model,\n    cond: str,\n    data: DataFrame | None = None,\n    idata: InferenceData | None = None,\n    predictive_group: Literal[\"posterior_predictive\", \"prior_predictive\"]\n    | None = \"posterior_predictive\",\n    n_samples: int = 20,\n    x: str = \"proportion\",\n    y: str = \"rt\",\n    hue: str = \"quantile\",\n    row: str | None = None,\n    col: str | None = None,\n    col_wrap: int | None = None,\n    groups: str | Iterable[str] | None = None,\n    groups_order: Iterable[str] | dict[str, Iterable[str]] | None = None,\n    correct: str | None = None,\n    q: int | Iterable[float] = 5,\n    title: str | None = \"Quantile Probability Plot\",\n    xlabel: str | None = \"Proportion\",\n    ylabel: str | None = None,\n    xticklabels: Iterable[str] | None = None,\n    grid_kwargs: dict[str, Any] | None = None,\n    data_kwargs: dict[str, Any] | None = None,\n    pps_kwargs: dict[str, Any] | None = None,\n    **kwargs,\n) -&gt; sns.FacetGrid\n</code></pre> <p>Plot the quantile probabilities against the observed data.</p> <p>Parameters:</p> <ul> <li> <code>model</code>           \u2013            <p>A model object that has a <code>plot_quantile_probability</code> method.</p> </li> <li> <code>cond</code>               (<code>str</code>)           \u2013            <p>The column in <code>data</code> that indicates the conditions.</p> </li> <li> <code>data</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>A pandas DataFrame containing the observed data. If None, the data from <code>idata.observed_data</code> will be used.</p> </li> <li> <code>idata</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>An arviz InferenceData object. If None, the model's trace will be used. If the model's trace does not contain posterior predictive samples, and \"plot_predictive\" is True, will use the model and <code>data</code> to produce posterior predictive samples.</p> </li> <li> <code>predictive_group</code>               (<code>optional</code>, default:                   <code>'posterior_predictive'</code> )           \u2013            <p>The type of predictive distribution to plot, by default \"posterior_predictive\". Can be \"posterior_predictive\" or \"prior_predictive\".</p> </li> <li> <code>n_samples</code>               (<code>optional</code>, default:                   <code>20</code> )           \u2013            <p>When idata is provided, the number or proportion of posterior predictive samples randomly drawn to be used from each chain for plotting. When idata is not provided, the number or proportion of posterior samples to be used to generate posterior predictive samples. The number or proportion are defined as follows:</p> <ul> <li>When an integer &gt;= 1, the number of samples to be extracted from the draw   dimension.</li> <li>When a float between 0 and 1, the proportion of samples to be extracted from   the draw dimension.</li> <li>When None, all samples are extracted.</li> </ul> </li> <li> <code>x</code>               (<code>optional</code>, default:                   <code>'proportion'</code> )           \u2013            <p>The column in <code>data</code> that indicates the x-axis variable. By default, this is \"proportion\", which is the proportion of (in)correct responses in each group in <code>cond</code>.</p> </li> <li> <code>y</code>               (<code>optional</code>, default:                   <code>'rt'</code> )           \u2013            <p>The column in <code>data</code> that indicates the y-axis variable. By default, this is \"rt\", which is the response time.</p> </li> <li> <code>hue</code>               (<code>optional</code>, default:                   <code>'quantile'</code> )           \u2013            <p>The column in <code>data</code> that indicates the hue variable. By default, this is \"quantile\", which is the quantile of the response time.</p> </li> <li> <code>row</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Variables that define subsets of the data, which will be drawn on the row dimension of the facets in the grid. When both row and col are None, one single plot will be produced, by default None.</p> </li> <li> <code>col</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Variables that define subsets of the data, which will be drawn on the column dimension of the facets in the grid. When both row and col are None, one single plot will be produced, by default None.</p> </li> <li> <code>col_wrap</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>\u201cWrap\u201d the column variable at this width, so that the column facets span multiple rows. Incompatible with a row facet., by default None.</p> </li> <li> <code>groups</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Additional dimensions along which to plot different groups. This is useful when there are 3 or more dimensions of covariates to plot against, by default None.</p> </li> <li> <code>groups_order</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>The order to plot the groups, by default None, in which case the order is the order in which the groups appear in the data. Only when <code>groups</code> is a string, this can be an iterable of strings. Otherwise, this is a dictionary mapping the dimension name to the order of the groups in that dimension.</p> </li> <li> <code>correct</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>The column in <code>data</code> that indicates the correct responses. If None, <code>response</code> column from <code>data</code> indicates whether the response is correct or not. By default None.</p> </li> <li> <code>q</code>               (<code>optional</code>, default:                   <code>5</code> )           \u2013            <p>If an <code>int</code>, quantiles will be determined using np.linspace(0, 1, q) (0 and 1 will be excluded. If an iterable, will generate quantiles according to this iterable.</p> </li> <li> <code>title</code>               (<code>optional</code>, default:                   <code>'Quantile Probability Plot'</code> )           \u2013            <p>The title of the plot, by default \"Quantile Predictive Plot\". Ignored when <code>groups</code> is provided.</p> </li> <li> <code>xlabel</code>               (<code>optional</code>, default:                   <code>'Proportion'</code> )           \u2013            <p>The label for the x-axis, by default \"Proportion\".</p> </li> <li> <code>ylabel</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>The label for the y-axis, by default None.</p> </li> <li> <code>xticklabels</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>The labels for groups on the top x-axis, by default None, which will be inferred from the data.</p> </li> <li> <code>grid_kwargs</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments passed to seaborn.FacetGrid.</p> </li> <li> <code>data_kwargs</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments passed to seaborn.lineplot.</p> </li> <li> <code>pps_kwargs</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments passed to seaborn.scatterplot.</p> </li> <li> <code>kwargs</code>               (<code>optional</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments passed to both seaborn.lineplot and seaborn.scatterplot.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Axes | FacetGrid | list[FacetGrid]</code>           \u2013            <p>A seaborn FacetGrid object containing the plot.</p> </li> </ul>"},{"location":"api/prior/","title":"hssm.Prior","text":""},{"location":"api/prior/#hssm.Prior","title":"hssm.Prior","text":"<pre><code>Prior(\n    name: str,\n    auto_scale: bool = True,\n    dist: Callable | None = None,\n    bounds: tuple[float, float] | None = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Prior</code></p> <p>Abstract specification of a prior.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name of prior distribution. Must be the name of a PyMC distribution (e.g., <code>\"Normal\"</code>, <code>\"Bernoulli\"</code>, etc.)</p> </li> <li> <code>auto_scale</code>               (<code>optional</code>, default:                   <code>True</code> )           \u2013            <p>Whether to adjust the parameters of the prior or use them as passed. Default to <code>True</code>.</p> </li> <li> <code>kwargs</code>           \u2013            <p>Optional keywords specifying the parameters of the named distribution.</p> </li> <li> <code>dist</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>A callable that returns a valid PyMC distribution. The signature must contain <code>name</code>, <code>dims</code>, and <code>shape</code>, as well as its own keyworded arguments.</p> </li> <li> <code>bounds</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>A tuple of two floats indicating the lower and upper bounds of the prior.</p> </li> </ul>"},{"location":"api/set_floatx/","title":"hssm.set_floatX","text":""},{"location":"api/set_floatx/#hssm.set_floatX","title":"hssm.set_floatX","text":"<pre><code>set_floatX(dtype: Literal['float32', 'float64'], update_jax: bool = True)\n</code></pre> <p>Set float types for pytensor and Jax.</p> <p>Often we wish to work with a specific type of float in both PyTensor and JAX. This function helps set float types in both packages.</p> <p>Parameters:</p> <ul> <li> <code>dtype</code>               (<code>Literal['float32', 'float64']</code>)           \u2013            <p>Either <code>float32</code> or <code>float64</code>. Float type for pytensor (and jax if <code>jax=True</code>).</p> </li> <li> <code>update_jax</code>               (<code>optional</code>, default:                   <code>True</code> )           \u2013            <p>Whether this function also sets float type for JAX by changing the <code>jax_enable_x64</code> setting in JAX config. Defaults to True.</p> </li> </ul>"},{"location":"api/show_defaults/","title":"hssm.show_defaults","text":""},{"location":"api/show_defaults/#hssm.show_defaults","title":"hssm.show_defaults","text":"<pre><code>show_defaults(model: SupportedModels, loglik_kind=Optional[LoglikKind]) -&gt; str\n</code></pre> <p>Show the defaults for supported models.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>SupportedModels</code>)           \u2013            <p>One of the supported model strings.</p> </li> <li> <code>loglik_kind</code>               (<code>optional</code>, default:                   <code>Optional[LoglikKind]</code> )           \u2013            <p>The kind of likelihood function, by default None, in which case the defaults for all likelihoods will be shown.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>A nicely organized printout for the defaults of provided model.</p> </li> </ul>"},{"location":"api/simulate_data/","title":"hssm.simulate_data","text":""},{"location":"api/simulate_data/#hssm.simulate_data","title":"hssm.simulate_data","text":"<pre><code>simulate_data(\n    model: str,\n    theta: dict[str, ArrayLike] | list[float] | ArrayLike,\n    size: int,\n    random_state: int | None = None,\n    output_df: bool = True,\n    **kwargs,\n) -&gt; np.ndarray | pd.DataFrame\n</code></pre> <p>Sample simulated data from specified distributions.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>A model name that must be supported in <code>ssm_simulators</code>. For a detailed list of supported models, please see all fields in the <code>model_config</code> dict here</p> </li> <li> <code>theta</code>               (<code>dict[str, ArrayLike] | list[float] | ArrayLike</code>)           \u2013            <p>Parameters of the process. Can be supplied as dictionary with parameter names as key and np.array or float as values. Can also be supplied as a list or 1D-array, however in this case the order of parameters is important and must match specifications here. Parameters can be specificed 'trial-wise', by supplying 1D arrays of shape <code>size</code> to the dictionary, or by supplying a 2D array of shape <code>(size, n_parameters)</code> dicrectly.</p> </li> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>The size of the data to be simulated. If <code>theta</code> is a 2D ArrayLike, this parameter indicates the size of data to be simulated for each trial.</p> </li> <li> <code>random_state</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>A random seed for reproducibility.</p> </li> <li> <code>output_df</code>               (<code>optional</code>, default:                   <code>True</code> )           \u2013            <p>If True, outputs a DataFrame with column names \"rt\", \"response\". Otherwise a 2-column numpy array, by default True.</p> </li> <li> <code>kwargs</code>               (<code>optional</code>, default:                   <code>{}</code> )           \u2013            <p>Other arguments passed to ssms.basic_simulators.simulator.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray | DataFrame</code>           \u2013            <p>An array or DataFrame with simulated data.</p> </li> </ul>"},{"location":"getting_started/getting_started/","title":"Getting Started","text":"In\u00a0[\u00a0]: Copied! <pre># !pip install hssm\n</pre> # !pip install hssm In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nimport hssm\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</pre> import numpy as np  import hssm  %matplotlib inline %config InlineBackend.figure_format='retina' In\u00a0[\u00a0]: Copied! <pre>v_true, a_true, z_true, t_true = [0.5, 1.5, 0.5, 0.5]\ndataset = hssm.simulate_data(\n    model=\"ddm\",\n    theta=[v_true, a_true, z_true, t_true],\n    size=1000,\n)\n\ndataset\n</pre> v_true, a_true, z_true, t_true = [0.5, 1.5, 0.5, 0.5] dataset = hssm.simulate_data(     model=\"ddm\",     theta=[v_true, a_true, z_true, t_true],     size=1000, )  dataset Out[\u00a0]: rt response 0 2.138525 1.0 1 4.298813 1.0 2 2.722723 1.0 3 1.338940 1.0 4 1.915012 -1.0 ... ... ... 995 2.367986 1.0 996 1.623948 1.0 997 1.893260 1.0 998 1.184971 1.0 999 4.209857 1.0 <p>1000 rows \u00d7 2 columns</p> In\u00a0[\u00a0]: Copied! <pre>simple_ddm_model = hssm.HSSM(data=dataset)\nsimple_ddm_model\n</pre> simple_ddm_model = hssm.HSSM(data=dataset) simple_ddm_model <pre>Model initialized successfully.\n</pre> Out[\u00a0]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 1000\n\nParameters:\n\nv:\n    Prior: Normal(mu: 0.0, sigma: 2.0)\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[\u00a0]: Copied! <pre># Uncomment if you have graphviz installed\nsimple_ddm_model.graph()\n</pre> # Uncomment if you have graphviz installed simple_ddm_model.graph() Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre>simple_ddm_model.sample()\n</pre> simple_ddm_model.sample() <pre>Using default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [z, t, a, v]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 19 seconds.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4000/4000 [00:01&lt;00:00, 2698.98it/s]\n</pre> Out[\u00a0]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 136kB\nDimensions:  (chain: 4, draw: 1000)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\nData variables:\n    v        (chain, draw) float64 32kB 0.5497 0.5603 0.5725 ... 0.6046 0.6327\n    t        (chain, draw) float64 32kB 0.5142 0.5036 0.5201 ... 0.5427 0.5165\n    z        (chain, draw) float64 32kB 0.4838 0.4935 0.4967 ... 0.5005 0.4871\n    a        (chain, draw) float64 32kB 1.455 1.444 1.481 ... 1.523 1.454 1.467\nAttributes:\n    created_at:                  2025-01-01T22:56:22.506497+00:00\n    arviz_version:               0.19.0\n    inference_library:           pymc\n    inference_library_version:   5.19.1\n    sampling_time:               18.824692964553833\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 1000</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li></ul></li><li>Data variables: (4)<ul><li>v(chain, draw)float640.5497 0.5603 ... 0.6046 0.6327<pre>array([[0.54970577, 0.56026962, 0.57254028, ..., 0.65770627, 0.60926159,\n        0.57188032],\n       [0.56698495, 0.62414784, 0.62414784, ..., 0.69756279, 0.60838028,\n        0.59956546],\n       [0.63364041, 0.62215605, 0.61402972, ..., 0.64570557, 0.57806909,\n        0.54335791],\n       [0.61107017, 0.66171729, 0.65194968, ..., 0.60551088, 0.60464883,\n        0.63274168]])</pre></li><li>t(chain, draw)float640.5142 0.5036 ... 0.5427 0.5165<pre>array([[0.51424905, 0.50363058, 0.52013742, ..., 0.49934094, 0.51503471,\n        0.52879378],\n       [0.50881794, 0.52406151, 0.52406151, ..., 0.52744524, 0.51095254,\n        0.54403327],\n       [0.53148476, 0.51829168, 0.51530686, ..., 0.54457198, 0.50441471,\n        0.53585271],\n       [0.51930111, 0.48109424, 0.51057573, ..., 0.50125602, 0.54265377,\n        0.51651044]])</pre></li><li>z(chain, draw)float640.4838 0.4935 ... 0.5005 0.4871<pre>array([[0.4837569 , 0.49350844, 0.4966984 , ..., 0.48755898, 0.49122704,\n        0.49755321],\n       [0.48660847, 0.51974036, 0.51974036, ..., 0.4657371 , 0.50483256,\n        0.50484662],\n       [0.48298108, 0.49957914, 0.48737165, ..., 0.50506395, 0.49206078,\n        0.51410704],\n       [0.49466379, 0.47574906, 0.46704874, ..., 0.50045994, 0.50052012,\n        0.48710852]])</pre></li><li>a(chain, draw)float641.455 1.444 1.481 ... 1.454 1.467<pre>array([[1.45521135, 1.44356845, 1.48098443, ..., 1.49655456, 1.47418624,\n        1.46217661],\n       [1.48499811, 1.49353323, 1.49353323, ..., 1.47532566, 1.52531485,\n        1.46118929],\n       [1.4687825 , 1.48311979, 1.48989569, ..., 1.4829365 , 1.48801815,\n        1.44453438],\n       [1.47337378, 1.51974775, 1.45499331, ..., 1.52310891, 1.45383622,\n        1.46746742]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li></ul></li><li>Attributes: (8)created_at :2025-01-01T22:56:22.506497+00:00arviz_version :0.19.0inference_library :pymcinference_library_version :5.19.1sampling_time :18.824692964553833tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 32MB\nDimensions:      (chain: 4, draw: 1000, __obs__: 1000)\nCoordinates:\n  * chain        (chain) int64 32B 0 1 2 3\n  * draw         (draw) int64 8kB 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\n  * __obs__      (__obs__) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    rt,response  (chain, draw, __obs__) float64 32MB -1.413 -2.947 ... -2.917\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 1000</li><li>__obs__: 1000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-1.413 -2.947 ... -0.8276 -2.917<pre>array([[[-1.41344934, -2.94665977, -1.82757127, ..., -1.24494756,\n         -0.91603009, -2.88445202],\n        [-1.42780816, -2.99911322, -1.85559533, ..., -1.25115284,\n         -0.86560552, -2.93574297],\n        [-1.39501039, -2.92207805, -1.81048175, ..., -1.22403504,\n         -0.87940237, -2.86038402],\n        ...,\n        [-1.35159185, -2.95685358, -1.78590136, ..., -1.17462142,\n         -0.82574406, -2.89183531],\n        [-1.37737034, -2.95667293, -1.80590955, ..., -1.20176159,\n         -0.84782489, -2.89281325],\n        [-1.39808335, -2.95525475, -1.82243521, ..., -1.22285633,\n         -0.85962589, -2.8924503 ]],\n\n       [[-1.39660155, -2.90317247, -1.80342145, ..., -1.23129108,\n         -0.91850006, -2.84198575],\n        [-1.38445747, -2.97400257, -1.82408405, ..., -1.19884499,\n         -0.77301606, -2.91055953],\n        [-1.38445747, -2.97400257, -1.82408405, ..., -1.19884499,\n         -0.77301606, -2.91055953],\n...\n        [-1.34801312, -2.97180251, -1.79262929, ..., -1.1634051 ,\n         -0.78045754, -2.9065713 ],\n        [-1.39617929, -2.91529292, -1.80809135, ..., -1.22758167,\n         -0.89235302, -2.85377594],\n        [-1.43150135, -2.99540671, -1.86262539, ..., -1.24976928,\n         -0.82808895, -2.93283056]],\n\n       [[-1.37690424, -2.96228276, -1.80811045, ..., -1.19950916,\n         -0.8357332 , -2.89828306],\n        [-1.34115014, -2.90733608, -1.76055748, ..., -1.17325233,\n         -0.87493773, -2.84341341],\n        [-1.34791503, -2.9989165 , -1.79008996, ..., -1.17093562,\n         -0.85013195, -2.93169234],\n        ...,\n        [-1.37626166, -2.88295916, -1.78660689, ..., -1.20744831,\n         -0.87140852, -2.82210021],\n        [-1.37771493, -2.99110649, -1.81855891, ..., -1.19492258,\n         -0.80860768, -2.92620775],\n        [-1.36419249, -2.98195272, -1.80235536, ..., -1.18522959,\n         -0.82758259, -2.91650488]]])</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 496kB\nDimensions:                (chain: 4, draw: 1000)\nCoordinates:\n  * chain                  (chain) int64 32B 0 1 2 3\n  * draw                   (draw) int64 8kB 0 1 2 3 4 5 ... 995 996 997 998 999\nData variables: (12/17)\n    acceptance_rate        (chain, draw) float64 32kB 0.9386 0.9401 ... 0.9572\n    diverging              (chain, draw) bool 4kB False False ... False False\n    energy                 (chain, draw) float64 32kB 1.896e+03 ... 1.891e+03\n    energy_error           (chain, draw) float64 32kB 0.2122 -0.2901 ... 0.05422\n    index_in_trajectory    (chain, draw) int64 32kB -3 -1 -2 -3 1 ... 2 2 -3 -3\n    largest_eigval         (chain, draw) float64 32kB nan nan nan ... nan nan\n    ...                     ...\n    process_time_diff      (chain, draw) float64 32kB 0.007661 ... 0.007419\n    reached_max_treedepth  (chain, draw) bool 4kB False False ... False False\n    smallest_eigval        (chain, draw) float64 32kB nan nan nan ... nan nan\n    step_size              (chain, draw) float64 32kB 0.7494 0.7494 ... 0.8027\n    step_size_bar          (chain, draw) float64 32kB 0.632 0.632 ... 0.6422\n    tree_depth             (chain, draw) int64 32kB 3 3 2 4 2 3 ... 3 3 3 3 3 3\nAttributes:\n    created_at:                  2025-01-01T22:56:22.519958+00:00\n    arviz_version:               0.19.0\n    inference_library:           pymc\n    inference_library_version:   5.19.1\n    sampling_time:               18.824692964553833\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 1000</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li></ul></li><li>Data variables: (17)<ul><li>acceptance_rate(chain, draw)float640.9386 0.9401 1.0 ... 1.0 0.9572<pre>array([[0.93859238, 0.94008691, 1.        , ..., 0.85550939, 0.81002973,\n        0.83299113],\n       [0.74398535, 0.89757502, 0.25990878, ..., 0.67898788, 0.95066537,\n        0.97502619],\n       [0.98645995, 0.53637624, 0.55863902, ..., 0.94864063, 0.95829174,\n        0.88451075],\n       [0.95381018, 0.86119478, 0.87497525, ..., 0.68985224, 1.        ,\n        0.95717856]])</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])</pre></li><li>energy(chain, draw)float641.896e+03 1.896e+03 ... 1.891e+03<pre>array([[1896.10442851, 1895.53214392, 1894.54911568, ..., 1893.5651165 ,\n        1892.77547175, 1891.85959832],\n       [1895.47297771, 1895.57362069, 1897.55270941, ..., 1896.47007313,\n        1894.80121334, 1893.6862696 ],\n       [1891.12228483, 1892.34904164, 1893.30443249, ..., 1893.07140827,\n        1893.23265629, 1896.1657012 ],\n       [1890.66711162, 1892.91145444, 1894.16113914, ..., 1893.69819439,\n        1892.6104971 , 1890.67942882]])</pre></li><li>energy_error(chain, draw)float640.2122 -0.2901 ... -0.1652 0.05422<pre>array([[ 0.21215292, -0.29010194, -0.41954787, ...,  0.14480428,\n        -0.26677343,  0.12934755],\n       [ 0.30507569,  0.51253783,  0.        , ..., -0.09626226,\n        -0.10892889, -0.1915657 ],\n       [ 0.01464248,  0.04701868, -0.02480712, ..., -0.34142039,\n        -0.08369416,  0.03792509],\n       [-0.06093447,  0.0642823 ,  0.11055521, ..., -0.09455818,\n        -0.16515617,  0.05422287]])</pre></li><li>index_in_trajectory(chain, draw)int64-3 -1 -2 -3 1 -4 ... 6 -5 2 2 -3 -3<pre>array([[-3, -1, -2, ..., -3, -5,  2],\n       [ 2, -3,  0, ..., -2, -5,  2],\n       [-4,  2,  6, ..., -3, -2, -2],\n       [ 7,  4,  2, ...,  2, -3, -3]])</pre></li><li>largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])</pre></li><li>lp(chain, draw)float64-1.895e+03 -1.894e+03 ... -1.89e+03<pre>array([[-1894.67897381, -1894.2334922 , -1891.22439692, ...,\n        -1891.10323381, -1890.01941614, -1890.97021381],\n       [-1892.93205057, -1894.22552934, -1894.22552934, ...,\n        -1893.18045676, -1892.08767072, -1890.51391744],\n       [-1890.34852549, -1890.33281282, -1890.22371309, ...,\n        -1892.08962788, -1891.62616436, -1892.8975849 ],\n       [-1889.94452927, -1891.86728696, -1891.65652195, ...,\n        -1891.86779146, -1890.36154336, -1890.02877458]])</pre></li><li>max_energy_error(chain, draw)float64-0.6402 -0.9719 ... -0.1652 0.08669<pre>array([[-0.64018366, -0.97185442, -0.41954787, ...,  0.42035857,\n         0.57327197,  0.31873529],\n       [ 0.70342289,  0.51253783,  1.34742454, ...,  1.09072469,\n         0.17826259, -0.1915657 ],\n       [ 0.03330553,  1.45819756,  1.31876995, ..., -0.74323684,\n        -0.28114852,  0.49571959],\n       [ 0.12531238,  0.21371031,  0.21934275, ...,  0.84624205,\n        -0.16515617,  0.08669273]])</pre></li><li>n_steps(chain, draw)float647.0 7.0 3.0 11.0 ... 7.0 7.0 7.0<pre>array([[7., 7., 3., ..., 7., 7., 7.],\n       [3., 7., 1., ..., 7., 7., 7.],\n       [7., 7., 7., ..., 3., 3., 7.],\n       [7., 7., 7., ..., 7., 7., 7.]])</pre></li><li>perf_counter_diff(chain, draw)float640.007669 0.007497 ... 0.007419<pre>array([[0.00766921, 0.00749713, 0.00384012, ..., 0.00728463, 0.00739396,\n        0.0074655 ],\n       [0.00369746, 0.00761471, 0.00191346, ..., 0.0073035 , 0.00727167,\n        0.00725363],\n       [0.00764817, 0.00770117, 0.00764067, ..., 0.00371546, 0.0036855 ,\n        0.00745537],\n       [0.00762333, 0.00770387, 0.00760725, ..., 0.00738733, 0.00743312,\n        0.00741858]])</pre></li><li>perf_counter_start(chain, draw)float643.373e+05 3.373e+05 ... 3.373e+05<pre>array([[337275.64862867, 337275.65635492, 337275.66391075, ...,\n        337283.23080083, 337283.23812754, 337283.24555933],\n       [337275.28754954, 337275.29130404, 337275.2989785 , ...,\n        337282.54750108, 337282.55484325, 337282.5621565 ],\n       [337275.267872  , 337275.2755775 , 337275.28335508, ...,\n        337282.45540871, 337282.45915825, 337282.46287813],\n       [337275.52300783, 337275.53067858, 337275.53844042, ...,\n        337282.50587404, 337282.51329729, 337282.52077383]])</pre></li><li>process_time_diff(chain, draw)float640.007661 0.007492 ... 0.007419<pre>array([[0.007661, 0.007492, 0.003839, ..., 0.007284, 0.007394, 0.007466],\n       [0.003698, 0.007611, 0.001914, ..., 0.007303, 0.007272, 0.007254],\n       [0.007637, 0.007694, 0.00763 , ..., 0.003715, 0.003685, 0.007456],\n       [0.007623, 0.007694, 0.007601, ..., 0.007387, 0.007434, 0.007419]])</pre></li><li>reached_max_treedepth(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])</pre></li><li>smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])</pre></li><li>step_size(chain, draw)float640.7494 0.7494 ... 0.8027 0.8027<pre>array([[0.74943933, 0.74943933, 0.74943933, ..., 0.74943933, 0.74943933,\n        0.74943933],\n       [0.66934713, 0.66934713, 0.66934713, ..., 0.66934713, 0.66934713,\n        0.66934713],\n       [0.73023901, 0.73023901, 0.73023901, ..., 0.73023901, 0.73023901,\n        0.73023901],\n       [0.80273936, 0.80273936, 0.80273936, ..., 0.80273936, 0.80273936,\n        0.80273936]])</pre></li><li>step_size_bar(chain, draw)float640.632 0.632 0.632 ... 0.6422 0.6422<pre>array([[0.63200669, 0.63200669, 0.63200669, ..., 0.63200669, 0.63200669,\n        0.63200669],\n       [0.61136213, 0.61136213, 0.61136213, ..., 0.61136213, 0.61136213,\n        0.61136213],\n       [0.59022707, 0.59022707, 0.59022707, ..., 0.59022707, 0.59022707,\n        0.59022707],\n       [0.64218318, 0.64218318, 0.64218318, ..., 0.64218318, 0.64218318,\n        0.64218318]])</pre></li><li>tree_depth(chain, draw)int643 3 2 4 2 3 2 3 ... 3 3 3 3 3 3 3 3<pre>array([[3, 3, 2, ..., 3, 3, 3],\n       [2, 3, 1, ..., 3, 3, 3],\n       [3, 3, 3, ..., 2, 2, 3],\n       [3, 3, 3, ..., 3, 3, 3]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li></ul></li><li>Attributes: (8)created_at :2025-01-01T22:56:22.519958+00:00arviz_version :0.19.0inference_library :pymcinference_library_version :5.19.1sampling_time :18.824692964553833tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 24kB\nDimensions:                  (__obs__: 1000, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 8kB 0 1 2 3 4 ... 996 997 998 999\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 16kB ...\nAttributes:\n    created_at:                  2025-01-01T22:56:22.522503+00:00\n    arviz_version:               0.19.0\n    inference_library:           pymc\n    inference_library_version:   5.19.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 1000</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float642.139 1.0 4.299 ... 1.0 4.21 1.0<pre>array([[2.13852501, 1.        ],\n       [4.29881287, 1.        ],\n       [2.72272325, 1.        ],\n       ...,\n       [1.89326012, 1.        ],\n       [1.18497074, 1.        ],\n       [4.20985746, 1.        ]])</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (6)created_at :2025-01-01T22:56:22.522503+00:00arviz_version :0.19.0inference_library :pymcinference_library_version :5.19.1modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>simple_ddm_model.summary()\n</pre> simple_ddm_model.summary() Out[\u00a0]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat v 0.623 0.035 0.555 0.686 0.001 0.0 2889.0 2946.0 1.0 t 0.521 0.020 0.484 0.561 0.000 0.0 2601.0 2822.0 1.0 z 0.490 0.014 0.463 0.515 0.000 0.0 2760.0 2545.0 1.0 a 1.476 0.027 1.425 1.528 0.001 0.0 2695.0 2712.0 1.0 In\u00a0[\u00a0]: Copied! <pre>simple_ddm_model.plot_trace();\n</pre> simple_ddm_model.plot_trace(); <p>Congratulations! You have just created and sampled from your first model in HSSM! Parameter recovery seems to be pretty successful.</p> In\u00a0[\u00a0]: Copied! <pre># Simulate data for an angle model\nangle_data = hssm.simulate_data(\n    model=\"angle\",\n    theta=[0.5, 1.5, 0.5, 0.5, 0.3],  # true values\n    size=1000,\n)\n\nangle_model = hssm.HSSM(data=angle_data, model=\"angle\")\nangle_model\n</pre> # Simulate data for an angle model angle_data = hssm.simulate_data(     model=\"angle\",     theta=[0.5, 1.5, 0.5, 0.5, 0.3],  # true values     size=1000, )  angle_model = hssm.HSSM(data=angle_data, model=\"angle\") angle_model <pre>Model initialized successfully.\n</pre> Out[\u00a0]: <pre>Hierarchical Sequential Sampling Model\nModel: angle\n\nResponse variable: rt,response\nLikelihood: approx_differentiable\nObservations: 1000\n\nParameters:\n\nv:\n    Prior: Uniform(lower: -3.0, upper: 3.0)\n    Explicit bounds: (-3.0, 3.0)\n\na:\n    Prior: Uniform(lower: 0.3, upper: 3.0)\n    Explicit bounds: (0.3, 3.0)\n\nz:\n    Prior: Uniform(lower: 0.1, upper: 0.9)\n    Explicit bounds: (0.1, 0.9)\n\nt:\n    Prior: Uniform(lower: 0.001, upper: 2.0)\n    Explicit bounds: (0.001, 2.0)\n\ntheta:\n    Prior: Uniform(lower: -0.1, upper: 1.3)\n    Explicit bounds: (-0.1, 1.3)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> <p><code>angle</code> models, by default, use an approximate differentiable likelihood function that relies on JAX for likelihood computation. At the moment, due to the different ways <code>JAX</code> and <code>Jupyter</code> handle parallelism, parallel sampling is not available for this type of likelihood computations. There are ways to get around this, but for now, let's just perform sequential sampling with just one core.</p> <p>Note: the <code>sample()</code> method internally calls <code>bambi</code>'s <code>fit()</code> fit method, which internally calls <code>pymc</code>'s <code>sample()</code> function. This means that HSSM's <code>sample()</code> method will accept most parameters accepted by the two other functions. You could pretty much use it just like the <code>pm.sample()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre># Unless otherwise specified, we default to the `nuts_numpyro` sampler\n# for \"approx_differentiable\" likelihoods.\n\nangle_model.sample()\n</pre> # Unless otherwise specified, we default to the `nuts_numpyro` sampler # for \"approx_differentiable\" likelihoods.  angle_model.sample() <pre>Using default initvals. \n\n</pre> <pre>  0%|          | 0/2000 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/2000 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/2000 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/2000 [00:00&lt;?, ?it/s]</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4000/4000 [00:04&lt;00:00, 869.89it/s]\n</pre> Out[\u00a0]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 168kB\nDimensions:  (chain: 4, draw: 1000)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\nData variables:\n    theta    (chain, draw) float64 32kB 0.3024 0.2963 0.3347 ... 0.3651 0.373\n    v        (chain, draw) float64 32kB 0.4196 0.4099 0.4478 ... 0.4249 0.4194\n    z        (chain, draw) float64 32kB 0.5282 0.528 0.5147 ... 0.5295 0.5169\n    t        (chain, draw) float64 32kB 0.5346 0.532 0.4893 ... 0.464 0.4651\n    a        (chain, draw) float64 32kB 1.476 1.468 1.565 ... 1.56 1.633 1.64\nAttributes:\n    created_at:                  2025-01-01T22:58:11.077741+00:00\n    arviz_version:               0.19.0\n    inference_library:           numpyro\n    inference_library_version:   0.16.1\n    sampling_time:               104.771827\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 1000</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li></ul></li><li>Data variables: (5)<ul><li>theta(chain, draw)float640.3024 0.2963 ... 0.3651 0.373<pre>array([[0.30242169, 0.29632991, 0.33472531, ..., 0.32819838, 0.32275869,\n        0.32002731],\n       [0.34304378, 0.3175317 , 0.33453714, ..., 0.32811982, 0.32212808,\n        0.35285781],\n       [0.36228318, 0.33512648, 0.36486818, ..., 0.35288072, 0.33775568,\n        0.34468611],\n       [0.34296713, 0.3363399 , 0.37107354, ..., 0.3302308 , 0.36510465,\n        0.37300772]])</pre></li><li>v(chain, draw)float640.4196 0.4099 ... 0.4249 0.4194<pre>array([[0.41961606, 0.4099227 , 0.44781464, ..., 0.41336737, 0.41189259,\n        0.41388682],\n       [0.4818998 , 0.41722413, 0.41674193, ..., 0.40706266, 0.41046839,\n        0.47432381],\n       [0.42794574, 0.45282415, 0.40308131, ..., 0.44785474, 0.44921289,\n        0.44099588],\n       [0.50179348, 0.51874919, 0.51420916, ..., 0.42511217, 0.42492402,\n        0.41938531]])</pre></li><li>z(chain, draw)float640.5282 0.528 ... 0.5295 0.5169<pre>array([[0.5282405 , 0.52796739, 0.51474406, ..., 0.53395986, 0.5347729 ,\n        0.52481726],\n       [0.51678166, 0.52197029, 0.53217784, ..., 0.53325184, 0.53905579,\n        0.50932918],\n       [0.52940554, 0.50718174, 0.52537599, ..., 0.51932069, 0.5195248 ,\n        0.5183871 ],\n       [0.49155091, 0.50275917, 0.49691754, ..., 0.51710907, 0.52947744,\n        0.51685154]])</pre></li><li>t(chain, draw)float640.5346 0.532 ... 0.464 0.4651<pre>array([[0.53456122, 0.53195901, 0.48932895, ..., 0.51283014, 0.50319511,\n        0.51421086],\n       [0.46666602, 0.51397774, 0.50637526, ..., 0.49315902, 0.54327866,\n        0.44804364],\n       [0.48087137, 0.47316823, 0.47091076, ..., 0.48888648, 0.48874634,\n        0.50449739],\n       [0.4665343 , 0.45819448, 0.47549256, ..., 0.48434443, 0.46398001,\n        0.46514009]])</pre></li><li>a(chain, draw)float641.476 1.468 1.565 ... 1.633 1.64<pre>array([[1.47620501, 1.46813007, 1.56515925, ..., 1.50564417, 1.51242426,\n        1.55811451],\n       [1.59123236, 1.50061372, 1.56116013, ..., 1.56151695, 1.5028171 ,\n        1.61331542],\n       [1.60007363, 1.55028016, 1.59830685, ..., 1.58854869, 1.57306445,\n        1.60902135],\n       [1.59023689, 1.60150976, 1.62944325, ..., 1.56040808, 1.63338224,\n        1.64029691]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li></ul></li><li>Attributes: (8)created_at :2025-01-01T22:58:11.077741+00:00arviz_version :0.19.0inference_library :numpyroinference_library_version :0.16.1sampling_time :104.771827tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 32MB\nDimensions:      (chain: 4, draw: 1000, __obs__: 1000)\nCoordinates:\n  * chain        (chain) int64 32B 0 1 2 3\n  * draw         (draw) int64 8kB 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\n  * __obs__      (__obs__) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    rt,response  (chain, draw, __obs__) float64 32MB -1.526 -0.8813 ... -0.7195\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 1000</li><li>__obs__: 1000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-1.526 -0.8813 ... -2.016 -0.7195<pre>array([[[-1.5263464 , -0.88130111, -3.56682299, ..., -1.37628693,\n         -2.04926568, -0.66044353],\n        [-1.53621679, -0.89482452, -3.53872102, ..., -1.38732161,\n         -2.04040227, -0.67243185],\n        [-1.47187256, -0.86375652, -3.56555197, ..., -1.32667997,\n         -2.07945531, -0.68990023],\n        ...,\n        [-1.54110206, -0.88310122, -3.80523901, ..., -1.38701829,\n         -2.03213688, -0.65555511],\n        [-1.54151185, -0.89468267, -3.70376879, ..., -1.39061462,\n         -2.04887091, -0.66828519],\n        [-1.46461527, -0.87984624, -3.33786483, ..., -1.32699842,\n         -2.05469777, -0.70676846]],\n\n       [[-1.47281678, -0.85122162, -3.61434964, ..., -1.32469025,\n         -2.15281121, -0.66637136],\n        [-1.52158078, -0.88463813, -3.66153867, ..., -1.37152555,\n         -2.02903449, -0.67938979],\n        [-1.48598737, -0.87352596, -3.5436905 , ..., -1.34208271,\n         -2.05513628, -0.67888957],\n...\n        [-1.46371405, -0.84942756, -3.69261704, ..., -1.31650082,\n         -2.06921228, -0.67533514],\n        [-1.47202485, -0.86090593, -3.56808482, ..., -1.32666422,\n         -2.09137197, -0.6809892 ],\n        [-1.42158368, -0.84791738, -3.38810055, ..., -1.28413355,\n         -2.07593879, -0.7018466 ]],\n\n       [[-1.43803545, -0.83650214, -3.59938446, ..., -1.29084304,\n         -2.13237396, -0.6952231 ],\n        [-1.44423662, -0.83594565, -3.48857089, ..., -1.29789813,\n         -2.20926616, -0.67229372],\n        [-1.40885634, -0.79977405, -3.77372992, ..., -1.25861679,\n         -2.12912981, -0.6656805 ],\n        ...,\n        [-1.48617113, -0.88603641, -3.53328853, ..., -1.34337145,\n         -2.05364672, -0.70984832],\n        [-1.46729611, -0.87038264, -3.65872817, ..., -1.32494731,\n         -2.06615952, -0.69455886],\n        [-1.44611806, -0.8659958 , -3.71585899, ..., -1.30510426,\n         -2.01596356, -0.71953736]]])</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 204kB\nDimensions:          (chain: 4, draw: 1000)\nCoordinates:\n  * chain            (chain) int64 32B 0 1 2 3\n  * draw             (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    acceptance_rate  (chain, draw) float64 32kB 0.9751 0.9826 ... 0.9353 0.9994\n    diverging        (chain, draw) bool 4kB False False False ... False False\n    energy           (chain, draw) float64 32kB 1.479e+03 1.48e+03 ... 1.478e+03\n    lp               (chain, draw) float64 32kB 1.478e+03 ... 1.478e+03\n    n_steps          (chain, draw) int64 32kB 15 7 15 15 31 15 ... 7 31 15 31 15\n    step_size        (chain, draw) float64 32kB 0.1867 0.1867 ... 0.2378 0.2378\n    tree_depth       (chain, draw) int64 32kB 4 3 4 4 5 4 3 4 ... 5 5 3 5 4 5 4\nAttributes:\n    created_at:                  2025-01-01T22:58:11.100235+00:00\n    arviz_version:               0.19.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 1000</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li></ul></li><li>Data variables: (7)<ul><li>acceptance_rate(chain, draw)float640.9751 0.9826 ... 0.9353 0.9994<pre>array([[0.97510844, 0.98264123, 0.97676201, ..., 0.99683191, 0.99429341,\n        0.87688701],\n       [0.884448  , 0.99270034, 0.99018676, ..., 0.88074251, 0.99079751,\n        0.86816597],\n       [0.88448935, 0.96377616, 0.96165767, ..., 0.99945114, 1.        ,\n        0.45194231],\n       [1.        , 0.87820733, 0.98985202, ..., 0.97507614, 0.93531421,\n        0.99936192]])</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])</pre></li><li>energy(chain, draw)float641.479e+03 1.48e+03 ... 1.478e+03<pre>array([[1479.4103251 , 1479.87112344, 1479.03155567, ..., 1484.44019313,\n        1481.58529691, 1481.95361   ],\n       [1481.20569902, 1481.45909204, 1478.68308869, ..., 1481.72093534,\n        1480.23596515, 1481.60210165],\n       [1478.57761754, 1479.21020346, 1481.70322917, ..., 1478.86277585,\n        1477.52391607, 1485.06857401],\n       [1481.38770897, 1480.96827621, 1481.25165606, ..., 1481.51697287,\n        1480.21725317, 1478.4484657 ]])</pre></li><li>lp(chain, draw)float641.478e+03 1.479e+03 ... 1.478e+03<pre>array([[1478.35277321, 1478.50455348, 1477.20733093, ..., 1479.54984468,\n        1479.2414713 , 1479.98556622],\n       [1478.58731433, 1478.08173145, 1477.53053524, ..., 1477.80344671,\n        1479.63354707, 1478.83927208],\n       [1478.09950757, 1478.98518196, 1478.90960457, ..., 1477.34920918,\n        1477.10385717, 1481.4516606 ],\n       [1479.66900873, 1479.70550224, 1480.67974257, ..., 1477.48807466,\n        1478.05506977, 1478.15904824]])</pre></li><li>n_steps(chain, draw)int6415 7 15 15 31 15 ... 7 31 15 31 15<pre>array([[15,  7, 15, ..., 31,  7, 15],\n       [ 7, 15, 31, ..., 27, 15, 31],\n       [19, 23, 31, ..., 15, 15,  7],\n       [ 7,  7,  7, ..., 15, 31, 15]])</pre></li><li>step_size(chain, draw)float640.1867 0.1867 ... 0.2378 0.2378<pre>array([[0.18665756, 0.18665756, 0.18665756, ..., 0.18665756, 0.18665756,\n        0.18665756],\n       [0.21640439, 0.21640439, 0.21640439, ..., 0.21640439, 0.21640439,\n        0.21640439],\n       [0.23939746, 0.23939746, 0.23939746, ..., 0.23939746, 0.23939746,\n        0.23939746],\n       [0.23780414, 0.23780414, 0.23780414, ..., 0.23780414, 0.23780414,\n        0.23780414]])</pre></li><li>tree_depth(chain, draw)int644 3 4 4 5 4 3 4 ... 5 5 5 3 5 4 5 4<pre>array([[4, 3, 4, ..., 5, 3, 4],\n       [3, 4, 5, ..., 5, 4, 5],\n       [5, 5, 5, ..., 4, 4, 3],\n       [3, 3, 3, ..., 4, 5, 4]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li></ul></li><li>Attributes: (4)created_at :2025-01-01T22:58:11.100235+00:00arviz_version :0.19.0modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 24kB\nDimensions:                  (__obs__: 1000, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 8kB 0 1 2 3 4 ... 996 997 998 999\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 16kB ...\nAttributes:\n    created_at:                  2025-01-01T22:58:11.101098+00:00\n    arviz_version:               0.19.0\n    inference_library:           numpyro\n    inference_library_version:   0.16.1\n    sampling_time:               104.771827\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 1000</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float642.258 1.0 1.583 ... -1.0 1.232 1.0<pre>array([[ 2.25767398,  1.        ],\n       [ 1.58293712,  1.        ],\n       [ 3.42535663,  1.        ],\n       ...,\n       [ 2.12007141,  1.        ],\n       [ 1.86002243, -1.        ],\n       [ 1.23192048,  1.        ]])</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-01-01T22:58:11.101098+00:00arviz_version :0.19.0inference_library :numpyroinference_library_version :0.16.1sampling_time :104.771827tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>angle_model.summary()\n</pre> angle_model.summary() Out[\u00a0]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat v 0.435 0.044 0.354 0.517 0.001 0.001 1903.0 2071.0 1.0 z 0.521 0.013 0.497 0.547 0.000 0.000 1772.0 2092.0 1.0 a 1.578 0.064 1.455 1.693 0.002 0.001 1336.0 1801.0 1.0 theta 0.341 0.028 0.288 0.396 0.001 0.001 1461.0 2105.0 1.0 t 0.487 0.029 0.435 0.541 0.001 0.001 1403.0 1668.0 1.0 In\u00a0[\u00a0]: Copied! <pre>angle_model.plot_trace();\n</pre> angle_model.plot_trace(); In\u00a0[\u00a0]: Copied! <pre># A Normal prior for `v` without explicit bounds\nparam_v = {\n    \"name\": \"v\",\n    \"prior\": {\n        \"name\": \"Normal\",\n        \"mu\": 0.0,\n        \"sigma\": 2.0,\n    },\n}\n\n# A Uniform prior for `a`. Using the `dict` function\nparam_a = hssm.Param(\n    \"a\",\n    prior=dict(\n        name=\"Uniform\",\n        lower=0.01,\n        upper=5,\n    ),\n    bounds=(0, np.inf),\n)\n\n# A Uniform prior for `z` over (0, 1) set using hssm.Prior.\n# bounds are not set, existing default bounds will be used\nparam_z = {\"name\": \"z\", \"prior\": hssm.Prior(\"Uniform\", lower=0.0, upper=1.0)}\n\n# A fixed value for t\nparam_t = {\"name\": \"t\", \"prior\": 0.5}\n\nexample_ddm_model = hssm.HSSM(\n    data=dataset,\n    model=\"ddm\",\n    include=[\n        param_v,\n        param_a,\n        param_z,\n        param_t,\n    ],\n)\n\nexample_ddm_model\n</pre> # A Normal prior for `v` without explicit bounds param_v = {     \"name\": \"v\",     \"prior\": {         \"name\": \"Normal\",         \"mu\": 0.0,         \"sigma\": 2.0,     }, }  # A Uniform prior for `a`. Using the `dict` function param_a = hssm.Param(     \"a\",     prior=dict(         name=\"Uniform\",         lower=0.01,         upper=5,     ),     bounds=(0, np.inf), )  # A Uniform prior for `z` over (0, 1) set using hssm.Prior. # bounds are not set, existing default bounds will be used param_z = {\"name\": \"z\", \"prior\": hssm.Prior(\"Uniform\", lower=0.0, upper=1.0)}  # A fixed value for t param_t = {\"name\": \"t\", \"prior\": 0.5}  example_ddm_model = hssm.HSSM(     data=dataset,     model=\"ddm\",     include=[         param_v,         param_a,         param_z,         param_t,     ], )  example_ddm_model <pre>Model initialized successfully.\n</pre> Out[\u00a0]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 1000\n\nParameters:\n\nv:\n    Prior: Normal(mu: 0.0, sigma: 2.0)\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: Uniform(lower: 0.01, upper: 5.0)\n    Explicit bounds: (0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: 0.5\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[\u00a0]: Copied! <pre>example_ddm_model.sample()\n</pre> example_ddm_model.sample() <pre>Using default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [z, a, v]\n/Users/afengler/miniconda3/envs/hssm519/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n</pre> <pre>Output()</pre> <pre>/Users/afengler/miniconda3/envs/hssm519/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n</pre> <pre></pre> <pre>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 12 seconds.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4000/4000 [00:01&lt;00:00, 2229.85it/s]\n</pre> Out[\u00a0]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 104kB\nDimensions:  (chain: 4, draw: 1000)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\nData variables:\n    z        (chain, draw) float64 32kB 0.4848 0.4887 0.4816 ... 0.4839 0.4778\n    v        (chain, draw) float64 32kB 0.6765 0.6443 0.6463 ... 0.6343 0.693\n    a        (chain, draw) float64 32kB 1.478 1.5 1.517 ... 1.498 1.478 1.484\nAttributes:\n    created_at:                  2025-01-01T22:58:30.612048+00:00\n    arviz_version:               0.19.0\n    inference_library:           pymc\n    inference_library_version:   5.19.1\n    sampling_time:               12.191963911056519\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 1000</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li></ul></li><li>Data variables: (3)<ul><li>z(chain, draw)float640.4848 0.4887 ... 0.4839 0.4778<pre>array([[0.48478111, 0.48867193, 0.48158051, ..., 0.48842231, 0.48795669,\n        0.49800552],\n       [0.47765371, 0.47432879, 0.46462595, ..., 0.503417  , 0.50125552,\n        0.46361682],\n       [0.47576172, 0.48135687, 0.46892544, ..., 0.47479588, 0.47373892,\n        0.47934485],\n       [0.4690884 , 0.47119327, 0.48064803, ..., 0.4846411 , 0.48388615,\n        0.47782361]])</pre></li><li>v(chain, draw)float640.6765 0.6443 ... 0.6343 0.693<pre>array([[0.67651654, 0.64427576, 0.64629733, ..., 0.62427287, 0.60245098,\n        0.60693231],\n       [0.62965382, 0.69835934, 0.68036712, ..., 0.60741109, 0.61406297,\n        0.67716866],\n       [0.67167341, 0.66556241, 0.64739995, ..., 0.64453554, 0.65204798,\n        0.57251589],\n       [0.63572807, 0.65794548, 0.65415691, ..., 0.65955584, 0.63431248,\n        0.6930075 ]])</pre></li><li>a(chain, draw)float641.478 1.5 1.517 ... 1.478 1.484<pre>array([[1.47815763, 1.50002678, 1.51663176, ..., 1.5185945 , 1.47663483,\n        1.52016029],\n       [1.50480679, 1.49451961, 1.46435803, ..., 1.51466189, 1.51061405,\n        1.48989539],\n       [1.46646853, 1.46552424, 1.47116325, ..., 1.49060433, 1.4557163 ,\n        1.42190131],\n       [1.47553895, 1.47338289, 1.51185522, ..., 1.49790659, 1.47829934,\n        1.48424851]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li></ul></li><li>Attributes: (8)created_at :2025-01-01T22:58:30.612048+00:00arviz_version :0.19.0inference_library :pymcinference_library_version :5.19.1sampling_time :12.191963911056519tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 32MB\nDimensions:      (chain: 4, draw: 1000, __obs__: 1000)\nCoordinates:\n  * chain        (chain) int64 32B 0 1 2 3\n  * draw         (draw) int64 8kB 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\n  * __obs__      (__obs__) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    rt,response  (chain, draw, __obs__) float64 32MB -1.349 -3.008 ... -2.932\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 1000</li><li>__obs__: 1000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-1.349 -3.008 ... -0.8029 -2.932<pre>array([[[-1.34942122, -3.0077885 , -1.79818589, ..., -1.1664335 ,\n         -0.79613985, -2.94070719],\n        [-1.35677124, -2.93928211, -1.78499511, ..., -1.18224541,\n         -0.84054128, -2.87516535],\n        [-1.34200015, -2.89628317, -1.75966379, ..., -1.17394632,\n         -0.8757488 , -2.83297677],\n        ...,\n        [-1.35844493, -2.88607231, -1.77085615, ..., -1.19118399,\n         -0.88167865, -2.82401926],\n        [-1.38767168, -2.9520277 , -1.81126002, ..., -1.21461643,\n         -0.86566068, -2.88867384],\n        [-1.37536091, -2.88613622, -1.78612712, ..., -1.20678729,\n         -0.87404217, -2.82504703]],\n\n       [[-1.35242211, -2.89957674, -1.76726923, ..., -1.18603036,\n         -0.89408268, -2.83647255],\n        [-1.32129729, -2.97727641, -1.76571462, ..., -1.14281902,\n         -0.81700506, -2.90990293],\n        [-1.33559923, -3.0085462 , -1.78307198, ..., -1.15691385,\n         -0.83348953, -2.94039217],\n...\n        [-1.34855928, -2.93584534, -1.77416511, ..., -1.1777941 ,\n         -0.87095449, -2.87115738],\n        [-1.36111593, -3.01545329, -1.80631922, ..., -1.18126108,\n         -0.83042649, -2.94831122],\n        [-1.42320092, -3.03852041, -1.86015122, ..., -1.24484906,\n         -0.86909545, -2.9731816 ]],\n\n       [[-1.35539317, -2.95139855, -1.78229422, ..., -1.18481232,\n         -0.88185386, -2.88627851],\n        [-1.34709094, -2.97811407, -1.78442303, ..., -1.17160286,\n         -0.84873658, -2.91171154],\n        [-1.33930387, -2.91080485, -1.76160397, ..., -1.16934529,\n         -0.8648158 , -2.84681835],\n        ...,\n        [-1.34677169, -2.95029375, -1.77965352, ..., -1.17107039,\n         -0.83325542, -2.88524662],\n        [-1.36730433, -2.96727268, -1.79954851, ..., -1.19147629,\n         -0.84381578, -2.90241478],\n        [-1.33227266, -2.99945365, -1.78123928, ..., -1.15081778,\n         -0.8029431 , -2.93180318]]])</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 496kB\nDimensions:                (chain: 4, draw: 1000)\nCoordinates:\n  * chain                  (chain) int64 32B 0 1 2 3\n  * draw                   (draw) int64 8kB 0 1 2 3 4 5 ... 995 996 997 998 999\nData variables: (12/17)\n    acceptance_rate        (chain, draw) float64 32kB 0.4948 0.8589 ... 0.6956\n    diverging              (chain, draw) bool 4kB False False ... False False\n    energy                 (chain, draw) float64 32kB 1.891e+03 ... 1.893e+03\n    energy_error           (chain, draw) float64 32kB 0.7037 -0.6844 ... 0.5542\n    index_in_trajectory    (chain, draw) int64 32kB 1 5 5 -2 2 1 ... 3 3 -2 -4 1\n    largest_eigval         (chain, draw) float64 32kB nan nan nan ... nan nan\n    ...                     ...\n    process_time_diff      (chain, draw) float64 32kB 0.001533 ... 0.005752\n    reached_max_treedepth  (chain, draw) bool 4kB False False ... False False\n    smallest_eigval        (chain, draw) float64 32kB nan nan nan ... nan nan\n    step_size              (chain, draw) float64 32kB 0.9622 0.9622 ... 0.9208\n    step_size_bar          (chain, draw) float64 32kB 0.7741 0.7741 ... 0.7759\n    tree_depth             (chain, draw) int64 32kB 1 3 3 2 2 3 ... 2 3 3 2 3 3\nAttributes:\n    created_at:                  2025-01-01T22:58:30.624651+00:00\n    arviz_version:               0.19.0\n    inference_library:           pymc\n    inference_library_version:   5.19.1\n    sampling_time:               12.191963911056519\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 1000</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li></ul></li><li>Data variables: (17)<ul><li>acceptance_rate(chain, draw)float640.4948 0.8589 ... 0.9874 0.6956<pre>array([[0.49475785, 0.85885697, 0.65227334, ..., 0.89256091, 0.95842768,\n        0.95714357],\n       [0.97872149, 0.76264086, 0.94950625, ..., 1.        , 1.        ,\n        0.82946638],\n       [0.80688555, 0.87490536, 0.9960673 , ..., 0.99764115, 0.79799319,\n        0.50384932],\n       [1.        , 0.66449315, 0.84414886, ..., 0.97869631, 0.98743857,\n        0.69557076]])</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])</pre></li><li>energy(chain, draw)float641.891e+03 1.894e+03 ... 1.893e+03<pre>array([[1891.34592295, 1893.96485634, 1892.5359328 , ..., 1891.42956253,\n        1890.90710325, 1890.82403986],\n       [1892.85744573, 1891.75979357, 1893.30611705, ..., 1892.38822633,\n        1890.69861376, 1892.71807261],\n       [1891.25603303, 1891.63845908, 1890.73099438, ..., 1889.76207184,\n        1891.76025384, 1899.290754  ],\n       [1890.67815305, 1891.69361162, 1891.90544757, ..., 1890.05665818,\n        1889.98916031, 1893.39448176]])</pre></li><li>energy_error(chain, draw)float640.7037 -0.6844 ... -0.08118 0.5542<pre>array([[ 0.70368682, -0.68441123,  0.0388633 , ..., -0.43501691,\n        -0.07578602,  0.06131132],\n       [-0.08923795,  0.19607314, -0.20394189, ..., -0.21645336,\n        -0.01018915,  0.00268384],\n       [ 0.38258471,  0.23043112, -0.54863788, ..., -0.03028182,\n         0.19692842,  0.62575557],\n       [-0.08811837, -0.14616255,  0.01151003, ...,  0.03578194,\n        -0.08117874,  0.55420638]])</pre></li><li>index_in_trajectory(chain, draw)int641 5 5 -2 2 1 3 ... -3 3 3 -2 -4 1<pre>array([[ 1,  5,  5, ..., -3,  4, -3],\n       [-4,  2,  3, ..., -1, -1,  2],\n       [ 3,  1,  1, ..., -2, -3,  6],\n       [-2,  2,  1, ..., -2, -4,  1]])</pre></li><li>largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])</pre></li><li>lp(chain, draw)float64-1.891e+03 ... -1.891e+03<pre>array([[-1891.2628207 , -1889.4433111 , -1889.80345045, ...,\n        -1889.89080687, -1889.75602354, -1890.37061305],\n       [-1889.82133977, -1891.08012439, -1891.12639651, ...,\n        -1890.62675488, -1890.30492845, -1890.5929553 ],\n       [-1890.70968764, -1890.94340213, -1890.05927179, ...,\n        -1889.41513938, -1890.68721931, -1895.47329994],\n       [-1890.1569686 , -1889.87447537, -1889.64622246, ...,\n        -1889.62913956, -1889.36513319, -1891.27096872]])</pre></li><li>max_energy_error(chain, draw)float640.7037 -0.6844 ... -0.1414 0.5542<pre>array([[ 0.70368682, -0.68441123,  0.75288501, ..., -0.43501691,\n         0.10723298,  0.06131132],\n       [-0.33646798,  0.47820015, -0.20394189, ..., -0.21645336,\n        -0.01018915,  0.43125799],\n       [ 0.44083245,  0.23043112, -0.62185405, ..., -0.03028182,\n         0.28450806,  1.41850286],\n       [-0.08811837,  0.89570219,  0.26100078, ...,  0.03578194,\n        -0.14138724,  0.55420638]])</pre></li><li>n_steps(chain, draw)float641.0 7.0 7.0 3.0 ... 7.0 3.0 7.0 7.0<pre>array([[1., 7., 7., ..., 3., 7., 3.],\n       [7., 7., 3., ..., 3., 1., 7.],\n       [7., 3., 3., ..., 3., 3., 7.],\n       [3., 3., 3., ..., 3., 7., 7.]])</pre></li><li>perf_counter_diff(chain, draw)float640.001562 0.005938 ... 0.00576<pre>array([[0.00156196, 0.00593758, 0.00629171, ..., 0.00286142, 0.00571362,\n        0.00290638],\n       [0.00617704, 0.00651667, 0.00299117, ..., 0.00288792, 0.00145817,\n        0.00594954],\n       [0.00763033, 0.00299279, 0.00313842, ..., 0.00289492, 0.00289946,\n        0.00645933],\n       [0.00299542, 0.00297325, 0.00296854, ..., 0.00293567, 0.00597246,\n        0.00576033]])</pre></li><li>perf_counter_start(chain, draw)float643.374e+05 3.374e+05 ... 3.374e+05<pre>array([[337406.84360979, 337406.84524258, 337406.85123946, ...,\n        337411.36189429, 337411.36479192, 337411.37053983],\n       [337407.02570733, 337407.03204854, 337407.0386305 , ...,\n        337411.25437275, 337411.25729813, 337411.25878883],\n       [337406.40145921, 337406.40915775, 337406.41235475, ...,\n        337410.87890275, 337410.88183492, 337410.88477096],\n       [337406.62446062, 337406.62751708, 337406.63055087, ...,\n        337411.10644129, 337411.10942008, 337411.11543754]])</pre></li><li>process_time_diff(chain, draw)float640.001533 0.005937 ... 0.005752<pre>array([[0.001533, 0.005937, 0.006196, ..., 0.002861, 0.005713, 0.002897],\n       [0.006089, 0.006156, 0.002984, ..., 0.002888, 0.001457, 0.005947],\n       [0.006134, 0.002993, 0.003116, ..., 0.002895, 0.0029  , 0.006187],\n       [0.00298 , 0.002956, 0.002957, ..., 0.002936, 0.005957, 0.005752]])</pre></li><li>reached_max_treedepth(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])</pre></li><li>smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])</pre></li><li>step_size(chain, draw)float640.9622 0.9622 ... 0.9208 0.9208<pre>array([[0.96218765, 0.96218765, 0.96218765, ..., 0.96218765, 0.96218765,\n        0.96218765],\n       [0.97287712, 0.97287712, 0.97287712, ..., 0.97287712, 0.97287712,\n        0.97287712],\n       [0.76443892, 0.76443892, 0.76443892, ..., 0.76443892, 0.76443892,\n        0.76443892],\n       [0.92084443, 0.92084443, 0.92084443, ..., 0.92084443, 0.92084443,\n        0.92084443]])</pre></li><li>step_size_bar(chain, draw)float640.7741 0.7741 ... 0.7759 0.7759<pre>array([[0.77413264, 0.77413264, 0.77413264, ..., 0.77413264, 0.77413264,\n        0.77413264],\n       [0.75414619, 0.75414619, 0.75414619, ..., 0.75414619, 0.75414619,\n        0.75414619],\n       [0.75586516, 0.75586516, 0.75586516, ..., 0.75586516, 0.75586516,\n        0.75586516],\n       [0.77589918, 0.77589918, 0.77589918, ..., 0.77589918, 0.77589918,\n        0.77589918]])</pre></li><li>tree_depth(chain, draw)int641 3 3 2 2 3 3 2 ... 2 2 2 3 3 2 3 3<pre>array([[1, 3, 3, ..., 2, 3, 2],\n       [3, 3, 2, ..., 2, 1, 3],\n       [3, 2, 2, ..., 2, 2, 3],\n       [2, 2, 2, ..., 2, 3, 3]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li></ul></li><li>Attributes: (8)created_at :2025-01-01T22:58:30.624651+00:00arviz_version :0.19.0inference_library :pymcinference_library_version :5.19.1sampling_time :12.191963911056519tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 24kB\nDimensions:                  (__obs__: 1000, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 8kB 0 1 2 3 4 ... 996 997 998 999\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 16kB ...\nAttributes:\n    created_at:                  2025-01-01T22:58:30.627251+00:00\n    arviz_version:               0.19.0\n    inference_library:           pymc\n    inference_library_version:   5.19.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 1000</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float642.139 1.0 4.299 ... 1.0 4.21 1.0<pre>array([[2.13852501, 1.        ],\n       [4.29881287, 1.        ],\n       [2.72272325, 1.        ],\n       ...,\n       [1.89326012, 1.        ],\n       [1.18497074, 1.        ],\n       [4.20985746, 1.        ]])</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (6)created_at :2025-01-01T22:58:30.627251+00:00arviz_version :0.19.0inference_library :pymcinference_library_version :5.19.1modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>example_ddm_model.summary()\n</pre> example_ddm_model.summary() Out[\u00a0]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat z 0.484 0.012 0.460 0.506 0.000 0.0 2194.0 2510.0 1.0 v 0.633 0.033 0.567 0.694 0.001 0.0 2384.0 2783.0 1.0 a 1.493 0.023 1.451 1.537 0.000 0.0 3163.0 3002.0 1.0 In\u00a0[\u00a0]: Copied! <pre># All ways to specify priors mentioned above are supported in the shortcut syntax\nshortcut_ddm_model = hssm.HSSM(\n    data=dataset,\n    model=\"ddm\",\n    v={\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 2.0},\n    a=dict(name=\"Uniform\", lower=0.01, upper=5),\n    z=hssm.Prior(\"Uniform\", lower=0.01, upper=1.0),\n    t=0.5,\n)\n\nshortcut_ddm_model\n</pre> # All ways to specify priors mentioned above are supported in the shortcut syntax shortcut_ddm_model = hssm.HSSM(     data=dataset,     model=\"ddm\",     v={\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 2.0},     a=dict(name=\"Uniform\", lower=0.01, upper=5),     z=hssm.Prior(\"Uniform\", lower=0.01, upper=1.0),     t=0.5, )  shortcut_ddm_model <pre>Model initialized successfully.\n</pre> Out[\u00a0]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 1000\n\nParameters:\n\nv:\n    Prior: Normal(mu: 0.0, sigma: 2.0)\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: Uniform(lower: 0.01, upper: 5.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.01, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: 0.5\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> <p>Note that the shortcut syntax also supports specifying bounds. It will be more polished in a future update. We will skip this step for now.</p> In\u00a0[\u00a0]: Copied! <pre># Generate simulated data\n\nintercept = 1.5\nx = np.random.uniform(-5.0, 5.0, size=1000)\ny = np.random.uniform(-5.0, 5.0, size=1000)\n\nv = intercept + 0.8 * x + 0.3 * y\n</pre> # Generate simulated data  intercept = 1.5 x = np.random.uniform(-5.0, 5.0, size=1000) y = np.random.uniform(-5.0, 5.0, size=1000)  v = intercept + 0.8 * x + 0.3 * y In\u00a0[\u00a0]: Copied! <pre>true_values = np.column_stack([v, np.repeat([[1.5, 0.5, 0.5]], axis=0, repeats=1000)])\ntrue_values.shape\n</pre> true_values = np.column_stack([v, np.repeat([[1.5, 0.5, 0.5]], axis=0, repeats=1000)]) true_values.shape Out[\u00a0]: <pre>(1000, 4)</pre> In\u00a0[\u00a0]: Copied! <pre>dataset_reg_v = hssm.simulate_data(\n    model=\"ddm\",\n    theta=true_values,\n    size=1,  # Generate one data point for each of the 1000 set of true values\n)\n\ndataset_reg_v[\"x\"] = x\ndataset_reg_v[\"y\"] = y\n\ndataset_reg_v\n</pre> dataset_reg_v = hssm.simulate_data(     model=\"ddm\",     theta=true_values,     size=1,  # Generate one data point for each of the 1000 set of true values )  dataset_reg_v[\"x\"] = x dataset_reg_v[\"y\"] = y  dataset_reg_v Out[\u00a0]: rt response x y 0 0.740289 1.0 1.819151 -4.117494 1 0.850780 1.0 2.491206 4.171911 2 0.843184 1.0 2.147904 -2.180605 3 0.833531 1.0 2.570652 -0.476937 4 0.997305 -1.0 -2.800995 -1.066227 ... ... ... ... ... 995 1.523809 1.0 -0.781649 3.039424 996 1.733002 1.0 -0.322806 2.131486 997 1.454640 1.0 1.452501 -3.462405 998 2.021644 1.0 0.933907 -4.275842 999 1.974865 1.0 -2.507812 4.814151 <p>1000 rows \u00d7 4 columns</p> In\u00a0[\u00a0]: Copied! <pre>model_reg_v = hssm.HSSM(\n    data=dataset_reg_v,\n    include=[\n        {\n            \"name\": \"v\",\n            \"formula\": \"v ~ 1 + x + y\",\n            \"prior\": {\n                \"Intercept\": {\"name\": \"Uniform\", \"lower\": -0.001, \"upper\": 0.5},\n                \"x\": dict(name=\"Uniform\", lower=0.0, upper=1.0),\n                \"y\": hssm.Prior(\"Uniform\", lower=0.0, upper=1.0),\n            },\n            \"link\": \"identity\",\n        }\n    ],\n)\nmodel_reg_v\n</pre> model_reg_v = hssm.HSSM(     data=dataset_reg_v,     include=[         {             \"name\": \"v\",             \"formula\": \"v ~ 1 + x + y\",             \"prior\": {                 \"Intercept\": {\"name\": \"Uniform\", \"lower\": -0.001, \"upper\": 0.5},                 \"x\": dict(name=\"Uniform\", lower=0.0, upper=1.0),                 \"y\": hssm.Prior(\"Uniform\", lower=0.0, upper=1.0),             },             \"link\": \"identity\",         }     ], ) model_reg_v <pre>Model initialized successfully.\n</pre> Out[\u00a0]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 1000\n\nParameters:\n\nv:\n    Formula: v ~ 1 + x + y\n    Priors:\n        v_Intercept ~ Uniform(lower: -0.001, upper: 0.5)\n        v_x ~ Uniform(lower: 0.0, upper: 1.0)\n        v_y ~ Uniform(lower: 0.0, upper: 1.0)\n    Link: identity\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[\u00a0]: Copied! <pre>model_reg_v.initvals\n</pre> model_reg_v.initvals Out[\u00a0]: <pre>{'z': array(0.5),\n 't': array(0.025),\n 'a': array(1.5),\n 'v_Intercept': array(0.),\n 'v_x': array(0.5),\n 'v_y': array(0.5)}</pre> In\u00a0[\u00a0]: Copied! <pre># Uncomment to see model graph if you have graphviz installed\nmodel_reg_v.graph()\n</pre> # Uncomment to see model graph if you have graphviz installed model_reg_v.graph() Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre>trace_reg_v = model_reg_v.sample()\n</pre> trace_reg_v = model_reg_v.sample() <pre>Using default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [z, t, a, v_Intercept, v_x, v_y]\n/Users/afengler/miniconda3/envs/hssm519/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n</pre> <pre>Output()</pre> <pre>/Users/afengler/miniconda3/envs/hssm519/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n</pre> <pre></pre> <pre>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 25 seconds.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4000/4000 [00:01&lt;00:00, 2687.22it/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre># Looks like parameter recovery was successful\nmodel_reg_v.summary()\n</pre> # Looks like parameter recovery was successful model_reg_v.summary() Out[\u00a0]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat v_Intercept 0.548 0.004 0.540 0.554 0.000 0.0 4540.0 3515.0 1.0 v_x 0.661 0.020 0.626 0.699 0.000 0.0 3951.0 3240.0 1.0 z 0.616 0.008 0.600 0.632 0.000 0.0 4129.0 3041.0 1.0 a 1.315 0.032 1.259 1.379 0.001 0.0 3679.0 2938.0 1.0 v_y 0.271 0.015 0.243 0.299 0.000 0.0 4134.0 3066.0 1.0 t 0.554 0.008 0.538 0.568 0.000 0.0 3413.0 2848.0 1.0 In\u00a0[\u00a0]: Copied! <pre>model_reg_v.plot_trace();\n</pre> model_reg_v.plot_trace();"},{"location":"getting_started/getting_started/#getting-started","title":"Getting Started\u00b6","text":"<p>This tutorial demonstrates how to quickly get started with the HSSM package. We will cover the following steps:</p> <ul> <li>How to create a model</li> <li>How to create some simple simulated data</li> <li>How to specify parameters</li> <li>How to specify parameters with regressions</li> <li>How to use ArviZ to summarize and visualize the traces.</li> </ul>"},{"location":"getting_started/getting_started/#colab-instructions","title":"Colab Instructions\u00b6","text":"<p>If you would like to run this tutorial on Google colab, please click this link.</p> <p>Once you are in the colab, follow the installation instructions below and then restart your runtime.</p> <p>Just uncomment the code in the next code cell and run it!</p> <p>NOTE:</p> <p>You may want to switch your runtime to have a GPU or TPU. To do so, go to Runtime &gt; Change runtime type and select the desired hardware accelerator.</p> <p>Note that if you switch your runtime you have to follow the installation instructions again.</p>"},{"location":"getting_started/getting_started/#import-modules","title":"Import Modules\u00b6","text":""},{"location":"getting_started/getting_started/#simulating-a-dataset","title":"Simulating a dataset\u00b6","text":"<p>The <code>hssm.simulate_data()</code> function generates data for most SSM types. Here we simulate some data from a Drift Diffusion Model (DDM) with known true parameter values.</p>"},{"location":"getting_started/getting_started/#model-specification","title":"Model specification\u00b6","text":""},{"location":"getting_started/getting_started/#1-ddm-using-defaults","title":"1. DDM using defaults\u00b6","text":"<p>We begin with a simple example. The only information required to create a model in <code>HSSM</code> is a dataset.</p> <p>A dataset in <code>HSSM</code> is typically a <code>pandas</code> <code>DataFrame</code> with at least <code>rt</code> and <code>response</code> columns, which indicates response time and choices respectively. Right now, <code>response</code> only accepts values of <code>1</code> and <code>-1</code>.</p> <p>If none of the optional parameters is provided, HSSM will assume that we are modeling a classical DDM model with <code>v</code>, <code>a</code>, <code>z</code>, and <code>t</code> as its parameters. HSSM also provides a default analytical likelihood function and some uninformative priors. These can all be overriden by user inputs.</p> <p>Note</p> <p>     From HSSM v0.1.2 on, lapse distributions will be enabled by default, with `p_outlier` fixed to 0.05. You can set `p_outlier` to 0 or `None` to disable lapse distributions.    </p>"},{"location":"getting_started/getting_started/#visualizing-the-model","title":"Visualizing the model\u00b6","text":"<p>If you have <code>graphviz</code> installed on your machine, you will also be able to visualize the model. Please uncomment the code and run it if you have <code>graphviz</code> installed.</p>"},{"location":"getting_started/getting_started/#performing-mcmc-sampling","title":"Performing MCMC sampling\u00b6","text":"<p>Similar to <code>PyMC</code>, HSSM provides the <code>sample()</code> method once the model is created to perform MCMC sampling. By default, it uses <code>PyMC</code>'s <code>NUTS</code> sampler. We can use other samplers, which we will cover soon.</p>"},{"location":"getting_started/getting_started/#visualizing-the-traces-with-arviz","title":"Visualizing the traces with <code>ArviZ</code>\u00b6","text":"<p>Like that of <code>pm.sample()</code>, the result of <code>model.sample()</code> is also an <code>az.InferenceData</code> object, which can be used with the <code>ArviZ</code> package. The last sample the model has performed is stored in the <code>model.traces</code> property for eazy access. Here we use the <code>az.summary()</code> and <code>az.plot_trace()</code> functions to generate a summary table and diagnostic plots for the samples.</p>"},{"location":"getting_started/getting_started/#2-specifying-different-model-types","title":"2. Specifying different model types\u00b6","text":"<p>The default model in HSSM is the classic DDM, but HSSM supports many other model types. Below is a full list of supported models as of <code>hssm</code> v0.2.5. New models may be added in future updates.</p> <ul> <li>ddm</li> <li>ddm_sdv</li> <li>full_ddm</li> <li>angle</li> <li>levy</li> <li>ornstein</li> <li>weibull</li> <li>race_no_bias_angle_4</li> <li>ddm_seq2_no_bias</li> </ul> <p>For more information about these models, please see here.</p> <p>HSSM has default specifications for the models above (such as response variables, likelihoods, choices, bounds, and others); no further specification is necessary.</p> <p>For custom type modeling, please use the <code>model</code> parameter of the HSSM class constructor (tutorials available here and here).</p> <p>Below is an example of specifying the <code>angle</code> model type.</p>"},{"location":"getting_started/getting_started/#3-specifying-priors-the-non-regression-case","title":"3. Specifying priors: the non-regression case\u00b6","text":"<p>Next, let's take a look at how to specify priors in the non-regression case. In HSSM, parameter specification can be done in two ways:</p> <ol> <li>through the <code>include</code> parameter, or</li> <li>through a shortcut</li> </ol>"},{"location":"getting_started/getting_started/#31-specifying-priors-through-the-include-parameter","title":"3.1 Specifying priors through the <code>include</code> parameter\u00b6","text":"<p>The <code>include</code> parameter accepts a list of dictionaries or <code>hssm.Param</code> objects. Both dictionaries and <code>hssm.Param</code> objects are equivalent, since the content of the dictionary will be passed as parameters to the <code>hssm.Param</code> class during model creation, so it is more of a matter of preference. We recommend the <code>hssm.Param</code> object because some IDEs will be able to provide prompts for possible options of parameters. In the non-regression case, each dictionary typically looks like this:</p> <pre>{\n    \"name\": \"v\",\n    \"prior\": {\n        \"name\": \"Uniform\",\n        \"lower\": -5.0,\n        \"upper\": 5.0,\n    },\n    \"bounds\": (-10.0, 10.0)\n}\n</pre> <p>This is equivalent to writing:</p> <pre>hssm.Param(\n    \"v\",\n    prior=dict(name=\"Uniform\", upper=-5.0, lower=5.0),\n    bounds=(-10.0, 10.0)\n)\n</pre> <p>The <code>name</code> field corresponds to the name of the parameter being specified.</p> <p>The <code>prior</code> field specifies the distribution of the prior. There are two ways to achieve this:</p> <ol> <li>A dictionary with the <code>name</code> of the distribution (typically captalized) and the parameters of the distribution that you would typically set if you were specifying a distribution in <code>PyMC</code>. For example, if you would like to specify <code>pm.Normal(mu=0.0, sigma=1.0)</code> as the prior, then in <code>HSSM</code>, this prior dictionary would be:</li> </ol> <pre>{\n    \"name\": \"Normal\", ## Note it is capitalized\n    \"mu\": 0.0,\n    \"sigma\": 1.0,\n}\n</pre> <p>or, using the <code>dict</code> constructor:</p> <pre>dict(name=\"Normal\", mu=0.0, sigma=1.0)\n</pre> <ol> <li>A <code>hssm.Prior</code> object. This is exactly how you would specify priors using <code>bambi</code> (In fact, <code>hssm.Prior</code> is a subclass of <code>bmb.Prior</code> and for the most part can be used interchangeably with <code>bmb.Prior</code>). To specify the same normal prior as above, you would write:</li> </ol> <pre>hssm.Prior(\"Normal\", mu=0.0, sigma=1.0)\n</pre> <p>The <code>bounds</code> field accepts a tuple of floats, indicating the lower and upper bounds for the parameter.</p> <p>Fixing parameters: sometimes you might want to fix the values of a parameter. You can easily do so by specifying that value to the <code>prior</code> field of the dictionary. In the following example, the paramter <code>v</code> is fixed to <code>0.5</code>.</p> <pre>{\n    \"name\": \"v\",\n    \"prior\": 0.5,\n}\n</pre> <p>Now let's make this concrete with an example:</p>"},{"location":"getting_started/getting_started/#32-specifying-priors-using-the-shortcut","title":"3.2 Specifying priors using the shortcut\u00b6","text":"<p>HSSM also supports a syntax very similar to <code>PyMC</code>: You can directly specify priors by passing the prior to the name of the parameter in <code>hssm.HSSM</code>. This is convenient if the prior is simple. Below is an example almost equivalent to the above example:</p>"},{"location":"getting_started/getting_started/#4-specifying-priors-the-regression-case","title":"4. Specifying priors: the regression case\u00b6","text":"<p>Built on top of <code>bambi</code>, HSSM uses an <code>lmer</code>-like syntax that makes it extremely straight-forward to specify regressions.</p> <p>Parameters that are targets of regressions are also specified using dictionaries in <code>include</code>. Below is an example for such dictionaries.</p> <pre>{\n    \"name\": \"v\",\n    \"formula\": \"v ~ 1 + x + y\",\n    \"prior\": {\n        # All ways to specify priors in the non-regression case\n        # work the same way here.\n        \"Intercept\": {\"name\": \"Uniform\", \"lower\": -10.0, \"upper\": 10.0},\n        \"x\": dict(name=\"Normal\", mu=0, sigma=1),\n        \"y\": hssm.Prior(\"HalfNormal\", sigma=0.5),\n        \"z\": 1.0\n    }\n    \"link\": \"identity\",\n    \"bounds\": (-10.0, 10.0)\n}\n</pre> <p>We see that in the regression case, <code>name</code> and <code>bounds</code> are specified the exact same way as in the non-regression case. The regression formula is specified in a way that's very similar to the <code>lmer</code> package in R. Users that have experience with R formulas should be very familar with this syntax. In this case, the formula means that the parameter <code>v</code> is regressed on variables <code>x</code> and <code>y</code>, which can be found in the dataframe passed to <code>hssm.HSSM</code>. The <code>1</code> explicitly specifies an intercept for the regression.</p> <p>In addition to the <code>formula</code>, users typically need to specify priors for the regression coefficients. This is done in the <code>prior</code> field of the dictionary. Instead of specifying priors for the parameter, the priors are now specified for the corresponding regression coefficients. If not specified, HSSM will use default priors generated in Bambi.</p> <p>Users might also want to specify a <code>link</code> function for generalized linear models. If left unspecified, the <code>identity</code> link function will be used.</p> <p>Now let's see an example of a regression:</p>"},{"location":"getting_started/hierarchical_modeling/","title":"Hierarchical Modeling","text":"In\u00a0[\u00a0]: Copied! <pre># !pip install hssm\n</pre> # !pip install hssm In\u00a0[\u00a0]: Copied! <pre>import hssm\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</pre> import hssm  %matplotlib inline %config InlineBackend.figure_format='retina' In\u00a0[\u00a0]: Copied! <pre>hssm.set_floatX(\"float32\")\n</pre> hssm.set_floatX(\"float32\") <pre>Setting PyTensor floatX type to float32.\nSetting \"jax_enable_x64\" to False. If this is not intended, please set `jax` to False.\n</pre> In\u00a0[\u00a0]: Copied! <pre># Load a package-supplied dataset\ncav_data = hssm.load_data(\"cavanagh_theta\")\n</pre> # Load a package-supplied dataset cav_data = hssm.load_data(\"cavanagh_theta\") In\u00a0[\u00a0]: Copied! <pre># Define a basic non-hierarchical model\nmodel_non_hierarchical = hssm.HSSM(data=cav_data)\nmodel_non_hierarchical\n</pre> # Define a basic non-hierarchical model model_non_hierarchical = hssm.HSSM(data=cav_data) model_non_hierarchical <pre>Model initialized successfully.\n</pre> Out[\u00a0]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 3988\n\nParameters:\n\nv:\n    Prior: Normal(mu: 0.0, sigma: 2.0)\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[\u00a0]: Copied! <pre># Specifying a global formula\n# This is equivalent to setting `hierarchical` to True\nmodel_hierarchical = hssm.HSSM(\n    data=cav_data, global_formula=\"y ~ 1 + (1|participant_id)\", prior_settings=\"safe\"\n)\nmodel_hierarchical\n</pre> # Specifying a global formula # This is equivalent to setting `hierarchical` to True model_hierarchical = hssm.HSSM(     data=cav_data, global_formula=\"y ~ 1 + (1|participant_id)\", prior_settings=\"safe\" ) model_hierarchical <pre>Model initialized successfully.\n</pre> Out[\u00a0]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 3988\n\nParameters:\n\nv:\n    Formula: v ~ 1 + (1|participant_id)\n    Priors:\n        v_Intercept ~ Normal(mu: 2.0, sigma: 3.0)\n        v_1|participant_id ~ Normal(mu: 0.0, sigma: Weibull(alpha: 1.5, beta: 0.30000001192092896))\n    Link: identity\n    Explicit bounds: (-inf, inf)\n\na:\n    Formula: a ~ 1 + (1|participant_id)\n    Priors:\n        a_Intercept ~ Gamma(mu: 1.5, sigma: 0.75)\n        a_1|participant_id ~ Normal(mu: 0.0, sigma: Weibull(alpha: 1.5, beta: 0.30000001192092896))\n    Link: identity\n    Explicit bounds: (0.0, inf)\n\nz:\n    Formula: z ~ 1 + (1|participant_id)\n    Priors:\n        z_Intercept ~ Beta(alpha: 10.0, beta: 10.0)\n        z_1|participant_id ~ Normal(mu: 0.0, sigma: Weibull(alpha: 1.5, beta: 0.30000001192092896))\n    Link: identity\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Formula: t ~ 1 + (1|participant_id)\n    Priors:\n        t_Intercept ~ Gamma(mu: 0.20000000298023224, sigma: 0.20000000298023224)\n        t_1|participant_id ~ Normal(mu: 0.0, sigma: Weibull(alpha: 1.5, beta: 0.30000001192092896))\n    Link: identity\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[\u00a0]: Copied! <pre>model_safe = hssm.HSSM(\n    data=cav_data,\n    global_formula=\"y ~ 1 + (1|participant_id)\",\n    prior_settings=\"safe\",\n    loglik_kind=\"approx_differentiable\",\n)\nmodel_safe\n</pre> model_safe = hssm.HSSM(     data=cav_data,     global_formula=\"y ~ 1 + (1|participant_id)\",     prior_settings=\"safe\",     loglik_kind=\"approx_differentiable\", ) model_safe <pre>Model initialized successfully.\n</pre> Out[\u00a0]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: approx_differentiable\nObservations: 3988\n\nParameters:\n\nv:\n    Formula: v ~ 1 + (1|participant_id)\n    Priors:\n        v_Intercept ~ Normal(mu: 0.0, sigma: 0.25)\n        v_1|participant_id ~ Normal(mu: 0.0, sigma: Weibull(alpha: 1.5, beta: 0.30000001192092896))\n    Link: identity\n    Explicit bounds: (-3.0, 3.0)\n\na:\n    Formula: a ~ 1 + (1|participant_id)\n    Priors:\n        a_Intercept ~ Normal(mu: 1.399999976158142, sigma: 0.25)\n        a_1|participant_id ~ Normal(mu: 0.0, sigma: Weibull(alpha: 1.5, beta: 0.30000001192092896))\n    Link: identity\n    Explicit bounds: (0.3, 2.5)\n\nz:\n    Formula: z ~ 1 + (1|participant_id)\n    Priors:\n        z_Intercept ~ Normal(mu: 0.5, sigma: 0.25)\n        z_1|participant_id ~ Normal(mu: 0.0, sigma: Weibull(alpha: 1.5, beta: 0.30000001192092896))\n    Link: identity\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Formula: t ~ 1 + (1|participant_id)\n    Priors:\n        t_Intercept ~ Normal(mu: 1.0, sigma: 0.25)\n        t_1|participant_id ~ Normal(mu: 0.0, sigma: Weibull(alpha: 1.5, beta: 0.30000001192092896))\n    Link: identity\n    Explicit bounds: (0.0, 2.0)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[\u00a0]: Copied! <pre>model_safe_off = hssm.HSSM(\n    data=cav_data,\n    global_formula=\"y ~ 1 + (1|participant_id)\",\n    prior_settings=None,\n    loglik_kind=\"approx_differentiable\",\n)\nmodel_safe_off\n</pre> model_safe_off = hssm.HSSM(     data=cav_data,     global_formula=\"y ~ 1 + (1|participant_id)\",     prior_settings=None,     loglik_kind=\"approx_differentiable\", ) model_safe_off <pre>Model initialized successfully.\n</pre> Out[\u00a0]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: approx_differentiable\nObservations: 3988\n\nParameters:\n\nv:\n    Formula: v ~ 1 + (1|participant_id)\n    Priors:\n        v_Intercept ~ Normal(mu: 0.0, sigma: 2.5)\n        v_1|participant_id ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 2.5))\n    Link: identity\n    Explicit bounds: (-3.0, 3.0)\n\na:\n    Formula: a ~ 1 + (1|participant_id)\n    Priors:\n        a_Intercept ~ Normal(mu: 0.0, sigma: 1.0)\n        a_1|participant_id ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 1.0))\n    Link: identity\n    Explicit bounds: (0.3, 2.5)\n\nz:\n    Formula: z ~ 1 + (1|participant_id)\n    Priors:\n        z_Intercept ~ Normal(mu: 0.0, sigma: 1.0)\n        z_1|participant_id ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 1.0))\n    Link: identity\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Formula: t ~ 1 + (1|participant_id)\n    Priors:\n        t_Intercept ~ Normal(mu: 0.0, sigma: 1.0)\n        t_1|participant_id ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 1.0))\n    Link: identity\n    Explicit bounds: (0.0, 2.0)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[\u00a0]: Copied! <pre>model_log_logit = hssm.HSSM(\n    data=cav_data,\n    global_formula=\"y ~ 1 + (1|participant_id)\",\n    prior_settings=None,\n    link_settings=\"log_logit\",\n)\nmodel_log_logit\n</pre> model_log_logit = hssm.HSSM(     data=cav_data,     global_formula=\"y ~ 1 + (1|participant_id)\",     prior_settings=None,     link_settings=\"log_logit\", ) model_log_logit <pre>Model initialized successfully.\n</pre> Out[\u00a0]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 3988\n\nParameters:\n\nv:\n    Formula: v ~ 1 + (1|participant_id)\n    Priors:\n        v_Intercept ~ Normal(mu: 0.0, sigma: 2.5)\n        v_1|participant_id ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 2.5))\n    Link: identity\n    Explicit bounds: (-inf, inf)\n (ignored due to link function)\na:\n    Formula: a ~ 1 + (1|participant_id)\n    Priors:\n        a_Intercept ~ Normal(mu: 0.0, sigma: 1.0)\n        a_1|participant_id ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 1.0))\n    Link: log\n    Explicit bounds: (0.0, inf)\n (ignored due to link function)\nz:\n    Formula: z ~ 1 + (1|participant_id)\n    Priors:\n        z_Intercept ~ Normal(mu: 0.0, sigma: 1.0)\n        z_1|participant_id ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 1.0))\n    Link: Generalized logit link function with bounds (0.0, 1.0)\n    Explicit bounds: (0.0, 1.0)\n (ignored due to link function)\nt:\n    Formula: t ~ 1 + (1|participant_id)\n    Priors:\n        t_Intercept ~ Normal(mu: 0.0, sigma: 1.0)\n        t_1|participant_id ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 1.0))\n    Link: log\n    Explicit bounds: (0.0, inf)\n (ignored due to link function)\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[\u00a0]: Copied! <pre>model_safe_loglogit = hssm.HSSM(\n    data=cav_data,\n    global_formula=\"y ~ 1 + (1|participant_id)\",\n    prior_settings=\"safe\",\n    link_settings=\"log_logit\",\n)\nmodel_safe_loglogit\n</pre> model_safe_loglogit = hssm.HSSM(     data=cav_data,     global_formula=\"y ~ 1 + (1|participant_id)\",     prior_settings=\"safe\",     link_settings=\"log_logit\", ) model_safe_loglogit <pre>Model initialized successfully.\n</pre> Out[\u00a0]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 3988\n\nParameters:\n\nv:\n    Formula: v ~ 1 + (1|participant_id)\n    Priors:\n        v_Intercept ~ Normal(mu: 0.0, sigma: 0.25)\n        v_1|participant_id ~ Normal(mu: 0.0, sigma: Weibull(alpha: 1.5, beta: 0.30000001192092896))\n    Link: identity\n    Explicit bounds: (-inf, inf)\n (ignored due to link function)\na:\n    Formula: a ~ 1 + (1|participant_id)\n    Priors:\n        a_Intercept ~ Normal(mu: 0.0, sigma: 0.25)\n        a_1|participant_id ~ Normal(mu: 0.0, sigma: Weibull(alpha: 1.5, beta: 0.30000001192092896))\n    Link: log\n    Explicit bounds: (0.0, inf)\n (ignored due to link function)\nz:\n    Formula: z ~ 1 + (1|participant_id)\n    Priors:\n        z_Intercept ~ Normal(mu: 0.0, sigma: 0.25)\n        z_1|participant_id ~ Normal(mu: 0.0, sigma: Weibull(alpha: 1.5, beta: 0.30000001192092896))\n    Link: Generalized logit link function with bounds (0.0, 1.0)\n    Explicit bounds: (0.0, 1.0)\n (ignored due to link function)\nt:\n    Formula: t ~ 1 + (1|participant_id)\n    Priors:\n        t_Intercept ~ Normal(mu: 0.0, sigma: 0.25)\n        t_1|participant_id ~ Normal(mu: 0.0, sigma: Weibull(alpha: 1.5, beta: 0.30000001192092896))\n    Link: log\n    Explicit bounds: (0.0, inf)\n (ignored due to link function)\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"getting_started/hierarchical_modeling/#hierarchical-modeling","title":"Hierarchical Modeling\u00b6","text":"<p>This tutorial demonstrates how to take advantage of HSSM's hierarchical modeling capabilities. We will cover the following:</p> <ul> <li>How to define a mixed-effect regression</li> <li>How to define a hierarchial HSSM model</li> <li>How to apply prior and link function settings to ensure successful sampling</li> </ul>"},{"location":"getting_started/hierarchical_modeling/#colab-instructions","title":"Colab Instructions\u00b6","text":"<p>If you would like to run this tutorial on Google colab, please click this link.</p> <p>Once you are in the colab, follow the installation instructions below and then restart your runtime.</p> <p>Just uncomment the code in the next code cell and run it!</p> <p>NOTE:</p> <p>You may want to switch your runtime to have a GPU or TPU. To do so, go to Runtime &gt; Change runtime type and select the desired hardware accelerator.</p> <p>Note that if you switch your runtime you have to follow the installation instructions again.</p>"},{"location":"getting_started/hierarchical_modeling/#import-modules","title":"Import Modules\u00b6","text":""},{"location":"getting_started/hierarchical_modeling/#setting-the-global-float-type","title":"Setting the global float type\u00b6","text":"<p>Note: Using the analytical DDM (Drift Diffusion Model) likelihood in PyMC without setting the float type in <code>PyTensor</code> may result in warning messages during sampling, which is a known bug in PyMC v5.6.0 and earlier versions. To avoid these warnings, we provide a convenience function:</p>"},{"location":"getting_started/hierarchical_modeling/#1-defining-regressions","title":"1. Defining Regressions\u00b6","text":"<p>Under the hood, HSSM uses <code>bambi</code> for model creation. <code>bambi</code> takes inspiration from the <code>lme4</code> package in R and supports the definition of generalized linear mixed-effect models through R-like formulas and concepts such as link functions. This makes it possible to create arbitrary mixed-effect regressions in HSSM, which is one advantage of HSSM over HDDM. Now let's walk through the ways to define a parameter with a regression in HSSM.</p>"},{"location":"getting_started/hierarchical_modeling/#specifying-fixed-and-random-effect-terms","title":"Specifying fixed- and random-effect terms\u00b6","text":"<p>Suppose that we want to define a parameter <code>v</code> that has a regression defined. There are two ways to define such a parameter - either through a dictionary or through a <code>hssm.Param</code> object:</p> <pre><code># The following code are equivalent,\n# including the definition of the formula.\n\n# The dictionary way:\nparam_v = {\n    \"name\": \"v\",\n    \"formula\": \"v ~ x + y + x:y + (1|participant_id)\",\n    \"link\": \"identity\",\n    \"prior\": {\n        \"Intercept\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 0.25},\n        \"1|participant_id\": {\n            \"name\": \"Normal\",\n            \"mu\": 0.0,\n            \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 0.2},  # this is a hyperprior\n        },\n        \"x\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 0.25},\n    },\n}\n\n# The object-oriented way\nparam_v = hssm.Param(\n    \"v\",\n    formula=\"v ~ 1 + x*y + (1|participant_id)\",\n    link=\"identity\",\n    prior={\n        \"Intercept\": hssm.Prior(\"Normal\", mu=0.0, sigma=0.25),\n        \"1|participant_id\": hssm.Prior(\n            \"Normal\",\n            mu=0.0,\n            sigma=hssm.Prior(\"HalfNormal\", sigma=0.2), # this is a hyperprior\n        ),\n        \"x\": hssm.Prior(\"Normal\", mu=0.0, sigma=0.25),\n    },\n)\n</code></pre> <p>The formula <code>\"v ~ x + y + x:y + (1|participant_id)\"</code> defines a random-intercept model. Like R, unless otherwise specified, a fixed-effect intercept term is added to the formula by default. You can make this explicit by adding a <code>1</code> to the formula. Or, if your regression does not have an intercept. you can explicitly remove the intercept term by using a <code>0</code> in the place of <code>1</code>: <code>\"v ~ 0 + x * y + (1|participant_id)\"</code>. We recommend that the random effect terms should be specified after the fixed effect terms.</p> <p>Other fixed effect covariates are <code>x</code>, <code>y</code>, and the interaction term <code>x:y</code>. When all three terms are present, you can use the shortcut <code>x * y</code> in place of the three terms.</p> <p>The only random effect term in this model is <code>1|participant_id</code>. It is a random-intercept term with <code>participant_id</code> indicating the grouping variable. You can add another random-effect term in a similar way: <code>\"v ~ x + y + x:y + (1|participant_id) + (x|participant_id)\"</code>, or more briefly, <code>\"v ~ x + y + x:y + (1 + x|participant_id)\"</code>.</p>"},{"location":"getting_started/hierarchical_modeling/#specifying-priors-for-fixed-and-random-effect-terms","title":"Specifying priors for fixed- and random-effect terms:\u00b6","text":"<p>As demonstrated in the above code, you can specify priors of each term through a dictionary, with the key being the name of each term, and the corresponding value being the prior specification, etiher through a dictionary, or a <code>hssm.Prior</code> object. There are a few things to note:</p> <ul> <li>The prior of fixed-effect intercept is specified with <code>\"Intercept\"</code>, capitalized.</li> <li>For random effects, you can specify hyperpriors for the parameters of of their priors.</li> </ul>"},{"location":"getting_started/hierarchical_modeling/#specifying-the-link-functions","title":"Specifying the link functions:\u00b6","text":"<p>Link functions is another concept in frequentist generalized linear models, which defines a transformation between the linear combination of the covariates and the response variable. This is helpful especially when the response variable is not normally distributed, e.g. in a logistic regression. In HSSM, the link function is identity by default. However, since some parameters of SSMs are defined on <code>(0, inf)</code> or <code>(0, 1)</code>, link function can be helpful in ensuring the result of the regression is defined for these parameters. We will come back to this later.</p>"},{"location":"getting_started/hierarchical_modeling/#2-defining-a-hierarchical-hssm-model","title":"2. Defining a hierarchical HSSM model\u00b6","text":"<p>In fact, HSSM does not differentiate between a hierarchical or non-hierarchical model. A hierarchical model in HSSM is simply a model with one or more parameters defined as regressions. However, HSSM does provide some useful functionalities in creating hierarchical models.</p>"},{"location":"getting_started/hierarchical_modeling/#breaking-changes-use-global_formula-instead-of-hierarchical-parameter-when-creating-an-hssm-model","title":"BREAKING CHANGES: Use <code>global_formula</code> instead of <code>hierarchical</code> parameter when creating an HSSM model\u00b6","text":"<p>In HSSM v0.2.5, we have removed the <code>hierarchical</code> parameter to the <code>hssm.HSSM</code> class. In older versions, HSSM had a <code>hierarchical</code> argument which was a <code>bool</code>. It serves as a convenient switch to add a random-intercept regression to any parameter that is not explicitly defined by the user, using <code>participant_id</code> as a grouping variable.</p> <p>However, this <code>hierarchical</code> parameter caused much confusion, because many believed that somehow <code>hierarchical</code> would magically turn the model into a hierarchical model, while in reality, it does nothing more than adding a <code>y ~ 1 + (1|participant_id)</code> to all parameter, where <code>y</code> stands for the name of that parameter. That is why we removed this confusing parameter in favor of the new <code>global_formula</code> parameter, which is less confusing and offers the users more convenience and transparent control over the models that they want to create.</p> <p>When specified, <code>global_formula</code> adds the specified formula to all parameters. Therefore, when set to <code>y ~ 1 + (1|participant_id)</code>, this is equivalent to setting <code>hierarchical=True</code> in older versions of HSSM. However, the users can set it to any formula they want to apply to all parameters. HSSM is agnostic to whatever parameter name to the left of the <code>~</code> sign, while using <code>y</code> is more customary.</p> <p>Note</p> <p>     In HSSM, the default grouping variable is now `participant_id`, which is different from `subj_idx` in HDDM.   </p>"},{"location":"getting_started/hierarchical_modeling/#3-intelligent-defaults-for-complex-hierarchical-models","title":"3. Intelligent defaults for complex hierarchical models\u00b6","text":"<p><code>bambi</code> is not designed with HSSM in mind. Therefore, in cases where priors for certain parameters are not defined, the default priors supplied by <code>bambi</code> sometimes are not optimal. The same goes for link functions. <code>\"identity\"</code> link functions tend not to work well for certain parameters that are not defined on <code>(inf, inf)</code>. Therefore, we provide some default settings that the users can experiment to ensure that sampling is successful.</p>"},{"location":"getting_started/hierarchical_modeling/#prior_settings","title":"<code>prior_settings</code>\u00b6","text":"<p>Currently we provide a <code>\"safe\"</code> strategy that uses HSSM default priors, which is turned on by default for parameters that are targets of regressions. One can compare the two models below, with <code>safe</code> strategy turned on and off:</p>"},{"location":"getting_started/hierarchical_modeling/#link_settings","title":"<code>link_settings</code>\u00b6","text":"<p>We also provide a <code>link_settings</code> switch, which changes default link functions for parameters according to their explicit bounds. See the model below with <code>link_settings</code> set to <code>\"log_logit\"</code>:</p>"},{"location":"getting_started/hierarchical_modeling/#mixing-strategies","title":"Mixing strategies:\u00b6","text":"<p>It is possible to turn on both <code>prior_settings</code> and <code>link_settings</code>:</p>"},{"location":"getting_started/installation/","title":"Installation","text":"<p>Important Update: From HSSM 0.2.2, <code>conda-forge</code> is the official way of installing HSSM. This will also install other libraries such as <code>libblas</code> that PyMC requires to run properly.</p>"},{"location":"getting_started/installation/#step-1-create-a-conda-environment","title":"Step 1: Create a conda environment","text":"<p>If you haven't already, please follow the Anaconda official website to install conda. We assume that you already have one of Anaconda, Miniconda, miniforge, or mambaforge installed on your system and have access to either <code>conda</code> or <code>mamba</code> available on your command line.</p> <p>To create a conda environment, use the following command. Substitute <code>mamba</code> for <code>conda</code> if <code>mamba</code> is available:</p> <pre><code>conda create -n &lt;your-env-name&gt; python=3.11\nconda activate &lt;your-env-name&gt;\n</code></pre> <p>Substitute <code>&lt;your-env-name&gt;</code> with the name of the virtual environment that you choose. HSSM 0.2.0 and above supports Python versions 3.10 and 3.11.</p>"},{"location":"getting_started/installation/#step-2-install-hssm","title":"Step 2: Install HSSM","text":"<p>HSSM can be directly installed into your conda environment on Linux and MacOS. Installing HSSM on windows takes only one more simple step.</p>"},{"location":"getting_started/installation/#install-hssm-on-linux-and-macos-cpu-only","title":"Install HSSM on Linux and MacOS (CPU only)","text":"<p>Use the following command to install HSSM into your virtual environment:</p> <pre><code>conda install -c conda-forge hssm\n</code></pre>"},{"location":"getting_started/installation/#install-hssm-on-linux-and-macos-with-gpu-support","title":"Install HSSM on Linux and MacOS (with GPU Support)","text":"<p>If you need to sample with GPU, please install JAX with GPU support before installing HSSM:</p> <pre><code>conda install jaxlib=*=*cuda* jax cuda-nvcc -c conda-forge -c nvidia\nconda install -c conda-forge hssm\n</code></pre>"},{"location":"getting_started/installation/#install-hssm-on-windows-cpu-only","title":"Install HSSM on Windows (CPU only)","text":"<p>Because <code>jaxlib</code> is not available through <code>conda-forge</code> on Windows, you need to install JAX on Windows through <code>pip</code> before getting HSSM:</p> <pre><code>pip install jax\nconda install -c conda-forge hssm\n</code></pre>"},{"location":"getting_started/installation/#install-hssm-on-windows-with-gpu-support","title":"Install HSSM on Windows (with GPU support)","text":"<p>You simply need to install JAX with GPU support before getting HSSM:</p> <pre><code>pip install jax[cuda12]\nconda install -c conda-forge hssm\n</code></pre>"},{"location":"getting_started/installation/#support-for-apple-silicon-amd-and-other-gpus","title":"Support for Apple Silicon, AMD, and other GPUs","text":"<p>JAX also has support other GPUs. Please follow the Official JAX installation guide to install the correct version of JAX before installing HSSM.</p>"},{"location":"getting_started/installation/#advanced-installation","title":"Advanced Installation","text":""},{"location":"getting_started/installation/#install-hssm-directly-with-pip","title":"Install HSSM directly with Pip","text":"<p>HSSM is also available through PyPI. You can directly install it with pip into any virtual environment via:</p> <pre><code>pip install hssm\n</code></pre> <p>Note</p> <p>While this installation is much simpler, you might encounter this warning message <code>WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.</code> You can follow this discussion to link a BLAS library with <code>pytensor</code>.</p>"},{"location":"getting_started/installation/#install-the-dev-version-of-hssm","title":"Install the dev version of HSSM","text":"<p>You can install the dev version of <code>hssm</code> directly from this repo:</p> <pre><code>pip install git+https://github.com/lnccbrown/HSSM.git\n</code></pre>"},{"location":"getting_started/installation/#install-hssm-on-google-colab","title":"Install HSSM on Google Colab","text":"<p>Google Colab comes with PyMC and JAX pre-configured. That holds true even if you are using the GPU and TPU backend, so you simply need to install HSSM via pip on Colab regardless of the backend you are using:</p> <pre><code>!pip install hssm\n</code></pre>"},{"location":"getting_started/installation/#install-optional-dependencies","title":"Install optional dependencies","text":"<p>Whether you have installed HSSM via <code>conda</code>, <code>pip</code>, or GitHub, you might still need additional packages installed for additional features such as sampling with <code>blackjax</code> or GPU support for <code>JAX</code>. Please follow the instructions below if you need any of these additional features:</p>"},{"location":"getting_started/installation/#1-sampling-with-jax-through-numpyro-or-blackjax","title":"1. Sampling with JAX through <code>numpyro</code> or <code>blackjax</code>","text":"<p>JAX-based sampling is done through <code>numpyro</code> and <code>blackjax</code>. <code>numpyro</code> is installed as a dependency by default. You need to have <code>blackjax</code> installed if you want to use the <code>nuts_blackjax</code> sampler.</p> <pre><code>pip install blackjax\n</code></pre>"},{"location":"getting_started/installation/#2-visualizing-the-model-with-graphviz","title":"2. Visualizing the model with <code>graphviz</code>","text":"<p>Model graphs are created with <code>model.graph()</code> through <code>graphviz</code>. If you have installed hssm in a conda environment, you can simply install <code>graphviz</code> in conda:</p> <pre><code>conda install -c conda-forge graphviz\n</code></pre> <p>If you have installed hssm in a non-conda environment, you need to have <code>graphviz</code> installed system-wide and then install its Python binding:</p>"},{"location":"getting_started/installation/#install-graphviz-system-wide","title":"Install graphviz system-wide","text":"<p>Please follow the instructions on the graphviz official site to install graphviz for your specific platform.</p>"},{"location":"getting_started/installation/#install-graphviz-python-binding","title":"Install graphviz python binding","text":"<p>Once graphviz is installed, you can install its Python binding via pip:</p> <pre><code>pip install graphviz\n</code></pre>"},{"location":"getting_started/installation/#common-issues","title":"Common issues","text":"<ol> <li>I run into warnings such as</li> </ol> <pre><code>WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n</code></pre> <p>This is because <code>pytensor</code>, the compute backend of PyMC, cannot find a BLAS library on your    system to optimize its computation. This can be resolved by following the recommended    steps to install HSSM into a conda environment. If conda cannot be used, you can follow    this discussion to link a BLAS library    with <code>pytensor</code>.</p> <ol> <li><code>pip</code> installation fails with missing dependencies:</li> </ol> <p>Here's an example:</p> <pre><code>ERROR: Could not find a version that satisfies the requirement jaxlib&lt;0.5.0,&gt;=0.4.0 (from hssm) (from versions: none)\nERROR: No matching distribution found for jaxlib&lt;0.5.0,&gt;=0.4.0 (from hssm)\n</code></pre> <p>HSSM has very specific requirements for the versions of <code>jax</code>, <code>pymc</code>, and <code>bambi</code>.    This problem can usually be resolved by installing HSSM into a dedicated virtual    environment.</p> <p>Note</p> <p>Possible solutions to any issues with installations with hssm can be located here. Also feel free to start a new discussion thread if you don't find answers there. We recommend installing HSSM into a new conda environment with Python 3.10 or 3.11 to prevent any problems with dependencies during the installation process. Please note that hssm is only tested for python 3.10, 3.11. As of HSSM v0.2.0, support for Python 3.9 is dropped. Use unsupported python versions with caution.</p>"},{"location":"getting_started/installation/#questions","title":"Questions?","text":"<p>If you have any questions, please open an issue in our GitHub repo.</p>"},{"location":"tutorials/add_custom_rlssm_model/","title":"Introducing a custom Reinforcement Learning - Sequential Sampling Model (RLSSM) into HSSM","text":"<p>You will have to edit <code>rldm.py</code> file and re-install hssm for this tutorial to work. Running this notebook without changes to the <code>rldm.py</code> file will throw errors.</p> In\u00a0[15]: Copied! <pre># Import necessary libraries\nimport numpy as np\nimport arviz as az\nimport matplotlib.pyplot as plt\nfrom functools import partial\n\n# Import HSSM and simulator package\nimport hssm\nfrom hssm.utils import decorate_atomic_simulator\nfrom hssm.likelihoods.rldm import make_rldm_logp_op\nfrom hssm.distribution_utils.dist import make_hssm_rv\nfrom ssms.basic_simulators.simulator import simulator\n</pre> # Import necessary libraries import numpy as np import arviz as az import matplotlib.pyplot as plt from functools import partial  # Import HSSM and simulator package import hssm from hssm.utils import decorate_atomic_simulator from hssm.likelihoods.rldm import make_rldm_logp_op from hssm.distribution_utils.dist import make_hssm_rv from ssms.basic_simulators.simulator import simulator In\u00a0[2]: Copied! <pre># Set the style for the plots\nplt.style.use('seaborn-v0_8-dark-palette')\n</pre> # Set the style for the plots plt.style.use('seaborn-v0_8-dark-palette') In\u00a0[3]: Copied! <pre># Load synthetic RLSSM dataset containing both behavioral data and ground truth parameters\nsavefile = np.load(\"../../tests/fixtures/rldm_data.npy\", allow_pickle=True).item()\ndataset = savefile['data']\n\n# Rename trial column to match HSSM conventions\ndataset.rename(columns={'trial': 'trial_id'}, inplace=True)\n\n# Examine the dataset structure\ndataset.head()\n</pre> # Load synthetic RLSSM dataset containing both behavioral data and ground truth parameters savefile = np.load(\"../../tests/fixtures/rldm_data.npy\", allow_pickle=True).item() dataset = savefile['data']  # Rename trial column to match HSSM conventions dataset.rename(columns={'trial': 'trial_id'}, inplace=True)  # Examine the dataset structure dataset.head() Out[3]: participant_id trial_id response rt feedback correct 0 0 0 0.0 0.935602 0.126686 0.0 1 0 1 0.0 1.114379 0.173100 0.0 2 0 2 0.0 0.564311 0.444935 0.0 3 0 3 0.0 2.885860 0.307207 0.0 4 0 4 0.0 0.532113 0.177911 0.0 In\u00a0[4]: Copied! <pre># Validate data structure and extract dataset configuration \ndataset, n_participants, n_trials = hssm.check_data_for_rl(dataset)\n\nprint(f\"Number of participants: {n_participants}\")\nprint(f\"Number of trials: {n_trials}\")\n</pre> # Validate data structure and extract dataset configuration  dataset, n_participants, n_trials = hssm.check_data_for_rl(dataset)  print(f\"Number of participants: {n_participants}\") print(f\"Number of trials: {n_trials}\") <pre>Number of participants: 20\nNumber of trials: 200\n</pre> In\u00a0[5]: Copied! <pre># Define parameters for the RLSSM model (RL + decision model parameters)\nlist_params = ['rl.alpha', 'rl.alpha_neg', 'scaler', 'a', 'z', 't', 'theta']\n\n# Create a dummy simulator for generating synthetic data (used for posterior predictives)\n# This bypasses the need for a full RLSSM simulator implementation\ndef create_dummy_simulator():\n    \"\"\"Create a dummy simulator function for RLSSM model.\"\"\"\n    def sim_wrapper(simulator_fun, theta, model, n_samples, random_state, **kwargs):\n        # Generate random RT and choice data as placeholders\n        sim_rt = np.random.uniform(0.2, 0.6, n_samples)\n        sim_ch = np.random.randint(0, 2, n_samples)\n        \n        return np.column_stack([sim_rt, sim_ch])\n\n    # Wrap the simulator function with required metadata\n    wrapped_simulator = partial(sim_wrapper, simulator_fun=simulator, model=\"custom\", n_samples=1)\n\n    # Decorate the simulator to make it compatible with HSSM\n    return decorate_atomic_simulator(model_name=\"custom\", choices=[0, 1], obs_dim=2)(wrapped_simulator)\n\n# Create the simulator and RandomVariable\ndecorated_simulator = create_dummy_simulator()\n\n# Create a PyTensor RandomVariable using `make_hssm_rv` for use in the PyMC model\nCustomRV = make_hssm_rv(\n    simulator_fun=decorated_simulator, list_params=list_params\n)\n</pre> # Define parameters for the RLSSM model (RL + decision model parameters) list_params = ['rl.alpha', 'rl.alpha_neg', 'scaler', 'a', 'z', 't', 'theta']  # Create a dummy simulator for generating synthetic data (used for posterior predictives) # This bypasses the need for a full RLSSM simulator implementation def create_dummy_simulator():     \"\"\"Create a dummy simulator function for RLSSM model.\"\"\"     def sim_wrapper(simulator_fun, theta, model, n_samples, random_state, **kwargs):         # Generate random RT and choice data as placeholders         sim_rt = np.random.uniform(0.2, 0.6, n_samples)         sim_ch = np.random.randint(0, 2, n_samples)                  return np.column_stack([sim_rt, sim_ch])      # Wrap the simulator function with required metadata     wrapped_simulator = partial(sim_wrapper, simulator_fun=simulator, model=\"custom\", n_samples=1)      # Decorate the simulator to make it compatible with HSSM     return decorate_atomic_simulator(model_name=\"custom\", choices=[0, 1], obs_dim=2)(wrapped_simulator)  # Create the simulator and RandomVariable decorated_simulator = create_dummy_simulator()  # Create a PyTensor RandomVariable using `make_hssm_rv` for use in the PyMC model CustomRV = make_hssm_rv(     simulator_fun=decorated_simulator, list_params=list_params ) In\u00a0[6]: Copied! <pre># Create a Pytensor Op for the likelihood function.\n# The `make_rldm_logp_op` function is a utility that wraps the base JAX likelihood function into a HSSM/PyMC-compatible callable.\n\nlogp_jax_op = make_rldm_logp_op(\n    n_participants=n_participants,\n    n_trials=n_trials,\n    n_params=len(list_params),\n)\n</pre> # Create a Pytensor Op for the likelihood function. # The `make_rldm_logp_op` function is a utility that wraps the base JAX likelihood function into a HSSM/PyMC-compatible callable.  logp_jax_op = make_rldm_logp_op(     n_participants=n_participants,     n_trials=n_trials,     n_params=len(list_params), ) In\u00a0[7]: Copied! <pre># Test the likelihood function\n\ndef extract_data_columns(dataset):\n    \"\"\"Extract required data columns from dataset.\"\"\"\n    return {\n        'participant_id': dataset[\"participant_id\"].values,\n        'trial': dataset[\"trial_id\"].values,\n        'response': dataset[\"response\"].values,\n        'feedback': dataset[\"feedback\"].values,\n        'rt': dataset[\"rt\"].values\n    }\n\ndef create_test_parameters(n_trials):\n    \"\"\"Create dummy parameters for testing the likelihood function.\"\"\"\n    return {\n        'rl_alpha': np.ones(n_trials) * 0.60,\n        'rl_alpha_neg': np.ones(n_trials) * 0.60,  \n        'scaler': np.ones(n_trials) * 3.2,\n        'a': np.ones(n_trials) * 1.2,\n        'z': np.ones(n_trials) * 0.1,\n        't': np.ones(n_trials) * 0.1,\n        'theta': np.ones(n_trials) * 0.1\n}\n\n# Extract data and create test parameters\ndata_columns = extract_data_columns(dataset)\nnum_subj = len(np.unique(data_columns['participant_id']))\nn_trials_total = num_subj * 200\n\ntest_params = create_test_parameters(n_trials_total)\n\n# Evaluate the likelihood function\ntest_logp_out = logp_jax_op(\n    np.column_stack((data_columns['rt'], data_columns['response'])),\n    test_params['rl_alpha'],\n    test_params['rl_alpha_neg'],\n    test_params['scaler'],\n    test_params['a'], \n    test_params['z'],\n    test_params['t'],\n    test_params['theta'],\n    data_columns['participant_id'],\n    data_columns['trial'],\n    data_columns['feedback'],\n)\n\nLL = test_logp_out.eval()\nprint(f\"Log likelihood: {np.sum(LL):.4f}\")\n</pre> # Test the likelihood function  def extract_data_columns(dataset):     \"\"\"Extract required data columns from dataset.\"\"\"     return {         'participant_id': dataset[\"participant_id\"].values,         'trial': dataset[\"trial_id\"].values,         'response': dataset[\"response\"].values,         'feedback': dataset[\"feedback\"].values,         'rt': dataset[\"rt\"].values     }  def create_test_parameters(n_trials):     \"\"\"Create dummy parameters for testing the likelihood function.\"\"\"     return {         'rl_alpha': np.ones(n_trials) * 0.60,         'rl_alpha_neg': np.ones(n_trials) * 0.60,           'scaler': np.ones(n_trials) * 3.2,         'a': np.ones(n_trials) * 1.2,         'z': np.ones(n_trials) * 0.1,         't': np.ones(n_trials) * 0.1,         'theta': np.ones(n_trials) * 0.1 }  # Extract data and create test parameters data_columns = extract_data_columns(dataset) num_subj = len(np.unique(data_columns['participant_id'])) n_trials_total = num_subj * 200  test_params = create_test_parameters(n_trials_total)  # Evaluate the likelihood function test_logp_out = logp_jax_op(     np.column_stack((data_columns['rt'], data_columns['response'])),     test_params['rl_alpha'],     test_params['rl_alpha_neg'],     test_params['scaler'],     test_params['a'],      test_params['z'],     test_params['t'],     test_params['theta'],     data_columns['participant_id'],     data_columns['trial'],     data_columns['feedback'], )  LL = test_logp_out.eval() print(f\"Log likelihood: {np.sum(LL):.4f}\") <pre>Log likelihood: -6879.1526\n</pre> In\u00a0[8]: Copied! <pre># Step 3: Define the model config\n\n# Configure the HSSM model \nmodel_config = hssm.ModelConfig(\n    response=[\"rt\", \"response\"],        # Dependent variables (RT and choice)\n    list_params=                        # List of model parameters\n        ['rl.alpha', 'rl.alpha_neg', 'scaler', 'a', 'z', 't', 'theta'],            \n    choices=[0, 1],                     # Possible choice options\n    default_priors={},                  # Use custom priors (defined below)\n    bounds=dict(                        # Parameter bounds for optimization\n        rl_alpha=(0.01, 1),             # Learning rate bounds\n        rl_alpha_neg=(0.01, 1),         # Negative learning rate bounds\n        scaler=(1, 4),                  # Scaler bounds\n        a=(0.3, 2.5),                   # Boundary separation bounds\n        z=(0.1, 0.9),                   # Bias bounds\n        t=(0.1, 2.0),                   # Non-decision time bounds\n        theta=(0.0, 1.2)                # Collapse rate bounds\n        ),\n    rv=CustomRV,                        # Custom RandomVariable that we created earlier\n    extra_fields=[                      # Additional data columns to be passed to the likelihood function as extra_fields\n        \"participant_id\", \n        \"trial_id\", \n        \"feedback\"],  \n    backend=\"jax\"                       # Use JAX for computation\n)\n</pre> # Step 3: Define the model config  # Configure the HSSM model  model_config = hssm.ModelConfig(     response=[\"rt\", \"response\"],        # Dependent variables (RT and choice)     list_params=                        # List of model parameters         ['rl.alpha', 'rl.alpha_neg', 'scaler', 'a', 'z', 't', 'theta'],                 choices=[0, 1],                     # Possible choice options     default_priors={},                  # Use custom priors (defined below)     bounds=dict(                        # Parameter bounds for optimization         rl_alpha=(0.01, 1),             # Learning rate bounds         rl_alpha_neg=(0.01, 1),         # Negative learning rate bounds         scaler=(1, 4),                  # Scaler bounds         a=(0.3, 2.5),                   # Boundary separation bounds         z=(0.1, 0.9),                   # Bias bounds         t=(0.1, 2.0),                   # Non-decision time bounds         theta=(0.0, 1.2)                # Collapse rate bounds         ),     rv=CustomRV,                        # Custom RandomVariable that we created earlier     extra_fields=[                      # Additional data columns to be passed to the likelihood function as extra_fields         \"participant_id\",          \"trial_id\",          \"feedback\"],       backend=\"jax\"                       # Use JAX for computation ) In\u00a0[9]: Copied! <pre># Create a hierarchical HSSM model with custom likelihood function\nhssm_model = hssm.HSSM(\n    data=dataset,                        # Input dataset\n    model_config=model_config,           # Model configuration\n    p_outlier=0,                         # No outlier modeling\n    lapse=None,                          # No lapse rate modeling\n    loglik=logp_jax_op,                  # Custom RLDM likelihood function\n    loglik_kind=\"approx_differentiable\", # Use approximate gradients\n    noncentered=True,                    # Use non-centered parameterization\n    process_initvals=False,              # Skip initial value processing in HSSM\n    include=[\n        # Define hierarchical priors: group-level intercepts + subject-level random effects\n        hssm.Param(\"rl.alpha\", \n                formula=\"rl_alpha ~ 1 + (1|participant_id)\", \n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.01, upper=1, mu=0.3)}),\n        hssm.Param(\"rl.alpha_neg\", \n                formula=\"rl_alpha_neg ~ 1 + (1|participant_id)\", \n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.01, upper=1, mu=0.3)}),\n        hssm.Param(\"scaler\", \n                formula=\"scaler ~ 1 + (1|participant_id)\", \n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=1, upper=4, mu=1.5)}),\n        hssm.Param(\"a\", \n                formula=\"a ~ 1 + (1|participant_id)\", \n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.3, upper=2.5, mu=1.0)}),\n        hssm.Param(\"z\", \n                formula=\"z ~ 1 + (1|participant_id)\", \n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.1, upper=0.9, mu=0.2)}),\n        hssm.Param(\"t\", \n                formula=\"t ~ 1 + (1|participant_id)\", \n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.01, upper=2, mu=0.2, initval=0.1)}),\n        hssm.Param(\"theta\", \n                formula=\"theta ~ 1 + (1|participant_id)\", \n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.00, upper=1.2, mu=0.3)}),\n    ]\n)\n</pre> # Create a hierarchical HSSM model with custom likelihood function hssm_model = hssm.HSSM(     data=dataset,                        # Input dataset     model_config=model_config,           # Model configuration     p_outlier=0,                         # No outlier modeling     lapse=None,                          # No lapse rate modeling     loglik=logp_jax_op,                  # Custom RLDM likelihood function     loglik_kind=\"approx_differentiable\", # Use approximate gradients     noncentered=True,                    # Use non-centered parameterization     process_initvals=False,              # Skip initial value processing in HSSM     include=[         # Define hierarchical priors: group-level intercepts + subject-level random effects         hssm.Param(\"rl.alpha\",                  formula=\"rl_alpha ~ 1 + (1|participant_id)\",                  prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.01, upper=1, mu=0.3)}),         hssm.Param(\"rl.alpha_neg\",                  formula=\"rl_alpha_neg ~ 1 + (1|participant_id)\",                  prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.01, upper=1, mu=0.3)}),         hssm.Param(\"scaler\",                  formula=\"scaler ~ 1 + (1|participant_id)\",                  prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=1, upper=4, mu=1.5)}),         hssm.Param(\"a\",                  formula=\"a ~ 1 + (1|participant_id)\",                  prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.3, upper=2.5, mu=1.0)}),         hssm.Param(\"z\",                  formula=\"z ~ 1 + (1|participant_id)\",                  prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.1, upper=0.9, mu=0.2)}),         hssm.Param(\"t\",                  formula=\"t ~ 1 + (1|participant_id)\",                  prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.01, upper=2, mu=0.2, initval=0.1)}),         hssm.Param(\"theta\",                  formula=\"theta ~ 1 + (1|participant_id)\",                  prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.00, upper=1.2, mu=0.3)}),     ] ) <pre>No common intercept. Bounds for parameter rl.alpha is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter rl.alpha_neg is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter scaler is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter a is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter z is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter t is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter theta is not applied due to a current limitation of Bambi. This will change in the future.\nModel initialized successfully.\n</pre> In\u00a0[10]: Copied! <pre># Run MCMC sampling using NUTS sampler with JAX backend\n# Note: Using small number of samples for demonstration (increase for real analysis)\nidata_mcmc = hssm_model.sample(\n    sampler='nuts_numpyro',  # JAX-based NUTS sampler for efficiency\n    chains=1,                # Number of parallel chains\n    draws=1000,                # Number of posterior samples\n    tune=1000,                 # Number of tuning/warmup samples\n)\n</pre> # Run MCMC sampling using NUTS sampler with JAX backend # Note: Using small number of samples for demonstration (increase for real analysis) idata_mcmc = hssm_model.sample(     sampler='nuts_numpyro',  # JAX-based NUTS sampler for efficiency     chains=1,                # Number of parallel chains     draws=1000,                # Number of posterior samples     tune=1000,                 # Number of tuning/warmup samples ) <pre>Using default initvals. \n\n</pre> <pre>sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [08:33&lt;00:00,  3.89it/s, 29 steps of size 1.56e-01. acc. prob=0.92]   \nThere were 1 divergences after tuning. Increase `target_accept` or reparameterize.\nOnly one chain was sampled, this makes it impossible to run some convergence checks\n/Users/krishnbera/Documents/revert_rldm/HSSM/.venv/lib/python3.12/site-packages/pymc/pytensorf.py:958: FutureWarning: compile_pymc was renamed to compile. Old name will be removed in a future release of PyMC\n  warnings.warn(\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:02&lt;00:00, 366.61it/s]\n</pre> In\u00a0[11]: Copied! <pre>idata_mcmc\n</pre> idata_mcmc Out[11]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 2MB\nDimensions:                                (chain: 1, draw: 1000,\n                                            participant_id__factor_dim: 20,\n                                            rl.alpha_1|participant_id__factor_dim: 20)\nCoordinates:\n  * chain                                  (chain) int64 8B 0\n  * draw                                   (draw) int64 8kB 0 1 2 ... 998 999\n  * rl.alpha_1|participant_id__factor_dim  (rl.alpha_1|participant_id__factor_dim) &lt;U2 160B ...\n  * participant_id__factor_dim             (participant_id__factor_dim) &lt;U2 160B ...\nData variables: (12/35)\n    rl.alpha_neg_1|participant_id          (chain, draw, participant_id__factor_dim) float64 160kB ...\n    rl.alpha_neg_1|participant_id_sigma    (chain, draw) float64 8kB 0.09328 ...\n    a_1|participant_id_mu                  (chain, draw) float64 8kB -0.2027 ...\n    rl.alpha_1|participant_id_sigma        (chain, draw) float64 8kB 0.03706 ...\n    scaler_1|participant_id_offset         (chain, draw, participant_id__factor_dim) float64 160kB ...\n    a_1|participant_id                     (chain, draw, participant_id__factor_dim) float64 160kB ...\n    ...                                     ...\n    theta_1|participant_id_mu              (chain, draw) float64 8kB -0.2717 ...\n    a_Intercept                            (chain, draw) float64 8kB 1.491 .....\n    scaler_Intercept                       (chain, draw) float64 8kB 2.639 .....\n    scaler_1|participant_id_mu             (chain, draw) float64 8kB 0.3354 ....\n    rl.alpha_1|participant_id_mu           (chain, draw) float64 8kB 0.02453 ...\n    theta_1|participant_id                 (chain, draw, participant_id__factor_dim) float64 160kB ...\nAttributes:\n    created_at:                  2025-07-15T19:55:45.042734+00:00\n    arviz_version:               0.22.0\n    inference_library:           numpyro\n    inference_library_version:   0.18.0\n    sampling_time:               517.168047\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 1000</li><li>participant_id__factor_dim: 20</li><li>rl.alpha_1|participant_id__factor_dim: 20</li></ul></li><li>Coordinates: (4)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li><li>rl.alpha_1|participant_id__factor_dim(rl.alpha_1|participant_id__factor_dim)&lt;U2'0' '1' '2' '3' ... '17' '18' '19'<pre>array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n       '13', '14', '15', '16', '17', '18', '19'], dtype='&lt;U2')</pre></li><li>participant_id__factor_dim(participant_id__factor_dim)&lt;U2'0' '1' '2' '3' ... '17' '18' '19'<pre>array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n       '13', '14', '15', '16', '17', '18', '19'], dtype='&lt;U2')</pre></li></ul></li><li>Data variables: (35)<ul><li>rl.alpha_neg_1|participant_id(chain, draw, participant_id__factor_dim)float640.08525 -0.03887 ... 0.1017 0.1859<pre>array([[[ 8.52524246e-02, -3.88736108e-02,  1.10405443e-01, ...,\n         -6.03191902e-02,  1.11141755e-02,  9.78937925e-02],\n        [ 1.25520353e-02, -6.54906468e-03, -1.25774425e-02, ...,\n         -5.00059581e-04,  4.13463470e-03,  9.16094207e-04],\n        [ 7.28220291e-05, -3.21929838e-03, -6.87044825e-03, ...,\n          2.58669913e-04,  5.35369655e-03,  5.29310912e-03],\n        ...,\n        [ 6.19613710e-02, -9.04301011e-02, -6.40939936e-02, ...,\n         -1.44371076e-01,  6.93436508e-02,  1.19720281e-01],\n        [ 1.17282010e-01, -4.68099183e-02, -5.80185783e-02, ...,\n          8.10048782e-02,  6.91800987e-02,  5.62488894e-02],\n        [ 4.16863398e-02,  4.87059201e-02, -8.28223216e-02, ...,\n          1.12394939e-01,  1.01691578e-01,  1.85925093e-01]]],\n      shape=(1, 1000, 20))</pre></li><li>rl.alpha_neg_1|participant_id_sigma(chain, draw)float640.09328 0.01113 ... 0.0792 0.08951<pre>array([[0.09327861, 0.01113374, 0.0067807 , 0.02224991, 0.18818852,\n        0.13693416, 0.0137104 , 0.12112734, 0.02774661, 0.01184964,\n        0.03313942, 0.10921727, 0.09487614, 0.01795892, 0.10265819,\n        0.06051296, 0.08517624, 0.08433337, 0.11109638, 0.06801696,\n        0.02649293, 0.07327044, 0.10900929, 0.1437742 , 0.11890193,\n        0.01861684, 0.03742197, 0.16033764, 0.0679923 , 0.11418525,\n        0.12090175, 0.06131621, 0.07123911, 0.09237216, 0.03997358,\n        0.02860808, 0.08295873, 0.13752383, 0.07696958, 0.0946859 ,\n        0.06441234, 0.0692772 , 0.03525691, 0.14056736, 0.03736379,\n        0.14179708, 0.00567365, 0.07516111, 0.0841962 , 0.18274141,\n        0.0527435 , 0.04581121, 0.01990324, 0.05346567, 0.16033973,\n        0.06810372, 0.10893175, 0.16168255, 0.03397938, 0.02351622,\n        0.08627316, 0.01601121, 0.01131003, 0.01035117, 0.02602841,\n        0.07322761, 0.03045117, 0.05172015, 0.05411817, 0.00919667,\n        0.08180663, 0.01927885, 0.06548393, 0.10205384, 0.0270732 ,\n        0.00521116, 0.02167259, 0.08553463, 0.01201813, 0.21275623,\n        0.06559833, 0.1847809 , 0.05703139, 0.0461503 , 0.03027838,\n        0.21694742, 0.12979277, 0.11097162, 0.08269843, 0.04748068,\n        0.28007182, 0.0231027 , 0.02844314, 0.15013276, 0.17306386,\n        0.10448734, 0.02659661, 0.14779651, 0.03827021, 0.11011419,\n...\n        0.03162604, 0.08527373, 0.12920783, 0.06979845, 0.10271215,\n        0.06767155, 0.03451224, 0.15729287, 0.11459939, 0.04799843,\n        0.03906045, 0.10477396, 0.06634792, 0.04507225, 0.211674  ,\n        0.01846484, 0.10071775, 0.02165307, 0.10840669, 0.04564035,\n        0.07983719, 0.1140913 , 0.14494868, 0.04455192, 0.02871864,\n        0.06410322, 0.07585762, 0.06543017, 0.09677864, 0.08256108,\n        0.07546982, 0.08211627, 0.03127799, 0.1853499 , 0.07410288,\n        0.03271516, 0.03283377, 0.09298921, 0.08625752, 0.04118969,\n        0.11363661, 0.11267627, 0.11052486, 0.10924007, 0.07988203,\n        0.07742597, 0.11399736, 0.08374279, 0.08072191, 0.1551673 ,\n        0.17327917, 0.07352062, 0.12750346, 0.09591898, 0.03928874,\n        0.08258566, 0.19549517, 0.05626114, 0.12524791, 0.08005122,\n        0.11528304, 0.08485011, 0.07287171, 0.00884009, 0.10423099,\n        0.06677659, 0.0590118 , 0.14027077, 0.11099063, 0.11157492,\n        0.16431369, 0.16567496, 0.09144268, 0.01535024, 0.01808003,\n        0.01111863, 0.17841836, 0.01905178, 0.1746642 , 0.10077163,\n        0.1998371 , 0.11643685, 0.11606395, 0.06974784, 0.06793704,\n        0.02075596, 0.05559657, 0.12647182, 0.02907609, 0.10676777,\n        0.0145071 , 0.0222392 , 0.0448217 , 0.10051719, 0.0922675 ,\n        0.06733797, 0.12014162, 0.13143749, 0.07920041, 0.08951135]])</pre></li><li>a_1|participant_id_mu(chain, draw)float64-0.2027 -0.4448 ... -0.1654 -0.1497<pre>array([[-2.02719840e-01, -4.44842080e-01, -3.03964924e-01,\n        -1.55609758e-01,  1.31007984e-01, -7.59249147e-02,\n         1.37072883e-01,  5.06277206e-02, -5.60636524e-02,\n         1.84903475e-01,  2.41388470e-01,  8.32442934e-02,\n        -4.56799516e-02,  1.33342697e-02, -1.42448090e-01,\n         6.53218550e-02,  1.71898816e-01, -1.09097340e-02,\n         3.98032202e-01, -2.99370619e-01,  3.05871182e-01,\n        -2.50432920e-01,  3.32773988e-01, -2.64016270e-01,\n         2.68458099e-01, -4.70057178e-01, -4.73121869e-01,\n         1.10616487e-01, -1.11512175e-01,  7.82328434e-02,\n         1.68435968e-01, -3.64161262e-01, -1.79503239e-02,\n         4.90399166e-01, -3.68400984e-01, -8.57769115e-02,\n         1.15795612e-02,  6.29667855e-03, -2.04432032e-01,\n         2.02540726e-01, -3.54064121e-01, -1.86987814e-01,\n        -1.78315042e-01,  4.05421330e-01, -6.33678532e-01,\n        -7.59063197e-03, -2.27643846e-01,  1.76635714e-01,\n        -4.64064271e-02,  1.08984697e-01, -1.31863439e-01,\n        -2.65443662e-01, -3.32474608e-01,  2.07935384e-01,\n        -3.40245641e-01,  4.48081785e-02,  2.40600620e-01,\n        -7.65335512e-02,  8.40249994e-02,  4.46943186e-02,\n...\n        -5.25219200e-02,  3.26874443e-02, -5.35956329e-03,\n         2.64440432e-03,  2.68161531e-01, -3.80900520e-01,\n        -3.63901515e-01,  4.43749513e-01, -2.62191913e-01,\n         4.85944373e-01, -3.73407644e-01,  3.08463063e-01,\n         3.08789186e-01, -3.26877636e-01,  7.66973819e-02,\n        -2.03142913e-01,  4.00175991e-01, -3.99422717e-01,\n         2.18065014e-01, -2.91034906e-01,  1.31350513e-01,\n         1.45869259e-01, -1.04896259e-01,  7.87857837e-02,\n         1.28148804e-01,  2.08061623e-01, -2.28293052e-01,\n         1.30602673e-01, -3.26786851e-01,  2.81410262e-01,\n        -3.34005531e-01,  2.84284392e-01, -3.80217659e-01,\n        -3.00765996e-01,  2.81146102e-01, -2.59827388e-01,\n         1.67198897e-01, -1.89070258e-01, -2.87456083e-01,\n         1.16248583e-01, -1.13738792e-01,  1.33560394e-01,\n        -1.66193673e-01, -2.09997026e-01,  2.52654322e-01,\n         5.11441764e-02, -1.58902464e-01,  1.61687130e-01,\n        -1.83569790e-01,  1.10535873e-01, -2.45399914e-01,\n         3.78714947e-01, -3.73786765e-01,  4.98948500e-01,\n         2.03085198e-01, -4.68995390e-02, -1.65352499e-01,\n        -1.49680850e-01]])</pre></li><li>rl.alpha_1|participant_id_sigma(chain, draw)float640.03706 0.03513 ... 0.06522 0.02886<pre>array([[0.03705929, 0.03513307, 0.02895748, 0.06591059, 0.16732787,\n        0.04916698, 0.15706138, 0.06801485, 0.09469248, 0.14255108,\n        0.12811558, 0.22328067, 0.05717146, 0.14317605, 0.15689983,\n        0.01769085, 0.02674053, 0.02704576, 0.02184269, 0.01961838,\n        0.1458844 , 0.05309886, 0.10834505, 0.1546012 , 0.0312736 ,\n        0.07673636, 0.07006105, 0.13105068, 0.11640804, 0.05656212,\n        0.06250989, 0.09320703, 0.12368692, 0.01581294, 0.07802086,\n        0.17113857, 0.09322637, 0.03732654, 0.12476267, 0.12650323,\n        0.0889501 , 0.11217427, 0.11363395, 0.22390902, 0.03064159,\n        0.02317336, 0.16881594, 0.03566653, 0.11702845, 0.12819443,\n        0.06096566, 0.08023278, 0.06755874, 0.06227903, 0.10302292,\n        0.06773351, 0.07629667, 0.1396328 , 0.19289015, 0.13320769,\n        0.04224377, 0.05855081, 0.22075669, 0.0448352 , 0.13730017,\n        0.03361158, 0.04601898, 0.23437392, 0.08898296, 0.029606  ,\n        0.07635805, 0.14949171, 0.08975536, 0.04087383, 0.08667208,\n        0.10378602, 0.05187667, 0.14663556, 0.01525829, 0.09895022,\n        0.1219454 , 0.03420199, 0.16709584, 0.07488889, 0.11075151,\n        0.11335392, 0.09615191, 0.13245751, 0.04418025, 0.10621877,\n        0.02593165, 0.07919483, 0.1304459 , 0.01230364, 0.00486108,\n        0.21220025, 0.12718461, 0.12606474, 0.11468334, 0.10354153,\n...\n        0.14117324, 0.07014331, 0.12533277, 0.06199117, 0.14506578,\n        0.15604541, 0.03515604, 0.20747414, 0.03717839, 0.02070695,\n        0.11343202, 0.03029284, 0.04269185, 0.03996101, 0.01773622,\n        0.07836577, 0.05189516, 0.21696966, 0.05786292, 0.05962564,\n        0.08947374, 0.04780684, 0.13209795, 0.06296797, 0.18522729,\n        0.12441249, 0.03614721, 0.21138901, 0.10233696, 0.10215774,\n        0.04820891, 0.16667374, 0.17328293, 0.03794996, 0.07960459,\n        0.11472293, 0.08932149, 0.01794318, 0.04524802, 0.07987832,\n        0.18342589, 0.0048896 , 0.00530082, 0.26090563, 0.02460495,\n        0.0313102 , 0.17797002, 0.01851477, 0.18591341, 0.08829196,\n        0.04356739, 0.07960325, 0.12178655, 0.01614376, 0.10168777,\n        0.02288544, 0.13287343, 0.05188245, 0.12853207, 0.12999807,\n        0.13337968, 0.03015042, 0.09442344, 0.08705122, 0.08415245,\n        0.17540988, 0.15582279, 0.14589611, 0.02524112, 0.09966723,\n        0.08574016, 0.16832156, 0.119172  , 0.052289  , 0.19453834,\n        0.05346085, 0.08721493, 0.09342031, 0.07427953, 0.09782203,\n        0.08154349, 0.02543633, 0.27488179, 0.04897705, 0.09932784,\n        0.2270581 , 0.10833157, 0.04251109, 0.11566376, 0.07376173,\n        0.0549153 , 0.16695815, 0.04692031, 0.04356361, 0.13768075,\n        0.04178491, 0.07694743, 0.08435779, 0.06521522, 0.02886193]])</pre></li><li>scaler_1|participant_id_offset(chain, draw, participant_id__factor_dim)float640.05816 0.8928 ... -0.5987 -0.07629<pre>array([[[ 0.05815785,  0.89281459, -1.72070993, ...,  0.0870186 ,\n         -1.0537266 , -2.68888698],\n        [-0.54158608,  0.96019242, -0.85502858, ...,  0.32913185,\n         -0.48634642, -1.51089929],\n        [ 0.2002351 ,  1.11554375, -0.15931935, ...,  0.66193168,\n         -1.16415439, -0.85202308],\n        ...,\n        [-1.12131754,  1.50988892, -0.0566591 , ...,  0.00762951,\n         -0.30476515, -1.2250236 ],\n        [-0.35235071,  0.14721624,  0.04704862, ..., -0.48884411,\n         -0.74038562, -0.26226789],\n        [-0.64990326,  1.41503762,  0.45225491, ..., -0.59423638,\n         -0.59874432, -0.07628799]]], shape=(1, 1000, 20))</pre></li><li>a_1|participant_id(chain, draw, participant_id__factor_dim)float64-0.01662 -0.03004 ... 0.01428<pre>array([[[-0.01662183, -0.03004255, -0.01417413, ...,  0.12101847,\n         -0.00584561,  0.0608771 ],\n        [ 0.08242411,  0.02024621,  0.03138023, ...,  0.11866479,\n         -0.01507013,  0.00681003],\n        [ 0.04010624,  0.05536735,  0.04770319, ...,  0.08566125,\n         -0.04251608,  0.00275873],\n        ...,\n        [ 0.02546625,  0.02139674,  0.02826233, ..., -0.02260432,\n          0.01751147,  0.05382402],\n        [ 0.00293344, -0.02290545,  0.02147935, ...,  0.02181695,\n          0.00510547,  0.0131984 ],\n        [ 0.0007985 , -0.00398267,  0.02638743, ...,  0.03311368,\n         -0.00804651,  0.01428182]]], shape=(1, 1000, 20))</pre></li><li>z_1|participant_id_sigma(chain, draw)float640.00492 0.005791 ... 0.02391<pre>array([[0.00491989, 0.00579056, 0.0074441 , 0.00643979, 0.00245682,\n        0.00543548, 0.01248916, 0.00607091, 0.00719514, 0.00150427,\n        0.00170021, 0.00801611, 0.00427387, 0.00092616, 0.02129892,\n        0.02307831, 0.02231576, 0.01088997, 0.01520426, 0.01160025,\n        0.00274968, 0.0089091 , 0.01775793, 0.01624618, 0.01101128,\n        0.00575512, 0.00861218, 0.02051636, 0.01270545, 0.01989488,\n        0.03869494, 0.02960841, 0.00713568, 0.02938418, 0.0086812 ,\n        0.00578054, 0.01448172, 0.00844005, 0.02639904, 0.01021372,\n        0.0107375 , 0.03207331, 0.03313816, 0.01127482, 0.00638755,\n        0.0071244 , 0.0058769 , 0.00612335, 0.00382838, 0.00726529,\n        0.02161403, 0.01047378, 0.02329702, 0.01367762, 0.01836122,\n        0.01663674, 0.01937311, 0.01342901, 0.01455493, 0.01139901,\n        0.02275929, 0.01347695, 0.00573815, 0.00083742, 0.00023218,\n        0.00708994, 0.00944464, 0.00590824, 0.02266395, 0.01884882,\n        0.01852673, 0.01424532, 0.02465111, 0.01469275, 0.01661927,\n        0.00711061, 0.01658726, 0.02262509, 0.01039266, 0.00839315,\n        0.00284973, 0.00836196, 0.03001547, 0.02045738, 0.00317558,\n        0.0064545 , 0.00987383, 0.01704496, 0.02013295, 0.00463856,\n        0.02519563, 0.00755671, 0.01060129, 0.01790545, 0.0139149 ,\n        0.01389597, 0.00696438, 0.00936576, 0.02195523, 0.0264061 ,\n...\n        0.00230654, 0.00212305, 0.01871171, 0.01251532, 0.0124612 ,\n        0.01696136, 0.02007865, 0.01339983, 0.01314714, 0.01010116,\n        0.01825203, 0.02100792, 0.01221691, 0.01717949, 0.00870647,\n        0.02607905, 0.01363287, 0.03002605, 0.01936816, 0.0145294 ,\n        0.01312378, 0.00750425, 0.00696661, 0.00690587, 0.01508316,\n        0.0071477 , 0.02344095, 0.01139574, 0.00398835, 0.0156852 ,\n        0.00407706, 0.00426644, 0.00634134, 0.00804195, 0.01828475,\n        0.01550731, 0.01108918, 0.01346289, 0.02826007, 0.03568245,\n        0.02140006, 0.02756971, 0.02731618, 0.02175558, 0.02316891,\n        0.02143751, 0.01973289, 0.0271815 , 0.03375149, 0.02418228,\n        0.04339554, 0.02298121, 0.03133625, 0.01931691, 0.02477765,\n        0.00344268, 0.01426477, 0.00303824, 0.00545446, 0.00512166,\n        0.00321535, 0.01226526, 0.02597917, 0.00764796, 0.01065695,\n        0.02250729, 0.02525534, 0.02066713, 0.01133463, 0.01191164,\n        0.01174052, 0.00282082, 0.00239297, 0.00751036, 0.01456332,\n        0.00919505, 0.00679071, 0.01195801, 0.01285645, 0.00935799,\n        0.01963981, 0.00911502, 0.02673968, 0.0133325 , 0.02642059,\n        0.0164162 , 0.02801497, 0.01176376, 0.00947875, 0.00273043,\n        0.00635398, 0.01679194, 0.01717792, 0.00051236, 0.01394578,\n        0.02879991, 0.02321684, 0.01631488, 0.00777396, 0.02390892]])</pre></li><li>theta_Intercept(chain, draw)float640.3538 0.3587 ... 0.3633 0.3621<pre>array([[0.35378513, 0.35871432, 0.36349141, 0.3569534 , 0.38645104,\n        0.37364719, 0.39190438, 0.34870226, 0.39528835, 0.36503875,\n        0.37380307, 0.39015077, 0.39216443, 0.38502772, 0.36366708,\n        0.38967961, 0.40475299, 0.39391405, 0.39148225, 0.37241536,\n        0.36125535, 0.37056421, 0.37421543, 0.38330909, 0.37828627,\n        0.36118888, 0.37825735, 0.37120072, 0.38070178, 0.37825277,\n        0.37077443, 0.38020013, 0.3791562 , 0.35011478, 0.37423386,\n        0.37471018, 0.3754635 , 0.3807245 , 0.38123446, 0.34858397,\n        0.39979103, 0.38171391, 0.3901461 , 0.37183075, 0.37548151,\n        0.36752798, 0.39265423, 0.37485955, 0.37652029, 0.37007421,\n        0.3694082 , 0.38591065, 0.38717194, 0.41618991, 0.37016173,\n        0.37663102, 0.3800128 , 0.3998677 , 0.36308518, 0.39958294,\n        0.38374275, 0.37822789, 0.37836398, 0.38151351, 0.36924892,\n        0.38210833, 0.36464998, 0.38216481, 0.39524334, 0.35991591,\n        0.39317798, 0.38769343, 0.38178329, 0.36453202, 0.39463086,\n        0.38978188, 0.37299139, 0.38740477, 0.41016846, 0.38995433,\n        0.364197  , 0.38840383, 0.38041277, 0.37452238, 0.36725453,\n        0.38565668, 0.3637653 , 0.37566207, 0.38089131, 0.36369541,\n        0.37689822, 0.37964618, 0.39119645, 0.3921312 , 0.38758545,\n        0.37537762, 0.39568623, 0.37041133, 0.39095267, 0.36744416,\n...\n        0.37274202, 0.38693538, 0.38454567, 0.39920365, 0.37956687,\n        0.38168832, 0.37162198, 0.37710879, 0.36798413, 0.3936924 ,\n        0.37865187, 0.38839493, 0.36901106, 0.39057439, 0.37246345,\n        0.38880536, 0.37711224, 0.40336544, 0.38517725, 0.39397022,\n        0.37759222, 0.35271155, 0.37873965, 0.36645727, 0.38628891,\n        0.38353996, 0.38626757, 0.3719754 , 0.37232457, 0.40274545,\n        0.34771228, 0.38651038, 0.38159536, 0.35639062, 0.3633836 ,\n        0.38986708, 0.37978639, 0.39185708, 0.36607203, 0.3676719 ,\n        0.37168225, 0.38264759, 0.38247754, 0.37833106, 0.39431684,\n        0.3914877 , 0.37840656, 0.38997239, 0.39998104, 0.38153429,\n        0.37384054, 0.37724475, 0.38367211, 0.36276317, 0.38396537,\n        0.37216683, 0.37623201, 0.38823402, 0.37210485, 0.38048027,\n        0.37770978, 0.36971562, 0.37398579, 0.36415585, 0.37121288,\n        0.3787213 , 0.37648446, 0.39896752, 0.35716992, 0.40190447,\n        0.41195481, 0.35760532, 0.36887229, 0.3842352 , 0.38936885,\n        0.40469189, 0.33035592, 0.41619787, 0.38499332, 0.37796143,\n        0.38123385, 0.36578041, 0.38010795, 0.37893521, 0.37469515,\n        0.33789093, 0.39393838, 0.40021458, 0.36334789, 0.38779268,\n        0.35800917, 0.3464761 , 0.4029188 , 0.37690717, 0.37493058,\n        0.35557172, 0.37269223, 0.34981788, 0.36327578, 0.36212036]])</pre></li><li>a_1|participant_id_offset(chain, draw, participant_id__factor_dim)float64-0.2317 -0.4188 ... -0.3271 0.5805<pre>array([[[-0.23171098, -0.41879792, -0.1975896 , ...,  1.68701676,\n         -0.08148876,  0.84863651],\n        [ 1.21977415,  0.29961867,  0.46438834, ...,  1.75609104,\n         -0.22301909,  0.10078001],\n        [ 0.8912923 ,  1.23044433,  1.0601216 , ...,  1.90367428,\n         -0.94484705,  0.06130805],\n        ...,\n        [ 0.61985202,  0.52079971,  0.68790904, ..., -0.55019229,\n          0.42623162,  1.31008424],\n        [ 0.11238105, -0.87751636,  0.8228818 , ...,  0.83581557,\n          0.19559232,  0.50563555],\n        [ 0.03245844, -0.16189364,  1.07263653, ...,  1.34605525,\n         -0.32708694,  0.58054933]]], shape=(1, 1000, 20))</pre></li><li>scaler_1|participant_id(chain, draw, participant_id__factor_dim)float640.008074 0.124 ... -0.007553<pre>array([[[ 0.00807429,  0.12395304, -0.23889308, ...,  0.01208114,\n         -0.1462931 , -0.373309  ],\n        [-0.07417332,  0.13150385, -0.11710106, ...,  0.04507649,\n         -0.06660793, -0.20692631],\n        [ 0.02372759,  0.13219046, -0.01887913, ...,  0.07843803,\n         -0.13795075, -0.10096361],\n        ...,\n        [-0.15480335,  0.20844752, -0.00782207, ...,  0.00105329,\n         -0.04207431, -0.16912047],\n        [-0.09420137,  0.03935843,  0.0125785 , ..., -0.13069304,\n         -0.19794295, -0.07011762],\n        [-0.06434423,  0.14009702,  0.04477589, ..., -0.05883289,\n         -0.0592792 , -0.00755296]]], shape=(1, 1000, 20))</pre></li><li>rl.alpha_Intercept(chain, draw)float640.6798 0.6869 ... 0.7553 0.6144<pre>array([[0.67981451, 0.68686736, 0.68082074, 0.7901338 , 0.59723767,\n        0.78260379, 0.57688863, 0.79139111, 0.6637761 , 0.74490533,\n        0.84441795, 0.80207425, 0.60019375, 0.8481621 , 0.68522582,\n        0.71024341, 0.66620078, 0.68433684, 0.71670737, 0.66328737,\n        0.78819137, 0.63560364, 0.87508355, 0.68184612, 0.75426002,\n        0.65516793, 0.78055336, 0.61218487, 0.68851134, 0.60475108,\n        0.6784454 , 0.63098478, 0.75832838, 0.6649001 , 0.80314143,\n        0.80186905, 0.71151081, 0.75428863, 0.68474017, 0.75493681,\n        0.80489748, 0.64513994, 0.68157431, 0.72463278, 0.72734395,\n        0.67916935, 0.79659412, 0.74244689, 0.79301655, 0.67322377,\n        0.66685744, 0.72338557, 0.77725868, 0.71432524, 0.7103513 ,\n        0.79667868, 0.65410764, 0.78664728, 0.69308851, 0.82279596,\n        0.68986628, 0.68103067, 0.85526019, 0.62039723, 0.77387761,\n        0.63352147, 0.76467683, 0.59216205, 0.80968998, 0.67409506,\n        0.73088954, 0.76919809, 0.74533741, 0.71609783, 0.66974316,\n        0.84001144, 0.64318688, 0.71115672, 0.66212016, 0.72574243,\n        0.71272818, 0.71898769, 0.74754889, 0.68193669, 0.80012137,\n        0.64005786, 0.6741602 , 0.76564657, 0.76930392, 0.75622214,\n        0.74222253, 0.81114309, 0.83619285, 0.61151945, 0.75947429,\n        0.67126027, 0.63481295, 0.6959517 , 0.6851581 , 0.82482544,\n...\n        0.68827554, 0.83737369, 0.87204962, 0.68163882, 0.76682429,\n        0.65783056, 0.65368253, 0.69043754, 0.82216799, 0.6538156 ,\n        0.93932833, 0.90391781, 0.66287597, 0.85534831, 0.83893325,\n        0.67309521, 0.75384936, 0.84359486, 0.79273253, 0.73031399,\n        0.61239545, 0.81222361, 0.62413585, 0.70724998, 0.63648772,\n        0.75173204, 0.7345834 , 0.81831758, 0.72086196, 0.72667022,\n        0.65978129, 0.7086867 , 0.71688173, 0.67320289, 0.64837575,\n        0.71288695, 0.67323977, 0.75037787, 0.66395608, 0.77510554,\n        0.73260492, 0.6391988 , 0.63660074, 0.59685252, 0.8475497 ,\n        0.83617637, 0.65506532, 0.71788155, 0.8429329 , 0.76410201,\n        0.62105728, 0.63528185, 0.69773924, 0.83157808, 0.67764764,\n        0.73283927, 0.68416303, 0.74281402, 0.63772126, 0.83335348,\n        0.67256604, 0.66415624, 0.66412155, 0.68714561, 0.76360103,\n        0.67999903, 0.68371581, 0.70509652, 0.67579059, 0.73179815,\n        0.66667485, 0.80329659, 0.54617839, 0.89184386, 0.80848606,\n        0.78835418, 0.68163496, 0.76682457, 0.66981904, 0.68532742,\n        0.71204748, 0.59344547, 0.849233  , 0.59440271, 0.83002553,\n        0.80473961, 0.70213396, 0.72167669, 0.71155166, 0.71790473,\n        0.68828105, 0.73439188, 0.76649404, 0.60220135, 0.82280767,\n        0.64489571, 0.73529588, 0.66229025, 0.75534503, 0.61442176]])</pre></li><li>t_1|participant_id_offset(chain, draw, participant_id__factor_dim)float64-0.3819 -1.056 ... -0.1613 -0.8788<pre>array([[[-0.38194485, -1.05586571,  0.02628284, ...,  0.49830232,\n          1.537318  , -1.08462159],\n        [-0.06793551, -0.39318713, -0.8529339 , ..., -0.69095614,\n          0.82344507,  0.62733361],\n        [ 0.06711075, -0.62281545, -0.80937158, ..., -0.20139458,\n          0.77929227,  1.08127646],\n        ...,\n        [-0.85929822,  0.23799789, -0.13492851, ...,  2.25398645,\n          0.97625651,  0.73325622],\n        [-0.77171718, -0.19800345, -0.02239689, ...,  0.96131147,\n          0.22079336, -0.53635593],\n        [-0.62258993,  0.28542593, -0.86663604, ...,  0.20092424,\n         -0.1612862 , -0.87878929]]], shape=(1, 1000, 20))</pre></li><li>rl.alpha_1|participant_id(chain, draw, rl.alpha_1|participant_id__factor_dim)float64-0.0368 -0.002968 ... 0.02581<pre>array([[[-0.03679909, -0.00296798,  0.02187738, ..., -0.0145418 ,\n          0.01633742, -0.04137242],\n        [-0.07928371,  0.0038912 ,  0.01967048, ..., -0.00428708,\n          0.01794866, -0.02987455],\n        [-0.0499396 , -0.0053482 ,  0.01877159, ..., -0.01667856,\n          0.01620367, -0.02774294],\n        ...,\n        [ 0.0325509 , -0.06555012,  0.09426108, ..., -0.04161504,\n         -0.03901267, -0.02799446],\n        [ 0.02629609,  0.02591385,  0.05631842, ...,  0.06308307,\n         -0.08170626, -0.06333946],\n        [-0.00738545,  0.00101153,  0.01249355, ...,  0.03750928,\n         -0.02891021,  0.02580884]]], shape=(1, 1000, 20))</pre></li><li>t_1|participant_id_sigma(chain, draw)float640.005011 0.01054 ... 0.01283<pre>array([[0.00501128, 0.01054137, 0.01524848, 0.01572233, 0.01123321,\n        0.02088035, 0.01831254, 0.00327885, 0.03241795, 0.02746393,\n        0.03341502, 0.02660728, 0.01023465, 0.00602655, 0.02133911,\n        0.01639495, 0.01641469, 0.00732684, 0.01476469, 0.01018447,\n        0.01110854, 0.01133345, 0.0181967 , 0.03814336, 0.01698099,\n        0.01707232, 0.02430018, 0.03937436, 0.01254125, 0.01056846,\n        0.01432492, 0.00708839, 0.01031127, 0.01530797, 0.02174614,\n        0.00465919, 0.01105182, 0.01017195, 0.00896005, 0.00954741,\n        0.01119766, 0.00811662, 0.01461603, 0.0127413 , 0.02828874,\n        0.02555107, 0.01734767, 0.0190448 , 0.02087055, 0.03583015,\n        0.04173709, 0.02971832, 0.03584903, 0.01910053, 0.01942048,\n        0.02354357, 0.03057129, 0.01812464, 0.01374013, 0.00973919,\n        0.02834382, 0.01809401, 0.01016741, 0.0220994 , 0.02996113,\n        0.02165008, 0.01708937, 0.02227103, 0.0179003 , 0.01921443,\n        0.06066329, 0.03116757, 0.02708662, 0.03702384, 0.02070866,\n        0.0223791 , 0.02895745, 0.03398312, 0.01433851, 0.02544746,\n        0.02896384, 0.02104393, 0.0236799 , 0.00513392, 0.0071528 ,\n        0.01230246, 0.01748626, 0.01516094, 0.01257643, 0.01030019,\n        0.02645552, 0.02005504, 0.00846291, 0.01688451, 0.03032933,\n        0.01407554, 0.01482068, 0.02838158, 0.01617578, 0.01436258,\n...\n        0.00769785, 0.00722986, 0.00483249, 0.00793816, 0.02273649,\n        0.02143647, 0.01953009, 0.01886915, 0.00800449, 0.03452717,\n        0.02255402, 0.00497536, 0.02768511, 0.01233675, 0.0166275 ,\n        0.02297582, 0.00931896, 0.01593861, 0.01705365, 0.01792476,\n        0.02646566, 0.00765189, 0.00926627, 0.01018181, 0.01377083,\n        0.00187705, 0.00207205, 0.01604242, 0.02983713, 0.01253906,\n        0.00325928, 0.00345325, 0.00371499, 0.01734627, 0.01096586,\n        0.00860599, 0.02413122, 0.02761615, 0.03154432, 0.03780112,\n        0.03245063, 0.02163091, 0.02759888, 0.02347867, 0.02736987,\n        0.02984709, 0.03566231, 0.03449733, 0.04822428, 0.01044563,\n        0.04570791, 0.03929428, 0.02525433, 0.02761916, 0.0190146 ,\n        0.0089807 , 0.01070961, 0.00946985, 0.00916451, 0.03357767,\n        0.02613565, 0.01686198, 0.02538053, 0.00329155, 0.02211504,\n        0.02594151, 0.02770055, 0.013735  , 0.015314  , 0.03002161,\n        0.02380949, 0.02196524, 0.01889795, 0.01659494, 0.00675431,\n        0.0043088 , 0.00802228, 0.00460835, 0.01817774, 0.02026742,\n        0.01911117, 0.03292088, 0.01199471, 0.02061779, 0.01263754,\n        0.01009841, 0.01969273, 0.00482563, 0.0040375 , 0.00457878,\n        0.03901345, 0.02216891, 0.02262017, 0.00386708, 0.02010124,\n        0.02148183, 0.01816215, 0.01979764, 0.02497194, 0.01283101]])</pre></li><li>a_1|participant_id_sigma(chain, draw)float640.07174 0.06757 ... 0.0261 0.0246<pre>array([[0.07173519, 0.06757325, 0.04499785, 0.04636885, 0.01237667,\n        0.04267171, 0.05554642, 0.06060978, 0.05215793, 0.02850317,\n        0.03103155, 0.04809045, 0.0514551 , 0.0457768 , 0.05866729,\n        0.08722717, 0.05298975, 0.04059   , 0.02766446, 0.02265341,\n        0.02748525, 0.06488209, 0.06979739, 0.0523025 , 0.04009004,\n        0.03088425, 0.04057532, 0.09104736, 0.03253716, 0.08663923,\n        0.09066234, 0.07936465, 0.0375405 , 0.05337286, 0.05909928,\n        0.05040438, 0.04776153, 0.04988124, 0.0486959 , 0.02952079,\n        0.06877796, 0.02551684, 0.03858532, 0.02931617, 0.04900554,\n        0.04670761, 0.03373212, 0.02967967, 0.01478621, 0.02988847,\n        0.03320885, 0.05277283, 0.02446208, 0.04197646, 0.04747993,\n        0.04228589, 0.02749925, 0.05285338, 0.06093665, 0.0511434 ,\n        0.02655579, 0.05046508, 0.02910316, 0.03803855, 0.04127443,\n        0.04516493, 0.02779485, 0.04700665, 0.07164582, 0.08754618,\n        0.0449107 , 0.02990185, 0.05205801, 0.05596508, 0.07221751,\n        0.05635773, 0.04838369, 0.04814239, 0.04486825, 0.05520174,\n        0.04344116, 0.03654437, 0.02366813, 0.06180248, 0.03462425,\n        0.04867306, 0.06028767, 0.06267276, 0.03060344, 0.05718269,\n        0.03798494, 0.05887488, 0.04177753, 0.05378804, 0.0655356 ,\n        0.01197282, 0.02412687, 0.03827697, 0.02230057, 0.01342497,\n...\n        0.07108901, 0.06264612, 0.05273294, 0.05212277, 0.04830978,\n        0.04682725, 0.04120849, 0.07315502, 0.06035002, 0.07576027,\n        0.03806807, 0.04511131, 0.02223927, 0.08206318, 0.0638094 ,\n        0.04557301, 0.05912782, 0.05119268, 0.072299  , 0.07378679,\n        0.04073594, 0.05577519, 0.02791974, 0.06947737, 0.03069103,\n        0.05158493, 0.02453049, 0.05007425, 0.0485951 , 0.04382479,\n        0.06784222, 0.03155337, 0.03299433, 0.04583786, 0.05865193,\n        0.0570478 , 0.05595955, 0.02895182, 0.03059495, 0.02830817,\n        0.07088474, 0.07970198, 0.0797958 , 0.06371377, 0.06982702,\n        0.06716872, 0.06892839, 0.04619751, 0.06248188, 0.0696298 ,\n        0.04804402, 0.08522746, 0.04549831, 0.0624039 , 0.03833435,\n        0.0506618 , 0.0578271 , 0.04922972, 0.02713636, 0.04292028,\n        0.05746022, 0.09334869, 0.04619903, 0.04758745, 0.05265474,\n        0.08781584, 0.09474239, 0.0687736 , 0.03655969, 0.04010972,\n        0.03397715, 0.03441265, 0.06821884, 0.06789032, 0.05941363,\n        0.0499974 , 0.04632217, 0.05074637, 0.04289316, 0.05860868,\n        0.05266718, 0.04189182, 0.11079897, 0.0381235 , 0.08006359,\n        0.04836054, 0.04960625, 0.05342488, 0.04216447, 0.05301546,\n        0.027539  , 0.03143972, 0.04974105, 0.0419891 , 0.07074808,\n        0.05718809, 0.03930726, 0.0410844 , 0.02610259, 0.02460054]])</pre></li><li>rl.alpha_neg_Intercept(chain, draw)float640.6904 0.7299 ... 0.7092 0.6561<pre>array([[0.69038683, 0.72993853, 0.7449339 , 0.64959241, 0.7358063 ,\n        0.56610348, 0.63400391, 0.70712486, 0.70312414, 0.58867178,\n        0.60437564, 0.56343472, 0.80628413, 0.5610069 , 0.84320671,\n        0.60689762, 0.79474184, 0.75285952, 0.78956679, 0.78258027,\n        0.65821826, 0.74786894, 0.58981609, 0.81946753, 0.64302529,\n        0.6890724 , 0.70809536, 0.66629117, 0.72622853, 0.74892018,\n        0.78987283, 0.80853928, 0.55621314, 0.70445503, 0.65728093,\n        0.63167132, 0.79657891, 0.57724362, 0.64194875, 0.66383771,\n        0.65373719, 0.72209192, 0.72280303, 0.69331169, 0.63539943,\n        0.66447656, 0.69556475, 0.67674146, 0.60861006, 0.70831231,\n        0.69182503, 0.7437137 , 0.67754572, 0.68430712, 0.77185816,\n        0.69540557, 0.6566537 , 0.70667704, 0.65619659, 0.71714523,\n        0.65192555, 0.68259091, 0.67759049, 0.68994186, 0.67629339,\n        0.69068016, 0.64254273, 0.81895804, 0.63204506, 0.71560487,\n        0.66035689, 0.65991725, 0.71963277, 0.67202727, 0.64756835,\n        0.66592885, 0.66469185, 0.59403817, 0.74619752, 0.72349888,\n        0.6434175 , 0.69099596, 0.59321089, 0.73872918, 0.60717964,\n        0.80676915, 0.6321594 , 0.66167166, 0.70488998, 0.64940243,\n        0.77911958, 0.58352784, 0.62645531, 0.76020457, 0.71271209,\n        0.62371312, 0.7181133 , 0.6306151 , 0.68634513, 0.57125826,\n...\n        0.6778626 , 0.64462731, 0.77318772, 0.62675624, 0.70165054,\n        0.6794069 , 0.71044425, 0.69523635, 0.68801706, 0.63134453,\n        0.67810832, 0.58096296, 0.81764392, 0.62740541, 0.67243539,\n        0.69584001, 0.71009613, 0.58610207, 0.76055133, 0.68040318,\n        0.75626844, 0.53410677, 0.71096767, 0.70722735, 0.68572318,\n        0.6858935 , 0.64891125, 0.76460494, 0.6334908 , 0.66689878,\n        0.69789812, 0.75922621, 0.65487747, 0.73391358, 0.69597517,\n        0.68389748, 0.68557007, 0.65230061, 0.68104211, 0.74261849,\n        0.67051668, 0.76690238, 0.75064105, 0.85506839, 0.49090012,\n        0.50390318, 0.83561221, 0.76189344, 0.54945598, 0.88430624,\n        0.9261316 , 0.9287266 , 0.86586296, 0.53418688, 0.66337421,\n        0.66773149, 0.72422049, 0.66359905, 0.6138246 , 0.70289211,\n        0.7862396 , 0.75785777, 0.70385503, 0.68031133, 0.66739917,\n        0.72694522, 0.72650672, 0.65807471, 0.79952224, 0.69639962,\n        0.64432701, 0.70862006, 0.6959433 , 0.61072779, 0.65562685,\n        0.69047368, 0.73734077, 0.65500192, 0.78178878, 0.76598014,\n        0.74271528, 0.7199929 , 0.61413797, 0.68290769, 0.64092818,\n        0.66921406, 0.6663995 , 0.66369316, 0.73950862, 0.66519456,\n        0.69377535, 0.69213989, 0.61052153, 0.81529134, 0.58891638,\n        0.69488287, 0.7097902 , 0.69254868, 0.70920397, 0.65613717]])</pre></li><li>rl.alpha_1|participant_id_offset(chain, draw, rl.alpha_1|participant_id__factor_dim)float64-0.993 -0.08009 ... -1.002 0.8942<pre>array([[[-0.99297873, -0.08008733,  0.5903345 , ..., -0.39239292,\n          0.44084537, -1.11638438],\n        [-2.25666873,  0.11075601,  0.55988493, ..., -0.12202411,\n          0.51087658, -0.85032562],\n        [-1.72458347, -0.18469131,  0.64824644, ..., -0.57596713,\n          0.55956764, -0.95805761],\n        ...,\n        [ 0.38586717, -0.77704876,  1.11739629, ..., -0.49331586,\n         -0.46246674, -0.33185388],\n        [ 0.4032201 ,  0.39735886,  0.86357775, ...,  0.96730587,\n         -1.25287092, -0.97123734],\n        [-0.25588914,  0.03504735,  0.43287293, ...,  1.29961124,\n         -1.00167305,  0.89421751]]], shape=(1, 1000, 20))</pre></li><li>t_1|participant_id_mu(chain, draw)float640.01511 -0.2057 ... 0.3788 0.2383<pre>array([[ 0.01511467, -0.20572407, -0.12132609,  0.29957555, -0.36195211,\n         0.08692273, -0.04953266,  0.25743939, -0.25711457,  0.13201344,\n        -0.10551054, -0.08230923, -0.2923595 ,  0.32416111, -0.1016563 ,\n         0.13287597,  0.43676477,  0.04412088,  0.39005455,  0.10163718,\n        -0.08182372, -0.05548793,  0.25117531, -0.12116738,  0.24189656,\n        -0.165815  , -0.33789161, -0.03977139,  0.29917878, -0.63894304,\n        -0.11424422, -0.64521075,  0.16773168, -0.20398306, -0.07798314,\n        -0.12613954,  0.16124638,  0.48045142, -0.3773307 ,  0.37257459,\n        -0.42499062,  0.26620606,  0.10669677, -0.14281455,  0.19341479,\n         0.33253584, -0.32607866,  0.33052039,  0.08143363, -0.05091278,\n         0.03071056, -0.45228429, -0.2243013 ,  0.11463483,  0.14855446,\n        -0.31041119,  0.30208406, -0.30463904,  0.29198677, -0.12261104,\n         0.02274629, -0.0068877 , -0.09590145, -0.5335171 ,  0.52055268,\n        -0.48551565,  0.25294147, -0.31554745,  0.37304265,  0.06846637,\n        -0.08281051,  0.20657642, -0.2660431 ,  0.13790166, -0.12815107,\n        -0.31911208, -0.31785483,  0.3160004 , -0.33539657,  0.07793024,\n        -0.00317632, -0.02126081, -0.06100694,  0.05115548, -0.02259676,\n         0.18874049, -0.31440947, -0.44099992, -0.52542371,  0.32414841,\n        -0.29373919,  0.15891968, -0.03847389,  0.06819933, -0.3513549 ,\n         0.46970234, -0.04929152,  0.0364425 , -0.02323526,  0.04323053,\n...\n        -0.23086025, -0.10247109,  0.01207696, -0.02682633,  0.07742928,\n        -0.08884781,  0.06236189,  0.17944289, -0.25673309,  0.28788289,\n        -0.32646717,  0.34280961, -0.37891675,  0.43436695,  0.51777332,\n        -0.43455461,  0.24662785, -0.31914911,  0.16405783, -0.04071189,\n         0.12011175,  0.13449565,  0.0397748 , -0.19722587,  0.46897564,\n        -0.44703548, -0.23853413,  0.12142134,  0.00669417, -0.13471392,\n         0.11397265, -0.14171136,  0.23128641,  0.08774496,  0.14052393,\n        -0.47310229, -0.04329747, -0.09637734,  0.12819152, -0.35142379,\n        -0.22245996,  0.26530796,  0.20436492, -0.16140517,  0.06555417,\n         0.05725287, -0.09859644, -0.01573384,  0.2504415 , -0.26193004,\n        -0.05368492,  0.17603271, -0.02226524,  0.11519561,  0.10856586,\n         0.04846163,  0.29524464, -0.02499308,  0.05609767, -0.06856198,\n         0.10040024, -0.08345717,  0.01023871,  0.10211745, -0.08120329,\n         0.15764293,  0.12175832, -0.32749953,  0.33365644, -0.24620413,\n        -0.17989596,  0.27716284,  0.42998776, -0.54015241, -0.26307347,\n        -0.04092977,  0.11086023, -0.12574464,  0.38986765, -0.35788988,\n        -0.21738949, -0.37756401,  0.4601979 , -0.45965829,  0.4714485 ,\n        -0.32598537, -0.01618817,  0.08385242,  0.06245422, -0.11120679,\n         0.09760044,  0.45732136,  0.20062178, -0.21334609,  0.16994305,\n        -0.21298984,  0.21129826,  0.01205766,  0.37883855,  0.23826645]])</pre></li><li>z_1|participant_id_offset(chain, draw, participant_id__factor_dim)float641.153 -1.022 ... -0.6438 -1.911<pre>array([[[ 1.15270492, -1.0217935 ,  0.88096742, ...,  1.1647648 ,\n          0.01443685,  1.86731971],\n        [ 1.25122774,  0.16012276, -0.1706174 , ...,  0.29988808,\n         -0.25394959,  1.19874627],\n        [ 0.11431206, -0.06624679, -0.66381407, ...,  0.31994262,\n         -0.25568053,  1.76222547],\n        ...,\n        [-0.59279102, -1.15911924, -1.84826571, ...,  0.11254186,\n         -1.10042359, -0.952858  ],\n        [-2.06814001, -1.28752815, -1.1797962 , ...,  0.21728314,\n          0.03660396, -1.43134713],\n        [-0.2176119 ,  0.5409327 , -0.4124358 , ..., -0.77447715,\n         -0.64377527, -1.91069582]]], shape=(1, 1000, 20))</pre></li><li>z_1|participant_id(chain, draw, participant_id__factor_dim)float640.005671 -0.005027 ... -0.04568<pre>array([[[ 5.67118581e-03, -5.02711552e-03,  4.33426616e-03, ...,\n          5.73051911e-03,  7.10277927e-05,  9.18701467e-03],\n        [ 7.24531345e-03,  9.27200993e-04, -9.87970832e-04, ...,\n          1.73652089e-03, -1.47051120e-03,  6.94141621e-03],\n        [ 8.50950493e-04, -4.93147731e-04, -4.94149862e-03, ...,\n          2.38168502e-03, -1.90331155e-03,  1.31181835e-02],\n        ...,\n        [-9.67131312e-03, -1.89108890e-02, -3.01542297e-02, ...,\n          1.83610679e-03, -1.79532767e-02, -1.55457621e-02],\n        [-1.60776395e-02, -1.00091935e-02, -9.17168949e-03, ...,\n          1.68915061e-03,  2.84557715e-04, -1.11272366e-02],\n        [-5.20286564e-03,  1.29331170e-02, -9.86089483e-03, ...,\n         -1.85169126e-02, -1.53919716e-02, -4.56826744e-02]]],\n      shape=(1, 1000, 20))</pre></li><li>scaler_1|participant_id_sigma(chain, draw)float640.1388 0.137 ... 0.2674 0.09901<pre>array([[0.13883402, 0.13695573, 0.11849868, 0.2370089 , 0.14949371,\n        0.15389322, 0.13428827, 0.12770186, 0.13803292, 0.11683156,\n        0.1172641 , 0.13595729, 0.10990215, 0.11187867, 0.09951282,\n        0.10291779, 0.10985967, 0.13976347, 0.1391927 , 0.13752252,\n        0.18606117, 0.10043801, 0.07932981, 0.02230673, 0.04396884,\n        0.12340407, 0.19749686, 0.08094822, 0.03136297, 0.00714891,\n        0.0065612 , 0.04850566, 0.12790267, 0.12973162, 0.07631307,\n        0.15333151, 0.15028776, 0.15365694, 0.01957529, 0.11991931,\n        0.04479693, 0.01791469, 0.06115986, 0.17033434, 0.06145518,\n        0.15351383, 0.17869024, 0.21799441, 0.15775311, 0.21986665,\n        0.11163786, 0.08950804, 0.08507637, 0.03602241, 0.14550513,\n        0.08859383, 0.12975492, 0.07733683, 0.12455398, 0.15202855,\n        0.13445165, 0.08721999, 0.14038856, 0.06489532, 0.05724345,\n        0.17623275, 0.13648628, 0.1160806 , 0.17573855, 0.10768908,\n        0.06717272, 0.18058678, 0.13521496, 0.17168171, 0.1304983 ,\n        0.09483273, 0.11210288, 0.09765789, 0.12329164, 0.12678427,\n        0.14670532, 0.13399923, 0.08466465, 0.10880114, 0.10123768,\n        0.17289792, 0.0464579 , 0.15901177, 0.10550302, 0.09532858,\n        0.09053835, 0.06695293, 0.06811207, 0.01233338, 0.12784902,\n        0.12446723, 0.11835842, 0.12501622, 0.09850538, 0.02374594,\n...\n        0.09596475, 0.1367606 , 0.18861998, 0.11180634, 0.06732487,\n        0.10590262, 0.08391199, 0.0614631 , 0.10983597, 0.05508622,\n        0.06829853, 0.09510736, 0.112354  , 0.07264883, 0.04303162,\n        0.02161306, 0.07118339, 0.04347767, 0.03558323, 0.0258348 ,\n        0.09562234, 0.17696993, 0.14855759, 0.19776204, 0.17646268,\n        0.22519951, 0.08488752, 0.02298579, 0.06287322, 0.19317189,\n        0.16866511, 0.06259875, 0.10964955, 0.10266083, 0.11543212,\n        0.08952323, 0.15077565, 0.01614116, 0.02593329, 0.06664143,\n        0.0457872 , 0.02988396, 0.02502704, 0.03138401, 0.03372625,\n        0.03236418, 0.01734978, 0.03269736, 0.10073787, 0.03431711,\n        0.05601549, 0.01738471, 0.01977466, 0.05514682, 0.14093553,\n        0.12028046, 0.11640691, 0.10818165, 0.10059389, 0.11934271,\n        0.18416174, 0.17706813, 0.04270399, 0.13361721, 0.08517431,\n        0.11690971, 0.12934158, 0.15462919, 0.06827055, 0.08945355,\n        0.09625059, 0.1678879 , 0.21573586, 0.11333648, 0.08457554,\n        0.09229381, 0.11676618, 0.04847994, 0.066074  , 0.06814324,\n        0.04756327, 0.04610921, 0.05970327, 0.05357878, 0.10954619,\n        0.09958176, 0.01296234, 0.02367471, 0.03755727, 0.0652909 ,\n        0.03995674, 0.07921431, 0.09379009, 0.14253633, 0.11984931,\n        0.13391687, 0.29358021, 0.13805487, 0.26735115, 0.09900586]])</pre></li><li>rl.alpha_neg_1|participant_id_offset(chain, draw, participant_id__factor_dim)float640.914 -0.4167 1.184 ... 1.136 2.077<pre>array([[[ 0.91395466, -0.4167473 ,  1.18360938, ..., -0.64665616,\n          0.11915031,  1.04947734],\n        [ 1.12738671, -0.58821763, -1.12966871, ..., -0.04491387,\n          0.37136067,  0.08228087],\n        [ 0.0107396 , -0.47477346, -1.01323521, ...,  0.03814794,\n          0.78954875,  0.78061348],\n        ...,\n        [ 0.47141322, -0.68800843, -0.4876386 , ..., -1.09840104,\n          0.52757893,  0.91085337],\n        [ 1.48082587, -0.59103129, -0.73255405, ...,  1.02278363,\n          0.87348162,  0.71020961],\n        [ 0.46571013,  0.54413125, -0.92527178, ...,  1.25565021,\n          1.13607474,  2.07711204]]], shape=(1, 1000, 20))</pre></li><li>t_1|participant_id(chain, draw, participant_id__factor_dim)float64-0.001914 -0.005291 ... -0.01128<pre>array([[[-0.00191403, -0.00529124,  0.00013171, ...,  0.00249713,\n          0.00770393, -0.00543534],\n        [-0.00071613, -0.00414473, -0.00899109, ..., -0.00728362,\n          0.00868024,  0.00661295],\n        [ 0.00102334, -0.00949699, -0.01234169, ..., -0.00307096,\n          0.01188302,  0.01648782],\n        ...,\n        [-0.01701208,  0.0047118 , -0.00267127, ...,  0.04462361,\n          0.01932757,  0.01451674],\n        [-0.01927128, -0.00494453, -0.00055929, ...,  0.02400581,\n          0.00551364, -0.01339385],\n        [-0.00798846,  0.0036623 , -0.01111982, ...,  0.00257806,\n         -0.00206947, -0.01127576]]], shape=(1, 1000, 20))</pre></li><li>rl.alpha_neg_1|participant_id_mu(chain, draw)float64-0.1912 0.1659 ... 0.2265 -0.254<pre>array([[-1.91163480e-01,  1.65893333e-01, -2.41744087e-01,\n         1.27074881e-02,  8.65219394e-02, -1.68640810e-01,\n        -2.80162179e-01,  2.48999455e-01, -2.53842287e-01,\n        -2.78177931e-02,  7.82455227e-02,  1.37383617e-01,\n         1.02398968e-01, -1.35284889e-01,  9.71249071e-02,\n        -1.61088444e-01,  4.34891944e-02, -1.68586124e-01,\n        -3.64298683e-01,  1.30713495e-01, -1.52538122e-01,\n         1.29101166e-01, -1.84451937e-01, -9.33884279e-03,\n        -1.35247569e-01, -2.01169718e-01, -4.90062036e-01,\n         2.27070775e-01, -2.49072008e-02, -4.63648471e-01,\n        -5.41428038e-01,  2.33896144e-01, -2.61038191e-01,\n         5.11841845e-01, -3.44227470e-01,  2.79644825e-01,\n        -2.59579326e-01,  4.18372793e-01, -4.27188476e-01,\n         4.40970589e-01, -2.44833096e-01,  1.76681010e-01,\n        -1.50931194e-01,  3.93936977e-02, -4.17818146e-02,\n        -1.39548354e-01, -2.97959458e-01,  1.60627197e-01,\n        -1.92047887e-01,  1.45341660e-01, -6.31980763e-02,\n         5.20091913e-01,  7.69077395e-02, -1.42362001e-01,\n         4.04356217e-02,  1.21898531e-02, -2.30951203e-01,\n         9.68739609e-02, -1.04688897e-01,  2.08765613e-01,\n...\n        -1.00663932e-01,  1.25416575e-01, -3.28134887e-02,\n        -8.43287247e-02,  2.46257734e-02,  1.78017922e-01,\n         2.97961190e-01, -3.92148217e-01, -1.82328010e-01,\n        -4.84184010e-02, -8.50307090e-02,  1.34247380e-01,\n        -1.62616159e-01,  3.34091534e-01, -1.01267376e-01,\n        -6.01855650e-02,  3.69622343e-02, -1.01871409e-02,\n         2.67536657e-01, -2.86256618e-01,  2.46299046e-01,\n        -2.56669684e-01,  1.30927446e-01, -2.22288078e-01,\n        -2.47523962e-01, -2.99566265e-01,  3.32075591e-01,\n        -9.51815525e-02, -2.50111982e-01,  8.20012610e-02,\n         5.18387265e-02,  1.19466233e-02,  2.52066309e-01,\n         2.20709044e-01, -2.01003141e-01,  1.96076467e-01,\n         4.18712536e-01, -4.57571314e-01, -4.40509016e-01,\n        -4.43448800e-01,  4.28154330e-01, -4.41594413e-01,\n         3.93838771e-01, -3.44465968e-01,  1.26766927e-01,\n         1.31476871e-01, -1.40187871e-01, -1.05196070e-02,\n         6.78692385e-02,  1.33894131e-01,  4.15525201e-01,\n        -6.88482234e-02,  4.93912388e-02,  1.16871128e-01,\n        -9.04504771e-02, -1.78027319e-02,  2.26549246e-01,\n        -2.53989099e-01]])</pre></li><li>t_Intercept(chain, draw)float640.428 0.4182 ... 0.413 0.4232<pre>array([[0.42795654, 0.41823627, 0.42314501, 0.41735082, 0.41619463,\n        0.4071601 , 0.44056347, 0.40919825, 0.44195089, 0.40883328,\n        0.41523489, 0.4073094 , 0.41784601, 0.41449605, 0.41930529,\n        0.41235779, 0.41161047, 0.42033639, 0.41774312, 0.41379587,\n        0.41383922, 0.40693655, 0.42955936, 0.41418968, 0.41254351,\n        0.42924322, 0.42080165, 0.43080557, 0.43605201, 0.41229373,\n        0.42057533, 0.42022927, 0.41618287, 0.42128428, 0.41670349,\n        0.42299527, 0.41732937, 0.42164025, 0.41553528, 0.42166416,\n        0.41137296, 0.42285852, 0.40846821, 0.41484647, 0.42059521,\n        0.42133152, 0.42158073, 0.42040357, 0.42984636, 0.42743805,\n        0.42278639, 0.42179975, 0.41363187, 0.42318039, 0.42539817,\n        0.4297697 , 0.4235959 , 0.41558856, 0.43636193, 0.4108576 ,\n        0.43260185, 0.42834442, 0.42367378, 0.41962605, 0.43243334,\n        0.42984584, 0.42561359, 0.41852482, 0.41641895, 0.43145826,\n        0.40381442, 0.41479119, 0.41702356, 0.43002545, 0.42010149,\n        0.41170148, 0.43302701, 0.39791374, 0.42541215, 0.40845445,\n        0.42475907, 0.43404504, 0.41883115, 0.41076757, 0.42565302,\n        0.40129423, 0.41524857, 0.40723944, 0.42275631, 0.42901171,\n        0.39332584, 0.43940802, 0.43148146, 0.41921352, 0.41008698,\n        0.4226552 , 0.4125507 , 0.43822428, 0.4175532 , 0.42676599,\n...\n        0.41480516, 0.41197798, 0.43395988, 0.41586865, 0.42108214,\n        0.42042501, 0.4267316 , 0.42136331, 0.42299723, 0.42363132,\n        0.41824481, 0.42352098, 0.42396242, 0.42704201, 0.43566468,\n        0.40643914, 0.43827992, 0.41769007, 0.4370298 , 0.41055008,\n        0.41835593, 0.41902956, 0.42407741, 0.43170136, 0.40590685,\n        0.42157009, 0.40587898, 0.42127369, 0.41705206, 0.42779208,\n        0.41438404, 0.42013267, 0.41157184, 0.42234159, 0.42644471,\n        0.40914155, 0.42900852, 0.41427944, 0.41756009, 0.41107444,\n        0.40630463, 0.43954635, 0.43817099, 0.43187778, 0.41244353,\n        0.41295888, 0.41980498, 0.40478264, 0.43506238, 0.41738462,\n        0.42641597, 0.44004422, 0.43103076, 0.42204631, 0.42046083,\n        0.41429288, 0.41158925, 0.41381992, 0.40917449, 0.43091907,\n        0.43580473, 0.42347087, 0.42031143, 0.43045823, 0.41568809,\n        0.4309304 , 0.43056802, 0.41485988, 0.41291569, 0.41557259,\n        0.43375865, 0.43006285, 0.42962073, 0.42424283, 0.41040709,\n        0.42301603, 0.42372394, 0.42292936, 0.40514352, 0.42786558,\n        0.43132346, 0.43730487, 0.40677776, 0.4376037 , 0.40862596,\n        0.43660138, 0.40711775, 0.41325803, 0.4133131 , 0.40825353,\n        0.41293523, 0.4140093 , 0.43740441, 0.41520185, 0.42968083,\n        0.4230516 , 0.42253972, 0.42738669, 0.41296092, 0.42321183]])</pre></li><li>z_1|participant_id_mu(chain, draw)float64-0.3845 -0.2661 ... 0.03367 0.3078<pre>array([[-0.38445513, -0.26610365, -0.08498686, -0.00518097,  0.08367884,\n        -0.29467574,  0.35431146, -0.14396236,  0.10381403,  0.21667349,\n        -0.17901817, -0.33327185,  0.24754052, -0.25919081,  0.2637896 ,\n        -0.14456564, -0.05142755, -0.28325005,  0.02739128,  0.09912178,\n        -0.09467615,  0.08298222, -0.09712408,  0.03332864, -0.07969637,\n         0.29089948,  0.13634389,  0.15101199, -0.13260154,  0.06449909,\n        -0.00387945, -0.25134126, -0.16414408,  0.03717098, -0.09373394,\n         0.07116787, -0.13867478,  0.12306532, -0.07585001,  0.06690887,\n         0.22945518,  0.03724327, -0.06931435, -0.28061768, -0.05765187,\n        -0.37273685,  0.58480359, -0.6801243 , -0.27759224,  0.20521889,\n        -0.22289734, -0.01145584,  0.15782034, -0.13648454,  0.36663022,\n         0.17724395,  0.08639322, -0.08428371,  0.04685814, -0.15394183,\n         0.17809488, -0.08092882,  0.24664464,  0.27328059, -0.2622707 ,\n         0.34573446,  0.14108368, -0.14403198,  0.16835908, -0.32129244,\n         0.31566407, -0.13657995,  0.07744273, -0.20195604,  0.24918578,\n         0.17200137, -0.22721794,  0.22269238, -0.20577418,  0.38482834,\n        -0.51282137,  0.47796978, -0.62885922,  0.64295301, -0.57610829,\n         0.46871674, -0.49890055, -0.47671215,  0.31561297, -0.33683737,\n         0.40052802, -0.19045125,  0.05500176, -0.05023526, -0.15322155,\n        -0.01350676,  0.22827694, -0.07612768,  0.17279737, -0.16783627,\n...\n         0.05810804,  0.14673046,  0.16092736, -0.12904012,  0.15562485,\n         0.10650882, -0.09062207, -0.25187423,  0.21226772, -0.14073981,\n         0.02111361, -0.08299419, -0.085154  ,  0.13812413, -0.30986806,\n         0.37500875, -0.2912714 ,  0.38541369, -0.05595662,  0.06197875,\n        -0.13128986, -0.02865051, -0.12845949, -0.28057316,  0.14035149,\n        -0.08529648,  0.12743635,  0.09571021, -0.47402108,  0.27496687,\n        -0.3983348 ,  0.26769788, -0.32741872, -0.1633967 ,  0.02167076,\n        -0.21747759,  0.30507532, -0.26869995, -0.01232208,  0.06016842,\n         0.07454836, -0.10253738, -0.13306231,  0.0198188 ,  0.07312948,\n         0.13930926, -0.05575929,  0.39240501, -0.20304019,  0.28963143,\n         0.50119251, -0.60446654,  0.68061968, -0.45383777, -0.21997373,\n         0.23366197,  0.26016504, -0.42278481, -0.12864326,  0.13376956,\n        -0.04380211,  0.00986593, -0.0798749 ,  0.39127279, -0.50679482,\n         0.39849345,  0.37714201, -0.00898171,  0.01024528,  0.00199428,\n         0.40421152, -0.3326053 , -0.21566443,  0.30176821,  0.13736922,\n         0.36878817, -0.31998447,  0.34030536, -0.26669462,  0.29168813,\n         0.26956602, -0.15026433,  0.11011255, -0.11005266,  0.1113702 ,\n         0.33204116, -0.2302861 , -0.04569952, -0.13321691,  0.13976592,\n        -0.17235007, -0.12974042, -0.02932819,  0.07935601, -0.15087585,\n         0.18029748,  0.08395692,  0.06804948,  0.03367068,  0.30779514]])</pre></li><li>theta_1|participant_id_offset(chain, draw, participant_id__factor_dim)float641.081 -0.05683 ... 0.6488 1.496<pre>array([[[ 1.08064786, -0.05683023,  0.76743687, ..., -0.44403018,\n          0.08708229,  2.03109539],\n        [ 0.12593339,  0.72860755,  0.53089381, ...,  0.24894858,\n         -0.08017794,  0.9067292 ],\n        [ 0.27675089,  0.05562065,  0.1021461 , ..., -0.18876699,\n         -0.01249637,  1.2954936 ],\n        ...,\n        [ 0.86379366,  1.01239982,  0.25856898, ..., -1.03481219,\n          1.3934954 ,  1.63520832],\n        [ 0.24106893,  1.12106309,  1.67942768, ..., -1.10487845,\n          0.51805745,  1.14407622],\n        [ 0.47765069,  0.80921827,  0.40942822, ..., -1.28309606,\n          0.64877481,  1.49613842]]], shape=(1, 1000, 20))</pre></li><li>z_Intercept(chain, draw)float640.2854 0.2993 ... 0.3059 0.3075<pre>array([[0.28535394, 0.2993119 , 0.29992442, 0.30001705, 0.29764792,\n        0.29515565, 0.32325465, 0.2878073 , 0.30882159, 0.29431201,\n        0.28591197, 0.29060289, 0.28452536, 0.30201674, 0.29842536,\n        0.30138836, 0.30396643, 0.29825543, 0.29970298, 0.29891266,\n        0.29848888, 0.29731889, 0.30826   , 0.29340137, 0.29532247,\n        0.30003169, 0.28520468, 0.30490132, 0.30658569, 0.2966049 ,\n        0.29902996, 0.29392082, 0.30393868, 0.29888099, 0.29781481,\n        0.29616385, 0.29591465, 0.30844128, 0.28781459, 0.30637182,\n        0.28915415, 0.29135074, 0.30371296, 0.29048833, 0.30470957,\n        0.29662229, 0.30691191, 0.2880616 , 0.29786101, 0.29779486,\n        0.29230968, 0.29317333, 0.28469788, 0.30813368, 0.28128241,\n        0.31180583, 0.30222493, 0.29981952, 0.29700221, 0.29289386,\n        0.2988997 , 0.30216948, 0.29083848, 0.29883806, 0.30178366,\n        0.2991145 , 0.30112754, 0.30014864, 0.30449497, 0.30978842,\n        0.29766392, 0.30096508, 0.28718927, 0.31056829, 0.29101269,\n        0.28741402, 0.30371914, 0.29243218, 0.30541333, 0.29106231,\n        0.30455728, 0.30463482, 0.31312318, 0.29349101, 0.30416171,\n        0.28437617, 0.30061386, 0.30475034, 0.2964082 , 0.29993091,\n        0.29909109, 0.30222035, 0.30274791, 0.29261432, 0.29820533,\n        0.29338119, 0.29479924, 0.3029484 , 0.29626621, 0.30255974,\n...\n        0.29141572, 0.29639209, 0.29098178, 0.30006698, 0.29971707,\n        0.29950705, 0.29020791, 0.2851421 , 0.30750325, 0.289773  ,\n        0.28760161, 0.30542147, 0.28737187, 0.30796041, 0.31811267,\n        0.2921845 , 0.30754245, 0.3023156 , 0.30447257, 0.28723449,\n        0.31161573, 0.29243505, 0.30401397, 0.30456455, 0.28960672,\n        0.29368438, 0.29711475, 0.30651598, 0.28622175, 0.31339755,\n        0.29557761, 0.29164988, 0.29221288, 0.30202907, 0.30065386,\n        0.29175901, 0.309105  , 0.29398295, 0.28869259, 0.30419119,\n        0.3017563 , 0.31125974, 0.31228975, 0.30649672, 0.30566988,\n        0.30437683, 0.29045334, 0.28716103, 0.3090097 , 0.30800029,\n        0.31872959, 0.2925329 , 0.30686818, 0.2955717 , 0.29679539,\n        0.28793236, 0.29631139, 0.30045706, 0.28527457, 0.31150355,\n        0.30507236, 0.30323466, 0.29785615, 0.30323594, 0.29872305,\n        0.29477875, 0.29388994, 0.30238001, 0.29453713, 0.29479562,\n        0.30652257, 0.29197701, 0.2992591 , 0.29440763, 0.29184112,\n        0.30621642, 0.29106598, 0.30719122, 0.29184642, 0.30246756,\n        0.30233196, 0.30047371, 0.29551409, 0.29106717, 0.30003597,\n        0.30828206, 0.28910783, 0.29019337, 0.29281793, 0.29934729,\n        0.30608585, 0.2897666 , 0.31509098, 0.29135533, 0.30471286,\n        0.30177028, 0.30854231, 0.31383581, 0.30588341, 0.30745167]])</pre></li><li>theta_1|participant_id_sigma(chain, draw)float640.04096 0.05766 ... 0.03672 0.05072<pre>array([[0.04095581, 0.05766162, 0.0456981 , 0.0449229 , 0.05680918,\n        0.0285089 , 0.03453046, 0.02925858, 0.01972195, 0.03455175,\n        0.05309737, 0.07252292, 0.06408525, 0.0457302 , 0.0264558 ,\n        0.00906853, 0.03019866, 0.05273966, 0.04607154, 0.04347917,\n        0.04645139, 0.01041346, 0.01957336, 0.04786794, 0.0426248 ,\n        0.03315946, 0.03757007, 0.02729861, 0.0260703 , 0.02594669,\n        0.01898064, 0.05302707, 0.04032139, 0.04312885, 0.04127796,\n        0.03611918, 0.04183559, 0.04930705, 0.05478179, 0.03391523,\n        0.04974164, 0.04620655, 0.03724837, 0.04137495, 0.02041434,\n        0.05200859, 0.03244668, 0.0282329 , 0.02945521, 0.0451452 ,\n        0.02539614, 0.03684225, 0.04653749, 0.0450934 , 0.06913856,\n        0.02417119, 0.03585186, 0.04317742, 0.02577326, 0.02779946,\n        0.03247738, 0.04533804, 0.03541986, 0.0360971 , 0.0238885 ,\n        0.02069438, 0.038878  , 0.02885935, 0.03182943, 0.02291509,\n        0.02891716, 0.02871782, 0.00673819, 0.01307651, 0.02384775,\n        0.02994273, 0.05565937, 0.03494539, 0.04928943, 0.05682162,\n        0.025282  , 0.04089624, 0.01782405, 0.02373336, 0.05024414,\n        0.03824396, 0.02558914, 0.02633754, 0.01215822, 0.01918326,\n        0.03405103, 0.06870833, 0.04053351, 0.03664117, 0.04853689,\n        0.04432756, 0.05496077, 0.01740131, 0.03378064, 0.05563747,\n...\n        0.03857755, 0.03296677, 0.03408886, 0.03318723, 0.03447868,\n        0.02319802, 0.02812294, 0.03807692, 0.03439944, 0.03135831,\n        0.02885402, 0.02957312, 0.02448075, 0.02967032, 0.01871197,\n        0.02549132, 0.03405451, 0.02321704, 0.05097397, 0.05686479,\n        0.03146744, 0.02395055, 0.03277589, 0.05182318, 0.0290177 ,\n        0.04424462, 0.02849435, 0.03148975, 0.02766244, 0.02010505,\n        0.05530256, 0.03575094, 0.05767258, 0.05120221, 0.07605192,\n        0.02817025, 0.04743378, 0.02479317, 0.03139863, 0.0199533 ,\n        0.02007286, 0.02464639, 0.02756504, 0.02932353, 0.03871003,\n        0.03595645, 0.0533503 , 0.04400041, 0.05284094, 0.06265968,\n        0.02906867, 0.04763049, 0.02675166, 0.03843379, 0.0316083 ,\n        0.04588487, 0.04134572, 0.03121208, 0.0698196 , 0.02165766,\n        0.01876386, 0.00709326, 0.00693389, 0.01086685, 0.01474221,\n        0.03171121, 0.02995716, 0.03258617, 0.04536167, 0.03933128,\n        0.04802055, 0.035161  , 0.0219788 , 0.02536217, 0.02379602,\n        0.03309731, 0.05466112, 0.04622765, 0.05156264, 0.03854952,\n        0.03937764, 0.03086262, 0.02307055, 0.05259533, 0.0628386 ,\n        0.04608284, 0.02803261, 0.02559631, 0.04605795, 0.00382283,\n        0.03184208, 0.04673074, 0.07552445, 0.03476246, 0.03218705,\n        0.04577025, 0.04780782, 0.05233627, 0.0367238 , 0.05071736]])</pre></li><li>theta_1|participant_id_mu(chain, draw)float64-0.2717 -0.5113 ... 0.093 0.1671<pre>array([[-2.71732086e-01, -5.11296059e-01, -3.23807539e-01,\n        -3.88298455e-01,  5.34152471e-01, -6.45064962e-01,\n         2.92092953e-01, -1.15834807e-01,  1.20258943e-01,\n        -2.32704550e-01, -9.86603644e-02,  3.62706827e-01,\n        -5.42650016e-01,  4.99065544e-01, -2.35553870e-01,\n         6.55976624e-02,  5.41005626e-01, -6.80756752e-02,\n         2.55898604e-02, -3.77840230e-01,  3.79367658e-01,\n        -1.75478487e-01,  4.28158474e-02,  5.01947540e-02,\n        -1.66641639e-01, -1.80162299e-02, -1.48131883e-01,\n        -4.80211956e-01,  1.28889653e-01, -5.85039917e-02,\n         1.97722042e-01, -1.78382062e-01,  1.76123556e-01,\n        -2.80066318e-01,  3.68651973e-01, -1.58549472e-01,\n         1.58607178e-01, -4.70915542e-02, -7.17698749e-02,\n        -1.93604652e-02, -1.41383006e-01,  3.78527743e-01,\n         6.06024379e-01, -9.69360771e-03, -7.91031146e-02,\n         3.82939195e-02, -4.90137076e-02,  7.73567244e-02,\n         2.51070512e-01, -3.15959680e-01,  3.79518585e-01,\n         2.07648276e-01,  5.36536340e-01, -5.81876638e-01,\n         4.33280793e-01, -8.84401962e-02,  1.31180870e-01,\n        -1.77637390e-01,  1.87326551e-01, -1.26145397e-01,\n...\n         2.03896256e-01, -2.07276852e-01,  3.99938475e-01,\n         3.81072487e-01, -6.01509056e-01,  4.04163342e-01,\n        -2.54470659e-01,  2.69125823e-01,  3.27810903e-01,\n        -1.98265901e-01,  1.79468204e-01, -3.32764218e-01,\n        -2.12081944e-01,  3.55625899e-01,  5.61877823e-02,\n         2.66856637e-01,  2.71341038e-01, -2.91072071e-01,\n         4.14796262e-01, -3.77755116e-01,  5.61251626e-01,\n        -2.65931885e-01,  9.03806642e-02, -5.71154398e-02,\n        -4.28009549e-02, -2.18587984e-01,  2.28556500e-01,\n        -1.20458900e-01,  1.93741018e-02,  7.53731510e-02,\n        -2.74299903e-01,  2.83044137e-01,  1.74957018e-01,\n         4.07889017e-01, -3.91677987e-01,  4.21731486e-01,\n        -2.21180421e-01,  2.09137613e-01,  1.10696908e-01,\n        -1.58355725e-01,  2.05783592e-01, -3.02446043e-01,\n         3.16819377e-01, -1.30245528e-01, -2.46015727e-02,\n        -1.43010087e-01, -9.10926214e-03, -1.87084671e-03,\n         5.14908138e-02, -1.99392419e-01, -1.94496117e-01,\n         5.97796497e-01, -5.67267454e-01,  4.08358419e-01,\n         2.58501141e-01, -1.50884445e-02,  9.29977106e-02,\n         1.67062702e-01]])</pre></li><li>a_Intercept(chain, draw)float641.491 1.481 1.476 ... 1.506 1.5<pre>array([[1.49106999, 1.48126667, 1.47639279, 1.51482428, 1.50865262,\n        1.52361065, 1.48292926, 1.48089985, 1.52774428, 1.51641303,\n        1.50125385, 1.51425078, 1.50399455, 1.55061734, 1.47089901,\n        1.55240448, 1.57159412, 1.50486368, 1.50658531, 1.49990441,\n        1.50932506, 1.53723781, 1.49345529, 1.52217964, 1.5101542 ,\n        1.49267497, 1.49156222, 1.5181493 , 1.51528951, 1.50778953,\n        1.51446093, 1.50199877, 1.51716078, 1.48183875, 1.52130714,\n        1.48025123, 1.51693023, 1.5187197 , 1.50452056, 1.48526455,\n        1.52482159, 1.51102852, 1.52950289, 1.50014217, 1.50763736,\n        1.51661578, 1.52993264, 1.49116638, 1.50304885, 1.50005338,\n        1.50128832, 1.51155904, 1.48586443, 1.53895175, 1.48493403,\n        1.50800216, 1.48984869, 1.54463042, 1.44024817, 1.52189155,\n        1.49089448, 1.50779014, 1.52601256, 1.51015058, 1.49904233,\n        1.51256363, 1.47671654, 1.51203369, 1.55689484, 1.47903757,\n        1.52868898, 1.52508092, 1.51234332, 1.4581175 , 1.54563815,\n        1.5344382 , 1.51925963, 1.51907705, 1.51984156, 1.5609167 ,\n        1.46900133, 1.53593988, 1.52054833, 1.50528245, 1.5450986 ,\n        1.52347655, 1.4912987 , 1.52281441, 1.51878571, 1.46769483,\n        1.58318316, 1.46443156, 1.47155848, 1.54287093, 1.54913438,\n        1.49412534, 1.54690288, 1.48718757, 1.52596109, 1.49171661,\n...\n        1.50076167, 1.51780297, 1.51974274, 1.52080525, 1.53608964,\n        1.52754152, 1.48424653, 1.50384339, 1.49535942, 1.52488713,\n        1.51997538, 1.50393091, 1.5123751 , 1.46663312, 1.47314153,\n        1.5481768 , 1.50812922, 1.54798082, 1.50850311, 1.51775067,\n        1.52102535, 1.50556193, 1.48845783, 1.46705822, 1.56701742,\n        1.50827543, 1.52010099, 1.50493966, 1.47618632, 1.52874861,\n        1.48591924, 1.51279508, 1.49518944, 1.45728928, 1.4777127 ,\n        1.53166941, 1.49077001, 1.54050944, 1.49283111, 1.5191932 ,\n        1.51024845, 1.5552524 , 1.54600166, 1.47134038, 1.54726912,\n        1.55164671, 1.48526564, 1.51776152, 1.50944517, 1.52056379,\n        1.49427421, 1.47350097, 1.49436375, 1.49835489, 1.52026534,\n        1.5234061 , 1.52846393, 1.53257015, 1.49227621, 1.51025262,\n        1.48093856, 1.5024025 , 1.5068363 , 1.50261023, 1.51505796,\n        1.52909602, 1.52158363, 1.54770245, 1.50065096, 1.54402857,\n        1.55078994, 1.44586488, 1.46686001, 1.56368086, 1.55694375,\n        1.56305391, 1.47740578, 1.56892577, 1.51783911, 1.51512358,\n        1.51067504, 1.47631877, 1.51687402, 1.49315979, 1.52616865,\n        1.46079157, 1.5408622 , 1.52288293, 1.51795565, 1.52488064,\n        1.4821562 , 1.4936641 , 1.48250714, 1.52415042, 1.48663057,\n        1.48984025, 1.50779838, 1.48306332, 1.50607781, 1.5002109 ]])</pre></li><li>scaler_Intercept(chain, draw)float642.639 2.517 2.512 ... 2.488 2.521<pre>array([[2.63910663, 2.51691166, 2.51225446, 2.50394976, 2.49235332,\n        2.56861516, 2.38574936, 2.61223267, 2.46493746, 2.58239165,\n        2.55697202, 2.53001329, 2.57718457, 2.5236611 , 2.50199018,\n        2.49736986, 2.50914813, 2.43001644, 2.4906074 , 2.48197888,\n        2.4422003 , 2.56567393, 2.45317435, 2.56923222, 2.51379831,\n        2.54941103, 2.54795639, 2.4587748 , 2.45543897, 2.5120037 ,\n        2.46256251, 2.53711446, 2.55840645, 2.56231227, 2.53787238,\n        2.56756467, 2.51622953, 2.47767018, 2.51989225, 2.48883691,\n        2.4817006 , 2.60241761, 2.51726432, 2.39061052, 2.47855292,\n        2.51590895, 2.41225253, 2.61566102, 2.54498845, 2.53277367,\n        2.53983564, 2.52883856, 2.60670763, 2.46493131, 2.59432252,\n        2.49173373, 2.48953252, 2.5131184 , 2.45508337, 2.50749323,\n        2.46401071, 2.48658291, 2.59379578, 2.52881726, 2.47950477,\n        2.59406517, 2.48253403, 2.4549487 , 2.50892391, 2.4599632 ,\n        2.54184659, 2.54042893, 2.53827883, 2.43320717, 2.5961008 ,\n        2.57902627, 2.46523994, 2.63643408, 2.47568848, 2.50485458,\n        2.51831386, 2.55490562, 2.46972471, 2.56882234, 2.53049603,\n        2.61496634, 2.51093182, 2.48115869, 2.60819806, 2.4910801 ,\n        2.51993507, 2.4809872 , 2.47400077, 2.54807138, 2.54834152,\n        2.57873048, 2.54062806, 2.48674233, 2.56379471, 2.41539864,\n...\n        2.54053524, 2.62670257, 2.43990491, 2.56047294, 2.51053279,\n        2.50139887, 2.48781993, 2.57938615, 2.49788789, 2.55662149,\n        2.48907317, 2.50500347, 2.53613317, 2.39754618, 2.39938218,\n        2.63577574, 2.44686925, 2.50542103, 2.47286687, 2.60810431,\n        2.41884892, 2.61786983, 2.56140383, 2.57290012, 2.51591682,\n        2.46159337, 2.49931475, 2.48368823, 2.55164459, 2.53763705,\n        2.54598055, 2.46553472, 2.54658704, 2.56899281, 2.59250017,\n        2.47813126, 2.53072736, 2.48522232, 2.49556073, 2.52319165,\n        2.50802776, 2.48303132, 2.47588433, 2.47269069, 2.50820391,\n        2.50587889, 2.52076347, 2.56045188, 2.51288805, 2.5014309 ,\n        2.39509302, 2.47107392, 2.42420906, 2.59096186, 2.48770825,\n        2.5398181 , 2.52048937, 2.52186696, 2.52165722, 2.46537344,\n        2.49488954, 2.46782325, 2.52552883, 2.50495407, 2.5975604 ,\n        2.43777872, 2.43884829, 2.52931248, 2.53669674, 2.5931412 ,\n        2.55408799, 2.50131799, 2.56339384, 2.56337303, 2.48751249,\n        2.51267927, 2.52480605, 2.58312103, 2.47648535, 2.46559084,\n        2.49009109, 2.47896343, 2.56563747, 2.49321698, 2.51398487,\n        2.50590937, 2.53747837, 2.57372077, 2.50462225, 2.54981397,\n        2.47396595, 2.58545358, 2.44669664, 2.5782798 , 2.47926008,\n        2.55740866, 2.43776369, 2.50720738, 2.48807669, 2.52110103]])</pre></li><li>scaler_1|participant_id_mu(chain, draw)float640.3354 -0.1478 ... -0.6267 -0.3424<pre>array([[ 3.35410178e-01, -1.47817388e-01, -1.29505346e-01,\n        -9.95447046e-02,  1.48550878e-01, -1.45272330e-01,\n         5.64389097e-02,  2.58439798e-01, -3.07121579e-01,\n         2.47131656e-01, -3.74355128e-02, -4.41246391e-01,\n        -3.55339841e-02, -6.61493049e-02,  7.86089491e-03,\n        -3.22463118e-02,  4.26736991e-01,  3.15564964e-01,\n         3.37606405e-01, -6.58557648e-02,  2.13147682e-02,\n        -2.23555312e-01,  1.24784105e-01, -1.36380436e-01,\n        -2.00297512e-01,  1.87740949e-01, -1.83839964e-01,\n         1.51814333e-01,  1.05890321e-01, -5.39602190e-01,\n        -3.76466194e-01,  3.70262959e-01,  3.74577344e-01,\n        -4.83475543e-02,  2.39623042e-01,  1.87940927e-01,\n        -7.76638459e-02, -8.40222570e-02,  4.23162474e-02,\n         7.71533232e-02, -2.31334939e-01, -5.87415153e-02,\n        -9.52340830e-03,  8.20465257e-02, -1.14552058e-01,\n         5.11487117e-02, -1.18406418e-01,  1.51066429e-01,\n         1.21080092e-01, -1.97537384e-03, -7.64747915e-02,\n         3.56210654e-01,  1.58577843e-01, -1.97749738e-02,\n        -4.23514806e-01,  5.08707966e-01, -3.93041499e-01,\n        -3.81850920e-02, -2.03973566e-02,  2.81368847e-01,\n...\n         1.44441085e-01, -3.21510713e-02,  6.74630400e-02,\n         1.55363320e-01,  1.05669776e-01, -2.75338606e-01,\n        -6.98307271e-02,  5.07817036e-02,  1.57380665e-01,\n        -1.99762656e-01,  3.70260564e-01, -3.64582950e-01,\n         6.79429243e-02, -1.22806046e-01,  6.67416334e-02,\n         1.55192579e-01, -7.51201938e-02,  9.64104994e-02,\n        -3.95147971e-02,  2.49047437e-01, -2.70196205e-01,\n         1.75321411e-02, -1.36961779e-01,  1.47138836e-01,\n         1.15353525e-01, -1.02888678e-01,  9.40433211e-02,\n        -1.31835053e-01,  4.61738236e-01, -5.01903674e-01,\n        -2.51878586e-01,  4.62765800e-01,  2.13769223e-01,\n        -1.77617406e-01,  1.92080579e-01, -2.65745592e-01,\n         1.02790308e-01, -5.43633027e-02, -7.56255605e-02,\n         1.83570611e-02,  1.37597633e-01, -1.57373728e-01,\n         1.42195438e-01, -2.83160977e-02, -3.88231994e-02,\n        -2.88198883e-01,  3.43720571e-01, -3.33420766e-01,\n         8.57145697e-02,  4.62241472e-01,  7.42232390e-02,\n        -7.95501228e-02,  1.11176478e-02, -6.78086935e-03,\n         1.61354969e-02, -5.99075658e-01, -6.26710537e-01,\n        -3.42395112e-01]])</pre></li><li>rl.alpha_1|participant_id_mu(chain, draw)float640.02453 0.2148 ... 0.5005 0.06875<pre>array([[ 2.45335207e-02,  2.14796487e-01,  1.59023778e-01,\n        -1.10912834e-01,  1.31601132e-01, -8.22081262e-02,\n        -9.23318902e-02,  2.37801805e-01, -2.01195126e-01,\n         1.89213209e-01,  3.54341194e-01,  1.01297758e-01,\n         1.28905023e-01, -1.77139008e-01,  1.70004571e-01,\n        -8.97545666e-02, -2.14285317e-01, -1.97283690e-01,\n        -2.57026718e-01, -3.34278552e-01,  2.33755813e-01,\n         8.99006718e-02,  9.78095054e-03,  4.27738408e-02,\n         5.82580380e-02,  5.60573943e-02,  2.96111786e-01,\n        -1.36015221e-01, -2.83772890e-01,  1.47937333e-01,\n         3.30174513e-01, -1.43387108e-02, -4.20157604e-01,\n         4.87248892e-01,  9.57507793e-02, -2.35180055e-01,\n         2.13393648e-01,  1.93616751e-01, -1.51811990e-01,\n         5.03795716e-02,  1.14075548e-01,  3.35698096e-01,\n        -2.26624828e-01,  2.58007172e-01,  1.21599887e-01,\n        -4.76237923e-03,  3.09452543e-01, -2.93644097e-01,\n        -1.61094179e-01,  1.47972030e-01, -2.52348129e-01,\n        -1.21281158e-01, -4.94751805e-02,  1.08836690e-01,\n         3.49520336e-01, -3.50835244e-01, -1.62943618e-01,\n         1.68383413e-01, -2.17511853e-01,  2.90470752e-01,\n...\n        -3.96729273e-01,  5.45058489e-01, -3.76554675e-01,\n        -3.89803512e-01,  5.22857532e-01, -1.58454589e-01,\n         3.65187126e-01, -3.53740120e-01, -1.44002191e-01,\n         1.18495608e-01, -2.38114629e-01,  3.15999115e-01,\n        -4.92713461e-01,  5.46923522e-01, -4.07180066e-01,\n         3.08611464e-01,  1.05023382e-01, -1.52256808e-01,\n         2.56272598e-01, -3.86752326e-01,  1.97593480e-01,\n        -2.67742740e-01,  3.80406775e-01, -3.26539325e-01,\n        -3.21686895e-01,  3.20793758e-01, -2.41667827e-01,\n         4.08886783e-02,  2.02737405e-01, -2.02496130e-01,\n         5.99754904e-02, -4.93650736e-02,  4.53871752e-01,\n         2.83484112e-01, -2.86883082e-01,  1.88780371e-01,\n         1.31020013e-01, -1.33490429e-01, -3.50386873e-01,\n        -1.06258280e-01,  9.97047710e-02, -2.00923168e-01,\n         2.42069280e-01,  7.71823744e-02, -1.16731215e-01,\n        -2.17011498e-03,  1.42888721e-01, -1.50911220e-01,\n         6.55533714e-02, -1.02608977e-01,  2.91953460e-01,\n        -2.81581167e-01,  3.51240999e-01, -3.73960994e-01,\n        -1.90849644e-02, -1.15760992e-01,  5.00534695e-01,\n         6.87497014e-02]])</pre></li><li>theta_1|participant_id(chain, draw, participant_id__factor_dim)float640.04426 -0.002328 ... 0.07588<pre>array([[[ 0.0442588 , -0.00232753,  0.031431  , ..., -0.01818561,\n          0.00356653,  0.08318515],\n        [ 0.00726152,  0.04201269,  0.0306122 , ...,  0.01435478,\n         -0.00462319,  0.05228347],\n        [ 0.01264699,  0.00254176,  0.00466788, ..., -0.00862629,\n         -0.00057106,  0.0592016 ],\n        ...,\n        [ 0.04520773,  0.05298523,  0.01353253, ..., -0.05415821,\n          0.07293035,  0.0855807 ],\n        [ 0.00885297,  0.04116969,  0.06167496, ..., -0.04057533,\n          0.01902504,  0.04201482],\n        [ 0.02422518,  0.04104141,  0.02076512, ..., -0.06507524,\n          0.03290414,  0.07588019]]], shape=(1, 1000, 20))</pre></li></ul></li><li>Indexes: (4)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li><li>rl.alpha_1|participant_id__factor_dimPandasIndex<pre>PandasIndex(Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n       '13', '14', '15', '16', '17', '18', '19'],\n      dtype='object', name='rl.alpha_1|participant_id__factor_dim'))</pre></li><li>participant_id__factor_dimPandasIndex<pre>PandasIndex(Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n       '13', '14', '15', '16', '17', '18', '19'],\n      dtype='object', name='participant_id__factor_dim'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-07-15T19:55:45.042734+00:00arviz_version :0.22.0inference_library :numpyroinference_library_version :0.18.0sampling_time :517.168047tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 32MB\nDimensions:      (chain: 1, draw: 1000, __obs__: 4000)\nCoordinates:\n  * chain        (chain) int64 8B 0\n  * draw         (draw) int64 8kB 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\n  * __obs__      (__obs__) int64 32kB 0 1 2 3 4 5 ... 3995 3996 3997 3998 3999\nData variables:\n    rt,response  (chain, draw, __obs__) float64 32MB -2.636 -3.071 ... -0.2937\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 1000</li><li>__obs__: 4000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 ... 3996 3997 3998 3999<pre>array([   0,    1,    2, ..., 3997, 3998, 3999], shape=(4000,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-2.636 -3.071 ... -0.1371 -0.2937<pre>array([[[-2.63575903, -3.07067751, -3.58235113, ..., -0.12729951,\n         -0.1199053 , -0.36625479],\n        [-3.02132081, -3.3054947 , -3.91587711, ..., -0.13660625,\n         -0.16814799, -0.28142859],\n        [-2.84212813, -3.19447648, -3.79152338, ..., -0.09283051,\n         -0.15829979, -0.21594327],\n        ...,\n        [-2.76569868, -3.16024893, -3.37349719, ..., -0.08677428,\n         -0.09641447, -0.32637665],\n        [-2.88307955, -3.18972267, -3.43701824, ..., -0.12496733,\n         -0.17307048, -0.28215016],\n        [-2.77531299, -3.13847069, -3.48107947, ..., -0.07518812,\n         -0.13713301, -0.29366249]]], shape=(1, 1000, 4000))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999],\n      dtype='int64', name='__obs__', length=4000))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 57kB\nDimensions:          (chain: 1, draw: 1000)\nCoordinates:\n  * chain            (chain) int64 8B 0\n  * draw             (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    acceptance_rate  (chain, draw) float64 8kB 0.9375 0.9946 ... 0.9262 0.9207\n    step_size        (chain, draw) float64 8kB 0.1562 0.1562 ... 0.1562 0.1562\n    diverging        (chain, draw) bool 1kB False False False ... False True\n    energy           (chain, draw) float64 8kB 4.153e+03 4.145e+03 ... 4.14e+03\n    n_steps          (chain, draw) int64 8kB 31 31 31 31 31 ... 31 31 31 31 29\n    tree_depth       (chain, draw) int64 8kB 5 5 5 5 5 5 5 5 ... 5 5 5 5 5 5 5 5\n    lp               (chain, draw) float64 8kB 4.075e+03 4.063e+03 ... 4.057e+03\nAttributes:\n    created_at:                  2025-07-15T19:55:45.056390+00:00\n    arviz_version:               0.22.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 1000</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li></ul></li><li>Data variables: (7)<ul><li>acceptance_rate(chain, draw)float640.9375 0.9946 ... 0.9262 0.9207<pre>array([[0.93746783, 0.99464907, 0.78335128, 0.95797434, 0.99233246,\n        0.72772066, 0.8083    , 0.9098505 , 0.99226918, 0.87728014,\n        0.9883639 , 0.97551034, 0.97466181, 0.92632492, 0.93055648,\n        0.96786379, 0.85248314, 0.82145043, 0.83077439, 0.97432863,\n        0.77482085, 0.98574552, 0.96301847, 0.98605014, 0.89544694,\n        0.82491724, 0.97375321, 0.90610057, 0.80933871, 0.99569276,\n        0.97246148, 0.98852382, 0.91488494, 0.92543557, 0.95542213,\n        0.9459299 , 0.99035437, 0.96909479, 0.68920198, 0.9319655 ,\n        0.99229207, 0.89232167, 0.98733122, 0.92868501, 0.92162471,\n        0.92612254, 0.99595961, 0.96314095, 0.91445265, 0.98012361,\n        0.85740421, 1.        , 0.8025675 , 0.92165831, 0.95490251,\n        0.97049448, 0.95547075, 0.86717685, 0.81445293, 0.90152814,\n        0.96529657, 0.8053747 , 0.84932834, 0.98925461, 0.89801942,\n        0.98762315, 0.9077886 , 0.91961807, 0.9903903 , 0.97290118,\n        0.96296578, 0.83357885, 0.94127187, 0.92136829, 0.98554265,\n        0.86675206, 0.96389629, 0.97863609, 0.73719634, 0.91480106,\n        0.75584848, 0.98375431, 0.94192359, 0.9701683 , 0.94238792,\n        0.94340282, 0.86692425, 0.8734812 , 0.99761667, 0.94767621,\n        0.97306596, 0.97144952, 0.99619761, 0.94286967, 0.97114099,\n        0.7854196 , 0.97197201, 0.93618253, 0.94680017, 0.9555904 ,\n...\n        0.98043818, 0.8484001 , 0.92505803, 0.9964326 , 0.97734856,\n        0.85332782, 0.98481506, 0.98976365, 0.97938598, 0.95581819,\n        0.84783113, 0.9806777 , 0.9830957 , 0.99585434, 0.88672508,\n        0.8944661 , 0.96017336, 0.9122853 , 0.84999101, 0.99580485,\n        0.72727552, 0.97315627, 0.99050627, 0.91078195, 0.97123389,\n        1.        , 0.72020091, 0.99532404, 0.99417339, 0.96859991,\n        0.94228703, 0.94988061, 0.99282074, 0.93741108, 0.97435416,\n        0.96960852, 0.98148113, 0.90450153, 0.976391  , 0.89243739,\n        0.99935696, 0.94182565, 0.92183045, 0.82193333, 0.90559742,\n        0.88475178, 0.98502026, 0.81614997, 0.89516061, 0.63867355,\n        0.84845189, 0.86626966, 0.56598662, 0.95144555, 0.97819088,\n        0.98557885, 0.92812997, 0.88290843, 0.95906395, 0.99318933,\n        0.94321865, 0.96058863, 0.87465459, 0.72875762, 0.96873433,\n        0.95978854, 0.67634552, 1.        , 0.89357575, 0.99802003,\n        0.83017207, 0.98170094, 0.97178252, 0.69427976, 0.99476899,\n        0.99094475, 0.9655324 , 0.91886361, 0.96863406, 0.99527549,\n        0.93875635, 0.95747232, 0.92177572, 0.99275991, 0.93778398,\n        1.        , 0.82590945, 0.71545662, 0.93001864, 0.97995158,\n        0.99083529, 0.98962037, 0.97583855, 0.93055005, 0.93762783,\n        0.97989826, 0.98518312, 0.83532651, 0.92622512, 0.92071524]])</pre></li><li>step_size(chain, draw)float640.1562 0.1562 ... 0.1562 0.1562<pre>array([[0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n...\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747,\n        0.15623747, 0.15623747, 0.15623747, 0.15623747, 0.15623747]])</pre></li><li>diverging(chain, draw)boolFalse False False ... False True<pre>array([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n         True]])</pre></li><li>energy(chain, draw)float644.153e+03 4.145e+03 ... 4.14e+03<pre>array([[4152.8043778 , 4144.82450909, 4148.38139415, 4143.00373002,\n        4135.62789272, 4151.29420234, 4150.84229654, 4163.73921033,\n        4148.25645363, 4141.6097248 , 4143.83514423, 4149.25498808,\n        4144.94840481, 4144.92827191, 4165.85209752, 4141.08969558,\n        4142.46819957, 4146.2066407 , 4151.556989  , 4141.5934863 ,\n        4155.62521129, 4173.79434024, 4157.66205025, 4146.42542336,\n        4156.63771842, 4163.51207784, 4162.57866429, 4150.73204961,\n        4146.88713123, 4158.07157744, 4152.02721705, 4157.85610685,\n        4164.21419867, 4158.04454948, 4149.57510348, 4144.97658352,\n        4135.22367583, 4132.52108399, 4152.84419424, 4147.13422226,\n        4147.27099577, 4148.49587291, 4141.0303175 , 4139.19560554,\n        4154.95582966, 4148.96104767, 4145.82197864, 4160.83487106,\n        4140.20776346, 4139.04622444, 4135.44233456, 4128.7445626 ,\n        4133.28793991, 4152.3652621 , 4137.29796532, 4169.5296735 ,\n        4128.30452017, 4125.32026664, 4121.75913841, 4145.00450979,\n        4145.85052004, 4134.1205781 , 4147.83976539, 4153.32480314,\n        4172.37621842, 4154.22906724, 4151.66596124, 4155.76953339,\n        4125.67365426, 4130.05813395, 4155.60936205, 4141.29021677,\n        4144.76581421, 4148.85379185, 4138.41234771, 4158.71841826,\n        4125.32526164, 4106.25035815, 4123.07270723, 4122.9435605 ,\n...\n        4163.26349349, 4162.57679154, 4159.52126577, 4144.84177042,\n        4158.51148266, 4161.67697789, 4161.04011587, 4158.55005567,\n        4136.79194455, 4151.25838972, 4155.08074328, 4157.78185418,\n        4151.20696751, 4158.15678218, 4160.30851596, 4151.91778926,\n        4151.15709298, 4151.51756813, 4144.15753088, 4148.0106501 ,\n        4150.37687424, 4133.6018184 , 4147.59108059, 4144.40150931,\n        4132.95867153, 4127.58480853, 4116.96534015, 4113.48178631,\n        4119.92024489, 4119.09396547, 4122.86163494, 4129.59772393,\n        4151.40018017, 4150.0226073 , 4153.32484054, 4153.3877969 ,\n        4152.24479255, 4146.73868101, 4154.9585202 , 4137.78987174,\n        4137.81245257, 4137.39716872, 4141.30812254, 4161.30752083,\n        4158.28823149, 4140.88702537, 4123.65234526, 4111.75117677,\n        4128.16158674, 4147.3880296 , 4155.28993367, 4154.15318667,\n        4153.58422458, 4159.13655021, 4164.15509408, 4162.23178063,\n        4154.71696997, 4145.47749341, 4165.57231749, 4137.49563836,\n        4143.68576578, 4136.85823865, 4140.85794623, 4136.11096117,\n        4131.60103294, 4139.92126096, 4161.41702492, 4150.06268103,\n        4155.57065986, 4157.26121126, 4161.65883374, 4148.00651276,\n        4169.69908783, 4169.70707514, 4158.70332378, 4130.59303241,\n        4136.65251942, 4136.40023908, 4144.01760984, 4140.38566453]])</pre></li><li>n_steps(chain, draw)int6431 31 31 31 31 ... 31 31 31 31 29<pre>array([[31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n...\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 29]])</pre></li><li>tree_depth(chain, draw)int645 5 5 5 5 5 5 5 ... 5 5 5 5 5 5 5 5<pre>array([[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n...\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5]])</pre></li><li>lp(chain, draw)float644.075e+03 4.063e+03 ... 4.057e+03<pre>array([[4074.62573622, 4062.73964321, 4069.04981151, 4052.74611852,\n        4061.49849071, 4063.76922335, 4076.52523457, 4069.63948177,\n        4060.75646519, 4072.3588423 , 4060.49813797, 4065.79246532,\n        4063.16854087, 4071.59674997, 4064.82980337, 4056.62942264,\n        4064.9806756 , 4054.78809698, 4070.44897698, 4060.33148408,\n        4077.51648719, 4085.46132927, 4064.66252907, 4051.24042259,\n        4074.11699965, 4064.98687284, 4075.20997611, 4051.33078833,\n        4081.38573257, 4065.85607551, 4064.2621233 , 4075.71027112,\n        4080.96269555, 4071.13762255, 4059.69651828, 4065.75218129,\n        4057.47506638, 4057.20257234, 4059.94645279, 4074.59612031,\n        4064.79020691, 4065.8729987 , 4057.37842782, 4072.17122322,\n        4079.68257039, 4063.20596645, 4078.19261204, 4070.52254139,\n        4056.86512261, 4047.03277394, 4063.71997667, 4051.34137474,\n        4059.37530636, 4065.77500407, 4068.534483  , 4062.23478438,\n        4055.31937598, 4047.01061467, 4052.9682814 , 4056.15681765,\n        4048.17533265, 4063.72531741, 4075.72255712, 4082.28857625,\n        4083.71225707, 4065.34432185, 4074.03881977, 4061.03760539,\n        4042.67357799, 4066.41366375, 4076.70043186, 4055.57243948,\n        4063.74244156, 4071.33358969, 4068.59915468, 4065.31502753,\n        4039.41083753, 4037.32088529, 4053.35503398, 4048.7368359 ,\n...\n        4073.40081066, 4083.05433416, 4066.59497343, 4058.40496848,\n        4074.28216462, 4068.45256453, 4082.63696528, 4067.14798171,\n        4071.54429212, 4070.23170823, 4069.94825493, 4080.43863771,\n        4066.74505006, 4073.87089853, 4069.62870336, 4072.01673198,\n        4064.61407622, 4070.71683214, 4063.0375435 , 4070.4545854 ,\n        4061.09782543, 4063.00378205, 4056.9676769 , 4050.79902432,\n        4049.83929646, 4046.86067898, 4039.39888465, 4036.85780105,\n        4046.13053548, 4050.08868334, 4047.49852955, 4050.17693638,\n        4062.76276266, 4071.67591108, 4064.34227061, 4070.74614367,\n        4067.84755938, 4070.03302864, 4073.71720689, 4058.71332424,\n        4051.77491159, 4058.48672421, 4063.80089438, 4077.52457397,\n        4065.58202859, 4044.53210322, 4049.76922554, 4036.58435827,\n        4072.27188282, 4051.18929634, 4075.64083177, 4063.98071104,\n        4074.98253961, 4087.76773911, 4078.88079791, 4078.09593957,\n        4072.00746249, 4080.32637059, 4066.14755021, 4065.84437144,\n        4063.16836039, 4065.2837993 , 4056.16981674, 4062.35564929,\n        4059.58475247, 4067.56560861, 4056.73727462, 4082.29372279,\n        4067.00874284, 4081.50930878, 4070.57738867, 4076.47623521,\n        4068.19043575, 4079.6334369 , 4063.85776269, 4056.7369128 ,\n        4051.96607075, 4057.50926366, 4062.42680635, 4057.27964191]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li></ul></li><li>Attributes: (4)created_at :2025-07-15T19:55:45.056390+00:00arviz_version :0.22.0modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 96kB\nDimensions:                  (__obs__: 4000, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 32kB 0 1 2 3 ... 3997 3998 3999\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 64kB ...\nAttributes:\n    created_at:                  2025-07-15T19:55:45.057017+00:00\n    arviz_version:               0.22.0\n    inference_library:           numpyro\n    inference_library_version:   0.18.0\n    sampling_time:               517.168047\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 4000</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 ... 3996 3997 3998 3999<pre>array([   0,    1,    2, ..., 3997, 3998, 3999], shape=(4000,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float640.9356 0.0 1.114 ... 1.0 1.197 1.0<pre>array([[0.93560183, 0.        ],\n       [1.11437929, 0.        ],\n       [0.56431085, 0.        ],\n       ...,\n       [1.32044899, 1.        ],\n       [1.39262187, 1.        ],\n       [1.19681287, 1.        ]], shape=(4000, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999],\n      dtype='int64', name='__obs__', length=4000))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-07-15T19:55:45.057017+00:00arviz_version :0.22.0inference_library :numpyroinference_library_version :0.18.0sampling_time :517.168047tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[12]: Copied! <pre># Define parameter names for analysis\n\nlist_group_mean_params = [\n    \"rl.alpha_Intercept\",\n    \"rl.alpha_neg_Intercept\",\n    \"scaler_Intercept\",\n    \"a_Intercept\",\n    \"z_Intercept\",\n    \"t_Intercept\",\n    \"theta_Intercept\",\n]\n\nlist_group_sd_params = [\n    \"rl.alpha_1|participant_id_sigma\",\n    \"rl.alpha_neg_1|participant_id_sigma\",\n    \"scaler_1|participant_id_sigma\",\n    \"a_1|participant_id_sigma\", \n    \"z_1|participant_id_sigma\",\n    \"t_1|participant_id_sigma\",\n    \"theta_1|participant_id_sigma\",\n]\n</pre> # Define parameter names for analysis  list_group_mean_params = [     \"rl.alpha_Intercept\",     \"rl.alpha_neg_Intercept\",     \"scaler_Intercept\",     \"a_Intercept\",     \"z_Intercept\",     \"t_Intercept\",     \"theta_Intercept\", ]  list_group_sd_params = [     \"rl.alpha_1|participant_id_sigma\",     \"rl.alpha_neg_1|participant_id_sigma\",     \"scaler_1|participant_id_sigma\",     \"a_1|participant_id_sigma\",      \"z_1|participant_id_sigma\",     \"t_1|participant_id_sigma\",     \"theta_1|participant_id_sigma\", ] In\u00a0[13]: Copied! <pre># Create mapping from HSSM model parameter names to ground truth values.\ndef create_ground_truth_mapping(savefile):\n\n    return {\n    \"rl.alpha_Intercept\": savefile['params_true_group']['rl_alpha_mean'],\n    \"scaler_Intercept\": savefile['params_true_group']['scaler_mean'],\n    \"a_Intercept\": savefile['params_true_group']['a_mean'],\n    \"z_Intercept\": savefile['params_true_group']['z_mean'],\n    \"t_Intercept\": savefile['params_true_group']['t_mean'],\n    \"theta_Intercept\": savefile['params_true_group']['theta_mean'],\n}\n\nground_truth_params = create_ground_truth_mapping(savefile)\nprint(\"Ground truth group means:\\n\")\nfor param, value in ground_truth_params.items():\n    print(f\"{param}: {value:.3f}\")\n</pre> # Create mapping from HSSM model parameter names to ground truth values. def create_ground_truth_mapping(savefile):      return {     \"rl.alpha_Intercept\": savefile['params_true_group']['rl_alpha_mean'],     \"scaler_Intercept\": savefile['params_true_group']['scaler_mean'],     \"a_Intercept\": savefile['params_true_group']['a_mean'],     \"z_Intercept\": savefile['params_true_group']['z_mean'],     \"t_Intercept\": savefile['params_true_group']['t_mean'],     \"theta_Intercept\": savefile['params_true_group']['theta_mean'], }  ground_truth_params = create_ground_truth_mapping(savefile) print(\"Ground truth group means:\\n\") for param, value in ground_truth_params.items():     print(f\"{param}: {value:.3f}\") <pre>Ground truth group means:\n\nrl.alpha_Intercept: 0.660\nscaler_Intercept: 2.785\na_Intercept: 1.481\nz_Intercept: 0.267\nt_Intercept: 0.443\ntheta_Intercept: 0.312\n</pre> In\u00a0[14]: Copied! <pre># Plot posterior distributions and MCMC traces for group-level parameters\n# Vertical lines show ground truth values for parameter recovery assessment\naz.plot_trace(idata_mcmc, var_names=list_group_mean_params,\n              lines=[(key_, {}, ground_truth_params[key_]) for key_ in ground_truth_params])\nplt.tight_layout()\n</pre> # Plot posterior distributions and MCMC traces for group-level parameters # Vertical lines show ground truth values for parameter recovery assessment az.plot_trace(idata_mcmc, var_names=list_group_mean_params,               lines=[(key_, {}, ground_truth_params[key_]) for key_ in ground_truth_params]) plt.tight_layout() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/add_custom_rlssm_model/#introducing-a-custom-reinforcement-learning-sequential-sampling-model-rlssm-into-hssm","title":"Introducing a custom Reinforcement Learning - Sequential Sampling Model (RLSSM) into HSSM\u00b6","text":"<p>Before proceeding with this tutorial, we recommend going through the main RLSSM tutorial (Tutorial for hierarchical Bayesian inference for Reinforcement Learning - Sequential Sampling Models for a general familiarity with RLSSM modeling in HSSM.</p> <p>This tutorial demonstrates how to incorporate custom, user-defined Reinforcement Learning Sequential Sampling Models (RLSSM) into the HSSM framework by modifying the <code>rldm.py</code> file. The tutorial walks through the key steps needed to define, implement, and integrate your custom RLSSM likelihood functions.</p> <p>More specifically, this tutorial shows how to add a custom model on a 2-armed bandit environment. Our model employs simple Rescorla-Wagner-style learning updates with two learning rates - for positive and negative prediction errors. The decision process is a drift-diffusion model with collapsing bounds ('angle' model).</p>"},{"location":"tutorials/add_custom_rlssm_model/#warning","title":"\u26a0\ufe0f Warning\u00b6","text":""},{"location":"tutorials/add_custom_rlssm_model/#section-i-introduce-the-likelihood-function-for-your-custom-model-in-hssm","title":"Section I: Introduce the likelihood function for your custom model in HSSM\u00b6","text":"<p>In the existing implementation, <code>rldm.py</code> already defines the model config (Step 1) and the likelihood function (Step 3) for the model mentioned above. The model is called <code>rlssm2</code> in the existing implementation. An easy way to follow this tutorial is simply considering <code>rlssm2</code> as your custom model that you want to incoporate into HSSM. Make the necessary changes outlined in steps 2, 4 and 5 to readily use the implemented model.</p>"},{"location":"tutorials/add_custom_rlssm_model/#step-1-define-your-custom-rlssm-model-configuration","title":"Step 1: Define Your Custom RLSSM Model Configuration\u00b6","text":"<p>Location: Add to the <code>rlssm_model_config_list</code> dictionary</p> <p>Purpose: Define the meta-data and parameters of your custom model in a configuration dictionary.</p> <p>Details:</p> <ul> <li>Create a new entry in the <code>rlssm_model_config_list</code> dictionary with a unique model name</li> <li>Specify the following required fields:<ul> <li><code>name</code>: Name your custom rlssm model</li> <li><code>description</code>: Optional description of the model</li> <li><code>n_params</code>: Number of model parameters</li> <li><code>n_extra_fields</code>: Number of extra_fields columns passed to the likelihood function (e.g., trial, feedback)</li> <li><code>list_params</code>: List of parameter names in the order they'll be passed to the likelihood function</li> <li><code>extra_fields</code>: List of extra_fields columns</li> <li><code>decision_model</code>: Type of the likelihood for the decision process model (typically \"LAN\" - likelihood approximation networks)</li> <li><code>LAN</code>: Specific LAN model to use (e.g., \"angle\")</li> </ul> </li> </ul> <p>Example:</p> <pre>\"my_custom_rlssm\": {\n    \"name\": \"my_custom_rlssm\", \n    \"description\": \"Custom RLSSM with special features\", \n    \"n_params\": 8, \n    \"n_extra_fields\": 3, \n    \"list_params\": [\"param1\", \"param2\", \"param3\", ...], \n    \"extra_fields\": [\"extra_fields1\", \"extra_fields2\", \"extra_fields3\", ...], \n    \"decision_model\": \"LAN\", \n    \"LAN\": \"angle\", \n}\n</pre>"},{"location":"tutorials/add_custom_rlssm_model/#step-2-specify-which-model-configuration-to-use","title":"Step 2: Specify Which Model Configuration to Use\u00b6","text":"<p>Location: Update the <code>MODEL_NAME</code> variable</p> <p>Purpose: Inform the HSSM package which model configuration to use from your defined list.</p> <p>Details:</p> <ul> <li><p>Set <code>MODEL_NAME</code> to match one of the keys in your <code>rlssm_model_config_list</code> dictionary</p> </li> <li><p>The system will automatically load the corresponding configuration</p> </li> <li><p>This makes it easy to switch between different model variants</p> </li> </ul> <p>Example:</p> <pre>MODEL_NAME = \"my_custom_rlssm\"\u00a0 # Must match a key in rlssm_model_config_list\n</pre>"},{"location":"tutorials/add_custom_rlssm_model/#step-3-implement-your-custom-likelihood-function","title":"Step 3: Implement Your Custom Likelihood Function\u00b6","text":"<p>Location: Create a new function following the naming pattern <code>{model_name}_logp_inner_func</code></p> <p>Purpose: Define the core computational logic for your RLSSM model. See the existing implementation in <code>rldm.py</code> for template details.</p> <p>Details:</p> <ul> <li><p>Function signature: Must follow the pattern:</p> <pre>def my_custom_rlssm_logp_inner_func(\n    subj,\n    ntrials_subj,\n    data,\n    *model_params,    # Your specific parameters\n    *extra_fields,    # Additional data fields\n):\n</pre> </li> <li><p>Input requirements:</p> <ul> <li><code>subj</code>: Subject index</li> <li><code>ntrials_subj</code>: Number of trials per subject</li> <li><code>data</code>: RT and response data matrix</li> <li>Parameters must match the order specified in <code>list_params</code></li> <li>Extra fields must match those specified in <code>extra_fields</code></li> </ul> </li> <li><p>Output requirements:</p> <ul> <li>Return a 1D array of log likelihoods for each trial</li> <li>Must be differentiable for gradient-based sampling/optimization</li> </ul> </li> <li><p>Implementation notes:</p> <ul> <li>Use JAX operations for automatic differentiation</li> <li>Handle parameter slicing for individual subjects using <code>dynamic_slice</code></li> <li>Implement your specific RL update rules and decision mechanisms</li> <li>Structure the LAN matrix according to your decision model requirements</li> </ul> </li> </ul>"},{"location":"tutorials/add_custom_rlssm_model/#step-4-update-the-vectorized-function-reference","title":"Step 4: Update the Vectorized Function Reference\u00b6","text":"<p>Location: Modify the <code>rldm_logp_inner_func_vmapped</code> assignment</p> <p>Purpose: Enable parallel computation across multiple subjects.</p> <p>Details:</p> <ul> <li>Update the function reference to point to your custom likelihood function</li> <li>The vectorization pattern remains the same - only the first argument (subject index) gets vectorized</li> <li>Ensure the <code>total_params</code> count includes all parameters plus extra fields plus data columns</li> </ul> <p>Example:</p> <pre>rldm_logp_inner_func_vmapped = jax.vmap(\n    my_custom_rlssm_logp_inner_func,  # Update this to your function name\n    in_axes=[0] + [None] * total_params,\n)\n</pre>"},{"location":"tutorials/add_custom_rlssm_model/#step-5-update-parameter-unpacking-in-the-main-likelihood-function","title":"Step 5: Update Parameter Unpacking in the Main Likelihood Function\u00b6","text":"<p>Location: Modify the <code>logp</code> function within <code>make_logp_func</code></p> <p>Purpose: Ensure parameters are correctly extracted and passed to your custom function.</p> <p>Details:</p> <ul> <li><p>Parameter extraction: Adjust the indexing to match your model's parameter structure:</p> <pre># Extract extra fields (adjust indices based on your model)\nparticipant_id = dist_params[n_params]      # Usually after all model params\ntrial = dist_params[n_params + 1]\nfeedback = dist_params[n_params + 2]\n# ... additional extra fields as needed\n\n# Extract model parameters\nparam1, param2, ..., paramN = dist_params[:MODEL_CONFIG[\"n_params\"]]\n</pre> </li> <li><p>Function call: Update the <code>vec_logp</code> call to pass parameters in the correct order:</p> <pre>return vec_logp(\n    subj,\n    n_trials,\n    data,\n    param1,    # Your specific parameters\n    param2,\n    # ...\n    paramN,\n    trial,     # Extra fields\n    feedback,\n    # ...\n)\n</pre> </li> </ul>"},{"location":"tutorials/add_custom_rlssm_model/#step-6-verify-vjp-function-compatibility","title":"Step 6: Verify VJP Function Compatibility\u00b6","text":"<p>Location: Check the <code>vjp_logp</code> function within <code>make_vjp_logp_func</code></p> <p>Purpose: Ensure gradient computation works correctly with your parameter structure.</p> <p>Details:</p> <ul> <li>The VJP (Vector-Jacobian Product) function should automatically work with your custom model</li> <li>The slicing <code>[1:MODEL_CONFIG[\"n_params\"] + 1]</code> excludes the data and extra fields from gradient computation</li> <li>No changes typically needed unless you have special gradient requirements</li> </ul> <p>Notes:</p> <ul> <li>VJP is used for automatic differentiation during MCMC sampling</li> <li>The function computes gradients with respect to model parameters only</li> <li>Extra fields (trial, feedback, etc.) are not differentiated</li> <li>If your model has unusual parameter dependencies, you may need custom gradient handling</li> </ul>"},{"location":"tutorials/add_custom_rlssm_model/#section-ii-using-the-custom-model-with-hssm","title":"Section II: Using the custom model with HSSM\u00b6","text":""},{"location":"tutorials/add_custom_rlssm_model/#load-and-prepare-the-demo-dataset","title":"Load and prepare the demo dataset\u00b6","text":"<p>This data file contains (synthetic) data from a simulated 2-armed bandit task. We examine the dataset -- it contains the typical columns that are expected from a canonical instrumental learning task. <code>participant_id</code> identifies the subject id, <code>trial</code> identifies the sequence of trials within the subject data, <code>response</code> and <code>rt</code> are the data columns recorded for each trial, <code>feedback</code> column shows the reward obtained on a given trial and <code>correct</code> records whether the response was correct.</p>"},{"location":"tutorials/add_custom_rlssm_model/#construct-hssm-compatible-pymc-distribution-from-a-simulator-and-jax-likelihood-callable","title":"Construct HSSM-compatible PyMC distribution from a simulator and JAX likelihood callable\u00b6","text":"<p>We now construct a custom model that is compatible with HSSM and PyMC. Note that HSSM internally constructs a PyMC object (which is used for sampling) based on the user-specified HSSM model. In other words, we are peeling the abstration layers conveniently afforded by HSSM to directly use the core machinery of HSSM. This advanced HSSM tutorial explains how to use HSSM when starting from the very basics of a model -- a simulator and a JAX likelihood callable.</p> <p>The simulator function is used for generating samples from the model (for posterior predictives, etc.) and the likelihood callable is employed for sampling/inference. This preview tutorial exposes the key flexibility of the HSSM for use in fitting RLSSM models. Therefore, the subsequent tutorial will focus only on the sampling/inference aspect. We create a dummy simulator function to bypass the need for defining the actual simulator.</p>"},{"location":"tutorials/add_custom_rlssm_model/#step-1-define-a-pytensor-randomvariable","title":"Step 1: Define a pytensor RandomVariable\u00b6","text":""},{"location":"tutorials/add_custom_rlssm_model/#step-2-define-a-likelihood-function","title":"Step 2: Define a likelihood function\u00b6","text":""},{"location":"tutorials/add_custom_rlssm_model/#step-3-define-a-model-config-and-hssm-model","title":"Step 3: Define a model config and HSSM model\u00b6","text":""},{"location":"tutorials/add_custom_rlssm_model/#sample-using-nuts-mcmc","title":"Sample using NUTS MCMC\u00b6","text":""},{"location":"tutorials/add_custom_rlssm_model/#assess-the-model-fits","title":"Assess the model fits\u00b6","text":"<p>We examine the quality of fits by comparing the recovered parameters with the ground-truth data generating parameters of the simulated dataset. We examine the quality of fits both at group-level as well as subject-level.</p>"},{"location":"tutorials/add_custom_rlssm_model/#examining-group-level-posteriors","title":"Examining group-level posteriors\u00b6","text":""},{"location":"tutorials/blackbox_contribution_onnx_example/","title":"Custom models from onnx files","text":"In\u00a0[1]: Copied! <pre>import os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport hssm\n</pre> import os import matplotlib.pyplot as plt import numpy as np import hssm In\u00a0[2]: Copied! <pre># Networks\nnetwork_path = os.path.join(\"data\", \"race_3_no_bias_lan_no_batch.onnx\")\n</pre> # Networks network_path = os.path.join(\"data\", \"race_3_no_bias_lan_no_batch.onnx\") <p>The network we load here does not have dynamic input dimensions, which prevents us from batching computations.</p> <p>Instead of fixing things behind the scenes and loading a fixed network, we provide a useful snippet below that shows how to rectify this situation.</p> In\u00a0[3]: Copied! <pre>import onnx\nimport onnxruntime as ort\n\n# Load model from path\nonnx_model = onnx.load(network_path)\n\n# Change input and output dimensions to be dynamic to allow for batching\n# (in case this is not already done)\nfor input_tensor in onnx_model.graph.input:\n    dim_proto = input_tensor.type.tensor_type.shape.dim[0]\n    if not dim_proto.dim_param == \"None\":\n        dim_proto.dim_param = \"None\"\n\nfor output_tensor in onnx_model.graph.output:\n    dim_proto = output_tensor.type.tensor_type.shape.dim[0]\n    if not dim_proto.dim_param == \"None\":\n        dim_proto.dim_param = \"None\"\n\ninput_name = onnx_model.graph.input[0].name\n\n# Please uncomment the below line to save the adjusted model\n# onnx.save(onnx_model, \"test_files/race_3_no_bias_lan_batch.onnx\")\n</pre> import onnx import onnxruntime as ort  # Load model from path onnx_model = onnx.load(network_path)  # Change input and output dimensions to be dynamic to allow for batching # (in case this is not already done) for input_tensor in onnx_model.graph.input:     dim_proto = input_tensor.type.tensor_type.shape.dim[0]     if not dim_proto.dim_param == \"None\":         dim_proto.dim_param = \"None\"  for output_tensor in onnx_model.graph.output:     dim_proto = output_tensor.type.tensor_type.shape.dim[0]     if not dim_proto.dim_param == \"None\":         dim_proto.dim_param = \"None\"  input_name = onnx_model.graph.input[0].name  # Please uncomment the below line to save the adjusted model # onnx.save(onnx_model, \"test_files/race_3_no_bias_lan_batch.onnx\") <p>Armed with the corrected network, let's test inference speed on a data-batch of $1000$ trials.</p> In\u00a0[4]: Copied! <pre># Load model batch ready model\nort_session = ort.InferenceSession(\"data/race_3_no_bias_lan_batch.onnx\")\n\n# Test inference speed\nimport time\n\nstart = time.time()\nfor i in range(100):\n    ort_session.run(\n        None, {input_name: np.random.uniform(size=(1000, 8)).astype(np.float32)}\n    )\nend = time.time()\nprint(f\"Time taken: {(end - start) / 100} seconds\")\n</pre> # Load model batch ready model ort_session = ort.InferenceSession(\"data/race_3_no_bias_lan_batch.onnx\")  # Test inference speed import time  start = time.time() for i in range(100):     ort_session.run(         None, {input_name: np.random.uniform(size=(1000, 8)).astype(np.float32)}     ) end = time.time() print(f\"Time taken: {(end - start) / 100} seconds\") <pre>Time taken: 0.0009977602958679199 seconds\n</pre> In\u00a0[5]: Copied! <pre>def my_blackbox_race_model(data, v0, v1, v2, a, t, z):\n    \"\"\"Calculate log-likelihood for a 3-choice race model.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Array of shape (n_trials, 2) containing response times in first column\n        and choices (0, 1, or 2) in second column\n    v0 : float\n        Drift rate for accumulator 0\n    v1 : float\n        Drift rate for accumulator 1\n    v2 : float\n        Drift rate for accumulator 2\n    a : float\n        Decision threshold/boundary\n    t : float\n        Non-decision time\n    z : float\n        Starting point bias\n\n    Returns\n    -------\n    np.ndarray\n        Array of log-likelihood values for each trial\n    \"\"\"\n    data_nrows = data.shape[0]\n    data = np.vstack(\n        [np.full(data_nrows, param_) for param_ in [v0, v1, v2, a, t, z]]\n        + [data[:, 0], data[:, 1]]\n    ).T.astype(np.float32)\n    return ort_session.run(None, {input_name: data})[0].squeeze()\n</pre> def my_blackbox_race_model(data, v0, v1, v2, a, t, z):     \"\"\"Calculate log-likelihood for a 3-choice race model.      Parameters     ----------     data : np.ndarray         Array of shape (n_trials, 2) containing response times in first column         and choices (0, 1, or 2) in second column     v0 : float         Drift rate for accumulator 0     v1 : float         Drift rate for accumulator 1     v2 : float         Drift rate for accumulator 2     a : float         Decision threshold/boundary     t : float         Non-decision time     z : float         Starting point bias      Returns     -------     np.ndarray         Array of log-likelihood values for each trial     \"\"\"     data_nrows = data.shape[0]     data = np.vstack(         [np.full(data_nrows, param_) for param_ in [v0, v1, v2, a, t, z]]         + [data[:, 0], data[:, 1]]     ).T.astype(np.float32)     return ort_session.run(None, {input_name: data})[0].squeeze() In\u00a0[6]: Copied! <pre># Set parameters\nv0 = 1.0\nv1 = 0.5\nv2 = 0.25\na = 1.5\nt = 0.3\nz = 0.5\n\n# simulate some data from the model\nobs_race3 = hssm.simulate_data(\n    theta=dict(v0=v0, v1=v1, v2=v2, a=a, t=t, z=z), model=\"race_no_bias_3\", size=1000\n)\n</pre> # Set parameters v0 = 1.0 v1 = 0.5 v2 = 0.25 a = 1.5 t = 0.3 z = 0.5  # simulate some data from the model obs_race3 = hssm.simulate_data(     theta=dict(v0=v0, v1=v1, v2=v2, a=a, t=t, z=z), model=\"race_no_bias_3\", size=1000 ) In\u00a0[7]: Copied! <pre># Test that outputs are reasonable\nfor choice in [0, 1, 2]:\n    rts = np.linspace(0, 20, 1000)\n    choices = np.repeat(choice, 1000)\n\n    data = np.vstack([rts, choices]).T\n    out = my_blackbox_race_model(data, v0, v1, v2, a, t, z)\n\n    plt.plot(rts, np.exp(out), label=f\"choice: {choice}\")\nplt.legend()\nplt.show()\n</pre> # Test that outputs are reasonable for choice in [0, 1, 2]:     rts = np.linspace(0, 20, 1000)     choices = np.repeat(choice, 1000)      data = np.vstack([rts, choices]).T     out = my_blackbox_race_model(data, v0, v1, v2, a, t, z)      plt.plot(rts, np.exp(out), label=f\"choice: {choice}\") plt.legend() plt.show() In\u00a0[8]: Copied! <pre>model = hssm.HSSM(\n    data=obs_race3,\n    model=\"race_no_bias_3\",  # some name for the model\n    model_config={\n        \"list_params\": [\"v0\", \"v1\", \"v2\", \"a\", \"z\", \"t\"],\n        \"bounds\": {\n            \"v0\": (0.0, 2.5),\n            \"v1\": (0.0, 2.5),\n            \"v2\": (0.0, 2.5),\n            \"a\": (1.0, 3.0),\n            \"z\": (0.0, 0.9),\n            \"t\": (0.001, 2),\n        },\n    },  # minimal specification of model parameters and parameter bounds\n    loglik_kind=\"blackbox\",  # use the blackbox loglik\n    loglik=my_blackbox_race_model,\n    choices=[0, 1, 2],  # list the legal choice options\n    z=0.5,\n    p_outlier=0,\n)\n</pre> model = hssm.HSSM(     data=obs_race3,     model=\"race_no_bias_3\",  # some name for the model     model_config={         \"list_params\": [\"v0\", \"v1\", \"v2\", \"a\", \"z\", \"t\"],         \"bounds\": {             \"v0\": (0.0, 2.5),             \"v1\": (0.0, 2.5),             \"v2\": (0.0, 2.5),             \"a\": (1.0, 3.0),             \"z\": (0.0, 0.9),             \"t\": (0.001, 2),         },     },  # minimal specification of model parameters and parameter bounds     loglik_kind=\"blackbox\",  # use the blackbox loglik     loglik=my_blackbox_race_model,     choices=[0, 1, 2],  # list the legal choice options     z=0.5,     p_outlier=0, ) <pre>You have specified the `lapse` argument to include a lapse distribution, but `p_outlier` is set to either 0 or None. Your lapse distribution will be ignored.\nModel initialized successfully.\n</pre> In\u00a0[9]: Copied! <pre>model.graph()\n</pre> model.graph() Out[9]: In\u00a0[10]: Copied! <pre>model.sample(draws=500, tune=200, discard_tuned_samples=False)\n</pre> model.sample(draws=500, tune=200, discard_tuned_samples=False) <pre>Using default initvals. \n\n</pre> <pre>Multiprocess sampling (4 chains in 4 jobs)\nCompoundStep\n&gt;Slice: [t]\n&gt;Slice: [v1]\n&gt;Slice: [v2]\n&gt;Slice: [a]\n&gt;Slice: [v0]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 200 tune and 500 draw iterations (800 + 2_000 draws total) took 30 seconds.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:00&lt;00:00, 2609.05it/s]\n</pre> Out[10]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 84kB\nDimensions:  (chain: 4, draw: 500)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    a        (chain, draw) float64 16kB 1.312 1.312 1.344 ... 1.452 1.386 1.406\n    t        (chain, draw) float64 16kB 0.3086 0.3093 0.3091 ... 0.2966 0.2965\n    v1       (chain, draw) float64 16kB 0.3128 0.4278 0.2988 ... 0.457 0.4424\n    v0       (chain, draw) float64 16kB 0.7973 0.8258 0.951 ... 0.9066 1.014\n    v2       (chain, draw) float64 16kB 0.009694 0.06363 0.1586 ... 0.2224 0.28\nAttributes:\n    created_at:                  2025-09-26T23:48:18.919922+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               30.106836795806885\n    tuning_steps:                200\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (5)<ul><li>a(chain, draw)float641.312 1.312 1.344 ... 1.386 1.406<pre>array([[1.31193064, 1.311503  , 1.34441495, ..., 1.38486057, 1.39469806,\n        1.38804221],\n       [1.39810604, 1.43227583, 1.3975672 , ..., 1.32215499, 1.37598438,\n        1.37761908],\n       [1.42573909, 1.44693993, 1.37126147, ..., 1.42217064, 1.40619107,\n        1.40173622],\n       [1.28394073, 1.32861708, 1.32951657, ..., 1.4518297 , 1.38564245,\n        1.4055122 ]], shape=(4, 500))</pre></li><li>t(chain, draw)float640.3086 0.3093 ... 0.2966 0.2965<pre>array([[0.30863691, 0.30927328, 0.30909637, ..., 0.30199349, 0.30147412,\n        0.29995504],\n       [0.30222955, 0.30242707, 0.30159923, ..., 0.31108362, 0.30944716,\n        0.30026422],\n       [0.30139501, 0.29372999, 0.29964204, ..., 0.30037394, 0.29934944,\n        0.30123435],\n       [0.30733552, 0.30956548, 0.30659972, ..., 0.29678435, 0.29661223,\n        0.29647996]], shape=(4, 500))</pre></li><li>v1(chain, draw)float640.3128 0.4278 ... 0.457 0.4424<pre>array([[0.31279282, 0.42780944, 0.29879831, ..., 0.38459919, 0.41823145,\n        0.43710928],\n       [0.31852704, 0.45609414, 0.40901858, ..., 0.35154243, 0.37626249,\n        0.4025467 ],\n       [0.56733633, 0.42678395, 0.48169882, ..., 0.56945563, 0.41838576,\n        0.4319669 ],\n       [0.25904243, 0.19958935, 0.43529349, ..., 0.57514926, 0.45695198,\n        0.44239226]], shape=(4, 500))</pre></li><li>v0(chain, draw)float640.7973 0.8258 ... 0.9066 1.014<pre>array([[0.79731568, 0.82584119, 0.9509516 , ..., 0.96503895, 0.93938638,\n        0.92577401],\n       [1.08875068, 0.96698707, 1.00409388, ..., 0.87855222, 0.91963945,\n        0.95764549],\n       [0.87365313, 0.93685407, 0.98854186, ..., 0.9263228 , 0.87849128,\n        1.06757373],\n       [0.92522976, 0.9591878 , 0.96195117, ..., 0.97348779, 0.90662338,\n        1.01383687]], shape=(4, 500))</pre></li><li>v2(chain, draw)float640.009694 0.06363 ... 0.2224 0.28<pre>array([[0.00969446, 0.06362543, 0.15860902, ..., 0.21732409, 0.20930354,\n        0.24105736],\n       [0.26526182, 0.25532424, 0.37054488, ..., 0.13870106, 0.33051853,\n        0.15126172],\n       [0.20153521, 0.35331713, 0.24435826, ..., 0.20544992, 0.1840864 ,\n        0.17592717],\n       [0.0041807 , 0.01658391, 0.09149108, ..., 0.2786662 , 0.22235376,\n        0.28001988]], shape=(4, 500))</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-26T23:48:18.919922+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :30.106836795806885tuning_steps :200modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 16MB\nDimensions:      (chain: 4, draw: 500, __obs__: 1000)\nCoordinates:\n  * chain        (chain) int64 32B 0 1 2 3\n  * draw         (draw) int64 4kB 0 1 2 3 4 5 6 ... 493 494 495 496 497 498 499\n  * __obs__      (__obs__) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    rt,response  (chain, draw, __obs__) float64 16MB -0.65 -1.911 ... 0.2811\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 500</li><li>__obs__: 1000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-0.65 -1.911 ... -2.129 0.2811<pre>array([[[-0.64996892, -1.91114008, -1.85204542, ..., -1.49051368,\n         -2.17268157,  0.25565812],\n        [-0.59564871, -2.00508189, -1.94276869, ..., -1.50132978,\n         -2.20827937,  0.26626259],\n        [-0.70550579, -1.91341066, -1.85037565, ..., -1.41936386,\n         -2.12504435,  0.30395404],\n        ...,\n        [-0.67872602, -1.94575775, -1.88181865, ..., -1.40610862,\n         -2.11191797,  0.27079123],\n        [-0.64650333, -1.94644177, -1.88296854, ..., -1.40317726,\n         -2.10484314,  0.24231008],\n        [-0.64788473, -1.99242687, -1.92826176, ..., -1.39686477,\n         -2.10098577,  0.24274126]],\n\n       [[-0.75627095, -1.91274226, -1.84676743, ..., -1.39629662,\n         -2.11807394,  0.33144709],\n        [-0.61449045, -1.92408943, -1.85987818, ..., -1.35515094,\n         -2.05862117,  0.20487306],\n        [-0.69732094, -2.02450466, -1.9572891 , ..., -1.32467818,\n         -2.04628778,  0.27329805],\n...\n        [-0.54123175, -1.99670672, -1.93184459, ..., -1.42754638,\n         -2.14120364,  0.1945892 ],\n        [-0.62984282, -1.92377079, -1.86236727, ..., -1.39254451,\n         -2.07568526,  0.19403209],\n        [-0.66531736, -1.92788684, -1.86209977, ..., -1.48024869,\n         -2.20883632,  0.3148787 ]],\n\n       [[-0.74061757, -1.93080592, -1.86886847, ..., -1.56705177,\n         -2.27517796,  0.36980036],\n        [-0.75315386, -1.81489599, -1.75451577, ..., -1.50149548,\n         -2.19763994,  0.33612424],\n        [-0.63555229, -1.99793959, -1.93297887, ..., -1.53862655,\n         -2.26862669,  0.33005333],\n        ...,\n        [-0.5632872 , -1.99755359, -1.93151879, ..., -1.38819468,\n         -2.10354471,  0.19297867],\n        [-0.64485991, -2.01127028, -1.94747543, ..., -1.42515874,\n         -2.12619781,  0.2379401 ],\n        [-0.68305224, -1.99732256, -1.93125308, ..., -1.41062522,\n         -2.12853718,  0.28105086]]], shape=(4, 500, 1000))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 164kB\nDimensions:          (chain: 4, draw: 500, nstep_in_dim_0: 5, nstep_out_dim_0: 5)\nCoordinates:\n  * chain            (chain) int64 32B 0 1 2 3\n  * draw             (draw) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\n  * nstep_in_dim_0   (nstep_in_dim_0) int64 40B 0 1 2 3 4\n  * nstep_out_dim_0  (nstep_out_dim_0) int64 40B 0 1 2 3 4\nData variables:\n    nstep_in         (chain, draw, nstep_in_dim_0) int64 80kB 1 1 0 1 ... 0 6 0\n    nstep_out        (chain, draw, nstep_out_dim_0) int64 80kB 0 0 2 0 ... 0 0 0\nAttributes:\n    created_at:                  2025-09-26T23:48:18.959562+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               30.106836795806885\n    tuning_steps:                200\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 500</li><li>nstep_in_dim_0: 5</li><li>nstep_out_dim_0: 5</li></ul></li><li>Coordinates: (4)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>nstep_in_dim_0(nstep_in_dim_0)int640 1 2 3 4<pre>array([0, 1, 2, 3, 4])</pre></li><li>nstep_out_dim_0(nstep_out_dim_0)int640 1 2 3 4<pre>array([0, 1, 2, 3, 4])</pre></li></ul></li><li>Data variables: (2)<ul><li>nstep_in(chain, draw, nstep_in_dim_0)int641 1 0 1 1 0 0 4 ... 5 5 3 5 4 0 6 0<pre>array([[[ 1,  1,  0,  1,  1],\n        [ 0,  0,  4,  2,  0],\n        [ 2,  5,  1,  3,  3],\n        ...,\n        [ 5,  5,  6,  0,  3],\n        [ 1,  5,  8,  2,  1],\n        [ 4,  5,  4,  1,  3]],\n\n       [[ 7,  6,  6,  2,  2],\n        [ 4,  9,  0,  3,  1],\n        [ 0,  8, 10,  1,  2],\n        ...,\n        [ 2, 10,  0,  2,  1],\n        [ 4,  7,  1,  1,  6],\n        [ 2,  4,  1,  0,  2]],\n\n       [[ 8,  4,  1,  2,  0],\n        [ 1,  1,  0,  1,  2],\n        [ 6,  0,  5,  3,  1],\n        ...,\n        [ 0,  1,  3,  2,  3],\n        [ 1,  1,  4,  3,  0],\n        [ 0,  1,  6,  1,  0]],\n\n       [[ 5,  2,  2,  5,  0],\n        [ 3,  6,  1,  4,  0],\n        [ 3,  1,  4,  8,  0],\n        ...,\n        [ 4,  5,  4, 11,  5],\n        [ 5,  3,  5,  5,  3],\n        [ 5,  4,  0,  6,  0]]], shape=(4, 500, 5))</pre></li><li>nstep_out(chain, draw, nstep_out_dim_0)int640 0 2 0 0 0 1 1 ... 0 0 1 1 0 0 0 0<pre>array([[[0, 0, 2, 0, 0],\n        [0, 1, 1, 1, 0],\n        [0, 1, 1, 0, 1],\n        ...,\n        [0, 1, 0, 1, 1],\n        [0, 0, 0, 1, 1],\n        [0, 0, 1, 1, 0]],\n\n       [[1, 0, 0, 0, 1],\n        [0, 0, 0, 0, 1],\n        [0, 0, 0, 1, 0],\n        ...,\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 1],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 0, 1, 1, 0],\n        [0, 1, 0, 0, 0],\n        [0, 0, 0, 1, 1],\n        ...,\n        [0, 0, 0, 1, 1],\n        [0, 0, 0, 0, 0],\n        [0, 1, 0, 1, 0]],\n\n       [[0, 0, 1, 0, 0],\n        [0, 0, 1, 0, 0],\n        [0, 0, 0, 0, 1],\n        ...,\n        [0, 0, 1, 0, 0],\n        [0, 0, 0, 0, 1],\n        [1, 0, 0, 0, 0]]], shape=(4, 500, 5))</pre></li></ul></li><li>Indexes: (4)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>nstep_in_dim_0PandasIndex<pre>PandasIndex(Index([0, 1, 2, 3, 4], dtype='int64', name='nstep_in_dim_0'))</pre></li><li>nstep_out_dim_0PandasIndex<pre>PandasIndex(Index([0, 1, 2, 3, 4], dtype='int64', name='nstep_out_dim_0'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-26T23:48:18.959562+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :30.106836795806885tuning_steps :200modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 24kB\nDimensions:                  (__obs__: 1000, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 8kB 0 1 2 3 4 ... 996 997 998 999\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 16kB ...\nAttributes:\n    created_at:                  2025-09-26T23:48:18.982413+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 1000</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float640.6031 1.0 1.03 ... 2.0 0.4422 0.0<pre>array([[0.60314298, 1.        ],\n       [1.03006518, 0.        ],\n       [1.01167059, 0.        ],\n       ...,\n       [0.72323811, 2.        ],\n       [0.88554007, 2.        ],\n       [0.44218844, 0.        ]], shape=(1000, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (6)created_at :2025-09-26T23:48:18.982413+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> warmup_posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 6MB\nDimensions:  (chain: 4, draw: 200, __obs__: 1000)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 2kB 0 1 2 3 4 5 6 7 ... 193 194 195 196 197 198 199\n  * __obs__  (__obs__) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\nData variables:\n    t        (chain, draw) float64 6kB 0.2961 0.3006 0.3009 ... 0.3061 0.304\n    v1       (chain, draw) float64 6kB 1.392e-05 0.001714 ... 0.3169 0.3989\n    v2       (chain, draw) float64 6kB 5.871e-16 0.001898 ... 0.1617 0.1333\n    a        (chain, draw) float64 6kB 1.338 1.316 1.298 ... 1.336 1.336 1.329\n    v0       (chain, draw) float64 6kB 0.5663 0.574 0.6625 ... 0.8088 0.8209\n    v0_mean  (chain, draw, __obs__) float64 6MB 0.5663 0.5663 ... 0.8209 0.8209\nAttributes:\n    created_at:                  2025-09-26T23:48:18.940401+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               30.106836795806885\n    tuning_steps:                200\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 200</li><li>__obs__: 1000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 195 196 197 198 199<pre>array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n       154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n       168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n       196, 197, 198, 199])</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li></ul></li><li>Data variables: (6)<ul><li>t(chain, draw)float640.2961 0.3006 ... 0.3061 0.304<pre>array([[0.29608389, 0.30058538, 0.30093608, 0.31209986, 0.31494246,\n        0.31322896, 0.30979464, 0.30975162, 0.30911697, 0.30943005,\n        0.30888947, 0.30976094, 0.3111981 , 0.31095395, 0.30838413,\n        0.30923984, 0.30680895, 0.30778782, 0.30069994, 0.30065748,\n        0.30518304, 0.30210353, 0.30164271, 0.29500907, 0.29718768,\n        0.29335119, 0.29178955, 0.29236815, 0.29432164, 0.29159137,\n        0.28842956, 0.28908456, 0.29437048, 0.29150361, 0.28771933,\n        0.28990419, 0.29268217, 0.29386157, 0.28925678, 0.29936962,\n        0.29652882, 0.29478032, 0.29621466, 0.29610394, 0.29239715,\n        0.30093141, 0.29511653, 0.29375598, 0.29467499, 0.29021671,\n        0.28731399, 0.28921552, 0.2842849 , 0.290164  , 0.28825115,\n        0.29066932, 0.29951409, 0.29999463, 0.29987122, 0.2972787 ,\n        0.29894094, 0.29983941, 0.29773971, 0.29915853, 0.30066149,\n        0.30462224, 0.30042778, 0.30623357, 0.30375405, 0.30290902,\n        0.29501164, 0.29434482, 0.29522328, 0.29729648, 0.30096841,\n        0.30227017, 0.30228376, 0.30292022, 0.30248967, 0.30205896,\n        0.2974235 , 0.3118411 , 0.31066087, 0.30513249, 0.30750909,\n        0.30605557, 0.30661858, 0.30109312, 0.30004261, 0.30236495,\n        0.30382427, 0.30982122, 0.30722767, 0.30671817, 0.30775864,\n        0.31607948, 0.31152372, 0.30672852, 0.30812805, 0.31265569,\n...\n        0.29575917, 0.30573031, 0.30135434, 0.30385708, 0.30144338,\n        0.301393  , 0.29800443, 0.29833066, 0.30121424, 0.29726951,\n        0.29952953, 0.3045751 , 0.30680329, 0.3064758 , 0.30974051,\n        0.31118616, 0.31265959, 0.31566132, 0.31444301, 0.31458488,\n        0.3129794 , 0.30816488, 0.31391789, 0.31268845, 0.31333011,\n        0.31500073, 0.3134123 , 0.3146959 , 0.31001371, 0.30827067,\n        0.31095369, 0.31068017, 0.31248739, 0.31263879, 0.31644892,\n        0.31617631, 0.31211535, 0.31130031, 0.31471812, 0.31102949,\n        0.30347518, 0.30738296, 0.30734762, 0.31111654, 0.30958459,\n        0.3036045 , 0.30468729, 0.30262205, 0.30700676, 0.30698338,\n        0.30711687, 0.305202  , 0.30795013, 0.30026642, 0.29978008,\n        0.30009785, 0.30052439, 0.30487155, 0.3062611 , 0.30413151,\n        0.30454278, 0.3018852 , 0.300551  , 0.30100768, 0.3023436 ,\n        0.30143814, 0.30768569, 0.30668903, 0.30579579, 0.30249653,\n        0.30297311, 0.30158501, 0.30298317, 0.29841721, 0.29582804,\n        0.29362359, 0.30079598, 0.29941868, 0.29890014, 0.30228645,\n        0.30396818, 0.30744651, 0.31664054, 0.31738533, 0.31679118,\n        0.31404561, 0.31224501, 0.3156229 , 0.30783479, 0.30577104,\n        0.30710225, 0.30551138, 0.30692896, 0.30799808, 0.30633407,\n        0.30432589, 0.30528451, 0.30803309, 0.30614926, 0.30395529]])</pre></li><li>v1(chain, draw)float641.392e-05 0.001714 ... 0.3989<pre>array([[1.39193095e-05, 1.71428783e-03, 2.50512462e-01, 2.89359612e-01,\n        2.61343821e-01, 4.20531920e-01, 2.56214947e-01, 2.76360294e-01,\n        3.67765942e-01, 3.29212743e-01, 3.64545646e-01, 3.11342810e-01,\n        3.69014930e-01, 3.92106979e-01, 3.60687275e-01, 4.89579051e-01,\n        4.61849207e-01, 3.22266242e-01, 4.37678991e-01, 3.00783555e-01,\n        3.54491622e-01, 4.92898492e-01, 5.76816599e-01, 5.15607289e-01,\n        4.56155874e-01, 5.59115908e-01, 5.08101439e-01, 5.14059992e-01,\n        4.28979832e-01, 6.76446308e-01, 6.36688258e-01, 6.13175441e-01,\n        5.94377383e-01, 5.27712314e-01, 6.18852313e-01, 5.74380869e-01,\n        5.59625142e-01, 5.38698661e-01, 4.87243307e-01, 5.00536553e-01,\n        5.75437771e-01, 4.95161305e-01, 4.64109703e-01, 5.02633350e-01,\n        5.71963468e-01, 5.25247677e-01, 5.45472124e-01, 5.50575563e-01,\n        5.92223065e-01, 6.98668786e-01, 6.61750871e-01, 5.53901370e-01,\n        5.76656403e-01, 7.25089952e-01, 5.61083182e-01, 4.56251946e-01,\n        5.49879312e-01, 5.71866069e-01, 3.96386382e-01, 4.11671769e-01,\n        4.82282534e-01, 4.87692737e-01, 5.34863247e-01, 4.41435766e-01,\n        4.24017090e-01, 4.39243914e-01, 4.44636965e-01, 4.64438333e-01,\n        4.07985500e-01, 5.43373688e-01, 4.08149339e-01, 3.79017688e-01,\n        4.12616865e-01, 4.35393725e-01, 4.53517725e-01, 4.73052054e-01,\n        4.77968497e-01, 4.74348600e-01, 4.78265681e-01, 2.79118274e-01,\n...\n        3.26887750e-01, 3.60390951e-01, 3.53495367e-01, 2.51162492e-01,\n        2.14197964e-01, 1.24722259e-01, 2.35012460e-01, 3.18257392e-01,\n        3.20206970e-01, 3.49228790e-01, 3.26817085e-01, 3.21682564e-01,\n        2.69926881e-01, 2.19198046e-01, 3.31305728e-01, 2.73349532e-01,\n        2.10578826e-01, 2.24735377e-01, 4.19844036e-01, 3.51060994e-01,\n        2.23416528e-01, 3.39784106e-01, 3.66433782e-01, 4.30365657e-01,\n        3.99324896e-01, 3.75986633e-01, 5.85945020e-01, 2.38806805e-01,\n        4.96031910e-01, 4.17625439e-01, 4.20751738e-01, 3.11117442e-01,\n        3.97522067e-01, 4.69444302e-01, 4.27553218e-01, 5.03435038e-01,\n        5.29915045e-01, 5.35951464e-01, 5.18225515e-01, 4.84011760e-01,\n        4.72179994e-01, 3.75793471e-01, 4.14189662e-01, 4.17416665e-01,\n        4.11516359e-01, 2.61881785e-01, 3.94439252e-01, 4.87176518e-01,\n        4.56765381e-01, 3.10198862e-01, 4.99453082e-01, 3.88338580e-01,\n        5.63057792e-01, 4.89646716e-01, 4.35914074e-01, 4.24810427e-01,\n        5.33031626e-01, 4.78065432e-01, 4.15869078e-01, 5.11531477e-01,\n        2.39965899e-01, 3.72193304e-01, 3.56820886e-01, 1.61034123e-01,\n        1.82336382e-01, 3.09823691e-01, 2.72777745e-01, 3.68237479e-01,\n        3.28954151e-01, 2.95494081e-01, 4.16204074e-01, 3.96640099e-01,\n        3.76910158e-01, 4.33093277e-01, 3.76166281e-01, 3.10347166e-01,\n        2.36553560e-01, 2.81354241e-01, 3.16940359e-01, 3.98885972e-01]])</pre></li><li>v2(chain, draw)float645.871e-16 0.001898 ... 0.1333<pre>array([[5.87088553e-16, 1.89833099e-03, 5.52649822e-03, 5.59796438e-03,\n        4.84018921e-02, 9.49112300e-02, 1.36681636e-01, 1.52921781e-01,\n        6.16756291e-02, 7.88832127e-02, 1.25166838e-01, 2.19133054e-01,\n        1.12091727e-01, 5.26632357e-02, 7.46317163e-02, 1.35206561e-01,\n        2.38423276e-01, 2.90301013e-01, 1.49383327e-01, 1.42856486e-01,\n        2.64864505e-01, 2.26035246e-01, 3.87272704e-01, 2.45163433e-01,\n        3.31731239e-01, 3.35791803e-01, 3.43957148e-01, 3.57048967e-01,\n        5.67797422e-01, 4.29350919e-01, 4.89632322e-01, 4.52565058e-01,\n        3.72935524e-01, 3.51406752e-01, 4.51690816e-01, 3.80605113e-01,\n        4.07912448e-01, 2.68126412e-01, 3.89495100e-01, 3.76194339e-01,\n        3.19183513e-01, 3.28782745e-01, 3.95074755e-01, 2.21772265e-01,\n        2.22684628e-01, 4.14777758e-01, 4.11506343e-01, 2.66601928e-01,\n        3.13889754e-01, 3.40196145e-01, 3.81959884e-01, 4.23342299e-01,\n        4.03161738e-01, 4.06843113e-01, 3.61168191e-01, 3.48379820e-01,\n        3.72965143e-01, 2.81243767e-01, 3.68344903e-01, 3.60645051e-01,\n        3.19794586e-01, 2.19808909e-01, 2.89029541e-01, 2.77366111e-01,\n        2.81118393e-01, 2.86977744e-01, 3.32357443e-01, 3.62815282e-01,\n        3.91236118e-01, 2.73271955e-01, 2.43253846e-01, 3.12293506e-01,\n        3.45352167e-01, 2.40397535e-01, 3.24875202e-01, 2.43151193e-01,\n        2.28816236e-01, 2.82518179e-01, 2.44237967e-01, 1.94083776e-01,\n...\n        2.14604400e-01, 7.21861991e-02, 9.67315109e-02, 1.17611694e-01,\n        2.21746728e-02, 2.01614455e-02, 3.92725310e-02, 5.89926864e-02,\n        1.01933846e-01, 1.40034916e-01, 1.10456613e-01, 8.47719901e-02,\n        9.15179987e-02, 6.51936590e-02, 2.99790961e-02, 1.62994944e-02,\n        6.42618406e-02, 2.03987140e-01, 3.80867549e-02, 6.84974673e-02,\n        2.52883893e-01, 1.49754222e-01, 2.65977686e-01, 2.23413130e-01,\n        1.47867651e-01, 1.66488007e-01, 1.62452261e-01, 1.90044695e-01,\n        2.20473734e-01, 2.35299022e-01, 1.68287889e-01, 2.02616048e-01,\n        2.91482793e-01, 5.40144614e-02, 1.57055448e-01, 3.41805404e-01,\n        1.42501718e-01, 1.19998173e-01, 1.56740400e-01, 3.14967347e-01,\n        1.44380118e-01, 2.93930154e-01, 2.38577946e-01, 2.90483592e-01,\n        1.71538355e-01, 1.24691157e-01, 2.90760058e-01, 2.46858954e-01,\n        2.83660816e-01, 1.43217475e-01, 2.59246618e-01, 2.91024344e-01,\n        2.26544961e-01, 1.91768052e-01, 2.78418440e-01, 2.24560300e-01,\n        3.03077277e-01, 3.01780118e-01, 1.60339747e-01, 1.47268497e-01,\n        6.26681673e-02, 2.72444469e-02, 5.21546328e-02, 9.06666453e-02,\n        4.94897476e-02, 1.52516707e-01, 7.45820275e-02, 7.08947671e-02,\n        5.03835747e-02, 5.36920871e-02, 2.23522258e-01, 1.45239419e-01,\n        6.42637844e-02, 3.77952121e-02, 4.93664242e-02, 6.45916011e-02,\n        2.09056339e-01, 1.10968156e-01, 1.61729907e-01, 1.33316520e-01]])</pre></li><li>a(chain, draw)float641.338 1.316 1.298 ... 1.336 1.329<pre>array([[1.33838505, 1.31555547, 1.29807385, 1.22638067, 1.28468744,\n        1.29607567, 1.30210459, 1.30819481, 1.31026901, 1.32621856,\n        1.31353015, 1.29471365, 1.31158071, 1.3243413 , 1.34843463,\n        1.33259812, 1.34957618, 1.36891536, 1.3553761 , 1.37409999,\n        1.38952122, 1.42906131, 1.45528077, 1.46140188, 1.46768626,\n        1.47453075, 1.50267949, 1.5178627 , 1.50629951, 1.55088819,\n        1.58752638, 1.50749551, 1.50461252, 1.51775106, 1.53309253,\n        1.51190899, 1.47884321, 1.47873582, 1.48096054, 1.45939218,\n        1.46163197, 1.47505098, 1.4454662 , 1.44090927, 1.44293413,\n        1.44151003, 1.4829965 , 1.48582629, 1.49048538, 1.54233784,\n        1.54727874, 1.54385374, 1.54945497, 1.53217967, 1.49686899,\n        1.45858695, 1.46996678, 1.42989967, 1.41045317, 1.41961059,\n        1.41507985, 1.41897634, 1.41808059, 1.41764156, 1.38527964,\n        1.388474  , 1.38587508, 1.3814851 , 1.41350766, 1.41465734,\n        1.42514017, 1.43928002, 1.43259806, 1.41864146, 1.40829256,\n        1.40681649, 1.39189111, 1.40163825, 1.37249579, 1.37671964,\n        1.3499283 , 1.27897666, 1.32116652, 1.34648625, 1.31452637,\n        1.33373573, 1.37523051, 1.36871171, 1.34799494, 1.39464844,\n        1.35774064, 1.33579834, 1.37381254, 1.32627461, 1.26987896,\n        1.25670033, 1.30311198, 1.30301448, 1.30407314, 1.28330047,\n...\n        1.38018836, 1.38933436, 1.41524   , 1.44062949, 1.42698722,\n        1.44167706, 1.46501457, 1.41723649, 1.42387728, 1.42459842,\n        1.39692355, 1.36039265, 1.3206369 , 1.31768831, 1.33966557,\n        1.31108669, 1.25866488, 1.24443283, 1.23959915, 1.30209084,\n        1.32777802, 1.28094749, 1.27792464, 1.30720296, 1.22609456,\n        1.2180846 , 1.28032215, 1.29010135, 1.30809205, 1.30940742,\n        1.26728013, 1.26703242, 1.24995789, 1.2533811 , 1.2404348 ,\n        1.23251019, 1.28785624, 1.27912647, 1.27092145, 1.3200594 ,\n        1.32891953, 1.37836113, 1.35006838, 1.34583793, 1.34458297,\n        1.3785371 , 1.36310896, 1.35426582, 1.36652912, 1.35876683,\n        1.34680976, 1.35927528, 1.35575467, 1.40338084, 1.41029248,\n        1.40274029, 1.40278509, 1.3566586 , 1.36377443, 1.38204316,\n        1.38200682, 1.40387601, 1.38088988, 1.40464105, 1.36956182,\n        1.34443783, 1.38598519, 1.38690031, 1.38880417, 1.38221235,\n        1.42423266, 1.39273726, 1.44336841, 1.44011534, 1.41743397,\n        1.42709567, 1.40812563, 1.40801897, 1.40875436, 1.33074364,\n        1.31195286, 1.27870975, 1.24125144, 1.24391262, 1.2759844 ,\n        1.27405777, 1.28963407, 1.29442961, 1.34843515, 1.3397761 ,\n        1.34198454, 1.33968077, 1.33305427, 1.33904128, 1.32947923,\n        1.29363801, 1.29660742, 1.3363628 , 1.33571901, 1.32874715]])</pre></li><li>v0(chain, draw)float640.5663 0.574 ... 0.8088 0.8209<pre>array([[0.56632251, 0.57402213, 0.66248745, 0.76528648, 0.75825767,\n        0.81431008, 0.88262587, 0.88809962, 0.91803275, 0.91399949,\n        0.86436182, 0.79989332, 0.79222671, 0.9341386 , 0.88761299,\n        0.88534215, 0.91031415, 0.89994704, 0.90917143, 0.85764005,\n        0.92727393, 1.06947936, 1.0489714 , 0.97516029, 1.07003841,\n        0.93465176, 1.04002148, 1.07771401, 1.11730368, 1.16474843,\n        1.15152422, 1.03522497, 1.13423241, 1.06854635, 1.11672866,\n        1.10928305, 1.02295894, 1.03589818, 1.04893581, 1.05557396,\n        1.01878438, 1.10001672, 0.92720823, 1.03614009, 1.11049553,\n        1.03762213, 1.09658015, 1.10913329, 1.11578492, 1.16142616,\n        1.09032838, 0.99199258, 1.22711984, 1.06412947, 1.00513572,\n        1.0321039 , 1.03541094, 0.9878244 , 0.93289094, 0.99856199,\n        1.04395183, 0.94673887, 0.98351132, 0.9771853 , 0.90994191,\n        1.02191612, 0.89892437, 0.97324448, 1.00913875, 0.95875801,\n        1.01862184, 0.94438927, 0.86989837, 0.88436304, 0.96886929,\n        1.06287897, 1.01054649, 0.92582616, 0.99726813, 0.9032038 ,\n        0.92552183, 0.81987472, 0.85890113, 0.95146041, 0.81576653,\n        0.93162319, 0.93757667, 0.80121481, 0.87969991, 0.83573152,\n        0.94843975, 0.91567916, 0.89985991, 0.81688166, 0.86024851,\n        0.79284615, 0.82647   , 0.8732161 , 0.89017748, 0.8304562 ,\n...\n        0.87219874, 0.95791817, 1.01572902, 0.97667506, 1.13544568,\n        1.1458329 , 0.94776267, 0.92433697, 0.91633394, 0.84761464,\n        0.91094221, 0.89815412, 0.82451683, 0.87722231, 0.89799175,\n        0.81548987, 0.7410378 , 0.70191152, 0.87082476, 0.84206025,\n        0.83909088, 0.74002791, 0.83854718, 0.79891467, 0.72372762,\n        0.68299538, 0.88807442, 0.80983312, 0.79527662, 0.85082481,\n        0.65474459, 0.66661528, 0.68001845, 0.84062677, 0.75450175,\n        0.873658  , 0.87598227, 0.80315623, 0.81726054, 0.81713551,\n        0.8734532 , 1.0297671 , 0.84491955, 0.87589952, 0.8957649 ,\n        0.98805963, 0.92932086, 0.88127262, 0.83088911, 0.98286989,\n        0.80328629, 1.00422015, 0.94394095, 1.02629524, 0.92133574,\n        0.89030754, 0.89680779, 0.91241493, 0.86625983, 0.93185829,\n        1.01225965, 0.94691848, 0.88890962, 0.91341673, 0.88015096,\n        0.97014217, 0.98471362, 0.98247918, 0.98600732, 1.02017125,\n        1.02070901, 1.07331277, 1.05076289, 1.02804743, 0.99360133,\n        1.00804063, 1.04391669, 1.06737003, 0.8947157 , 0.8188081 ,\n        0.83237362, 0.70727669, 0.66873723, 0.87770492, 0.8893138 ,\n        0.91868337, 0.99886017, 0.86446944, 0.97064766, 0.89498152,\n        0.85775017, 0.89043703, 0.87575494, 0.9073318 , 0.79990725,\n        0.89800446, 0.85973858, 0.84717667, 0.80875276, 0.8209003 ]])</pre></li><li>v0_mean(chain, draw, __obs__)float640.5663 0.5663 ... 0.8209 0.8209<pre>array([[[0.56632251, 0.56632251, 0.56632251, ..., 0.56632251,\n         0.56632251, 0.56632251],\n        [0.57402213, 0.57402213, 0.57402213, ..., 0.57402213,\n         0.57402213, 0.57402213],\n        [0.66248745, 0.66248745, 0.66248745, ..., 0.66248745,\n         0.66248745, 0.66248745],\n        ...,\n        [0.77260134, 0.77260134, 0.77260134, ..., 0.77260134,\n         0.77260134, 0.77260134],\n        [0.85232564, 0.85232564, 0.85232564, ..., 0.85232564,\n         0.85232564, 0.85232564],\n        [0.78418039, 0.78418039, 0.78418039, ..., 0.78418039,\n         0.78418039, 0.78418039]],\n\n       [[0.17634131, 0.17634131, 0.17634131, ..., 0.17634131,\n         0.17634131, 0.17634131],\n        [0.98001694, 0.98001694, 0.98001694, ..., 0.98001694,\n         0.98001694, 0.98001694],\n        [0.85339859, 0.85339859, 0.85339859, ..., 0.85339859,\n         0.85339859, 0.85339859],\n...\n        [1.00294527, 1.00294527, 1.00294527, ..., 1.00294527,\n         1.00294527, 1.00294527],\n        [0.85767767, 0.85767767, 0.85767767, ..., 0.85767767,\n         0.85767767, 0.85767767],\n        [0.85614861, 0.85614861, 0.85614861, ..., 0.85614861,\n         0.85614861, 0.85614861]],\n\n       [[1.67821701, 1.67821701, 1.67821701, ..., 1.67821701,\n         1.67821701, 1.67821701],\n        [1.60218147, 1.60218147, 1.60218147, ..., 1.60218147,\n         1.60218147, 1.60218147],\n        [1.71983912, 1.71983912, 1.71983912, ..., 1.71983912,\n         1.71983912, 1.71983912],\n        ...,\n        [0.84717667, 0.84717667, 0.84717667, ..., 0.84717667,\n         0.84717667, 0.84717667],\n        [0.80875276, 0.80875276, 0.80875276, ..., 0.80875276,\n         0.80875276, 0.80875276],\n        [0.8209003 , 0.8209003 , 0.8209003 , ..., 0.8209003 ,\n         0.8209003 , 0.8209003 ]]], shape=(4, 200, 1000))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       190, 191, 192, 193, 194, 195, 196, 197, 198, 199],\n      dtype='int64', name='draw', length=200))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-26T23:48:18.940401+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :30.106836795806885tuning_steps :200modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> warmup_sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 66kB\nDimensions:          (chain: 4, draw: 200, nstep_in_dim_0: 5, nstep_out_dim_0: 5)\nCoordinates:\n  * chain            (chain) int64 32B 0 1 2 3\n  * draw             (draw) int64 2kB 0 1 2 3 4 5 6 ... 194 195 196 197 198 199\n  * nstep_in_dim_0   (nstep_in_dim_0) int64 40B 0 1 2 3 4\n  * nstep_out_dim_0  (nstep_out_dim_0) int64 40B 0 1 2 3 4\nData variables:\n    nstep_in         (chain, draw, nstep_in_dim_0) int64 32kB 0 0 0 1 ... 2 8 0\n    nstep_out        (chain, draw, nstep_out_dim_0) int64 32kB 3 24 67 ... 0 0 0\nAttributes:\n    created_at:                  2025-09-26T23:48:18.977556+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               30.106836795806885\n    tuning_steps:                200\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 200</li><li>nstep_in_dim_0: 5</li><li>nstep_out_dim_0: 5</li></ul></li><li>Coordinates: (4)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 195 196 197 198 199<pre>array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n       154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n       168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n       196, 197, 198, 199])</pre></li><li>nstep_in_dim_0(nstep_in_dim_0)int640 1 2 3 4<pre>array([0, 1, 2, 3, 4])</pre></li><li>nstep_out_dim_0(nstep_out_dim_0)int640 1 2 3 4<pre>array([0, 1, 2, 3, 4])</pre></li></ul></li><li>Data variables: (2)<ul><li>nstep_in(chain, draw, nstep_in_dim_0)int640 0 0 1 1 7 0 1 ... 2 9 2 0 2 2 8 0<pre>array([[[0, 0, 0, 1, 1],\n        [7, 0, 1, 2, 1],\n        [6, 2, 5, 2, 2],\n        ...,\n        [1, 2, 5, 3, 2],\n        [0, 1, 1, 0, 0],\n        [0, 2, 1, 0, 1]],\n\n       [[0, 0, 0, 0, 1],\n        [2, 2, 2, 5, 0],\n        [0, 8, 5, 5, 8],\n        ...,\n        [3, 6, 5, 1, 0],\n        [6, 5, 3, 5, 1],\n        [1, 6, 4, 2, 4]],\n\n       [[0, 0, 0, 0, 2],\n        [6, 1, 1, 0, 0],\n        [3, 8, 0, 7, 3],\n        ...,\n        [1, 2, 0, 3, 0],\n        [0, 5, 0, 1, 0],\n        [1, 2, 4, 1, 0]],\n\n       [[0, 0, 0, 0, 1],\n        [0, 1, 4, 2, 4],\n        [1, 0, 6, 4, 3],\n        ...,\n        [3, 0, 1, 4, 4],\n        [3, 5, 2, 9, 2],\n        [0, 2, 2, 8, 0]]], shape=(4, 200, 5))</pre></li><li>nstep_out(chain, draw, nstep_out_dim_0)int643 24 67 1 2 0 0 1 ... 0 0 0 0 0 0 0<pre>array([[[  3,  24,  67,   1,   2],\n        [  0,   0,   1,   1,   1],\n        [  0,   0,   0,   0,   0],\n        ...,\n        [  1,   0,   0,   0,   1],\n        [  0,   0,   1,   1,   1],\n        [  0,   0,   1,   0,   0]],\n\n       [[  3, 267, 321,   4,   4],\n        [  1,   1,   0,   0,   0],\n        [  1,   1,   0,   1,   0],\n        ...,\n        [  1,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   1]],\n\n       [[  3,  26,  71,   0,   1],\n        [  0,   0,   0,   0,   1],\n        [  0,   0,   0,   0,   0],\n        ...,\n        [  0,   1,   0,   0,   1],\n        [  0,   1,   0,   0,   1],\n        [  0,   0,   1,   1,   0]],\n\n       [[  3, 342, 391, 253,   2],\n        [  0,   0,   0,   1,   1],\n        [  1,   0,   0,   0,   0],\n        ...,\n        [  0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0]]], shape=(4, 200, 5))</pre></li></ul></li><li>Indexes: (4)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       190, 191, 192, 193, 194, 195, 196, 197, 198, 199],\n      dtype='int64', name='draw', length=200))</pre></li><li>nstep_in_dim_0PandasIndex<pre>PandasIndex(Index([0, 1, 2, 3, 4], dtype='int64', name='nstep_in_dim_0'))</pre></li><li>nstep_out_dim_0PandasIndex<pre>PandasIndex(Index([0, 1, 2, 3, 4], dtype='int64', name='nstep_out_dim_0'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-26T23:48:18.977556+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :30.106836795806885tuning_steps :200modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[11]: Copied! <pre>import arviz as az\n\naz.plot_trace(model.traces, var_names=[\"~v0_mean\"])\nplt.tight_layout()\n</pre> import arviz as az  az.plot_trace(model.traces, var_names=[\"~v0_mean\"]) plt.tight_layout() <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/arviz/utils.py:146: UserWarning: Items starting with ~: ['v0_mean'] have not been found and will be ignored\n  warnings.warn(\n</pre> In\u00a0[12]: Copied! <pre>az.plot_pair(model.traces, var_names=[\"~v0_mean\"])\nplt.tight_layout()\n</pre> az.plot_pair(model.traces, var_names=[\"~v0_mean\"]) plt.tight_layout() <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/arviz/utils.py:146: UserWarning: Items starting with ~: ['v0_mean'] have not been found and will be ignored\n  warnings.warn(\n</pre>"},{"location":"tutorials/blackbox_contribution_onnx_example/#build-hssm-models-starting-from-onnx-files","title":"Build HSSM models starting from ONNX files\u00b6","text":"<p>In this tutorial we build a <code>HSSM</code> model directly from an <code>onnx</code> file. For our purposes, the <code>onnx</code> file-format provides nice translation layer from deep learning frameworks into a common layer from which we can then reconstruct computation graph to use through PyMC.</p>"},{"location":"tutorials/blackbox_contribution_onnx_example/#loading-the-network","title":"Loading the network\u00b6","text":""},{"location":"tutorials/blackbox_contribution_onnx_example/#defining-the-likelihood","title":"Defining the Likelihood\u00b6","text":"<p>The network we loaded corresponds to a <code>LAN</code>, for a Race model with three choice alternatives. This model has three drift parameters <code>v0, v1, v2</code>, a boundary parameter <code>a</code>, a starting point bias <code>z</code> and a non-decision-time <code>t</code>.</p> <p>Data from this model has the usual <code>rt, choice</code> format.</p> <p>We use this to construct a simple blackbox likelihood function below. This likelihood function takes the respective data and model parameters as arguments. The function body shapes these input arguments into a matrix and performs a batched forward pass through the loaded network via the <code>onnx.runtime</code>.</p>"},{"location":"tutorials/blackbox_contribution_onnx_example/#simulate-example-data","title":"Simulate example data\u00b6","text":""},{"location":"tutorials/blackbox_contribution_onnx_example/#test-likelihood-outputs","title":"Test Likelihood Outputs\u00b6","text":""},{"location":"tutorials/blackbox_contribution_onnx_example/#build-hssm-model","title":"Build HSSM Model\u00b6","text":"<p>We can now build a simple <code>HSSM</code> model that takes in our new blackbox likelihood.</p>"},{"location":"tutorials/compile_logp/","title":"Using compiled log-likelihood functions","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport hssm\n</pre> import matplotlib.pyplot as plt import numpy as np import hssm In\u00a0[2]: Copied! <pre>obs_ddm = hssm.simulate_data(\n    theta={\"v\": 0.5, \"a\": 1.5, \"t\": 0.3, \"z\": 0.5},\n    model=\"angle\",\n    size=500,\n)\n</pre> obs_ddm = hssm.simulate_data(     theta={\"v\": 0.5, \"a\": 1.5, \"t\": 0.3, \"z\": 0.5},     model=\"angle\",     size=500, ) In\u00a0[3]: Copied! <pre>model = hssm.HSSM(\n    data=obs_ddm,\n    loglik_kind=\"analytical\",\n    process_initvals=True,\n    p_outlier=0\n)\n</pre> model = hssm.HSSM(     data=obs_ddm,     loglik_kind=\"analytical\",     process_initvals=True,     p_outlier=0 ) <pre>You have specified the `lapse` argument to include a lapse distribution, but `p_outlier` is set to either 0 or None. Your lapse distribution will be ignored.\nModel initialized successfully.\n</pre> In\u00a0[4]: Copied! <pre>model.graph()\n</pre> model.graph() Out[4]: <p>We can now use the <code>compile_logp()</code> method to compile the log-likelihood function created by the <code>hssm</code>. This illustrates the simplest use case, <code>compile_logp()</code> with no additional arguments. Check the documentation for more details on how to make <code>compile_logp()</code> work for more customized use cases.</p> In\u00a0[5]: Copied! <pre>logp_fun = model.compile_logp()  # msynth.pymc_model.compile_logp()\nprint(logp_fun(model.initial_point(transformed=False)))\n</pre> logp_fun = model.compile_logp()  # msynth.pymc_model.compile_logp() print(logp_fun(model.initial_point(transformed=False))) <pre>-33054.449962779865\n</pre> <p>Note that <code>logp_fun</code> takes as input a dictionary of parameter values with keys corresponding to the parameters names created by <code>hssm</code>. It might be helpful to take a look at the <code>initial_point()</code> method to see how the parameters are passed.</p> In\u00a0[6]: Copied! <pre>print(model.initial_point(transformed=False))\n</pre> print(model.initial_point(transformed=False)) <pre>{'t': array(2.), 'a': array(2.), 'z': array(0.5), 'v': array(0.)}\n</pre> In\u00a0[7]: Copied! <pre># time\nimport time\n\nmy_start_point = model.initial_point(transformed=False)\nstart_time = time.time()\nfor i in range(1000):\n    logp_fun(my_start_point)\nprint((time.time() - start_time) / 1000)\n</pre> # time import time  my_start_point = model.initial_point(transformed=False) start_time = time.time() for i in range(1000):     logp_fun(my_start_point) print((time.time() - start_time) / 1000) <pre>0.0001394040584564209\n</pre> In\u00a0[8]: Copied! <pre>def mylogp(theta: list[float]) -&gt; float:\n    \"\"\"Wrap function for compiled log probability function to work with zeus sampler.\n\n    Args\n    ----\n        theta: List of model parameters [v, a, z, t] where:\n            v: Drift rate\n            a: Boundary separation\n            z: Starting point\n            t: Non-decision time\n\n    Returns\n    -------\n        float: Log probability value for the given parameters\n    \"\"\"\n    v, a, z, t = theta\n    return logp_fun({\"v\": v, \"a\": a, \"z\": z, \"t\": t})\n</pre> def mylogp(theta: list[float]) -&gt; float:     \"\"\"Wrap function for compiled log probability function to work with zeus sampler.      Args     ----         theta: List of model parameters [v, a, z, t] where:             v: Drift rate             a: Boundary separation             z: Starting point             t: Non-decision time      Returns     -------         float: Log probability value for the given parameters     \"\"\"     v, a, z, t = theta     return logp_fun({\"v\": v, \"a\": a, \"z\": z, \"t\": t}) In\u00a0[9]: Copied! <pre>import zeus\n\nstart = np.random.uniform(low=-0.2, high=0.2, size=(8, 4)) + np.tile(\n    [0.5, 1.5, 0.5, 0.3], (8, 1)\n)\nsampler = zeus.EnsembleSampler(8, 4, mylogp)\n</pre> import zeus  start = np.random.uniform(low=-0.2, high=0.2, size=(8, 4)) + np.tile(     [0.5, 1.5, 0.5, 0.3], (8, 1) ) sampler = zeus.EnsembleSampler(8, 4, mylogp) In\u00a0[10]: Copied! <pre>sampler.run_mcmc(start, 1000)\n</pre> sampler.run_mcmc(start, 1000) <pre>Initialising ensemble of 8 walkers...\nSampling progress : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:08&lt;00:00, 118.93it/s]\n</pre> In\u00a0[11]: Copied! <pre>plt.figure(figsize=(16, 1.5 * 4))\nfor n in range(4):\n    plt.subplot2grid((4, 1), (n, 0))\n    plt.plot(sampler.get_chain()[:, :, n], alpha=0.5)\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(16, 1.5 * 4)) for n in range(4):     plt.subplot2grid((4, 1), (n, 0))     plt.plot(sampler.get_chain()[:, :, n], alpha=0.5) plt.tight_layout() plt.show()"},{"location":"tutorials/compile_logp/#compile-log-likelihood-function","title":"Compile log-likelihood function\u00b6","text":""},{"location":"tutorials/compile_logp/#simulate-data","title":"Simulate Data\u00b6","text":""},{"location":"tutorials/compile_logp/#basic-hssm-model","title":"Basic HSSM model\u00b6","text":""},{"location":"tutorials/compile_logp/#timing-the-compiled-log-likelihood-function","title":"Timing the compiled log-likelihood function\u00b6","text":""},{"location":"tutorials/compile_logp/#using-the-compiled-log-likelihood-function","title":"Using the compiled log-likelihood function\u00b6","text":"<p>For illustration, we use our compile log-likelihood function with a different MCMC sampler, using the <code>zeus</code> package.</p> <p>Note: <code>zeus</code> is an optional dependency of <code>hssm</code> and can be installed using the command <code>pip install hssm[notebook]</code>. Alternatively, you can install it directly with <code>pip install zeus-mcmc</code>.</p>"},{"location":"tutorials/compile_logp/#wrap-the-compiled-log-likelihood-to-accomodate-zeus","title":"Wrap the compiled log-likelihood to accomodate <code>zeus</code>\u00b6","text":""},{"location":"tutorials/compile_logp/#test-sampling-with-zeus","title":"Test sampling with <code>zeus</code>\u00b6","text":""},{"location":"tutorials/do_operator/","title":"Using the to simulate data","text":"In\u00a0[1]: Copied! <pre>import arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport hssm\nimport pymc as pm\n</pre> import arviz as az import matplotlib.pyplot as plt import numpy as np import pandas as pd import hssm import pymc as pm In\u00a0[2]: Copied! <pre># Simple model\ndata = hssm.simulate_data(\n    model=\"ddm\", theta=dict(v=0.5, a=1.5, z=0.5, t=0.1), size=500\n)\n</pre> # Simple model data = hssm.simulate_data(     model=\"ddm\", theta=dict(v=0.5, a=1.5, z=0.5, t=0.1), size=500 ) In\u00a0[3]: Copied! <pre># Model 1\nhssm_model = hssm.HSSM(model=\"ddm\", data=data)\n</pre> # Model 1 hssm_model = hssm.HSSM(model=\"ddm\", data=data) <pre>Model initialized successfully.\n</pre> In\u00a0[4]: Copied! <pre>pm.model_to_graphviz(hssm_model.pymc_model)\n</pre> pm.model_to_graphviz(hssm_model.pymc_model) Out[4]: In\u00a0[5]: Copied! <pre>from pymc.model.transform.conditioning import do\n\nsynth_idata, synth_model = hssm_model.sample_do(params = {\"v\": 0,\n                                                          \"a\": 1.5,\n                                                          \"z\": 0.5,\n                                                          \"t\": 0.1},\n                                                draws = 100,\n                                                return_model = True)\n</pre> from pymc.model.transform.conditioning import do  synth_idata, synth_model = hssm_model.sample_do(params = {\"v\": 0,                                                           \"a\": 1.5,                                                           \"z\": 0.5,                                                           \"t\": 0.1},                                                 draws = 100,                                                 return_model = True) <pre>Sampling: [rt,response]\n</pre> <p>Note that the process defines an new <code>PyMC</code> model, which sets the parameters we passed in our <code>dictionary</code> to <code>Data</code> objects.</p> In\u00a0[6]: Copied! <pre>pm.model_to_graphviz(synth_model)\n</pre> pm.model_to_graphviz(synth_model) Out[6]: In\u00a0[7]: Copied! <pre>synth_idata\n</pre> synth_idata Out[7]: arviz.InferenceData <ul> <li> prior <ul> <pre>&lt;xarray.Dataset&gt; Size: 405kB\nDimensions:  (chain: 1, draw: 100, __obs__: 500)\nCoordinates:\n  * chain    (chain) int64 8B 0\n  * draw     (draw) int64 800B 0 1 2 3 4 5 6 7 8 ... 91 92 93 94 95 96 97 98 99\n  * __obs__  (__obs__) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    v_mean   (chain, draw, __obs__) float64 400kB 0.0 0.0 0.0 ... 0.0 0.0 0.0\nAttributes:\n    created_at:                 2025-10-17T01:18:54.705842+00:00\n    arviz_version:              0.22.0\n    inference_library:          pymc\n    inference_library_version:  5.25.1</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 100</li><li>__obs__: 500</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99<pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (1)<ul><li>v_mean(chain, draw, __obs__)float640.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0<pre>array([[[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]], shape=(1, 100, 500))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n      dtype='int64', name='draw'))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='__obs__', length=500))</pre></li></ul></li><li>Attributes: (4)created_at :2025-10-17T01:18:54.705842+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1</li></ul> </ul> </li> <li> prior_predictive <ul> <pre>&lt;xarray.Dataset&gt; Size: 805kB\nDimensions:          (chain: 1, draw: 100, __obs__: 500, rt,response_dim: 2)\nCoordinates:\n  * chain            (chain) int64 8B 0\n  * draw             (draw) int64 800B 0 1 2 3 4 5 6 7 ... 93 94 95 96 97 98 99\n  * __obs__          (__obs__) int64 4kB 0 1 2 3 4 5 ... 494 495 496 497 498 499\n  * rt,response_dim  (rt,response_dim) int64 16B 0 1\nData variables:\n    rt,response      (chain, draw, __obs__, rt,response_dim) float64 800kB 1....\nAttributes:\n    created_at:                 2025-10-17T01:18:54.708836+00:00\n    arviz_version:              0.22.0\n    inference_library:          pymc\n    inference_library_version:  5.25.1</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 100</li><li>__obs__: 500</li><li>rt,response_dim: 2</li></ul></li><li>Coordinates: (4)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99<pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>rt,response_dim(rt,response_dim)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__, rt,response_dim)float641.447 1.0 1.199 ... 1.0 1.531 1.0<pre>array([[[[ 1.44655442,  1.        ],\n         [ 1.19892251,  1.        ],\n         [ 0.89263052, -1.        ],\n         ...,\n         [ 0.71472335, -1.        ],\n         [ 0.73178536, -1.        ],\n         [ 2.36448574,  1.        ]],\n\n        [[ 2.13502669,  1.        ],\n         [ 2.94716907, -1.        ],\n         [ 3.47942352,  1.        ],\n         ...,\n         [ 3.59060931,  1.        ],\n         [ 3.22709524, -1.        ],\n         [ 1.05840957, -1.        ]],\n\n        [[ 3.17093587, -1.        ],\n         [ 2.39028549,  1.        ],\n         [ 2.4552834 , -1.        ],\n         ...,\n...\n         ...,\n         [ 2.50948834, -1.        ],\n         [ 4.12036276,  1.        ],\n         [ 1.79317844,  1.        ]],\n\n        [[ 2.14002967, -1.        ],\n         [ 2.82443666,  1.        ],\n         [ 1.58908927, -1.        ],\n         ...,\n         [ 0.53689468,  1.        ],\n         [ 1.74238849,  1.        ],\n         [ 6.57336617, -1.        ]],\n\n        [[ 0.77919495, -1.        ],\n         [ 2.25651217,  1.        ],\n         [ 2.38643312,  1.        ],\n         ...,\n         [ 1.18781722, -1.        ],\n         [ 0.32062083,  1.        ],\n         [ 1.53068888,  1.        ]]]], shape=(1, 100, 500, 2))</pre></li></ul></li><li>Indexes: (4)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n      dtype='int64', name='draw'))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='__obs__', length=500))</pre></li><li>rt,response_dimPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_dim'))</pre></li></ul></li><li>Attributes: (4)created_at :2025-10-17T01:18:54.708836+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 12kB\nDimensions:                  (__obs__: 500, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 4kB 0 1 2 3 4 ... 496 497 498 499\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 8kB 0...\nAttributes:\n    created_at:                 2025-10-17T01:18:54.709714+00:00\n    arviz_version:              0.22.0\n    inference_library:          pymc\n    inference_library_version:  5.25.1</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 500</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float640.5801 1.0 2.519 ... 1.0 1.422 1.0<pre>array([[ 0.58008313,  1.        ],\n       [ 2.51914597,  1.        ],\n       [ 5.1725502 , -1.        ],\n       [ 5.38408518,  1.        ],\n       [ 2.92615318,  1.        ],\n       [ 2.60508227,  1.        ],\n       [ 1.60326254,  1.        ],\n       [ 1.45287967,  1.        ],\n       [ 1.62478471, -1.        ],\n       [ 1.49670947,  1.        ],\n       [ 2.34649038,  1.        ],\n       [ 2.30925393,  1.        ],\n       [ 2.05400562,  1.        ],\n       [ 0.68057668,  1.        ],\n       [ 0.88679963,  1.        ],\n       [ 0.41172174,  1.        ],\n       [ 2.29736328,  1.        ],\n       [ 2.66417313,  1.        ],\n       [ 1.93516386,  1.        ],\n       [ 1.83781409,  1.        ],\n...\n       [ 4.18482494,  1.        ],\n       [ 1.79381561,  1.        ],\n       [ 0.59991771,  1.        ],\n       [ 3.06441665,  1.        ],\n       [ 2.87169862,  1.        ],\n       [ 1.98977184,  1.        ],\n       [ 2.80292845,  1.        ],\n       [ 0.73743659,  1.        ],\n       [ 3.1652298 ,  1.        ],\n       [ 2.587255  ,  1.        ],\n       [ 3.69039965,  1.        ],\n       [ 0.96508873,  1.        ],\n       [ 0.45725834, -1.        ],\n       [ 0.81268889,  1.        ],\n       [ 2.14377213,  1.        ],\n       [ 1.3394289 ,  1.        ],\n       [ 2.96452594,  1.        ],\n       [ 3.14054394,  1.        ],\n       [ 0.86679167,  1.        ],\n       [ 1.42190468,  1.        ]])</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='__obs__', length=500))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (4)created_at :2025-10-17T01:18:54.709714+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1</li></ul> </ul> </li> <li> constant_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 32B\nDimensions:  ()\nData variables:\n    z        float64 8B 0.5\n    a        float64 8B 1.5\n    t        float64 8B 0.1\n    v        float64 8B 0.0\nAttributes:\n    created_at:                 2025-10-17T01:18:54.710879+00:00\n    arviz_version:              0.22.0\n    inference_library:          pymc\n    inference_library_version:  5.25.1</pre>xarray.Dataset<ul><li>Dimensions:</li><li>Coordinates: (0)<ul></ul></li><li>Data variables: (4)<ul><li>z()float640.5<pre>array(0.5)</pre></li><li>a()float641.5<pre>array(1.5)</pre></li><li>t()float640.1<pre>array(0.1)</pre></li><li>v()float640.0<pre>array(0.)</pre></li></ul></li><li>Indexes: (0)<ul></ul></li><li>Attributes: (4)created_at :2025-10-17T01:18:54.710879+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1</li></ul> </ul> </li> </ul> <p>Observe two things:</p> <ol> <li>A closer look at the <code>prior</code> group reveals that no parameters were actually sampled (it collects on the deterministic <code>v_mean</code> which, arguably, we can skip too)</li> <li>The <code>prior_predictive</code> group is where we collected our synthetic data</li> </ol> <p>Our synthetic data sits in the <code>prior_predictive</code> group, since, under the hood, were de facto called the <code>sample_prior_predictive()</code> method, on our new do-model.</p> <p>If you prefer to get your outputs as a dataframe, you can use the <code>predictive_idata_to_dataframe</code> method from the <code>hssm.utils</code> module.</p> In\u00a0[8]: Copied! <pre>synth_df = hssm.utils.predictive_idata_to_dataframe(synth_idata,\n                                                    predictive_group = \"prior_predictive\")\n</pre> synth_df = hssm.utils.predictive_idata_to_dataframe(synth_idata,                                                     predictive_group = \"prior_predictive\") In\u00a0[9]: Copied! <pre>synth_df\n</pre> synth_df Out[9]: chain draw __obs__ rt response 0 0 0 0 1.446554 1.0 1 0 0 1 1.198923 1.0 2 0 0 2 0.892631 -1.0 3 0 0 3 3.382068 1.0 4 0 0 4 0.436959 -1.0 ... ... ... ... ... ... 49995 0 99 495 3.510932 1.0 49996 0 99 496 1.799391 1.0 49997 0 99 497 1.187817 -1.0 49998 0 99 498 0.320621 1.0 49999 0 99 499 1.530689 1.0 <p>50000 rows \u00d7 5 columns</p> <p>From here you may re-attach the original data used to define the <code>hssm_model</code> (e.g.to re-attach covariates that you want as part of the model you are testing on your synthetic data).</p> <p>The <code>sample_do()</code> function can be extremely helpful in setting up numerical experiments to test e.g. for parameter identifiability in scenarios that you wish to control precisely.</p>"},{"location":"tutorials/do_operator/#using-the-do-operator-to-simulate-data","title":"Using the <code>do-operator</code> to simulate data\u00b6","text":"<p>This tutorial illustrates how we can use the underlying <code>PyMC</code> model of a given HSSM model, to forward simulate datasets that you may use for numerical studies.</p>"},{"location":"tutorials/do_operator/#simulate-data","title":"Simulate Data\u00b6","text":"<p>We will simulate a simple dataset that contains two conditions.</p>"},{"location":"tutorials/do_operator/#specify-hssm-model","title":"Specify HSSM Model\u00b6","text":"<p>We will fit two separate models to the data.</p>"},{"location":"tutorials/do_operator/#underlying-pymc-model","title":"Underlying PyMC model\u00b6","text":""},{"location":"tutorials/do_operator/#using-the-do-operator","title":"Using the <code>do-operator</code>\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/","title":"HSSM Tutorial Winterbrain 2025","text":"In\u00a0[\u00a0]: Copied! <pre># Import modules\nimport arviz as az\nimport jax\nimport matplotlib as plt\nimport pytensor\n\nimport hssm\n\npytensor.config.floatX = \"float32\"\njax.config.update(\"jax_enable_x64\", False)\n\nplt.use(\"Agg\")\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n# hssm.set_floatX(\"float32\")\n</pre> # Import modules import arviz as az import jax import matplotlib as plt import pytensor  import hssm  pytensor.config.floatX = \"float32\" jax.config.update(\"jax_enable_x64\", False)  plt.use(\"Agg\") %matplotlib inline %config InlineBackend.figure_format='retina'  # hssm.set_floatX(\"float32\") <pre>Setting PyTensor floatX type to float32.\nSetting \"jax_enable_x64\" to False. If this is not intended, please set `jax` to False.\n</pre> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Image\n\n# Display the image\nImage(filename=\"hssm_tutorial_workshop_1/ssm_overview.png\")\n</pre> from IPython.display import Image  # Display the image Image(filename=\"hssm_tutorial_workshop_1/ssm_overview.png\") Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre>### Check supported models\nhssm.defaults.SupportedModels\n</pre> ### Check supported models hssm.defaults.SupportedModels Out[\u00a0]: <pre>typing.Literal['ddm', 'ddm_sdv', 'full_ddm', 'angle', 'levy', 'ornstein', 'weibull', 'race_no_bias_angle_4', 'ddm_seq2_no_bias', 'lba3', 'lba2']</pre> In\u00a0[\u00a0]: Copied! <pre>### Details about a particular model\nhssm.defaults.default_model_config[\"weibull\"]\n##### the bounds within \"likelihoods\" provide reasonable parameter values (take this with a grain of salt)\n</pre> ### Details about a particular model hssm.defaults.default_model_config[\"weibull\"] ##### the bounds within \"likelihoods\" provide reasonable parameter values (take this with a grain of salt) Out[\u00a0]: <pre>{'response': ['rt', 'response'],\n 'list_params': ['v', 'a', 'z', 't', 'alpha', 'beta'],\n 'choices': [-1, 1],\n 'description': None,\n 'likelihoods': {'approx_differentiable': {'loglik': 'weibull.onnx',\n   'backend': 'jax',\n   'default_priors': {},\n   'bounds': {'v': (-2.5, 2.5),\n    'a': (0.3, 2.5),\n    'z': (0.2, 0.8),\n    't': (0.001, 2.0),\n    'alpha': (0.31, 4.99),\n    'beta': (0.31, 6.99)},\n   'extra_fields': None}}}</pre> In\u00a0[\u00a0]: Copied! <pre>hssm.defaults.default_model_config[\"ddm\"]\n</pre> hssm.defaults.default_model_config[\"ddm\"] Out[\u00a0]: <pre>{'response': ['rt', 'response'],\n 'list_params': ['v', 'a', 'z', 't'],\n 'choices': [-1, 1],\n 'description': 'The Drift Diffusion Model (DDM)',\n 'likelihoods': {'analytical': {'loglik': &lt;function hssm.likelihoods.analytical.logp_ddm(data: numpy.ndarray, v: float, a: float, z: float, t: float, err: float = 1e-15, k_terms: int = 20, epsilon: float = 1e-15) -&gt; numpy.ndarray&gt;,\n   'backend': None,\n   'bounds': {'v': (-inf, inf),\n    'a': (0.0, inf),\n    'z': (0.0, 1.0),\n    't': (0.0, inf)},\n   'default_priors': {'t': {'name': 'HalfNormal', 'sigma': 2.0}},\n   'extra_fields': None},\n  'approx_differentiable': {'loglik': 'ddm.onnx',\n   'backend': 'jax',\n   'default_priors': {'t': {'name': 'HalfNormal', 'sigma': 2.0}},\n   'bounds': {'v': (-3.0, 3.0),\n    'a': (0.3, 2.5),\n    'z': (0.0, 1.0),\n    't': (0.0, 2.0)},\n   'extra_fields': None},\n  'blackbox': {'loglik': &lt;function hssm.likelihoods.blackbox.hddm_to_hssm.&lt;locals&gt;.outer(data: numpy.ndarray, *args, **kwargs)&gt;,\n   'backend': None,\n   'bounds': {'v': (-inf, inf),\n    'a': (0.0, inf),\n    'z': (0.0, 1.0),\n    't': (0.0, inf)},\n   'default_priors': {'t': {'name': 'HalfNormal', 'sigma': 2.0}},\n   'extra_fields': None}}}</pre> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom ssms.basic_simulators.simulator import simulator\n\nv_true = 0.5\na_true = 1.5\nz_true = 0.5\nt_true = 0.5\n# a changes trial wise\n# a_trialwise = np.random.normal(loc=2, scale=0.3, size=1000)\nIntercept = 1.5\nzSTN = np.random.normal(loc=1, scale=1, size=1000).astype(np.float32)\na_trialwise = Intercept + 0.5 * zSTN\n\n# a changes trial wise\ntheta_mat = np.zeros((1000, 4))\ntheta_mat[:, 0] = v_true  # v\ntheta_mat[:, 1] = a_trialwise  # a\ntheta_mat[:, 2] = z_true  # z\ntheta_mat[:, 3] = t_true  # t\n\n# simulate data\nsim_out_trialwise = simulator(\n    theta=theta_mat,  # parameter_matrix\n    model=\"ddm\",  # specify model (many are included in ssms)\n    n_samples=1,  # number of samples for each set of parameters\n    # (plays the role of `size` parameter in `hssm.simulate_data`)\n)\n\n# Turn into nice dataset\ndf_ddm_reg_case1 = pd.DataFrame(\n    np.column_stack(\n        [sim_out_trialwise[\"rts\"][:, 0], sim_out_trialwise[\"choices\"][:, 0]]\n    ),\n    columns=[\"rt\", \"response\"],\n)\n\ndf_ddm_reg_case1[\"zSTN\"] = zSTN\ndf_ddm_reg_case1\n</pre> import numpy as np import pandas as pd from ssms.basic_simulators.simulator import simulator  v_true = 0.5 a_true = 1.5 z_true = 0.5 t_true = 0.5 # a changes trial wise # a_trialwise = np.random.normal(loc=2, scale=0.3, size=1000) Intercept = 1.5 zSTN = np.random.normal(loc=1, scale=1, size=1000).astype(np.float32) a_trialwise = Intercept + 0.5 * zSTN  # a changes trial wise theta_mat = np.zeros((1000, 4)) theta_mat[:, 0] = v_true  # v theta_mat[:, 1] = a_trialwise  # a theta_mat[:, 2] = z_true  # z theta_mat[:, 3] = t_true  # t  # simulate data sim_out_trialwise = simulator(     theta=theta_mat,  # parameter_matrix     model=\"ddm\",  # specify model (many are included in ssms)     n_samples=1,  # number of samples for each set of parameters     # (plays the role of `size` parameter in `hssm.simulate_data`) )  # Turn into nice dataset df_ddm_reg_case1 = pd.DataFrame(     np.column_stack(         [sim_out_trialwise[\"rts\"][:, 0], sim_out_trialwise[\"choices\"][:, 0]]     ),     columns=[\"rt\", \"response\"], )  df_ddm_reg_case1[\"zSTN\"] = zSTN df_ddm_reg_case1 Out[\u00a0]: rt response zSTN 0 2.337352 1.0 -0.557265 1 3.005860 1.0 0.928339 2 1.733934 1.0 0.308642 3 1.857290 1.0 0.352734 4 2.558753 -1.0 1.138395 ... ... ... ... 995 2.165475 1.0 2.408830 996 4.439553 1.0 0.351558 997 1.401433 1.0 2.409043 998 3.524226 1.0 1.129747 999 16.383413 1.0 2.785511 <p>1000 rows \u00d7 3 columns</p> In\u00a0[\u00a0]: Copied! <pre>model_ddm_reg_case1 = hssm.HSSM(\n    data=df_ddm_reg_case1,\n    model=\"ddm\",\n    include=[\n        {\n            \"name\": \"a\",\n            \"formula\": \"a ~ 1 + zSTN\",\n            \"prior\": {\n                # All ways to specify priors in the non-regression case work the same way here.\n                \"Intercept\": {\"name\": \"Normal\", \"mu\": 1.5, \"sigma\": 1.0},\n                \"zSTN\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n            },\n            \"link\": \"identity\",\n        }\n    ],\n)\n</pre> model_ddm_reg_case1 = hssm.HSSM(     data=df_ddm_reg_case1,     model=\"ddm\",     include=[         {             \"name\": \"a\",             \"formula\": \"a ~ 1 + zSTN\",             \"prior\": {                 # All ways to specify priors in the non-regression case work the same way here.                 \"Intercept\": {\"name\": \"Normal\", \"mu\": 1.5, \"sigma\": 1.0},                 \"zSTN\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},             },             \"link\": \"identity\",         }     ], ) <pre>Model initialized successfully.\n</pre> Out[\u00a0]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 1000\n\nParameters:\n\nv:\n    Prior: Normal(mu: 0.0, sigma: 2.0)\n    Explicit bounds: (-inf, inf)\n\na:\n    Formula: a ~ 1 + zSTN\n    Priors:\n        a_Intercept ~ Normal(mu: 1.5, sigma: 1.0)\n        a_zSTN ~ Normal(mu: 0.0, sigma: 1.0)\n    Link: identity\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[\u00a0]: Copied! <pre># graphical illustration of model\nmodel_ddm_reg_case1.graph()\n</pre> # graphical illustration of model model_ddm_reg_case1.graph() Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre># suppress warnings\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nsamples_model_ddm_reg_case1 = model_ddm_reg_case1.sample(\n    sampler=\"nuts_numpyro\",\n    cores=2,\n    chains=2,\n    draws=500,\n    tune=500,\n    idata_kwargs=dict(log_likelihood=True),\n)\n</pre> # suppress warnings import warnings  warnings.filterwarnings(\"ignore\") samples_model_ddm_reg_case1 = model_ddm_reg_case1.sample(     sampler=\"nuts_numpyro\",     cores=2,     chains=2,     draws=500,     tune=500,     idata_kwargs=dict(log_likelihood=True), ) <pre>Using default initvals. \n\n</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>We recommend running at least 4 chains for robust computation of convergence diagnostics\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00&lt;00:00, 2582.16it/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre>#### quick check on posterior statistics\nmodel_ddm_reg_case1.summary()\n</pre> #### quick check on posterior statistics model_ddm_reg_case1.summary() Out[\u00a0]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat t 0.508 0.021 0.470 0.547 0.001 0.001 722.0 541.0 1.0 z 0.507 0.014 0.479 0.532 0.001 0.000 492.0 583.0 1.0 a_zSTN 0.518 0.022 0.476 0.558 0.001 0.001 658.0 722.0 1.0 a_Intercept 1.516 0.033 1.456 1.577 0.001 0.001 751.0 864.0 1.0 v 0.546 0.028 0.493 0.598 0.001 0.001 651.0 547.0 1.0 In\u00a0[\u00a0]: Copied! <pre>#### trace plots\nmodel_ddm_reg_case1.plot_trace();\n</pre> #### trace plots model_ddm_reg_case1.plot_trace(); <ul> <li>besides from STN, we also have an indirect nogo pathway via GPe</li> <li>this time, we want to assume that activity in these components modulates drift rate</li> <li>even though there is evidence that STN modulates the boundary, we always want to check different model specifications</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Function to simulate data for one participant\ndef simulate_participant(participant_id, size=300):\n    intercept = 1.5\n    zSTN = np.random.normal(loc=1, scale=2, size=size)\n    zGPe = np.random.normal(loc=1, scale=2, size=size)\n    v = intercept + 0.8 * zSTN + 0.3 * zGPe\n\n    # Assume `hssm.simulate_data` returns a DataFrame\n    true_values = np.column_stack(\n        [v, np.repeat([[1.5, 0.5, 0.5]], axis=0, repeats=300)]\n    )\n\n    dataset_reg_v = hssm.simulate_data(\n        model=\"ddm\",\n        theta=true_values,\n        size=1,  # Generate one data point for each of the 1000 set of true values\n    )\n\n    # Adding additional variables to the dataset\n    dataset_reg_v[\"zSTN\"] = zSTN\n    dataset_reg_v[\"zGPe\"] = zGPe\n    dataset_reg_v[\"participant_id\"] = str(participant_id)\n    return dataset_reg_v\n\n\n# Simulate data for 10 participants\n\n# Combine datasets into one DataFrame\ncombined_dataset = pd.concat(\n    [simulate_participant(i, size=300) for i in range(1, 11)], ignore_index=True\n)\ncombined_dataset\n</pre> # Function to simulate data for one participant def simulate_participant(participant_id, size=300):     intercept = 1.5     zSTN = np.random.normal(loc=1, scale=2, size=size)     zGPe = np.random.normal(loc=1, scale=2, size=size)     v = intercept + 0.8 * zSTN + 0.3 * zGPe      # Assume `hssm.simulate_data` returns a DataFrame     true_values = np.column_stack(         [v, np.repeat([[1.5, 0.5, 0.5]], axis=0, repeats=300)]     )      dataset_reg_v = hssm.simulate_data(         model=\"ddm\",         theta=true_values,         size=1,  # Generate one data point for each of the 1000 set of true values     )      # Adding additional variables to the dataset     dataset_reg_v[\"zSTN\"] = zSTN     dataset_reg_v[\"zGPe\"] = zGPe     dataset_reg_v[\"participant_id\"] = str(participant_id)     return dataset_reg_v   # Simulate data for 10 participants  # Combine datasets into one DataFrame combined_dataset = pd.concat(     [simulate_participant(i, size=300) for i in range(1, 11)], ignore_index=True ) combined_dataset Out[\u00a0]: rt response zSTN zGPe participant_id 0 1.280871 1.0 0.851466 -2.818660 1 1 1.338707 1.0 0.008094 1.254727 1 2 0.827624 1.0 0.892344 2.773173 1 3 1.224458 1.0 3.908126 1.674779 1 4 0.783313 1.0 3.571661 0.838951 1 ... ... ... ... ... ... 2995 1.299362 -1.0 -2.218223 -4.054360 10 2996 2.183317 1.0 -0.109671 0.244783 10 2997 1.006327 1.0 0.268048 3.415423 10 2998 2.929376 1.0 -1.537712 0.608413 10 2999 1.878767 -1.0 -2.846485 1.423902 10 <p>3000 rows \u00d7 5 columns</p> In\u00a0[\u00a0]: Copied! <pre>model_reg_v_ex2_A1 = hssm.HSSM(\n    data=combined_dataset,\n    model=\"ddm\",\n    include=[\n        {\n            \"name\": \"v\",\n            \"formula\": \"v ~ 0 + (1 + zSTN + zGPe | participant_id)\",\n            \"prior\": {\n                \"1|participant_id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": {\"name\": \"Normal\", \"mu\": 1, \"sigma\": 1.0, \"initval\": 0},\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1, \"initval\": 0.25},\n                },\n                \"zSTN|participant_id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0, \"initval\": 0},\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1, \"initval\": 0.25},\n                },\n                \"zGPe|participant_id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0, \"initval\": 0},\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1, \"initval\": 0.25},\n                },\n            },\n            \"link\": \"identity\",\n        }\n    ],\n    noncentered=False,\n    p_outlier=0.05,\n)\n</pre> model_reg_v_ex2_A1 = hssm.HSSM(     data=combined_dataset,     model=\"ddm\",     include=[         {             \"name\": \"v\",             \"formula\": \"v ~ 0 + (1 + zSTN + zGPe | participant_id)\",             \"prior\": {                 \"1|participant_id\": {                     \"name\": \"Normal\",                     \"mu\": {\"name\": \"Normal\", \"mu\": 1, \"sigma\": 1.0, \"initval\": 0},                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1, \"initval\": 0.25},                 },                 \"zSTN|participant_id\": {                     \"name\": \"Normal\",                     \"mu\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0, \"initval\": 0},                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1, \"initval\": 0.25},                 },                 \"zGPe|participant_id\": {                     \"name\": \"Normal\",                     \"mu\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0, \"initval\": 0},                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1, \"initval\": 0.25},                 },             },             \"link\": \"identity\",         }     ],     noncentered=False,     p_outlier=0.05, ) <pre>Model initialized successfully.\n</pre> Out[\u00a0]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 3000\n\nParameters:\n\nv:\n    Formula: v ~ 0 + (1 + zSTN + zGPe | participant_id)\n    Priors:\n        v_1|participant_id ~ Normal(mu: Normal(mu: 1.0, sigma: 1.0, initval: 0.0), sigma: HalfNormal(sigma: 1.0, initval: 0.25))\n        v_zSTN|participant_id ~ Normal(mu: Normal(mu: 0.0, sigma: 1.0, initval: 0.0), sigma: HalfNormal(sigma: 1.0, initval: 0.25))\n        v_zGPe|participant_id ~ Normal(mu: Normal(mu: 0.0, sigma: 1.0, initval: 0.0), sigma: HalfNormal(sigma: 1.0, initval: 0.25))\n    Link: identity\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[\u00a0]: Copied! <pre># graphical illustration of model\nmodel_reg_v_ex2_A1.graph()\n</pre> # graphical illustration of model model_reg_v_ex2_A1.graph() Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre>model_reg_v_ex2_A2 = hssm.HSSM(\n    data=combined_dataset,\n    include=[\n        {\n            \"name\": \"v\",\n            \"formula\": \"v ~ 1 + zSTN + zGPe + (1 + zSTN + zGPe | participant_id)\",\n            \"prior\": {\n                # All ways to specify priors in the non-regression case work the same way here.\n                \"Intercept\": {\"name\": \"Normal\", \"mu\": 1.0, \"sigma\": 1.0},\n                \"zSTN\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n                \"zGPe\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n                \"1|participant_id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": 0,\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1.0},\n                },\n                \"zSTN|participant_id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": 0,\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1.0},\n                },\n                \"zGPe|participant_id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": 0,\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1.0},\n                },\n            },\n            \"link\": \"identity\",\n        }\n    ],\n    noncentered=True,\n    p_outlier=0.05,\n)\n</pre> model_reg_v_ex2_A2 = hssm.HSSM(     data=combined_dataset,     include=[         {             \"name\": \"v\",             \"formula\": \"v ~ 1 + zSTN + zGPe + (1 + zSTN + zGPe | participant_id)\",             \"prior\": {                 # All ways to specify priors in the non-regression case work the same way here.                 \"Intercept\": {\"name\": \"Normal\", \"mu\": 1.0, \"sigma\": 1.0},                 \"zSTN\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},                 \"zGPe\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},                 \"1|participant_id\": {                     \"name\": \"Normal\",                     \"mu\": 0,                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1.0},                 },                 \"zSTN|participant_id\": {                     \"name\": \"Normal\",                     \"mu\": 0,                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1.0},                 },                 \"zGPe|participant_id\": {                     \"name\": \"Normal\",                     \"mu\": 0,                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1.0},                 },             },             \"link\": \"identity\",         }     ],     noncentered=True,     p_outlier=0.05, ) <pre>Model initialized successfully.\n</pre> Out[\u00a0]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 3000\n\nParameters:\n\nv:\n    Formula: v ~ 1 + zSTN + zGPe + (1 + zSTN + zGPe | participant_id)\n    Priors:\n        v_Intercept ~ Normal(mu: 1.0, sigma: 1.0)\n        v_zSTN ~ Normal(mu: 0.0, sigma: 1.0)\n        v_zGPe ~ Normal(mu: 0.0, sigma: 1.0)\n        v_1|participant_id ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 1.0))\n        v_zSTN|participant_id ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 1.0))\n        v_zGPe|participant_id ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 1.0))\n    Link: identity\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[\u00a0]: Copied! <pre>model_reg_v_ex2_A2.graph()\n</pre> model_reg_v_ex2_A2.graph() Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre>## centered model version\nsamples_model_reg_v_ex2_A1 = model_reg_v_ex2_A1.sample(\n    sampler=\"nuts_numpyro\",\n    cores=3,\n    chains=3,\n    draws=500,\n    tune=500,\n    idata_kwargs=dict(log_likelihood=True),\n)\n</pre> ## centered model version samples_model_reg_v_ex2_A1 = model_reg_v_ex2_A1.sample(     sampler=\"nuts_numpyro\",     cores=3,     chains=3,     draws=500,     tune=500,     idata_kwargs=dict(log_likelihood=True), ) <pre>Using default initvals. \n\n</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>There were 322 divergences after tuning. Increase `target_accept` or reparameterize.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1500/1500 [00:01&lt;00:00, 845.05it/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre>## centered model version\nsamples_model_reg_v_ex2_A2 = model_reg_v_ex2_A2.sample(\n    sampler=\"nuts_numpyro\",\n    cores=3,\n    chains=3,\n    draws=500,\n    tune=500,\n    idata_kwargs=dict(log_likelihood=True),\n)\n</pre> ## centered model version samples_model_reg_v_ex2_A2 = model_reg_v_ex2_A2.sample(     sampler=\"nuts_numpyro\",     cores=3,     chains=3,     draws=500,     tune=500,     idata_kwargs=dict(log_likelihood=True), ) <pre>Using default initvals. \n\n</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>We recommend running at least 4 chains for robust computation of convergence diagnostics\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1500/1500 [00:02&lt;00:00, 704.70it/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre>#### posterior statistics of centered model\nmodel_reg_v_ex2_A1.summary(var_names=[\"~_id\"], filter_vars=\"like\")\n</pre> #### posterior statistics of centered model model_reg_v_ex2_A1.summary(var_names=[\"~_id\"], filter_vars=\"like\") Out[\u00a0]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat t 0.498 0.006 0.488 0.511 0.001 0.000 113.0 200.0 1.06 z 0.486 0.011 0.463 0.504 0.002 0.002 31.0 482.0 1.07 a 1.487 0.028 1.428 1.533 0.006 0.004 22.0 172.0 1.13 In\u00a0[\u00a0]: Copied! <pre># Plotting the posterior predictive\nhssm.plotting.plot_predictive(\n    model_reg_v_ex2_A1, col=\"participant_id\", col_wrap=5\n)\n</pre> # Plotting the posterior predictive hssm.plotting.plot_predictive(     model_reg_v_ex2_A1, col=\"participant_id\", col_wrap=5 ) <pre>No posterior predictive samples found. Generating posterior predictive samples using the provided InferenceData object and the original data. This will modify the provided InferenceData object, or if not provided, the traces object stored inside the model.\n</pre> Out[\u00a0]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x361497610&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>#### posterior statistics of non-centered model\nmodel_reg_v_ex2_A2.summary(var_names=[\"~_offset\"], filter_vars=\"like\")\n</pre> #### posterior statistics of non-centered model model_reg_v_ex2_A2.summary(var_names=[\"~_offset\"], filter_vars=\"like\") Out[\u00a0]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat v_Intercept 1.538 0.041 1.466 1.620 0.001 0.001 887.0 1052.0 1.00 v_zGPe|participant_id[1] 0.027 0.029 -0.014 0.085 0.001 0.001 881.0 1198.0 1.00 v_zGPe|participant_id[10] -0.007 0.023 -0.056 0.036 0.001 0.001 1343.0 1122.0 1.00 v_zGPe|participant_id[2] -0.023 0.026 -0.076 0.017 0.001 0.001 795.0 1272.0 1.00 v_zGPe|participant_id[3] 0.011 0.025 -0.036 0.064 0.001 0.001 1570.0 1159.0 1.01 v_zGPe|participant_id[4] 0.015 0.025 -0.024 0.069 0.001 0.001 1106.0 1078.0 1.00 v_zGPe|participant_id[5] -0.022 0.026 -0.074 0.022 0.001 0.001 763.0 1023.0 1.00 v_zGPe|participant_id[6] 0.005 0.022 -0.038 0.047 0.001 0.001 1736.0 1410.0 1.00 v_zGPe|participant_id[7] 0.001 0.023 -0.044 0.047 0.001 0.001 1874.0 1300.0 1.00 v_zGPe|participant_id[8] 0.006 0.023 -0.038 0.052 0.001 0.000 1561.0 1413.0 1.00 v_zGPe|participant_id[9] -0.012 0.024 -0.062 0.030 0.001 0.001 1411.0 1310.0 1.00 v_zGPe 0.294 0.016 0.266 0.324 0.000 0.000 1061.0 818.0 1.00 v_1|participant_id_sigma 0.033 0.027 0.000 0.081 0.001 0.001 853.0 1005.0 1.00 t 0.499 0.006 0.487 0.510 0.000 0.000 1205.0 1000.0 1.00 z 0.483 0.011 0.464 0.507 0.000 0.000 1192.0 965.0 1.00 v_zGPe|participant_id_sigma 0.028 0.018 0.000 0.058 0.001 0.001 416.0 597.0 1.00 v_zSTN 0.815 0.018 0.781 0.849 0.001 0.000 1172.0 803.0 1.01 a 1.478 0.026 1.429 1.526 0.001 0.000 1625.0 1145.0 1.01 v_1|participant_id[1] 0.006 0.033 -0.056 0.071 0.001 0.001 1450.0 1247.0 1.00 v_1|participant_id[10] 0.015 0.034 -0.040 0.089 0.001 0.001 1251.0 1277.0 1.00 v_1|participant_id[2] -0.011 0.034 -0.078 0.053 0.001 0.001 1603.0 1310.0 1.00 v_1|participant_id[3] -0.018 0.035 -0.095 0.037 0.001 0.001 1748.0 1383.0 1.00 v_1|participant_id[4] 0.013 0.034 -0.048 0.085 0.001 0.001 1585.0 1074.0 1.00 v_1|participant_id[5] -0.004 0.033 -0.074 0.060 0.001 0.001 1596.0 1194.0 1.00 v_1|participant_id[6] 0.002 0.034 -0.067 0.065 0.001 0.001 2092.0 1308.0 1.00 v_1|participant_id[7] 0.005 0.033 -0.052 0.079 0.001 0.001 1963.0 1303.0 1.00 v_1|participant_id[8] 0.005 0.036 -0.056 0.083 0.001 0.001 1592.0 1257.0 1.00 v_1|participant_id[9] -0.012 0.034 -0.081 0.047 0.001 0.001 1835.0 1333.0 1.01 v_zSTN|participant_id_sigma 0.021 0.016 0.000 0.049 0.001 0.000 685.0 1108.0 1.00 v_zSTN|participant_id[1] -0.012 0.023 -0.059 0.024 0.001 0.000 1683.0 1310.0 1.00 v_zSTN|participant_id[10] 0.003 0.019 -0.033 0.044 0.000 0.000 1765.0 1454.0 1.00 v_zSTN|participant_id[2] 0.004 0.020 -0.033 0.045 0.000 0.000 1960.0 1477.0 1.00 v_zSTN|participant_id[3] 0.007 0.020 -0.030 0.050 0.001 0.000 1498.0 1249.0 1.00 v_zSTN|participant_id[4] -0.005 0.019 -0.046 0.032 0.001 0.000 1465.0 1114.0 1.00 v_zSTN|participant_id[5] -0.013 0.022 -0.061 0.020 0.001 0.000 1340.0 1119.0 1.00 v_zSTN|participant_id[6] 0.001 0.020 -0.036 0.043 0.001 0.000 1779.0 1268.0 1.00 v_zSTN|participant_id[7] 0.012 0.022 -0.020 0.064 0.001 0.000 1604.0 1141.0 1.00 v_zSTN|participant_id[8] -0.004 0.019 -0.042 0.033 0.000 0.000 1700.0 1158.0 1.00 v_zSTN|participant_id[9] 0.009 0.021 -0.024 0.054 0.001 0.000 1422.0 1278.0 1.00 In\u00a0[\u00a0]: Copied! <pre># Plotting the posterior predictive\nhssm.plotting.plot_predictive(\n    model_reg_v_ex2_A2, col=\"participant_id\", col_wrap=5\n)\n</pre> # Plotting the posterior predictive hssm.plotting.plot_predictive(     model_reg_v_ex2_A2, col=\"participant_id\", col_wrap=5 ) <pre>No posterior predictive samples found. Generating posterior predictive samples using the provided InferenceData object and the original data. This will modify the provided InferenceData object, or if not provided, the traces object stored inside the model.\n</pre> Out[\u00a0]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x3531b2b50&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Function to simulate data for one participant\ndef simulate_participant2(participant_id, sevScore, size=300):\n    intercept = 0.5\n    zSTN = np.random.normal(loc=1, scale=2, size=size)\n    zGPe = np.random.normal(loc=1, scale=2, size=size)\n\n    ## Strength of neural modulation depends on PD severity score\n\n    #### Case 1: only the interaction btw sevScore &amp; STN/GPe will turn out sign, the main effects of STN &amp; GPe will disappear\n    # v = intercept + 0.8 * zSTN * sevScore + 0.3 * zGPe * sevScore\n\n    #### Case 2: main effects of STN/GPe will remain, plus a significant interaction, but no modulation on intercept\n    if sevScore &lt; 0.2:\n        v = intercept + 0.8 * zSTN + 0.3 * zGPe\n    elif sevScore &gt;= 0.2 and sevScore &lt; 0.4:\n        v = intercept + 0.7 * zSTN + 0.25 * zGPe\n    elif sevScore &gt;= 0.4 and sevScore &lt; 0.6:\n        v = intercept + 0.6 * zSTN + 0.2 * zGPe\n    elif sevScore &gt;= 0.6 and sevScore &lt; 0.8:\n        v = intercept + 0.5 * zSTN + 0.15 * zGPe\n    else:\n        v = intercept + 0.4 * zSTN + 0.1 * zGPe\n\n    # Assume `hssm.simulate_data` returns a DataFrame\n    true_values = np.column_stack(\n        [v, np.repeat([[1.5, 0.5, 0.5]], axis=0, repeats=size)]\n    )\n    dataset_reg_v = hssm.simulate_data(\n        model=\"ddm\",\n        theta=true_values,\n        size=1,  # Generate one data point for each of the 1000 set of true values\n    )\n\n    # Adding additional variables to the dataset\n    dataset_reg_v[\"zSTN\"] = zSTN\n    dataset_reg_v[\"zGPe\"] = zGPe\n    dataset_reg_v[\"participant_id\"] = str(participant_id)\n    dataset_reg_v[\"sevScore\"] = sevScore\n    return dataset_reg_v\n\n\n# Simulate data for four participants\n### note that we assume that STN &amp; GPe\n\n# patients with severity scores\nsubj_list = [1, 1, 2, 3, 4, 5, 6, 7, 8, 8, 8]\nsevscore_list = [0.00, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 1.00]\n\n# Combine datasets into one DataFrame\ncombined_dataset2 = pd.concat(\n    [\n        simulate_participant2(subj, sevscore)\n        for subj, sevscore in zip(subj_list, sevscore_list)\n    ],\n    ignore_index=True,\n)\ncombined_dataset2\n</pre> # Function to simulate data for one participant def simulate_participant2(participant_id, sevScore, size=300):     intercept = 0.5     zSTN = np.random.normal(loc=1, scale=2, size=size)     zGPe = np.random.normal(loc=1, scale=2, size=size)      ## Strength of neural modulation depends on PD severity score      #### Case 1: only the interaction btw sevScore &amp; STN/GPe will turn out sign, the main effects of STN &amp; GPe will disappear     # v = intercept + 0.8 * zSTN * sevScore + 0.3 * zGPe * sevScore      #### Case 2: main effects of STN/GPe will remain, plus a significant interaction, but no modulation on intercept     if sevScore &lt; 0.2:         v = intercept + 0.8 * zSTN + 0.3 * zGPe     elif sevScore &gt;= 0.2 and sevScore &lt; 0.4:         v = intercept + 0.7 * zSTN + 0.25 * zGPe     elif sevScore &gt;= 0.4 and sevScore &lt; 0.6:         v = intercept + 0.6 * zSTN + 0.2 * zGPe     elif sevScore &gt;= 0.6 and sevScore &lt; 0.8:         v = intercept + 0.5 * zSTN + 0.15 * zGPe     else:         v = intercept + 0.4 * zSTN + 0.1 * zGPe      # Assume `hssm.simulate_data` returns a DataFrame     true_values = np.column_stack(         [v, np.repeat([[1.5, 0.5, 0.5]], axis=0, repeats=size)]     )     dataset_reg_v = hssm.simulate_data(         model=\"ddm\",         theta=true_values,         size=1,  # Generate one data point for each of the 1000 set of true values     )      # Adding additional variables to the dataset     dataset_reg_v[\"zSTN\"] = zSTN     dataset_reg_v[\"zGPe\"] = zGPe     dataset_reg_v[\"participant_id\"] = str(participant_id)     dataset_reg_v[\"sevScore\"] = sevScore     return dataset_reg_v   # Simulate data for four participants ### note that we assume that STN &amp; GPe  # patients with severity scores subj_list = [1, 1, 2, 3, 4, 5, 6, 7, 8, 8, 8] sevscore_list = [0.00, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 1.00]  # Combine datasets into one DataFrame combined_dataset2 = pd.concat(     [         simulate_participant2(subj, sevscore)         for subj, sevscore in zip(subj_list, sevscore_list)     ],     ignore_index=True, ) combined_dataset2 Out[\u00a0]: rt response zSTN zGPe participant_id sevScore 0 1.158008 1.0 -0.881464 2.870271 1 0.0 1 0.886376 1.0 4.203513 1.580031 1 0.0 2 0.990652 1.0 3.064582 0.920301 1 0.0 3 0.998088 1.0 2.655949 1.267802 1 0.0 4 1.628673 1.0 0.968137 -0.848018 1 0.0 ... ... ... ... ... ... ... 3295 1.361452 1.0 3.671557 -0.752880 8 1.0 3296 1.022262 1.0 4.033932 0.788325 8 1.0 3297 1.167423 1.0 3.281838 -1.008586 8 1.0 3298 4.083560 1.0 -1.072045 3.363209 8 1.0 3299 1.108116 1.0 -1.233768 -1.161701 8 1.0 <p>3300 rows \u00d7 6 columns</p> In\u00a0[\u00a0]: Copied! <pre>model_reg_v_ex3_A1 = hssm.HSSM(\n    data=combined_dataset2,\n    include=[\n        {\n            \"name\": \"v\",\n            \"formula\": \"v ~ 1 + (zSTN + zGPe)*sevScore + (1 + zSTN + zGPe | participant_id)\",\n            \"prior\": {\n                # All ways to specify priors in the non-regression case work the same way here.\n                \"Intercept\": {\"name\": \"Normal\", \"mu\": 1.5, \"sigma\": 1.0},\n                \"zSTN\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n                \"zGPe\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n                \"sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n                \"zSTN:sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n                \"zGPe:sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n                \"1|participant_id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": 0,\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1.0},\n                },\n                \"zSTN|participant_id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": 0,\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1.0},\n                },\n                \"zGPe|participant_id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": 0,\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1.0},\n                },\n            },\n            \"link\": \"identity\",\n        }\n    ],\n    noncentered=True,\n    p_outlier=0.05,\n)\n</pre> model_reg_v_ex3_A1 = hssm.HSSM(     data=combined_dataset2,     include=[         {             \"name\": \"v\",             \"formula\": \"v ~ 1 + (zSTN + zGPe)*sevScore + (1 + zSTN + zGPe | participant_id)\",             \"prior\": {                 # All ways to specify priors in the non-regression case work the same way here.                 \"Intercept\": {\"name\": \"Normal\", \"mu\": 1.5, \"sigma\": 1.0},                 \"zSTN\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},                 \"zGPe\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},                 \"sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},                 \"zSTN:sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},                 \"zGPe:sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},                 \"1|participant_id\": {                     \"name\": \"Normal\",                     \"mu\": 0,                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1.0},                 },                 \"zSTN|participant_id\": {                     \"name\": \"Normal\",                     \"mu\": 0,                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1.0},                 },                 \"zGPe|participant_id\": {                     \"name\": \"Normal\",                     \"mu\": 0,                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1.0},                 },             },             \"link\": \"identity\",         }     ],     noncentered=True,     p_outlier=0.05, ) <pre>Model initialized successfully.\n</pre> In\u00a0[\u00a0]: Copied! <pre>samples_model_reg_v_ex3_A1 = model_reg_v_ex3_A1.sample(\n    sampler=\"nuts_numpyro\",\n    cores=3,\n    chains=3,\n    draws=500,\n    tune=500,\n    idata_kwargs=dict(log_likelihood=True),\n)\n</pre> samples_model_reg_v_ex3_A1 = model_reg_v_ex3_A1.sample(     sampler=\"nuts_numpyro\",     cores=3,     chains=3,     draws=500,     tune=500,     idata_kwargs=dict(log_likelihood=True), ) <pre>Using default initvals. \n\n</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>There were 15 divergences after tuning. Increase `target_accept` or reparameterize.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1500/1500 [00:02&lt;00:00, 684.65it/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre>#### posterior statistics of non-centered model\nmodel_reg_v_ex3_A1.summary()\n</pre> #### posterior statistics of non-centered model model_reg_v_ex3_A1.summary() Out[\u00a0]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat v_zGPe:sevScore -0.227 0.039 -0.303 -0.158 0.002 0.001 582.0 482.0 1.00 v_Intercept 0.605 0.052 0.506 0.698 0.002 0.001 793.0 759.0 1.00 v_zGPe 0.300 0.023 0.255 0.341 0.001 0.001 662.0 652.0 1.00 v_1|participant_id_sigma 0.045 0.038 0.000 0.112 0.002 0.002 341.0 232.0 1.01 t 0.500 0.008 0.485 0.514 0.000 0.000 1476.0 1039.0 1.01 z 0.484 0.008 0.470 0.500 0.000 0.000 1518.0 1220.0 1.01 v_zGPe|participant_id_sigma 0.017 0.014 0.000 0.040 0.001 0.000 416.0 727.0 1.00 v_zSTN 0.763 0.073 0.603 0.868 0.006 0.004 228.0 103.0 1.00 a 1.507 0.020 1.468 1.544 0.000 0.000 1736.0 1156.0 1.00 v_zSTN:sevScore -0.336 0.156 -0.532 -0.024 0.014 0.010 193.0 103.0 1.01 v_zSTN|participant_id_sigma 0.056 0.053 0.000 0.158 0.005 0.004 179.0 114.0 1.01 v_1|participant_id_offset[1] 0.025 0.861 -1.597 1.587 0.021 0.025 1638.0 1039.0 1.00 v_1|participant_id_offset[2] -0.156 0.868 -1.883 1.394 0.022 0.023 1509.0 1059.0 1.00 v_1|participant_id_offset[3] 0.308 0.820 -1.196 1.872 0.021 0.020 1497.0 1043.0 1.01 v_1|participant_id_offset[4] -0.127 0.839 -1.657 1.462 0.022 0.023 1448.0 906.0 1.00 v_1|participant_id_offset[5] 0.486 0.849 -1.103 2.135 0.021 0.018 1636.0 992.0 1.01 v_1|participant_id_offset[6] -0.688 0.887 -2.317 1.036 0.023 0.020 1466.0 1074.0 1.00 v_1|participant_id_offset[7] -0.053 0.901 -1.885 1.454 0.027 0.024 1119.0 1032.0 1.00 v_1|participant_id_offset[8] 0.279 0.892 -1.556 1.768 0.028 0.024 978.0 1008.0 1.00 v_zGPe|participant_id_offset[1] -0.247 0.900 -1.835 1.469 0.026 0.025 1158.0 1278.0 1.00 v_zGPe|participant_id_offset[2] -0.120 0.909 -1.653 1.926 0.041 0.036 580.0 153.0 1.01 v_zGPe|participant_id_offset[3] 0.540 0.965 -1.155 2.404 0.026 0.022 1423.0 997.0 1.00 v_zGPe|participant_id_offset[4] -0.159 0.894 -1.743 1.616 0.037 0.034 666.0 121.0 1.01 v_zGPe|participant_id_offset[5] 0.283 0.901 -1.447 1.880 0.031 0.024 879.0 1014.0 1.01 v_zGPe|participant_id_offset[6] -0.021 0.898 -1.684 1.652 0.023 0.022 1527.0 1075.0 1.00 v_zGPe|participant_id_offset[7] -0.281 0.909 -2.068 1.361 0.034 0.032 725.0 340.0 1.00 v_zGPe|participant_id_offset[8] -0.165 0.926 -1.858 1.602 0.028 0.025 1143.0 883.0 1.00 v_zSTN|participant_id_offset[1] 0.884 0.833 -0.592 2.539 0.029 0.021 825.0 790.0 1.00 v_zSTN|participant_id_offset[2] 0.089 0.753 -1.337 1.560 0.024 0.021 1007.0 943.0 1.00 v_zSTN|participant_id_offset[3] -0.100 0.726 -1.559 1.197 0.025 0.020 901.0 901.0 1.00 v_zSTN|participant_id_offset[4] 0.014 0.716 -1.471 1.306 0.020 0.025 1284.0 853.0 1.00 v_zSTN|participant_id_offset[5] 0.015 0.693 -1.121 1.451 0.019 0.021 1293.0 904.0 1.00 v_zSTN|participant_id_offset[6] -0.255 0.729 -1.608 1.128 0.024 0.020 947.0 784.0 1.00 v_zSTN|participant_id_offset[7] 0.421 0.801 -0.984 1.921 0.037 0.026 485.0 850.0 1.00 v_zSTN|participant_id_offset[8] -1.104 0.855 -2.702 0.560 0.032 0.023 743.0 740.0 1.00 v_sevScore -0.099 0.086 -0.263 0.052 0.003 0.003 953.0 707.0 1.00 <ul> <li>there is no main effect of severity (makes sense because we didn't change the intercept by severity!)</li> <li>you can see that by looking at the credible interval (hdi's). They are interpret similar to confidence intervals</li> <li>the intercept itself is recovered well (input: 0.5)</li> <li>the effect of severity onto STN is significant. This is good because we did manipulate that (0.6 was around the mean effect)</li> </ul> <ul> <li>we still have the intracranial recordings</li> <li>but now you have different patients and they vary in symptom severity for Parkinson</li> <li>you wonder whether those with higher symptom severity have a lower drift rate because the modulation from STN is less efficient</li> </ul> <p>BUT NOW: your friend is a clinician and tells you that some people might have Dystonia rather than Parkinson and they show different symptoms. So, you want to take this into account now</p> In\u00a0[\u00a0]: Copied! <pre># Function to simulate data for one participant\ndef simulate_participant3(participant_id, sevScore, diagnosis, size=300):\n    # intercept = 0.5\n    zSTN = np.random.normal(loc=1, scale=2, size=size)\n    zGPe = np.random.normal(loc=1, scale=2, size=size)\n\n    if diagnosis == \"PD\":\n        intercept = 0.3\n    else:\n        intercept = 0.6\n\n    ## Strength of neural modulation depends on PD severity score\n    if sevScore &lt;= 0.2:\n        v = intercept + 0.8 * zSTN + 0.3 * zGPe\n    elif sevScore &gt; 0.2 and sevScore &lt;= 0.4:\n        v = intercept + 0.6 * zSTN + 0.2 * zGPe\n    elif sevScore &gt; 0.4 and sevScore &lt; 0.6:\n        v = intercept + 0.5 * zSTN + 0.1 * zGPe\n    else:\n        v = intercept + 0.4 * zSTN + 0.0 * zGPe\n\n    # Assume `hssm.simulate_data` returns a DataFrame\n    true_values = np.column_stack(\n        [v, np.repeat([[1.5, 0.5, 0.5]], axis=0, repeats=size)]\n    )\n    dataset_reg_v = hssm.simulate_data(\n        model=\"ddm\",\n        theta=true_values,\n        size=1,  # Generate one data point for each of the 1000 set of true values\n    )\n\n    # Adding additional variables to the dataset\n    dataset_reg_v[\"zSTN\"] = zSTN\n    dataset_reg_v[\"zGPe\"] = zGPe\n    dataset_reg_v[\"participant_id\"] = str(participant_id)\n    dataset_reg_v[\"sevScore\"] = sevScore\n    dataset_reg_v[\"diagnosis\"] = diagnosis\n\n    return dataset_reg_v\n\n\n# Simulate data for four participants\n### note that we assume that STN &amp; GPe\n\n## PD patients:\n# patients with low severity scores\nsubj_list = [1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8]\nsevscore_list = [\n    0.10,\n    0.20,\n    0.30,\n    0.40,\n    0.50,\n    0.60,\n    0.70,\n    0.80,\n    0.10,\n    0.20,\n    0.30,\n    0.40,\n    0.50,\n    0.60,\n    0.70,\n    0.80,\n]\ndiagnosis_list = [\n    \"PD\",\n    \"PD\",\n    \"PD\",\n    \"PD\",\n    \"PD\",\n    \"PD\",\n    \"PD\",\n    \"PD\",\n    \"DD\",\n    \"DD\",\n    \"DD\",\n    \"DD\",\n    \"DD\",\n    \"DD\",\n    \"DD\",\n    \"DD\",\n]\n\n# [simulate_participant3(subj, sevscore, diagnosis) for subj, sevscore, diagnosis in zip(subj_list, sevscore_list, diagnosis_list)]\n# dataset2_participant1 = simulate_participant3(1,0.10,\"PD\")\n# dataset2_participant2 = simulate_participant3(2,0.20,\"PD\")\n# dataset2_participant3 = simulate_participant3(3,0.30,\"PD\")\n# dataset2_participant4 = simulate_participant3(4,0.40,\"PD\")\n# # patients with high severity scores\n# dataset2_participant5 = simulate_participant3(5,0.50,\"PD\")\n# dataset2_participant6 = simulate_participant3(6,0.60,\"PD\")\n# dataset2_participant7 = simulate_participant3(7,0.70,\"PD\")\n# dataset2_participant8 = simulate_participant3(8,0.80,\"PD\")\n\n# ## Dystonia patients:\n# # patients with low severity scores\n# dataset2_participant9 = simulate_participant3(1,0.10,\"DD\")\n# dataset2_participant10 = simulate_participant3(2,0.20,\"DD\")\n# dataset2_participant11 = simulate_participant3(3,0.30,\"DD\")\n# dataset2_participant12 = simulate_participant3(4,0.40,\"DD\")\n# # patients with high severity scores\n# dataset2_participant13 = simulate_participant3(5,0.50,\"DD\")\n# dataset2_participant14 = simulate_participant3(6,0.60,\"DD\")\n# dataset2_participant15 = simulate_participant3(7,0.70,\"DD\")\n# dataset2_participant16 = simulate_participant3(8,0.80,\"DD\")\n\n# Combine datasets into one DataFrame\ncombined_dataset3 = pd.concat(\n    [\n        simulate_participant3(subj, sevscore, diagnosis)\n        for subj, sevscore, diagnosis in zip(subj_list, sevscore_list, diagnosis_list)\n    ],\n    ignore_index=True,\n)\ncombined_dataset3\n</pre> # Function to simulate data for one participant def simulate_participant3(participant_id, sevScore, diagnosis, size=300):     # intercept = 0.5     zSTN = np.random.normal(loc=1, scale=2, size=size)     zGPe = np.random.normal(loc=1, scale=2, size=size)      if diagnosis == \"PD\":         intercept = 0.3     else:         intercept = 0.6      ## Strength of neural modulation depends on PD severity score     if sevScore &lt;= 0.2:         v = intercept + 0.8 * zSTN + 0.3 * zGPe     elif sevScore &gt; 0.2 and sevScore &lt;= 0.4:         v = intercept + 0.6 * zSTN + 0.2 * zGPe     elif sevScore &gt; 0.4 and sevScore &lt; 0.6:         v = intercept + 0.5 * zSTN + 0.1 * zGPe     else:         v = intercept + 0.4 * zSTN + 0.0 * zGPe      # Assume `hssm.simulate_data` returns a DataFrame     true_values = np.column_stack(         [v, np.repeat([[1.5, 0.5, 0.5]], axis=0, repeats=size)]     )     dataset_reg_v = hssm.simulate_data(         model=\"ddm\",         theta=true_values,         size=1,  # Generate one data point for each of the 1000 set of true values     )      # Adding additional variables to the dataset     dataset_reg_v[\"zSTN\"] = zSTN     dataset_reg_v[\"zGPe\"] = zGPe     dataset_reg_v[\"participant_id\"] = str(participant_id)     dataset_reg_v[\"sevScore\"] = sevScore     dataset_reg_v[\"diagnosis\"] = diagnosis      return dataset_reg_v   # Simulate data for four participants ### note that we assume that STN &amp; GPe  ## PD patients: # patients with low severity scores subj_list = [1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8] sevscore_list = [     0.10,     0.20,     0.30,     0.40,     0.50,     0.60,     0.70,     0.80,     0.10,     0.20,     0.30,     0.40,     0.50,     0.60,     0.70,     0.80, ] diagnosis_list = [     \"PD\",     \"PD\",     \"PD\",     \"PD\",     \"PD\",     \"PD\",     \"PD\",     \"PD\",     \"DD\",     \"DD\",     \"DD\",     \"DD\",     \"DD\",     \"DD\",     \"DD\",     \"DD\", ]  # [simulate_participant3(subj, sevscore, diagnosis) for subj, sevscore, diagnosis in zip(subj_list, sevscore_list, diagnosis_list)] # dataset2_participant1 = simulate_participant3(1,0.10,\"PD\") # dataset2_participant2 = simulate_participant3(2,0.20,\"PD\") # dataset2_participant3 = simulate_participant3(3,0.30,\"PD\") # dataset2_participant4 = simulate_participant3(4,0.40,\"PD\") # # patients with high severity scores # dataset2_participant5 = simulate_participant3(5,0.50,\"PD\") # dataset2_participant6 = simulate_participant3(6,0.60,\"PD\") # dataset2_participant7 = simulate_participant3(7,0.70,\"PD\") # dataset2_participant8 = simulate_participant3(8,0.80,\"PD\")  # ## Dystonia patients: # # patients with low severity scores # dataset2_participant9 = simulate_participant3(1,0.10,\"DD\") # dataset2_participant10 = simulate_participant3(2,0.20,\"DD\") # dataset2_participant11 = simulate_participant3(3,0.30,\"DD\") # dataset2_participant12 = simulate_participant3(4,0.40,\"DD\") # # patients with high severity scores # dataset2_participant13 = simulate_participant3(5,0.50,\"DD\") # dataset2_participant14 = simulate_participant3(6,0.60,\"DD\") # dataset2_participant15 = simulate_participant3(7,0.70,\"DD\") # dataset2_participant16 = simulate_participant3(8,0.80,\"DD\")  # Combine datasets into one DataFrame combined_dataset3 = pd.concat(     [         simulate_participant3(subj, sevscore, diagnosis)         for subj, sevscore, diagnosis in zip(subj_list, sevscore_list, diagnosis_list)     ],     ignore_index=True, ) combined_dataset3 Out[\u00a0]: rt response zSTN zGPe participant_id sevScore diagnosis 0 0.920093 1.0 1.460063 2.599932 1 0.1 PD 1 1.895956 1.0 0.528022 2.650326 1 0.1 PD 2 2.352382 1.0 -0.013171 1.096793 1 0.1 PD 3 3.713829 1.0 0.741769 -1.114357 1 0.1 PD 4 1.071956 1.0 1.857717 1.370395 1 0.1 PD ... ... ... ... ... ... ... ... 4795 1.808623 1.0 1.677419 1.744093 8 0.8 DD 4796 0.964277 1.0 1.541400 -1.548762 8 0.8 DD 4797 0.952152 1.0 1.619092 1.205605 8 0.8 DD 4798 4.067556 -1.0 -0.271316 2.719653 8 0.8 DD 4799 1.668725 1.0 1.134164 2.763838 8 0.8 DD <p>4800 rows \u00d7 7 columns</p> In\u00a0[\u00a0]: Copied! <pre>model_reg_v_ex4_A1 = hssm.HSSM(\n    data=combined_dataset3,\n    include=[\n        {\n            \"name\": \"v\",\n            \"formula\": \"v ~ 1 + (zSTN + zGPe)*sevScore*C(diagnosis) + ((1 + zSTN + zGPe)|participant_id)\",\n            \"prior\": {\n                # All ways to specify priors in the non-regression case work the same way here.\n                \"Intercept\": {\"name\": \"Normal\", \"mu\": 1.5, \"sigma\": 1.5},\n                \"zSTN\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n                \"zGPe\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n                \"sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n                \"zSTN:sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n                \"zGPe:sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n                \"1|participant_id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": 0,\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1},\n                },\n                \"zSTN|participant_id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": 0,\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1},\n                },\n                \"zGPe|participant_id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": 0,\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1},\n                },\n            },\n            \"link\": \"identity\",\n        }\n    ],\n    noncentered=True,\n    p_outlier=0.05,\n)\nmodel_reg_v_ex4_A1\n</pre> model_reg_v_ex4_A1 = hssm.HSSM(     data=combined_dataset3,     include=[         {             \"name\": \"v\",             \"formula\": \"v ~ 1 + (zSTN + zGPe)*sevScore*C(diagnosis) + ((1 + zSTN + zGPe)|participant_id)\",             \"prior\": {                 # All ways to specify priors in the non-regression case work the same way here.                 \"Intercept\": {\"name\": \"Normal\", \"mu\": 1.5, \"sigma\": 1.5},                 \"zSTN\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},                 \"zGPe\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},                 \"sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},                 \"zSTN:sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},                 \"zGPe:sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},                 \"1|participant_id\": {                     \"name\": \"Normal\",                     \"mu\": 0,                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1},                 },                 \"zSTN|participant_id\": {                     \"name\": \"Normal\",                     \"mu\": 0,                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1},                 },                 \"zGPe|participant_id\": {                     \"name\": \"Normal\",                     \"mu\": 0,                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1},                 },             },             \"link\": \"identity\",         }     ],     noncentered=True,     p_outlier=0.05, ) model_reg_v_ex4_A1 <pre>Model initialized successfully.\n</pre> Out[\u00a0]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 4800\n\nParameters:\n\nv:\n    Formula: v ~ 1 + (zSTN + zGPe)*sevScore*C(diagnosis) + ((1 + zSTN + zGPe)|participant_id)\n    Priors:\n        v_Intercept ~ Normal(mu: 1.5, sigma: 1.5)\n        v_zSTN ~ Normal(mu: 0.0, sigma: 1.0)\n        v_zGPe ~ Normal(mu: 0.0, sigma: 1.0)\n        v_sevScore ~ Normal(mu: 0.0, sigma: 1.0)\n        v_zSTN:sevScore ~ Normal(mu: 0.0, sigma: 1.0)\n        v_zGPe:sevScore ~ Normal(mu: 0.0, sigma: 1.0)\n        v_C(diagnosis) ~ Normal(mu: 0.0, sigma: 0.25)\n        v_zSTN:C(diagnosis) ~ Normal(mu: 0.0, sigma: 0.25)\n        v_zGPe:C(diagnosis) ~ Normal(mu: 0.0, sigma: 0.25)\n        v_sevScore:C(diagnosis) ~ Normal(mu: 0.0, sigma: 0.25)\n        v_zSTN:sevScore:C(diagnosis) ~ Normal(mu: 0.0, sigma: 0.25)\n        v_zGPe:sevScore:C(diagnosis) ~ Normal(mu: 0.0, sigma: 0.25)\n        v_1|participant_id ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 1.0))\n        v_zSTN|participant_id ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 1.0))\n        v_zGPe|participant_id ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 1.0))\n    Link: identity\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[\u00a0]: Copied! <pre>samples_model_reg_v_ex4_A1 = model_reg_v_ex4_A1.sample(\n    sampler=\"nuts_numpyro\",\n    cores=3,\n    chains=3,\n    draws=500,\n    tune=500,\n    idata_kwargs=dict(log_likelihood=True),\n)\n</pre> samples_model_reg_v_ex4_A1 = model_reg_v_ex4_A1.sample(     sampler=\"nuts_numpyro\",     cores=3,     chains=3,     draws=500,     tune=500,     idata_kwargs=dict(log_likelihood=True), ) <pre>Using default initvals. \n\n</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>We recommend running at least 4 chains for robust computation of convergence diagnostics\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1500/1500 [00:03&lt;00:00, 479.95it/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre>#### posterior statistics of non-centered model\nmodel_reg_v_ex4_A1.summary(\n    var_names=[\"~_offset\", \"~_id\"], filter_vars=\"like\"\n)  # var_names= [\"~_offset\"])\n</pre> #### posterior statistics of non-centered model model_reg_v_ex4_A1.summary(     var_names=[\"~_offset\", \"~_id\"], filter_vars=\"like\" )  # var_names= [\"~_offset\"]) Out[\u00a0]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat v_Intercept 0.576 0.058 0.467 0.677 0.002 0.001 1096.0 1319.0 1.00 v_zSTN:C(diagnosis)[PD] 0.014 0.038 -0.055 0.087 0.001 0.001 1044.0 993.0 1.00 v_zGPe:sevScore -0.612 0.083 -0.755 -0.446 0.003 0.002 620.0 669.0 1.00 v_zGPe 0.418 0.043 0.334 0.491 0.002 0.001 653.0 530.0 1.01 v_zSTN 0.868 0.064 0.741 0.981 0.003 0.002 616.0 617.0 1.00 v_zSTN:sevScore:C(diagnosis)[PD] -0.002 0.069 -0.136 0.125 0.002 0.002 1088.0 946.0 1.00 v_zGPe:C(diagnosis)[PD] -0.053 0.031 -0.112 0.005 0.001 0.001 1113.0 1052.0 1.00 v_C(diagnosis)[PD] -0.277 0.063 -0.392 -0.156 0.002 0.002 879.0 1035.0 1.00 t 0.509 0.006 0.497 0.519 0.000 0.000 1724.0 1186.0 1.00 z 0.502 0.006 0.490 0.514 0.000 0.000 2063.0 1284.0 1.00 v_zGPe:sevScore:C(diagnosis)[PD] 0.091 0.059 -0.015 0.202 0.002 0.001 1104.0 1087.0 1.00 a 1.501 0.016 1.471 1.529 0.000 0.000 2140.0 1124.0 1.00 v_zSTN:sevScore -0.648 0.125 -0.879 -0.402 0.005 0.004 535.0 608.0 1.00 v_sevScore 0.098 0.108 -0.120 0.281 0.004 0.003 897.0 878.0 1.00 v_sevScore:C(diagnosis)[PD] -0.051 0.118 -0.289 0.154 0.004 0.003 870.0 962.0 1.00 In\u00a0[\u00a0]: Copied! <pre>model_reg_v_ex4_A2 = hssm.HSSM(\n    data=combined_dataset3,\n    include=[\n        {\n            \"name\": \"v\",\n            \"formula\": \"v ~ 1 + (zSTN + zGPe)*sevScore + ((1 + zSTN + zGPe)|participant_id/C(diagnosis))\",\n            \"prior\": {\n                # All ways to specify priors in the non-regression case work the same way here.\n                \"Intercept\": {\"name\": \"Normal\", \"mu\": 1.5, \"sigma\": 1.0},\n                \"zSTN\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n                \"zGPe\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n                \"sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n                \"zSTN:sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n                \"zGPe:sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},\n                \"1|participant_id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": 0,\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1},\n                },\n                \"zSTN|participant_id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": 0,\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1},\n                },\n                \"zGPe|participant_id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": 0,\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1},\n                },\n            },\n            \"link\": \"identity\",\n        }\n    ],\n    noncentered=True,\n    p_outlier=0.05,\n)\n</pre> model_reg_v_ex4_A2 = hssm.HSSM(     data=combined_dataset3,     include=[         {             \"name\": \"v\",             \"formula\": \"v ~ 1 + (zSTN + zGPe)*sevScore + ((1 + zSTN + zGPe)|participant_id/C(diagnosis))\",             \"prior\": {                 # All ways to specify priors in the non-regression case work the same way here.                 \"Intercept\": {\"name\": \"Normal\", \"mu\": 1.5, \"sigma\": 1.0},                 \"zSTN\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},                 \"zGPe\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},                 \"sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},                 \"zSTN:sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},                 \"zGPe:sevScore\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1.0},                 \"1|participant_id\": {                     \"name\": \"Normal\",                     \"mu\": 0,                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1},                 },                 \"zSTN|participant_id\": {                     \"name\": \"Normal\",                     \"mu\": 0,                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1},                 },                 \"zGPe|participant_id\": {                     \"name\": \"Normal\",                     \"mu\": 0,                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 1},                 },             },             \"link\": \"identity\",         }     ],     noncentered=True,     p_outlier=0.05, ) <pre>No common intercept. Bounds for parameter v is not applied due to a current limitation of Bambi. This will change in the future.\n</pre> <pre>Model initialized successfully.\n</pre> In\u00a0[\u00a0]: Copied! <pre>samples_model_reg_v_ex4_A2 = model_reg_v_ex4_A2.sample(\n    sampler=\"nuts_numpyro\",\n    cores=3,\n    chains=3,\n    draws=500,\n    tune=500,\n    idata_kwargs=dict(log_likelihood=True),\n)\n</pre> samples_model_reg_v_ex4_A2 = model_reg_v_ex4_A2.sample(     sampler=\"nuts_numpyro\",     cores=3,     chains=3,     draws=500,     tune=500,     idata_kwargs=dict(log_likelihood=True), ) <pre>Using default initvals. \n\n</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>There were 8 divergences after tuning. Increase `target_accept` or reparameterize.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1500/1500 [00:03&lt;00:00, 425.24it/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre>#### posterior statistics of non-centered model\nmodel_reg_v_ex4_A2.summary(var_names=[\"~_offset\", \"~_id\"], filter_vars=\"like\")\n</pre> #### posterior statistics of non-centered model model_reg_v_ex4_A2.summary(var_names=[\"~_offset\", \"~_id\"], filter_vars=\"like\") Out[\u00a0]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat v_Intercept 0.442 0.116 0.218 0.656 0.005 0.003 611.0 784.0 1.00 v_zGPe:sevScore -0.549 0.092 -0.714 -0.362 0.005 0.004 497.0 137.0 1.01 v_zGPe 0.387 0.043 0.307 0.468 0.002 0.001 711.0 673.0 1.00 v_zSTN 0.876 0.062 0.762 0.986 0.003 0.002 491.0 705.0 1.01 t 0.509 0.006 0.497 0.520 0.000 0.000 1888.0 1225.0 1.00 z 0.501 0.006 0.491 0.515 0.000 0.000 2045.0 1137.0 1.00 a 1.500 0.016 1.473 1.531 0.000 0.000 1966.0 1170.0 1.00 v_zSTN:sevScore -0.651 0.120 -0.899 -0.450 0.005 0.004 541.0 613.0 1.01 v_sevScore 0.067 0.225 -0.359 0.476 0.009 0.007 573.0 785.0 1.01 In\u00a0[\u00a0]: Copied! <pre>az.compare(\n    {\"Model 1\": samples_model_reg_v_ex4_A1, \"Model 2\": samples_model_reg_v_ex4_A2}\n)\n</pre> az.compare(     {\"Model 1\": samples_model_reg_v_ex4_A1, \"Model 2\": samples_model_reg_v_ex4_A2} ) Out[\u00a0]: rank elpd_loo p_loo elpd_diff weight se dse warning scale Model 1 0 -5474.694548 26.597726 0.00000 1.0 89.767316 0.000000 False log Model 2 1 -5480.910478 36.499641 6.21593 0.0 89.848885 2.855761 False log"},{"location":"tutorials/hssm_tutorial_workshop_1/#hssm-tutorial-winterbrain-2025","title":"HSSM Tutorial Winterbrain 2025\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#nadja-r-ging-jehli","title":"Nadja R. Ging-Jehli\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#brown-university","title":"Brown University\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#lncc-lab-michael-j-frank","title":"LNCC Lab (Michael J. Frank)\u00b6","text":"<p>This is a slightly simplified version of the original workshop tutorial which you can find here.</p>"},{"location":"tutorials/hssm_tutorial_workshop_1/#load-modules","title":"Load Modules\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#1-overview-of-sequential-sampling-models","title":"1. Overview of Sequential Sampling Models\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#some-useful-papers-relevant-for-neuroscience-a-small-selection","title":"Some Useful Papers relevant for Neuroscience (a small selection)\u00b6","text":"<ul> <li><p>Ratcliff, R., &amp; Frank, M. J. (2012). Reinforcement-based decision making in corticostriatal circuits: mutual constraints by neurocomputational and diffusion models. Neural computation, 24(5), 1186-1229.</p> </li> <li><p>Turner, B. M., Palestro, J. J., Mileti\u0107, S., &amp; Forstmann, B. U. (2019). Advances in techniques for imposing reciprocity in brain-behavior relations. Neuroscience &amp; Biobehavioral Reviews, 102, 327-336.</p> </li> <li><p>Forstmann, B. U., Ratcliff, R., &amp; Wagenmakers, E. J. (2016). Sequential sampling models in cognitive neuroscience: Advantages, applications, and extensions. Annual review of psychology, 67(1), 641-666.</p> </li> </ul>"},{"location":"tutorials/hssm_tutorial_workshop_1/#blogs-self-plug","title":"Blogs (self-plug :))\u00b6","text":"<p>https://www.gingjehli.com/research-blog</p>"},{"location":"tutorials/hssm_tutorial_workshop_1/#notes","title":"Notes\u00b6","text":"<ul> <li>there is a difference between \"ddm\" and \"full_ddm\"</li> <li>the bounds within \"likelihoods\" provide reasonable param values (but take this with a grain of salt)</li> <li>the order of \"list_params\" is important</li> </ul>"},{"location":"tutorials/hssm_tutorial_workshop_1/#2-regression-based-modeling","title":"2. Regression-based modeling\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#case-study-1-1-hierarchical-layer-trial-level-with-1-within-subject-coefficient","title":"Case Study 1: 1 hierarchical layer (trial-level) with 1 within-subject coefficient)\u00b6","text":"<ul> <li>you have intracranial recording in STN and want to understand the decision-relevant dynamics in the activity</li> <li>we assume that activity in STN varies on a trial-by-trial basis in a systematic way</li> <li>your Hypothesis: STN is implicated in response cautiousness</li> <li>you want to test this hypothesis using a classical DDM</li> </ul>"},{"location":"tutorials/hssm_tutorial_workshop_1/#question-to-audience-which-model-parameter-does-stn-modulate","title":"Question to audience: which model parameter does STN modulate?\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#step-1-regression-based-data-simulation-one-subjects","title":"Step 1: Regression-based data simulation (one subjects)\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#looking-at-the-data-what-do-you-notice","title":"Looking at the data, what do you notice?\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#required-data-structure-for-hssm","title":"Required Data Structure for HSSM\u00b6","text":"<ul> <li>we need variables labeled as \"rt\" and \"response\"</li> <li>make sure rt are in seconds</li> <li>responses are (1,-1) because you are in a classical DDM setting</li> <li>make sure regressors are z-scored (particularly important when you have more than two regressors)</li> </ul>"},{"location":"tutorials/hssm_tutorial_workshop_1/#step-2-model-setup-priors","title":"Step 2: Model Setup &amp; Priors\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#notes-to-step-2","title":"Notes to Step 2\u00b6","text":"<ul> <li>make sure your priors are reasonable (always double-check)</li> <li>see my blog for useful notes for defining priors on each of these params https://www.gingjehli.com/single-post/choosing-effective-samplers-and-setting-priors</li> <li>priors are particularly important in these more complex regression-based models</li> <li>it can also be helpful to set \"init_val\" for the sampler so it starts in a reasonable region.</li> </ul>"},{"location":"tutorials/hssm_tutorial_workshop_1/#step-3-double-check-model-specification","title":"Step 3: Double-check model specification\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#step-4-sampling-from-the-posterior-distribution","title":"Step 4: Sampling from the Posterior Distribution\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#notes-to-step-4","title":"Notes to Step 4\u00b6","text":"<ul> <li>in a real analysis, you would want to run more chains (usually &gt;=4)</li> <li>if models don't converge, as a first step, it is typically a good idea to increase the chain length</li> <li>depending on task structure, some model params are more/less likely to trade-off (e.g., simple task design, <code>t</code> &amp; <code>a</code> are often anti-correlated)</li> <li>if there are parameter trade-offs or depending on your regression equation, it can be helpful to play around with the sampler</li> <li>though: nuts_numpyro sampler does not work with blackbox likelihoods.</li> <li>sometime variational inference (VI) can be a helpful alternative to MCMC (check out the dedicated tutorials in the HSSM documentation)</li> </ul>"},{"location":"tutorials/hssm_tutorial_workshop_1/#step-5-model-validation","title":"Step 5: Model Validation\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#what-do-you-think-about-these-results-below-also-given-jensens-part","title":"What do you think about these results below (also given Jensen's part)?\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#case-study-2-participant-hierarchy-on-two-within-subject-coefficients","title":"Case Study 2: Participant hierarchy on two within-subject coefficients\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#step-1-regression-based-data-simulation-multiple-subjects","title":"Step 1: Regression-based data simulation (multiple subjects)\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#step-2-model-setup-priors","title":"Step 2: Model Setup &amp; Priors\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#alternative-1-a-centered-model","title":"Alternative 1: A centered model\u00b6","text":"<ul> <li>specification where the subject-specific coefficients are coming from one hierarchical prior (mother distribution with mu and sigma)</li> <li>we can also call it as \"0+\" models</li> <li>this is the specification used in HDDM</li> <li>in this case, it's important to set \"noncentered=False\" (by default it's set to \"True\")</li> <li>The group distributions here will be recovered as v_1|subject_mu and v_x|subject_mu</li> </ul>"},{"location":"tutorials/hssm_tutorial_workshop_1/#alternative-2-a-non-centered-model","title":"Alternative 2: A non-centered model\u00b6","text":"<ul> <li>Non-centered means that each subject is estimated as offset from the group mean</li> <li>in this case the group means will be zSTN and zGPe.</li> <li>The priors are different here because now we want the mu for the random subject effects to be 0</li> <li>the mu should not be estimated from a hyperprior simultaneously with the fixed effects, as those will tradeoff and could produce convergence issues  - we still estimate the sigmas with a hyperprior to allow us to get indiv params</li> <li>now we keep noncentered=true.</li> </ul>"},{"location":"tutorials/hssm_tutorial_workshop_1/#important-note","title":"Important note\u00b6","text":"<p>Note that in this non-centered model, you will need to specify each individual coefficient's prior (at least for your regression equation). See example below</p>"},{"location":"tutorials/hssm_tutorial_workshop_1/#step-4-sampling-from-the-posterior-distribution-bayesian-model-fitting-based-on-mcmc-procedure","title":"Step 4: Sampling from the Posterior Distribution (Bayesian Model Fitting based on MCMC Procedure)\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#final-notes","title":"Final notes:\u00b6","text":"<ul> <li>whether centered is better depends on the structure of your dataset</li> <li>the two models are mathematically equivalent, however the posterior geometry is affected by the parameterization</li> <li>in practice, you can try both and see which one works better</li> <li>here is a great (albeit non-trivial to digest) long blog post which touches on the topic in some detail`</li> </ul>"},{"location":"tutorials/hssm_tutorial_workshop_1/#step-5-model-validation","title":"Step 5: Model Validation\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#additional-case-studies-between-subject-group-variables","title":"Additional Case studies (between-subject &amp; group variables)\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#case-study-3-participant-level-hierarchy-two-within-subject-coefficients-and-one-between-subject-coefficient","title":"Case Study 3: Participant-level hierarchy,  two within-subject coefficients and one between-subject coefficient\u00b6","text":"<ul> <li>we still have the intracranial recordings</li> <li>but now you have different patients and they vary in symptom severity for Parkinson</li> <li>you wonder whether those with higher symptom severity have a lower drift rate because the modulation from STN is less efficient</li> </ul>"},{"location":"tutorials/hssm_tutorial_workshop_1/#step-1-regression-based-data-simulation-multiple-subjects","title":"Step 1: Regression-based data simulation (multiple subjects)\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#step-2-model-setup-priors","title":"Step 2: Model Setup &amp; Priors\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#case-study-4-two-hierarchical-layers-trial-and-sbj-level-two-within-sbj-coefficients-one-btw-sbj-coefficient-group","title":"Case Study 4: Two hierarchical layers (trial- and sbj-level) &amp; Two within-sbj coefficients &amp; one btw-sbj coefficient &amp; group\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#audience-question-how-would-you-do-that-how-would-you-change-the-data-simulation","title":"Audience Question: how would you do that? How would you change the data simulation?\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#step-1-regression-based-data-simulation-multiple-subjects","title":"Step 1: Regression-based data simulation (multiple subjects)\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#other-example-2","title":"Other Example 2\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_1/#comparison","title":"Comparison\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/","title":"Getting Started with Drift Diffusion Models: A Python Tutorial","text":"<p>By: Jensen Palmer, Neuroscience PhD Candidate, American University</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Image\n</pre> from IPython.display import Image <p></p> <p>Link to publication: https://www.jneurosci.org/content/44/46/e0550242024.long</p> <p></p> <ul> <li>Figure 1, Palmer et al. (2024)</li> <li>Rats initiated a trial by nose-poking below a central LED grid when a square stimulus was presented.</li> <li>Upon initation, rats were presented with lateralized stimuli. Trials could be single-offer or dual-offer.</li> <li>In this tutorial, we will just be focusing on dual-offer trials: simultaneous presentation of a high- and low-luminance stimulus. Stimuli are referred to as high- and low-value.</li> <li>Responding to the high-value stimulus yielded 16% sucrose, and the low-value stimulus yielded 4% sucrose.</li> <li>We will look at behavior of male and female rats over the first five sessions where they experience dual-offer trials. We refer to this as \"choice learning.\"</li> <li>Behavioral measures of interest: response latencies (how long it takes an animal to respond after initiation) and choice preference (the ratio of responses to the high-value stimuli; could also be considered accuracy).</li> </ul> <p></p> <ul> <li>Figure 4, Palmer et al. (2024)</li> <li>Males had a reduction in choice latencies over learning sessions. Females had stable latencies over learning.</li> </ul> <p></p> <ul> <li>Figure 4, Palmer et al. (2024)</li> <li>Across sessions, females chose the high-value stimulus more than males.</li> </ul> <p></p> <ul> <li>Figure 1, Palmer et al. (2024)</li> </ul> <p>Males have a reduction in latencies over sessions. Females choose the high-value stimulus (the \"better\" option) more frequently.</p> <p>We can use Drift Diffusion Modeling to understand how these behavioral measures can explain the differences in the decision making process between males and females during learning.</p> <p>Below is an example of the effects that we found using HDDM, the predecessor of HSSM. We will step through how to replicate these findings with HSSM and how to interpret the results with regards to the behavioral effects.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport arviz as az  \nimport hssm\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import arviz as az   import hssm <pre>WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n</pre> In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv('hssm_tutorial_workshop_2/Choice_Learning.csv',index_col=0)\ndf.head()\n</pre> df = pd.read_csv('hssm_tutorial_workshop_2/Choice_Learning.csv',index_col=0) df.head() Out[\u00a0]: subj_idx date time trialtype cue response side error rt reward iti sex sess 0 BG01 02-20 61.288 D 2 0 R 0 0.358 3.082 -3469.542 F 1 1 BG01 02-20 104.832 D 2 0 L 0 0.460 4.230 10.256 F 1 2 BG01 02-20 122.062 D 8 1 L 0 0.278 1.904 17.230 F 1 3 BG01 02-20 134.828 D 8 1 L 0 0.390 1.274 12.766 F 1 4 BG01 02-20 193.924 D 8 1 R 0 0.364 3.962 13.372 F 1 In\u00a0[\u00a0]: Copied! <pre>df = df[df.trialtype == 'D'] # only including dual-offer trials ('D') and not single-offer trials ('S')\ndf = df[df.iti &gt; 0] # the first trial in each session has a negative iti, so we drop these values.\ndf = df.rename(columns={\"subj_idx\": \"participant_id\"}) # renaming column with rat ID to participant_id\ndf['response'] = np.where(df['response'] == 0, -1, 1) # changing error responses from 0 to -1 \n</pre> df = df[df.trialtype == 'D'] # only including dual-offer trials ('D') and not single-offer trials ('S') df = df[df.iti &gt; 0] # the first trial in each session has a negative iti, so we drop these values. df = df.rename(columns={\"subj_idx\": \"participant_id\"}) # renaming column with rat ID to participant_id df['response'] = np.where(df['response'] == 0, -1, 1) # changing error responses from 0 to -1  In\u00a0[\u00a0]: Copied! <pre># setting our variables of interest, sex and session to categorical, as ordered for session\ndf['sess'] = pd.Categorical(df['sess'], categories=[1,2,3,4,5], ordered=True)\ndf['sex'] = pd.Categorical(df['sex'], categories=['F','M'])\n</pre> # setting our variables of interest, sex and session to categorical, as ordered for session df['sess'] = pd.Categorical(df['sess'], categories=[1,2,3,4,5], ordered=True) df['sex'] = pd.Categorical(df['sex'], categories=['F','M']) <p>Reminder: We observed in the behavior that males have a reduction in latencies over sessions, and that females have a higher choice ratio.</p> <p>Based on these effects, we are interested in how sex and session effect the DDM parameters.</p> <p>Below we will define our model. The model parameters used here &amp; what they specify are:</p> <ol> <li>model: the type of model you want to fit (here, we are fitting a ddm)</li> <li>data: the data to be used (here, it is df, the dataframe that we read in above)</li> <li>prior_settings: a useful specification from HSSM developers to use when we do not have any information about our priors! (here, \"safe\")</li> <li>include: specifications for the parameters of interest &amp; which variables we want to look at effects of</li> </ol> <p>The specifications we state to include:</p> <ol> <li>name: the name of the ddm parameter of interest (here, \"v\" is drift rate - more on this below)</li> <li>formula: regression-style equation. the 0 after ~ removes a \"global intercept\", resulting in fits for each level of a categorical variable. 1|participant_id allows for random effects of rat.</li> <li>link: the link function to use; here we use \"identity\". this is the default of HSSM. In short: it means the predictor directly predicts the response.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># defining our model.\nv_sex = hssm.HSSM(\n    model=\"ddm\", \n    data=df, \n    prior_settings=\"safe\", \n    include=[\n        {   \"name\": \"v\", \n            \"formula\": \"v ~ 0 + sex + (1|participant_id)\",\n            \"link\": \"identity\" \n        }\n    ],\n)\n</pre> # defining our model. v_sex = hssm.HSSM(     model=\"ddm\",      data=df,      prior_settings=\"safe\",      include=[         {   \"name\": \"v\",              \"formula\": \"v ~ 0 + sex + (1|participant_id)\",             \"link\": \"identity\"          }     ], ) <pre>No common intercept. Bounds for parameter v is not applied due to a current limitation of Bambi. This will change in the future.\n</pre> <p>In the above model, we are looking at how sex impacts drift rate. Next we will sample from the model we just defined.</p> <ul> <li>Sampling: estimating the parameter of interest by repeatedly drawing values from the prior distributions. This occurs for many (usually 1000s) of iterations where the selected value is updated &amp; evaluated each time. This forms a chain of values that eventually converges on a solution, which is a posterior distribution of the parameter.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>v_sex.sample(target_accept=0.9)  # increasing target_accept is sometimes necessary to reduce divergences. the default is 0.8.\n</pre> v_sex.sample(target_accept=0.9)  # increasing target_accept is sometimes necessary to reduce divergences. the default is 0.8. <pre>Auto-assigning NUTS sampler...\nInitializing NUTS using adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [t, z, a, v_sex, v_1|participant_id_mu, v_1|participant_id_sigma, v_1|participant_id_offset]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1890 seconds.\n</pre> Out[\u00a0]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:                         (chain: 4, draw: 1000, v_sex_dim: 2,\n                                     v_1|participant_id__factor_dim: 18,\n                                     rt,response_obs: 9504)\nCoordinates:\n  * chain                           (chain) int64 0 1 2 3\n  * draw                            (draw) int64 0 1 2 3 4 ... 996 997 998 999\n  * v_sex_dim                       (v_sex_dim) &lt;U1 'F' 'M'\n  * v_1|participant_id__factor_dim  (v_1|participant_id__factor_dim) &lt;U4 'BG0...\n  * rt,response_obs                 (rt,response_obs) int64 0 1 2 ... 9502 9503\nData variables:\n    v_sex                           (chain, draw, v_sex_dim) float64 0.9215 ....\n    v_1|participant_id_mu           (chain, draw) float64 -0.03572 ... 0.2357\n    t                               (chain, draw) float64 0.2139 ... 0.2131\n    z                               (chain, draw) float64 0.5118 ... 0.512\n    a                               (chain, draw) float64 0.7843 ... 0.7801\n    v_1|participant_id_sigma        (chain, draw) float64 0.2612 ... 0.2704\n    v_1|participant_id              (chain, draw, v_1|participant_id__factor_dim) float64 ...\n    v                               (chain, draw, rt,response_obs) float64 1....\nAttributes:\n    created_at:                  2025-01-22T18:03:37.756598+00:00\n    arviz_version:               0.20.0\n    inference_library:           pymc\n    inference_library_version:   5.17.0\n    sampling_time:               1890.0189800262451\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 1000</li><li>v_sex_dim: 2</li><li>v_1|participant_id__factor_dim: 18</li><li>rt,response_obs: 9504</li></ul></li><li>Coordinates: (5)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li><li>v_sex_dim(v_sex_dim)&lt;U1'F' 'M'<pre>array(['F', 'M'], dtype='&lt;U1')</pre></li><li>v_1|participant_id__factor_dim(v_1|participant_id__factor_dim)&lt;U4'BG01' 'BG02' ... 'MV04' 'MV06'<pre>array(['BG01', 'BG02', 'BG03', 'BG04', 'BG05', 'BG06', 'BG08', 'BG11', 'BG12',\n       'JK03', 'JK07', 'JK08', 'JK09', 'JK12', 'MV01', 'MV03', 'MV04', 'MV06'],\n      dtype='&lt;U4')</pre></li><li>rt,response_obs(rt,response_obs)int640 1 2 3 4 ... 9500 9501 9502 9503<pre>array([   0,    1,    2, ..., 9501, 9502, 9503])</pre></li></ul></li><li>Data variables: (8)<ul><li>v_sex(chain, draw, v_sex_dim)float640.9215 0.7603 ... 0.9521 0.7439<pre>array([[[0.92145091, 0.76029638],\n        [0.85701755, 0.74690316],\n        [0.9816648 , 0.70679045],\n        ...,\n        [0.99860298, 0.84278774],\n        [1.01263308, 0.83705256],\n        [1.0325418 , 0.81354265]],\n\n       [[1.1121015 , 0.58378294],\n        [1.15008017, 0.55849074],\n        [1.15630065, 0.73402363],\n        ...,\n        [1.04231044, 0.54392365],\n        [0.96270667, 0.55671036],\n        [1.00532   , 0.49837451]],\n\n       [[1.16959304, 0.78737512],\n        [1.1570274 , 0.70510663],\n        [1.11639328, 0.63429516],\n        ...,\n        [1.05968212, 0.77919338],\n        [0.87033771, 0.45911206],\n        [0.8774255 , 0.50612135]],\n\n       [[1.1563872 , 0.70468496],\n        [1.20687522, 0.69138727],\n        [1.16251465, 0.66806239],\n        ...,\n        [1.03305492, 0.58733273],\n        [0.99381997, 0.57296379],\n        [0.9521184 , 0.74385212]]])</pre></li><li>v_1|participant_id_mu(chain, draw)float64-0.03572 0.1413 ... -0.2165 0.2357<pre>array([[-0.03571835,  0.14130218, -0.30824706, ...,  0.22010324,\n         0.30764299,  0.26152555],\n       [ 0.25720192, -0.21183339, -0.17911537, ...,  0.54753973,\n         0.45968928,  0.4240682 ],\n       [-0.27153346,  0.08611597, -0.22851215, ..., -0.25927243,\n         0.22461605, -0.21732445],\n       [-0.07472027,  0.25253804,  0.39162237, ...,  0.01067704,\n        -0.2165277 ,  0.23567618]])</pre></li><li>t(chain, draw)float640.2139 0.2145 ... 0.2131 0.2131<pre>array([[0.21393596, 0.21451   , 0.21488298, ..., 0.21175063, 0.2117641 ,\n        0.21214855],\n       [0.21276028, 0.20906188, 0.21089983, ..., 0.2116837 , 0.21314955,\n        0.21246568],\n       [0.21296943, 0.2115452 , 0.2163623 , ..., 0.21375593, 0.21040273,\n        0.21540924],\n       [0.2129909 , 0.21165768, 0.21191507, ..., 0.21516917, 0.21305115,\n        0.21311392]])</pre></li><li>z(chain, draw)float640.5118 0.5133 ... 0.5164 0.512<pre>array([[0.51176588, 0.51326453, 0.51377593, ..., 0.50544877, 0.50498794,\n        0.50472206],\n       [0.51492113, 0.51090578, 0.51043688, ..., 0.50908721, 0.50866716,\n        0.50904402],\n       [0.50857968, 0.51176577, 0.51815738, ..., 0.51011872, 0.5144987 ,\n        0.52211289],\n       [0.51566581, 0.51256922, 0.51611879, ..., 0.51450908, 0.51640845,\n        0.51202592]])</pre></li><li>a(chain, draw)float640.7843 0.7832 ... 0.7786 0.7801<pre>array([[0.78429516, 0.78317001, 0.78042534, ..., 0.77759836, 0.77716541,\n        0.77563442],\n       [0.78748938, 0.77862262, 0.78950503, ..., 0.77632567, 0.77772702,\n        0.77642716],\n       [0.78529814, 0.77674812, 0.77916961, ..., 0.77824054, 0.77682646,\n        0.78544794],\n       [0.78259374, 0.7816303 , 0.78552914, ..., 0.7773492 , 0.77861338,\n        0.78009534]])</pre></li><li>v_1|participant_id_sigma(chain, draw)float640.2612 0.3317 ... 0.2031 0.2704<pre>array([[0.26118393, 0.33169757, 0.29981773, ..., 0.46160168, 0.48613545,\n        0.45338341],\n       [0.22922788, 0.21136085, 0.22359346, ..., 0.45805229, 0.4123804 ,\n        0.42769886],\n       [0.25434889, 0.26201441, 0.27050957, ..., 0.2653256 , 0.37023273,\n        0.32325072],\n       [0.22676024, 0.22348119, 0.23277733, ..., 0.22133432, 0.2031169 ,\n        0.27038024]])</pre></li><li>v_1|participant_id(chain, draw, v_1|participant_id__factor_dim)float640.1878 0.09123 ... 0.09267 -0.3392<pre>array([[[ 0.18781726,  0.09122553,  0.47048276, ...,  0.06002095,\n          0.07228596, -0.24678886],\n        [ 0.3029986 ,  0.03556639,  0.48086896, ...,  0.04047622,\n          0.12492799, -0.33554918],\n        [ 0.08667293, -0.04322459,  0.28715491, ...,  0.10516111,\n          0.17643565, -0.16044692],\n        ...,\n        [ 0.08442762,  0.0788605 ,  0.37743407, ..., -0.05963362,\n         -0.03497917, -0.3442222 ],\n        [ 0.23250932, -0.10314878,  0.27936039, ..., -0.09476163,\n         -0.01158881, -0.40289471],\n        [ 0.26453952, -0.05141765,  0.22881492, ..., -0.08535446,\n         -0.09050585, -0.39398491]],\n\n       [[-0.07209501, -0.08719374,  0.23037494, ...,  0.23408313,\n          0.17605105, -0.22475213],\n        [-0.0725625 , -0.27269631,  0.15430493, ...,  0.12627017,\n          0.18475414, -0.0855301 ],\n        [-0.00768197, -0.03419274,  0.19840661, ...,  0.0443186 ,\n          0.10955685, -0.26756094],\n...\n        [ 0.0282366 ,  0.01520251,  0.36850422, ..., -0.07531482,\n          0.04699755, -0.34336668],\n        [ 0.19971676,  0.13529414,  0.42763641, ...,  0.26488953,\n          0.29870468, -0.14304862],\n        [ 0.24050398,  0.16255024,  0.36647543, ...,  0.26940854,\n          0.19786633, -0.13416946]],\n\n       [[-0.09191805, -0.19741343,  0.08084321, ..., -0.02568209,\n          0.0317146 , -0.18009674],\n        [-0.05281341, -0.1896988 ,  0.14545304, ...,  0.12107363,\n          0.20062088, -0.2573199 ],\n        [-0.07316173, -0.24333633,  0.14440446, ...,  0.11011439,\n          0.1828945 , -0.20133639],\n        ...,\n        [ 0.05272608, -0.11579721,  0.30080617, ...,  0.13158335,\n          0.26548714, -0.14008225],\n        [ 0.11498123, -0.01608346,  0.31281567, ...,  0.14445631,\n          0.24461286, -0.22084921],\n        [ 0.22732246,  0.01541193,  0.30840214, ...,  0.02303487,\n          0.09267455, -0.33923938]]])</pre></li><li>v(chain, draw, rt,response_obs)float641.109 1.109 1.109 ... 0.4046 0.4046<pre>array([[[1.10926816, 1.10926816, 1.10926816, ..., 0.51350751,\n         0.51350751, 0.51350751],\n        [1.16001615, 1.16001615, 1.16001615, ..., 0.41135398,\n         0.41135398, 0.41135398],\n        [1.06833773, 1.06833773, 1.06833773, ..., 0.54634353,\n         0.54634353, 0.54634353],\n        ...,\n        [1.0830306 , 1.0830306 , 1.0830306 , ..., 0.49856554,\n         0.49856554, 0.49856554],\n        [1.2451424 , 1.2451424 , 1.2451424 , ..., 0.43415785,\n         0.43415785, 0.43415785],\n        [1.29708132, 1.29708132, 1.29708132, ..., 0.41955774,\n         0.41955774, 0.41955774]],\n\n       [[1.04000649, 1.04000649, 1.04000649, ..., 0.35903081,\n         0.35903081, 0.35903081],\n        [1.07751767, 1.07751767, 1.07751767, ..., 0.47296064,\n         0.47296064, 0.47296064],\n        [1.14861868, 1.14861868, 1.14861868, ..., 0.46646269,\n         0.46646269, 0.46646269],\n...\n        [1.08791872, 1.08791872, 1.08791872, ..., 0.4358267 ,\n         0.4358267 , 0.4358267 ],\n        [1.07005447, 1.07005447, 1.07005447, ..., 0.31606344,\n         0.31606344, 0.31606344],\n        [1.11792948, 1.11792948, 1.11792948, ..., 0.3719519 ,\n         0.3719519 , 0.3719519 ]],\n\n       [[1.06446915, 1.06446915, 1.06446915, ..., 0.52458821,\n         0.52458821, 0.52458821],\n        [1.1540618 , 1.1540618 , 1.1540618 , ..., 0.43406737,\n         0.43406737, 0.43406737],\n        [1.08935292, 1.08935292, 1.08935292, ..., 0.466726  ,\n         0.466726  , 0.466726  ],\n        ...,\n        [1.085781  , 1.085781  , 1.085781  , ..., 0.44725048,\n         0.44725048, 0.44725048],\n        [1.1088012 , 1.1088012 , 1.1088012 , ..., 0.35211458,\n         0.35211458, 0.35211458],\n        [1.17944086, 1.17944086, 1.17944086, ..., 0.40461274,\n         0.40461274, 0.40461274]]])</pre></li></ul></li><li>Indexes: (5)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li><li>v_sex_dimPandasIndex<pre>PandasIndex(Index(['F', 'M'], dtype='object', name='v_sex_dim'))</pre></li><li>v_1|participant_id__factor_dimPandasIndex<pre>PandasIndex(Index(['BG01', 'BG02', 'BG03', 'BG04', 'BG05', 'BG06', 'BG08', 'BG11', 'BG12',\n       'JK03', 'JK07', 'JK08', 'JK09', 'JK12', 'MV01', 'MV03', 'MV04', 'MV06'],\n      dtype='object', name='v_1|participant_id__factor_dim'))</pre></li><li>rt,response_obsPandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       9494, 9495, 9496, 9497, 9498, 9499, 9500, 9501, 9502, 9503],\n      dtype='int64', name='rt,response_obs', length=9504))</pre></li></ul></li><li>Attributes: (8)created_at :2025-01-22T18:03:37.756598+00:00arviz_version :0.20.0inference_library :pymcinference_library_version :5.17.0sampling_time :1890.0189800262451tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.13.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:                (chain: 4, draw: 1000)\nCoordinates:\n  * chain                  (chain) int64 0 1 2 3\n  * draw                   (draw) int64 0 1 2 3 4 5 ... 994 995 996 997 998 999\nData variables: (12/17)\n    smallest_eigval        (chain, draw) float64 nan nan nan nan ... nan nan nan\n    reached_max_treedepth  (chain, draw) bool False False False ... False False\n    energy                 (chain, draw) float64 6.471e+03 ... 6.466e+03\n    diverging              (chain, draw) bool False False False ... False False\n    energy_error           (chain, draw) float64 -0.1905 0.3237 ... 0.08699\n    process_time_diff      (chain, draw) float64 0.7942 0.3949 ... 0.3832 0.7663\n    ...                     ...\n    largest_eigval         (chain, draw) float64 nan nan nan nan ... nan nan nan\n    index_in_trajectory    (chain, draw) int64 12 4 -14 -8 8 8 ... -9 -9 3 -9 22\n    tree_depth             (chain, draw) int64 5 4 4 5 4 5 4 5 ... 5 4 5 5 4 4 5\n    lp                     (chain, draw) float64 -6.454e+03 ... -6.451e+03\n    perf_counter_diff      (chain, draw) float64 0.7943 0.3949 ... 0.3832 0.7664\n    acceptance_rate        (chain, draw) float64 0.9737 0.8702 ... 0.9154 0.9248\nAttributes:\n    created_at:                  2025-01-22T18:03:37.768449+00:00\n    arviz_version:               0.20.0\n    inference_library:           pymc\n    inference_library_version:   5.17.0\n    sampling_time:               1890.0189800262451\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 1000</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li></ul></li><li>Data variables: (17)<ul><li>smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])</pre></li><li>reached_max_treedepth(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])</pre></li><li>energy(chain, draw)float646.471e+03 6.467e+03 ... 6.466e+03<pre>array([[6471.08385907, 6467.08693835, 6466.37040616, ..., 6463.30746162,\n        6460.44714756, 6465.46204491],\n       [6467.57006522, 6474.33334713, 6473.58895774, ..., 6464.34915346,\n        6453.70952715, 6455.76817024],\n       [6464.13213574, 6467.79647858, 6468.13651626, ..., 6458.23437233,\n        6458.30089357, 6464.2269457 ],\n       [6471.70126045, 6462.7567999 , 6465.5425939 , ..., 6462.6881706 ,\n        6460.69176445, 6465.51904257]])</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])</pre></li><li>energy_error(chain, draw)float64-0.1905 0.3237 ... 0.1198 0.08699<pre>array([[-0.19046525,  0.32368211, -0.06505588, ..., -0.88595576,\n         0.1048205 ,  0.21295371],\n       [ 0.30934736,  0.58755606, -0.41657214, ..., -0.31654412,\n        -0.22727259,  0.61609942],\n       [ 0.01397336,  0.27537902,  0.36704898, ...,  0.06493897,\n        -0.21857436,  0.71774221],\n       [-0.11174763, -0.08873559,  0.13816966, ..., -0.10754441,\n         0.11983228,  0.08699006]])</pre></li><li>process_time_diff(chain, draw)float640.7942 0.3949 ... 0.3832 0.7663<pre>array([[0.79422312, 0.39488104, 0.39707512, ..., 0.79472288, 0.19831334,\n        0.19761506],\n       [0.39490688, 0.39489654, 0.7911984 , ..., 1.4902231 , 0.74582731,\n        0.7073526 ],\n       [0.39873384, 0.39597862, 0.39714226, ..., 0.39519379, 0.97443004,\n        0.38759706],\n       [0.39803483, 0.39456217, 0.39482612, ..., 0.38336397, 0.38317399,\n        0.76631489]])</pre></li><li>step_size(chain, draw)float640.1596 0.1596 ... 0.1901 0.1901<pre>array([[0.15964913, 0.15964913, 0.15964913, ..., 0.15964913, 0.15964913,\n        0.15964913],\n       [0.12386318, 0.12386318, 0.12386318, ..., 0.12386318, 0.12386318,\n        0.12386318],\n       [0.119367  , 0.119367  , 0.119367  , ..., 0.119367  , 0.119367  ,\n        0.119367  ],\n       [0.1901348 , 0.1901348 , 0.1901348 , ..., 0.1901348 , 0.1901348 ,\n        0.1901348 ]])</pre></li><li>perf_counter_start(chain, draw)float641.204e+06 1.204e+06 ... 1.204e+06<pre>array([[1203553.08466059, 1203553.8791682 , 1203554.27431641, ...,\n        1204193.87695845, 1204194.67205102, 1204194.87062654],\n       [1203543.08203606, 1203543.4772851 , 1203543.87250379, ...,\n        1204255.74674039, 1204257.23771624, 1204257.98391436],\n       [1203571.56789572, 1203571.96688448, 1203572.36342351, ...,\n        1204194.79619987, 1204195.19163582, 1204196.16633934],\n       [1203542.56131708, 1203542.95963517, 1203543.35443101, ...,\n        1204207.74933969, 1204208.13372382, 1204208.51714646]])</pre></li><li>step_size_bar(chain, draw)float640.1678 0.1678 ... 0.1754 0.1754<pre>array([[0.1677528 , 0.1677528 , 0.1677528 , ..., 0.1677528 , 0.1677528 ,\n        0.1677528 ],\n       [0.15221018, 0.15221018, 0.15221018, ..., 0.15221018, 0.15221018,\n        0.15221018],\n       [0.16936144, 0.16936144, 0.16936144, ..., 0.16936144, 0.16936144,\n        0.16936144],\n       [0.17537563, 0.17537563, 0.17537563, ..., 0.17537563, 0.17537563,\n        0.17537563]])</pre></li><li>n_steps(chain, draw)float6431.0 15.0 15.0 ... 15.0 15.0 31.0<pre>array([[31., 15., 15., ..., 31.,  7.,  7.],\n       [15., 15., 31., ..., 63., 31., 31.],\n       [15., 15., 15., ..., 15., 39., 15.],\n       [15., 15., 15., ..., 15., 15., 31.]])</pre></li><li>max_energy_error(chain, draw)float64-0.3616 -0.4263 ... 0.157 0.2481<pre>array([[-0.36159523, -0.42632594, -0.75308358, ...,  1.06429361,\n         0.49869392,  0.30266155],\n       [ 0.30934736,  0.86010252, -0.73555867, ...,  0.64595138,\n         1.34745234,  0.98999031],\n       [ 0.2105989 ,  0.77673406,  0.39625154, ..., -0.38521462,\n        -0.45874341,  0.98563672],\n       [-0.26053311, -0.1206915 ,  0.53362175, ..., -0.10754441,\n         0.15700491,  0.2481349 ]])</pre></li><li>largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])</pre></li><li>index_in_trajectory(chain, draw)int6412 4 -14 -8 8 8 ... -9 -9 3 -9 22<pre>array([[ 12,   4, -14, ...,  15,   4,  -2],\n       [-12,   9, -13, ..., -14,   7,   2],\n       [  9,  -7, -11, ...,  10,  26, -12],\n       [-12,  10,   3, ...,   3,  -9,  22]])</pre></li><li>tree_depth(chain, draw)int645 4 4 5 4 5 4 5 ... 4 5 4 5 5 4 4 5<pre>array([[5, 4, 4, ..., 5, 3, 3],\n       [4, 4, 5, ..., 6, 5, 5],\n       [4, 4, 4, ..., 4, 6, 4],\n       [4, 4, 4, ..., 4, 4, 5]])</pre></li><li>lp(chain, draw)float64-6.454e+03 ... -6.451e+03<pre>array([[-6453.99373996, -6455.47261561, -6454.21207774, ...,\n        -6447.30168714, -6451.08227063, -6454.24107651],\n       [-6459.96084516, -6461.1367558 , -6455.9959798 , ...,\n        -6444.17796183, -6441.35192316, -6446.06334025],\n       [-6453.99086811, -6454.31737055, -6458.28994829, ...,\n        -6449.72326915, -6446.37555507, -6454.24853857],\n       [-6455.83093536, -6450.2044764 , -6452.10625377, ...,\n        -6449.13696861, -6456.24731323, -6451.37320796]])</pre></li><li>perf_counter_diff(chain, draw)float640.7943 0.3949 ... 0.3832 0.7664<pre>array([[0.79425566, 0.3949122 , 0.39708535, ..., 0.79483398, 0.19832729,\n        0.19761879],\n       [0.39500016, 0.39498086, 0.79207792, ..., 1.4907523 , 0.74597081,\n        0.70750832],\n       [0.39874418, 0.39629993, 0.39717115, ..., 0.39521585, 0.97446708,\n        0.38761731],\n       [0.39807969, 0.39457173, 0.39483643, ..., 0.38415579, 0.3831928 ,\n        0.76637245]])</pre></li><li>acceptance_rate(chain, draw)float640.9737 0.8702 ... 0.9154 0.9248<pre>array([[0.97365255, 0.8702139 , 0.94464048, ..., 0.72300939, 0.84124712,\n        0.84929491],\n       [0.85316585, 0.7140769 , 0.96718006, ..., 0.79471959, 0.57349824,\n        0.59263229],\n       [0.93304991, 0.69466245, 0.82817621, ..., 0.9653926 , 0.99687019,\n        0.59952677],\n       [0.9898122 , 0.99729202, 0.76279855, ..., 0.99576907, 0.91540948,\n        0.92483419]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li></ul></li><li>Attributes: (8)created_at :2025-01-22T18:03:37.768449+00:00arviz_version :0.20.0inference_library :pymcinference_library_version :5.17.0sampling_time :1890.0189800262451tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.13.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:                  (rt,response_obs: 9504, rt,response_extra_dim_0: 2)\nCoordinates:\n  * rt,response_obs          (rt,response_obs) int64 0 1 2 3 ... 9501 9502 9503\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 0 1\nData variables:\n    rt,response              (rt,response_obs, rt,response_extra_dim_0) float64 ...\nAttributes:\n    created_at:                  2025-01-22T18:03:37.771923+00:00\n    arviz_version:               0.20.0\n    inference_library:           pymc\n    inference_library_version:   5.17.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>rt,response_obs: 9504</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>rt,response_obs(rt,response_obs)int640 1 2 3 4 ... 9500 9501 9502 9503<pre>array([   0,    1,    2, ..., 9501, 9502, 9503])</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(rt,response_obs, rt,response_extra_dim_0)float640.46 -1.0 0.278 ... 1.0 1.07 -1.0<pre>array([[ 0.46 , -1.   ],\n       [ 0.278,  1.   ],\n       [ 0.39 ,  1.   ],\n       ...,\n       [ 0.51 ,  1.   ],\n       [ 0.884,  1.   ],\n       [ 1.07 , -1.   ]])</pre></li></ul></li><li>Indexes: (2)<ul><li>rt,response_obsPandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       9494, 9495, 9496, 9497, 9498, 9499, 9500, 9501, 9502, 9503],\n      dtype='int64', name='rt,response_obs', length=9504))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (6)created_at :2025-01-22T18:03:37.771923+00:00arviz_version :0.20.0inference_library :pymcinference_library_version :5.17.0modeling_interface :bambimodeling_interface_version :0.13.0</li></ul> </ul> </li> </ul> <p>We can now look at summary statistics from our sampled model.</p> <p>Some measures to focus on:</p> <ol> <li>mean - can see that females have a greater drift rate than males, on average</li> <li>hdi_3% and hdi_97% - the credible interval containing most probable value of drift rate; comparing males and females, they seem to be non-overlapping</li> <li>r_hat - Gelman-Rubin statistic, which is a convergence diagnostic when we sample from multiple chains (here, we sampled from 4 chains, which is the default in HSSM). r_hat should be close to 1 to indicate good convergence.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>az.summary(v_sex.traces, var_names=['v_sex','t','z','a'])\n</pre> az.summary(v_sex.traces, var_names=['v_sex','t','z','a']) Out[\u00a0]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat v_sex[F] 1.058 0.109 0.840 1.237 0.004 0.003 893.0 1327.0 1.01 v_sex[M] 0.639 0.096 0.447 0.804 0.003 0.002 982.0 1529.0 1.00 t 0.213 0.002 0.208 0.217 0.000 0.000 2177.0 2581.0 1.00 z 0.513 0.005 0.505 0.522 0.000 0.000 2676.0 2676.0 1.00 a 0.780 0.005 0.771 0.788 0.000 0.000 2523.0 2529.0 1.00 In\u00a0[\u00a0]: Copied! <pre> v_sex.plot_trace()\n</pre>  v_sex.plot_trace() <p>Next, we want to check our fits to the observed data. Below, we show how to do that. We are plotting the predicted posterior versus the observed data.</p> In\u00a0[\u00a0]: Copied! <pre>hssm.plotting.plot_predictive(\n    v_sex, # the model which you have fit\n    x_range=(-5,5) # the range on the x-axis, which is the response time\n)\n;\n</pre> hssm.plotting.plot_predictive(     v_sex, # the model which you have fit     x_range=(-5,5) # the range on the x-axis, which is the response time ) ; <pre>No posterior predictive samples found. Generating posterior predictive samples using the provided InferenceData object and the original data. This will modify the provided InferenceData object, or if not provided, the traces object stored inside the model.\n</pre> Out[\u00a0]: <pre>''</pre> <p>We can also look at fits to individual subjects. Below is an example on how to do that.</p> In\u00a0[\u00a0]: Copied! <pre>hssm.plotting.plot_predictive(\n    v_sex, # the model which you have fit\n    data = df[df.participant_id=='BG04'], # specifying a specific subject\n    x_range=(-5,5) # the range on the x-axis, which is the response time\n)\n;\n</pre> hssm.plotting.plot_predictive(     v_sex, # the model which you have fit     data = df[df.participant_id=='BG04'], # specifying a specific subject     x_range=(-5,5) # the range on the x-axis, which is the response time ) ; Out[\u00a0]: <pre>''</pre> In\u00a0[\u00a0]: Copied! <pre>hssm.plotting.plot_predictive(\n    v_sex, # the model which you have fit\n    data = df[df.participant_id=='JK03'], # specifying a specific subject\n    x_range=(-5,5) # the range on the x-axis, which is the response time\n)\n;\n</pre> hssm.plotting.plot_predictive(     v_sex, # the model which you have fit     data = df[df.participant_id=='JK03'], # specifying a specific subject     x_range=(-5,5) # the range on the x-axis, which is the response time ) ; Out[\u00a0]: <pre>''</pre> <p>If you want to look at fits for all participants, the below cell could do so - for now it is commented out.</p> hssm.plotting.plot_predictive(     v_sex, # the model which you have fit     col=\"participant_id\", # each column will have a plot for each participant/subject     col_wrap=3,  # each row will have 3 plots     x_range=(-5,5) # the range on the x-axis, which is the response time ) ;  In\u00a0[\u00a0]: Copied! <pre>az.summary(v_sex.traces, var_names=['v_sex','t','z','a'])\n</pre> az.summary(v_sex.traces, var_names=['v_sex','t','z','a']) Out[\u00a0]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat v_sex[F] 1.058 0.109 0.840 1.237 0.004 0.003 893.0 1327.0 1.01 v_sex[M] 0.639 0.096 0.447 0.804 0.003 0.002 982.0 1529.0 1.00 t 0.213 0.002 0.208 0.217 0.000 0.000 2177.0 2581.0 1.00 z 0.513 0.005 0.505 0.522 0.000 0.000 2676.0 2676.0 1.00 a 0.780 0.005 0.771 0.788 0.000 0.000 2523.0 2529.0 1.00 In\u00a0[\u00a0]: Copied! <pre>az.plot_posterior(v_sex.traces, var_names=('v_sex')) # the default is 94% HDI. you can use hdi_prob to change it to a different interval, for example hdi_prob=0.95\n</pre> az.plot_posterior(v_sex.traces, var_names=('v_sex')) # the default is 94% HDI. you can use hdi_prob to change it to a different interval, for example hdi_prob=0.95 Out[\u00a0]: <pre>''</pre> In\u00a0[\u00a0]: Copied! <pre>az.plot_forest(v_sex.traces, var_names=('v_sex')) # here, we can visualize the HDI for males and females from each of the 4 chains we sampled from\n</pre> az.plot_forest(v_sex.traces, var_names=('v_sex')) # here, we can visualize the HDI for males and females from each of the 4 chains we sampled from Out[\u00a0]: <pre>''</pre> In\u00a0[\u00a0]: Copied! <pre>az.plot_forest(v_sex.traces, var_names=('v_sex'), combined=True) # same as above, just combining all 4 chains into a single chain\n</pre> az.plot_forest(v_sex.traces, var_names=('v_sex'), combined=True) # same as above, just combining all 4 chains into a single chain Out[\u00a0]: <pre>''</pre> In\u00a0[\u00a0]: Copied! <pre>v_sex_stats = v_sex.summary() # saving our stats table as a dataframe for more flexibility in visualization of HDI\nv_sex_stats.head()\n</pre> v_sex_stats = v_sex.summary() # saving our stats table as a dataframe for more flexibility in visualization of HDI v_sex_stats.head() Out[\u00a0]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat v_sex[F] 1.058 0.109 0.840 1.237 0.004 0.003 893.0 1327.0 1.01 v_sex[M] 0.639 0.096 0.447 0.804 0.003 0.002 982.0 1529.0 1.00 v_1|participant_id_mu 0.012 0.251 -0.471 0.476 0.004 0.004 3235.0 2692.0 1.00 t 0.213 0.002 0.208 0.217 0.000 0.000 2177.0 2581.0 1.00 z 0.513 0.005 0.505 0.522 0.000 0.000 2676.0 2676.0 1.00 In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(2,4))\n\nplt.errorbar([0],v_sex_stats['mean'].loc['v_sex[F]'], \n             yerr=v_sex_stats['hdi_97%'].loc['v_sex[F]']-v_sex_stats['hdi_3%'].loc['v_sex[F]'], \n              marker='o',mew=2.5, ls='',color='C0')\n\nplt.errorbar([1],v_sex_stats['mean'].loc['v_sex[M]'], \n             yerr=v_sex_stats['hdi_97%'].loc['v_sex[M]']-v_sex_stats['hdi_3%'].loc['v_sex[M]'], \n              marker='o',mew=2.5, ls='',color='C2')\n\nplt.ylim(0,1.6)\nplt.xlim(-1,2)\n\nx = [0,1]\nlabels = ['F','M']\nplt.xticks(x,labels)\n\nsns.despine()\n</pre> plt.figure(figsize=(2,4))  plt.errorbar([0],v_sex_stats['mean'].loc['v_sex[F]'],               yerr=v_sex_stats['hdi_97%'].loc['v_sex[F]']-v_sex_stats['hdi_3%'].loc['v_sex[F]'],                marker='o',mew=2.5, ls='',color='C0')  plt.errorbar([1],v_sex_stats['mean'].loc['v_sex[M]'],               yerr=v_sex_stats['hdi_97%'].loc['v_sex[M]']-v_sex_stats['hdi_3%'].loc['v_sex[M]'],                marker='o',mew=2.5, ls='',color='C2')  plt.ylim(0,1.6) plt.xlim(-1,2)  x = [0,1] labels = ['F','M'] plt.xticks(x,labels)  sns.despine() Out[\u00a0]: <pre>''</pre> In\u00a0[\u00a0]: Copied! <pre>a_interaction = hssm.HSSM(\n    model=\"ddm\",\n    data=df,\n    prior_settings=\"safe\",\n    include=[\n        {   \"name\": \"a\",\n            \"formula\": \"a ~ sex*sess + (1|participant_id)\", # note the 0 after ~ was removed. this is the default set up (could also replace the 0 with a 1).\n        }\n    ],\n)\n</pre> a_interaction = hssm.HSSM(     model=\"ddm\",     data=df,     prior_settings=\"safe\",     include=[         {   \"name\": \"a\",             \"formula\": \"a ~ sex*sess + (1|participant_id)\", # note the 0 after ~ was removed. this is the default set up (could also replace the 0 with a 1).         }     ], ) In\u00a0[\u00a0]: Copied! <pre>a_interaction.sample(target_accept=0.9)\n</pre> a_interaction.sample(target_accept=0.9) <pre>Auto-assigning NUTS sampler...\nInitializing NUTS using adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [t, v, z, a_Intercept, a_sex, a_sess, a_sex:sess, a_1|participant_id_sigma, a_1|participant_id_offset]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2880 seconds.\n</pre> Out[\u00a0]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:                         (chain: 4, draw: 1000, a_sex_dim: 1,\n                                     a_sess_dim: 4, a_sex:sess_dim: 4,\n                                     a_1|participant_id__factor_dim: 18,\n                                     rt,response_obs: 9504)\nCoordinates:\n  * chain                           (chain) int64 0 1 2 3\n  * draw                            (draw) int64 0 1 2 3 4 ... 996 997 998 999\n  * a_sex_dim                       (a_sex_dim) &lt;U1 'M'\n  * a_sess_dim                      (a_sess_dim) &lt;U1 '2' '3' '4' '5'\n  * a_sex:sess_dim                  (a_sex:sess_dim) &lt;U4 'M, 2' ... 'M, 5'\n  * a_1|participant_id__factor_dim  (a_1|participant_id__factor_dim) &lt;U4 'BG0...\n  * rt,response_obs                 (rt,response_obs) int64 0 1 2 ... 9502 9503\nData variables:\n    v                               (chain, draw) float64 0.9193 ... 0.9951\n    a_sex                           (chain, draw, a_sex_dim) float64 0.1095 ....\n    a_sess                          (chain, draw, a_sess_dim) float64 -0.0198...\n    a_sex:sess                      (chain, draw, a_sex:sess_dim) float64 -0....\n    t                               (chain, draw) float64 0.2257 ... 0.2241\n    z                               (chain, draw) float64 0.5188 ... 0.5185\n    a_Intercept                     (chain, draw) float64 0.7218 ... 0.7269\n    a_1|participant_id_sigma        (chain, draw) float64 0.1307 ... 0.1209\n    a_1|participant_id              (chain, draw, a_1|participant_id__factor_dim) float64 ...\n    a                               (chain, draw, rt,response_obs) float64 0....\nAttributes:\n    created_at:                  2025-01-22T18:56:21.505311+00:00\n    arviz_version:               0.20.0\n    inference_library:           pymc\n    inference_library_version:   5.17.0\n    sampling_time:               2880.088999271393\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 1000</li><li>a_sex_dim: 1</li><li>a_sess_dim: 4</li><li>a_sex:sess_dim: 4</li><li>a_1|participant_id__factor_dim: 18</li><li>rt,response_obs: 9504</li></ul></li><li>Coordinates: (7)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li><li>a_sex_dim(a_sex_dim)&lt;U1'M'<pre>array(['M'], dtype='&lt;U1')</pre></li><li>a_sess_dim(a_sess_dim)&lt;U1'2' '3' '4' '5'<pre>array(['2', '3', '4', '5'], dtype='&lt;U1')</pre></li><li>a_sex:sess_dim(a_sex:sess_dim)&lt;U4'M, 2' 'M, 3' 'M, 4' 'M, 5'<pre>array(['M, 2', 'M, 3', 'M, 4', 'M, 5'], dtype='&lt;U4')</pre></li><li>a_1|participant_id__factor_dim(a_1|participant_id__factor_dim)&lt;U4'BG01' 'BG02' ... 'MV04' 'MV06'<pre>array(['BG01', 'BG02', 'BG03', 'BG04', 'BG05', 'BG06', 'BG08', 'BG11', 'BG12',\n       'JK03', 'JK07', 'JK08', 'JK09', 'JK12', 'MV01', 'MV03', 'MV04', 'MV06'],\n      dtype='&lt;U4')</pre></li><li>rt,response_obs(rt,response_obs)int640 1 2 3 4 ... 9500 9501 9502 9503<pre>array([   0,    1,    2, ..., 9501, 9502, 9503])</pre></li></ul></li><li>Data variables: (10)<ul><li>v(chain, draw)float640.9193 0.9965 ... 0.9576 0.9951<pre>array([[0.91929989, 0.99646991, 1.00476696, ..., 0.95646475, 0.99878967,\n        0.97273798],\n       [0.99567628, 0.98161239, 0.98705494, ..., 0.95694884, 0.99132792,\n        0.97249558],\n       [0.95641583, 0.96927178, 0.98503021, ..., 0.9699779 , 0.98988451,\n        0.94993414],\n       [0.9571126 , 0.97985427, 0.96674887, ..., 0.95818099, 0.95763999,\n        0.99512512]])</pre></li><li>a_sex(chain, draw, a_sex_dim)float640.1095 0.1586 ... 0.04736 0.1639<pre>array([[[ 0.10947133],\n        [ 0.15856369],\n        [ 0.08441146],\n        ...,\n        [ 0.05863233],\n        [-0.02165307],\n        [ 0.08427187]],\n\n       [[ 0.10640251],\n        [ 0.12234081],\n        [ 0.12691589],\n        ...,\n        [ 0.08687017],\n        [ 0.11396635],\n        [ 0.10654392]],\n\n       [[ 0.17244029],\n        [ 0.1430487 ],\n        [ 0.13759602],\n        ...,\n        [ 0.05831626],\n        [ 0.15888998],\n        [ 0.13294226]],\n\n       [[-0.03088106],\n        [ 0.03242854],\n        [ 0.03667997],\n        ...,\n        [ 0.04648536],\n        [ 0.0473561 ],\n        [ 0.16389364]]])</pre></li><li>a_sess(chain, draw, a_sess_dim)float64-0.01987 -0.01211 ... -0.004421<pre>array([[[-0.01987177, -0.01210618, -0.03842134, -0.01905901],\n        [-0.00720694, -0.00227669, -0.02299769, -0.01070019],\n        [-0.01853776, -0.00465897, -0.02281222, -0.02450829],\n        ...,\n        [-0.0248662 , -0.01966173, -0.03292623, -0.00660541],\n        [-0.0310884 , -0.01462397, -0.05268748, -0.04052013],\n        [-0.01184773, -0.01660186, -0.0177073 ,  0.0045717 ]],\n\n       [[ 0.00885193,  0.00386796,  0.00370268,  0.01871842],\n        [-0.00807461, -0.01690194, -0.01595385, -0.00177616],\n        [-0.01127418, -0.00890186, -0.02381751, -0.0162373 ],\n        ...,\n        [ 0.00479673,  0.01915363, -0.0089792 , -0.00760568],\n        [-0.02204807,  0.00443799, -0.03388831, -0.02534183],\n        [-0.00662399, -0.00097671, -0.02875303, -0.00851458]],\n\n       [[-0.02322497, -0.00157024, -0.00564774, -0.01374129],\n        [ 0.00911076, -0.00980959,  0.01217252, -0.00136743],\n        [ 0.02251241,  0.00143925,  0.0075778 , -0.00026268],\n        ...,\n        [-0.02579605,  0.00726623, -0.01684104, -0.00756363],\n        [ 0.02102417,  0.00449891, -0.01933671,  0.01993606],\n        [-0.00336848,  0.00511331, -0.0267383 , -0.00336128]],\n\n       [[ 0.01002174,  0.01362983,  0.00646603,  0.00369951],\n        [-0.00222347, -0.01164784, -0.0142721 ,  0.0055691 ],\n        [-0.0015953 ,  0.00101639, -0.01707062, -0.01971374],\n        ...,\n        [ 0.00465938, -0.00243217, -0.01445513, -0.00598096],\n        [ 0.00032394, -0.01183328, -0.00359951, -0.00462622],\n        [-0.01023119, -0.01325555, -0.02824186, -0.00442101]]])</pre></li><li>a_sex:sess(chain, draw, a_sex:sess_dim)float64-0.01191 0.007137 ... -0.05359<pre>array([[[-0.01190829,  0.00713726, -0.08190141, -0.06163151],\n        [ 0.0087669 ,  0.04520702, -0.06138544, -0.03766681],\n        [ 0.00103116, -0.00050388, -0.09901681, -0.06675604],\n        ...,\n        [ 0.01845382,  0.06084249, -0.06270319, -0.04116731],\n        [-0.00853864,  0.02497962, -0.06214833, -0.01771867],\n        [-0.00118969,  0.06406938, -0.06161547, -0.0454615 ]],\n\n       [[-0.00210937,  0.0553646 , -0.0879372 , -0.07046909],\n        [ 0.03260753,  0.07536813, -0.0709475 , -0.04169001],\n        [ 0.02208674,  0.0428168 , -0.03770568, -0.02440449],\n        ...,\n        [-0.00370055, -0.0129735 , -0.0896801 , -0.05853509],\n        [ 0.01228348,  0.01346945, -0.07164623, -0.04040465],\n        [-0.01911233,  0.02155481, -0.08539019, -0.04232056]],\n\n       [[-0.00131952,  0.01630901, -0.09789964, -0.06581513],\n        [-0.03401181,  0.03554698, -0.11169565, -0.07414916],\n        [-0.03196865,  0.00564102, -0.13158358, -0.06025968],\n        ...,\n        [ 0.04827418,  0.01593937, -0.05697054, -0.06499737],\n        [-0.05269264,  0.00368719, -0.09457853, -0.08683295],\n        [-0.00101588,  0.02265222, -0.08334774, -0.05759735]],\n\n       [[-0.02112229,  0.01907007, -0.10631666, -0.05645078],\n        [-0.01135679,  0.03370062, -0.08628255, -0.06835567],\n        [-0.01956099, -0.00727999, -0.11876121, -0.07400598],\n        ...,\n        [-0.02026805,  0.00392375, -0.07862478, -0.06033865],\n        [-0.0219125 ,  0.02593428, -0.09353973, -0.05627573],\n        [-0.00935762,  0.05515875, -0.07639308, -0.05359306]]])</pre></li><li>t(chain, draw)float640.2257 0.2263 ... 0.226 0.2241<pre>array([[0.22574149, 0.22631617, 0.2238645 , ..., 0.22433502, 0.22567505,\n        0.2234051 ],\n       [0.22663949, 0.22657086, 0.22634585, ..., 0.22746554, 0.22468588,\n        0.2293272 ],\n       [0.22585718, 0.22503654, 0.2251865 , ..., 0.22481034, 0.22610592,\n        0.22547627],\n       [0.22514632, 0.22524594, 0.22550711, ..., 0.22632351, 0.22603968,\n        0.22407048]])</pre></li><li>z(chain, draw)float640.5188 0.5077 ... 0.522 0.5185<pre>array([[0.51882191, 0.50765412, 0.51794832, ..., 0.51464129, 0.51947325,\n        0.51155824],\n       [0.51775129, 0.51277668, 0.51749499, ..., 0.5191494 , 0.51478701,\n        0.5224087 ],\n       [0.5179647 , 0.51297859, 0.51836508, ..., 0.51491563, 0.51547006,\n        0.51949142],\n       [0.51833737, 0.5142589 , 0.52068204, ..., 0.51772905, 0.52195222,\n        0.51847691]])</pre></li><li>a_Intercept(chain, draw)float640.7218 0.6493 ... 0.7466 0.7269<pre>array([[0.72179666, 0.64934536, 0.82923159, ..., 0.80100867, 0.828489  ,\n        0.78969864],\n       [0.70498418, 0.75818004, 0.76128389, ..., 0.73827267, 0.74665877,\n        0.7560112 ],\n       [0.72409225, 0.71091783, 0.71471124, ..., 0.81921723, 0.71724734,\n        0.74197897],\n       [0.79356179, 0.78655906, 0.78380101, ..., 0.74912411, 0.74655665,\n        0.7269338 ]])</pre></li><li>a_1|participant_id_sigma(chain, draw)float640.1307 0.1355 ... 0.104 0.1209<pre>array([[0.13071971, 0.13547926, 0.11983034, ..., 0.10680181, 0.13367199,\n        0.09735293],\n       [0.14429656, 0.08842393, 0.07250272, ..., 0.12052221, 0.10932314,\n        0.08314373],\n       [0.08528763, 0.12268591, 0.10853404, ..., 0.11063612, 0.0974974 ,\n        0.08731052],\n       [0.14333106, 0.10393959, 0.100452  , ..., 0.09784127, 0.10400718,\n        0.12094137]])</pre></li><li>a_1|participant_id(chain, draw, a_1|participant_id__factor_dim)float64-0.0714 0.105 ... 0.1114 0.09425<pre>array([[[-0.07139516,  0.10499539,  0.05732301, ...,  0.16912042,\n          0.22852811,  0.16441064],\n        [-0.0238993 ,  0.1532514 ,  0.09109115, ...,  0.17966582,\n          0.17546821,  0.17751304],\n        [-0.17670664, -0.02840294, -0.01691052, ...,  0.07454257,\n          0.12110869,  0.07884694],\n        ...,\n        [-0.14817428,  0.01734317,  0.00485103, ...,  0.11170681,\n          0.15278868,  0.11839689],\n        [-0.18202552,  0.01050249, -0.0284098 , ...,  0.18845248,\n          0.22372399,  0.1730619 ],\n        [-0.15199313,  0.00553856,  0.01519169, ...,  0.10081564,\n          0.13730236,  0.096758  ]],\n\n       [[-0.09703704,  0.10204405,  0.07138965, ...,  0.20615601,\n          0.20620214,  0.16057211],\n        [-0.14125597,  0.04974116,  0.0167163 , ...,  0.10834167,\n          0.12246219,  0.06198998],\n        [-0.12077523,  0.04749917, -0.01003427, ...,  0.05955734,\n          0.11253729,  0.06876511],\n...\n        [-0.18756327, -0.00483027, -0.01530225, ...,  0.10556649,\n          0.13457213,  0.10871073],\n        [-0.10715637,  0.07355284,  0.07072598, ...,  0.13422581,\n          0.14797469,  0.12358238],\n        [-0.1341528 ,  0.04825637,  0.0153212 , ...,  0.12060999,\n          0.15727177,  0.08096028]],\n\n       [[-0.18401984, -0.01737058, -0.02452824, ...,  0.21646668,\n          0.27293293,  0.18996934],\n        [-0.16078149,  0.01014502, -0.01790202, ...,  0.19576527,\n          0.18963803,  0.16718656],\n        [-0.1469759 ,  0.03172651,  0.009244  , ...,  0.19018305,\n          0.26027172,  0.17959199],\n        ...,\n        [-0.13120307,  0.04586969,  0.00087167, ...,  0.21560371,\n          0.18203641,  0.12906113],\n        [-0.1071379 ,  0.04228315,  0.01910075, ...,  0.22211626,\n          0.20128805,  0.18600327],\n        [-0.08874675,  0.06979468,  0.06885021, ...,  0.06814122,\n          0.11142831,  0.09425265]]])</pre></li><li>a(chain, draw, rt,response_obs)float640.6504 0.6504 ... 0.9271 0.9271<pre>array([[[0.65040149, 0.65040149, 0.65040149, ..., 0.91498811,\n         0.91498811, 0.91498811],\n        [0.62544606, 0.62544606, 0.62544606, ..., 0.93705509,\n         0.93705509, 0.93705509],\n        [0.65252495, 0.65252495, 0.65252495, ..., 0.90122565,\n         0.90122565, 0.90122565],\n        ...,\n        [0.65283439, 0.65283439, 0.65283439, ..., 0.93026517,\n         0.93026517, 0.93026517],\n        [0.64646348, 0.64646348, 0.64646348, ..., 0.92165903,\n         0.92165903, 0.92165903],\n        [0.63770551, 0.63770551, 0.63770551, ..., 0.92983871,\n         0.92983871, 0.92983871]],\n\n       [[0.60794714, 0.60794714, 0.60794714, ..., 0.92020812,\n         0.92020812, 0.92020812],\n        [0.61692408, 0.61692408, 0.61692408, ..., 0.89904467,\n         0.89904467, 0.89904467],\n        [0.64050866, 0.64050866, 0.64050866, ..., 0.9163231 ,\n         0.9163231 , 0.9163231 ],\n...\n        [0.63165396, 0.63165396, 0.63165396, ..., 0.91368321,\n         0.91368321, 0.91368321],\n        [0.61009097, 0.61009097, 0.61009097, ..., 0.93282281,\n         0.93282281, 0.93282281],\n        [0.60782617, 0.60782617, 0.60782617, ..., 0.89492288,\n         0.89492288, 0.89492288]],\n\n       [[0.60954195, 0.60954195, 0.60954195, ..., 0.89989879,\n         0.89989879, 0.89989879],\n        [0.62577757, 0.62577757, 0.62577757, ..., 0.92338758,\n         0.92338758, 0.92338758],\n        [0.63682511, 0.63682511, 0.63682511, ..., 0.90635325,\n         0.90635325, 0.90635325],\n        ...,\n        [0.61792103, 0.61792103, 0.61792103, ..., 0.858351  ,\n         0.858351  , 0.858351  ],\n        [0.63941875, 0.63941875, 0.63941875, ..., 0.91901407,\n         0.91901407, 0.91901407],\n        [0.63818704, 0.63818704, 0.63818704, ..., 0.92706602,\n         0.92706602, 0.92706602]]])</pre></li></ul></li><li>Indexes: (7)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li><li>a_sex_dimPandasIndex<pre>PandasIndex(Index(['M'], dtype='object', name='a_sex_dim'))</pre></li><li>a_sess_dimPandasIndex<pre>PandasIndex(Index(['2', '3', '4', '5'], dtype='object', name='a_sess_dim'))</pre></li><li>a_sex:sess_dimPandasIndex<pre>PandasIndex(Index(['M, 2', 'M, 3', 'M, 4', 'M, 5'], dtype='object', name='a_sex:sess_dim'))</pre></li><li>a_1|participant_id__factor_dimPandasIndex<pre>PandasIndex(Index(['BG01', 'BG02', 'BG03', 'BG04', 'BG05', 'BG06', 'BG08', 'BG11', 'BG12',\n       'JK03', 'JK07', 'JK08', 'JK09', 'JK12', 'MV01', 'MV03', 'MV04', 'MV06'],\n      dtype='object', name='a_1|participant_id__factor_dim'))</pre></li><li>rt,response_obsPandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       9494, 9495, 9496, 9497, 9498, 9499, 9500, 9501, 9502, 9503],\n      dtype='int64', name='rt,response_obs', length=9504))</pre></li></ul></li><li>Attributes: (8)created_at :2025-01-22T18:56:21.505311+00:00arviz_version :0.20.0inference_library :pymcinference_library_version :5.17.0sampling_time :2880.088999271393tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.13.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:                (chain: 4, draw: 1000)\nCoordinates:\n  * chain                  (chain) int64 0 1 2 3\n  * draw                   (draw) int64 0 1 2 3 4 5 ... 994 995 996 997 998 999\nData variables: (12/17)\n    smallest_eigval        (chain, draw) float64 nan nan nan nan ... nan nan nan\n    reached_max_treedepth  (chain, draw) bool False False False ... False False\n    energy                 (chain, draw) float64 6.15e+03 ... 6.148e+03\n    diverging              (chain, draw) bool False False False ... False False\n    energy_error           (chain, draw) float64 -0.03771 -0.2076 ... -0.02723\n    process_time_diff      (chain, draw) float64 1.152 1.145 ... 0.5565 2.215\n    ...                     ...\n    largest_eigval         (chain, draw) float64 nan nan nan nan ... nan nan nan\n    index_in_trajectory    (chain, draw) int64 31 -34 105 -39 19 ... 59 9 7 81\n    tree_depth             (chain, draw) int64 6 6 7 6 5 5 6 6 ... 6 6 7 7 5 5 7\n    lp                     (chain, draw) float64 -6.132e+03 ... -6.134e+03\n    perf_counter_diff      (chain, draw) float64 1.152 1.146 ... 0.5567 2.216\n    acceptance_rate        (chain, draw) float64 0.8838 0.8973 ... 0.9015 0.9585\nAttributes:\n    created_at:                  2025-01-22T18:56:21.517288+00:00\n    arviz_version:               0.20.0\n    inference_library:           pymc\n    inference_library_version:   5.17.0\n    sampling_time:               2880.088999271393\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 1000</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li></ul></li><li>Data variables: (17)<ul><li>smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])</pre></li><li>reached_max_treedepth(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])</pre></li><li>energy(chain, draw)float646.15e+03 6.148e+03 ... 6.148e+03<pre>array([[6149.63635876, 6148.48666191, 6147.66500242, ..., 6142.66537785,\n        6152.63366113, 6167.97919473],\n       [6159.46908923, 6157.14326869, 6155.78162401, ..., 6141.64824009,\n        6144.98056976, 6151.20974706],\n       [6158.53341632, 6153.50766864, 6153.78855447, ..., 6156.52656415,\n        6154.97815232, 6147.21014332],\n       [6137.28835268, 6141.86765375, 6139.01464178, ..., 6146.80158646,\n        6149.04360045, 6147.75216095]])</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])</pre></li><li>energy_error(chain, draw)float64-0.03771 -0.2076 ... -0.02723<pre>array([[-0.03770601, -0.20758502,  0.00130941, ..., -0.17204555,\n         0.00642355,  0.37346048],\n       [-0.0519579 ,  0.53997234, -0.05796225, ...,  0.00836485,\n         0.44390981, -0.10988453],\n       [-0.1617163 , -0.11714075, -0.20871811, ...,  0.10089004,\n        -0.25154026,  0.10377167],\n       [ 0.09740162,  0.12389337,  0.10315273, ...,  0.08336922,\n         0.05479171, -0.02723071]])</pre></li><li>process_time_diff(chain, draw)float641.152 1.145 2.284 ... 0.5565 2.215<pre>array([[1.15182187, 1.14536232, 2.28386981, ..., 0.53910743, 2.1549183 ,\n        2.16137302],\n       [1.13945761, 1.13722012, 0.57042311, ..., 0.56345312, 3.36308053,\n        1.13258047],\n       [1.14510923, 1.70342597, 2.26578621, ..., 1.05501743, 1.58063701,\n        1.05250169],\n       [2.26395528, 1.14026213, 1.13488409, ..., 0.55530842, 0.556542  ,\n        2.21514762]])</pre></li><li>step_size(chain, draw)float640.08122 0.08122 ... 0.07201 0.07201<pre>array([[0.08121544, 0.08121544, 0.08121544, ..., 0.08121544, 0.08121544,\n        0.08121544],\n       [0.09194912, 0.09194912, 0.09194912, ..., 0.09194912, 0.09194912,\n        0.09194912],\n       [0.08708321, 0.08708321, 0.08708321, ..., 0.08708321, 0.08708321,\n        0.08708321],\n       [0.07201382, 0.07201382, 0.07201382, ..., 0.07201382, 0.07201382,\n        0.07201382]])</pre></li><li>perf_counter_start(chain, draw)float641.206e+06 1.206e+06 ... 1.207e+06<pre>array([[1206017.96448806, 1206019.11693553, 1206020.26294733, ...,\n        1207405.25680885, 1207405.79619545, 1207407.95158446],\n       [1205927.94610297, 1205929.085937  , 1205930.22348556, ...,\n        1207168.04801368, 1207168.61192185, 1207171.97629273],\n       [1206054.65424414, 1206055.79972043, 1206057.50355159, ...,\n        1207418.2994713 , 1207419.35501769, 1207420.93635949],\n       [1206071.07371843, 1206073.33807203, 1206074.47862795, ...,\n        1207399.93618924, 1207400.49195009, 1207401.04888616]])</pre></li><li>step_size_bar(chain, draw)float640.09008 0.09008 ... 0.08204 0.08204<pre>array([[0.09008097, 0.09008097, 0.09008097, ..., 0.09008097, 0.09008097,\n        0.09008097],\n       [0.09333281, 0.09333281, 0.09333281, ..., 0.09333281, 0.09333281,\n        0.09333281],\n       [0.08387268, 0.08387268, 0.08387268, ..., 0.08387268, 0.08387268,\n        0.08387268],\n       [0.08203915, 0.08203915, 0.08203915, ..., 0.08203915, 0.08203915,\n        0.08203915]])</pre></li><li>n_steps(chain, draw)float6463.0 63.0 127.0 ... 31.0 31.0 127.0<pre>array([[ 63.,  63., 127., ...,  31., 127., 127.],\n       [ 63.,  63.,  31., ...,  31., 191.,  63.],\n       [ 63.,  95., 127., ...,  63.,  95.,  63.],\n       [127.,  63.,  63., ...,  31.,  31., 127.]])</pre></li><li>max_energy_error(chain, draw)float640.5555 -0.6588 ... 0.313 -0.3569<pre>array([[ 0.55553779, -0.65880397,  0.35314878, ..., -0.40773749,\n         0.41864486,  1.23688365],\n       [ 0.70737762,  0.94911853,  0.58331771, ...,  0.26240621,\n         0.51315259, -0.44490623],\n       [-0.53025352, -0.44803192, -0.38639155, ..., -0.19312716,\n        -0.37358843,  0.13683774],\n       [ 0.69729842,  0.28327694, -0.27605815, ...,  0.13794402,\n         0.31297302, -0.35692874]])</pre></li><li>largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])</pre></li><li>index_in_trajectory(chain, draw)int6431 -34 105 -39 19 ... 33 59 9 7 81<pre>array([[ 31, -34, 105, ..., -12, -28,  29],\n       [-11,  57, -10, ...,  -9,  64, -51],\n       [ 38, -50,  12, ..., -23, -42,  22],\n       [-52, -53,  20, ...,   9,   7,  81]])</pre></li><li>tree_depth(chain, draw)int646 6 7 6 5 5 6 6 ... 6 6 6 7 7 5 5 7<pre>array([[6, 6, 7, ..., 5, 7, 7],\n       [6, 6, 5, ..., 5, 8, 6],\n       [6, 7, 7, ..., 6, 7, 6],\n       [7, 6, 6, ..., 5, 5, 7]])</pre></li><li>lp(chain, draw)float64-6.132e+03 ... -6.134e+03<pre>array([[-6132.42421133, -6133.20009749, -6133.93273641, ...,\n        -6130.13290807, -6136.67230684, -6138.55627538],\n       [-6137.2530853 , -6139.54993172, -6141.98558722, ...,\n        -6129.40482508, -6132.09906217, -6140.09722511],\n       [-6139.06264412, -6137.70734337, -6131.97375965, ...,\n        -6139.72650286, -6130.87888986, -6132.97528036],\n       [-6125.0623416 , -6126.71806346, -6130.45248553, ...,\n        -6134.23163529, -6132.09011098, -6134.19535288]])</pre></li><li>perf_counter_diff(chain, draw)float641.152 1.146 2.285 ... 0.5567 2.216<pre>array([[1.15213987, 1.14571624, 2.28452789, ..., 0.53912073, 2.15509333,\n        2.16156401],\n       [1.13955378, 1.13727443, 0.57045088, ..., 0.56360753, 3.36401046,\n        1.13291346],\n       [1.14516547, 1.70353904, 2.26596041, ..., 1.05529648, 1.58108754,\n        1.05285363],\n       [2.26405723, 1.14029576, 1.13495879, ..., 0.55547606, 0.55666928,\n        2.21584139]])</pre></li><li>acceptance_rate(chain, draw)float640.8838 0.8973 ... 0.9015 0.9585<pre>array([[0.88383643, 0.89726297, 0.92666723, ..., 0.95176479, 0.91012537,\n        0.58335267],\n       [0.72600372, 0.65114252, 0.88435943, ..., 0.90909548, 0.81272678,\n        0.98676053],\n       [0.99765814, 0.99071337, 0.99509744, ..., 0.96922064, 0.99962325,\n        0.94794665],\n       [0.79082734, 0.88230758, 0.9561455 , ..., 0.95804915, 0.9014655 ,\n        0.95847792]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li></ul></li><li>Attributes: (8)created_at :2025-01-22T18:56:21.517288+00:00arviz_version :0.20.0inference_library :pymcinference_library_version :5.17.0sampling_time :2880.088999271393tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.13.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:                  (rt,response_obs: 9504, rt,response_extra_dim_0: 2)\nCoordinates:\n  * rt,response_obs          (rt,response_obs) int64 0 1 2 3 ... 9501 9502 9503\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 0 1\nData variables:\n    rt,response              (rt,response_obs, rt,response_extra_dim_0) float64 ...\nAttributes:\n    created_at:                  2025-01-22T18:56:21.521791+00:00\n    arviz_version:               0.20.0\n    inference_library:           pymc\n    inference_library_version:   5.17.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>rt,response_obs: 9504</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>rt,response_obs(rt,response_obs)int640 1 2 3 4 ... 9500 9501 9502 9503<pre>array([   0,    1,    2, ..., 9501, 9502, 9503])</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(rt,response_obs, rt,response_extra_dim_0)float640.46 -1.0 0.278 ... 1.0 1.07 -1.0<pre>array([[ 0.46 , -1.   ],\n       [ 0.278,  1.   ],\n       [ 0.39 ,  1.   ],\n       ...,\n       [ 0.51 ,  1.   ],\n       [ 0.884,  1.   ],\n       [ 1.07 , -1.   ]])</pre></li></ul></li><li>Indexes: (2)<ul><li>rt,response_obsPandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       9494, 9495, 9496, 9497, 9498, 9499, 9500, 9501, 9502, 9503],\n      dtype='int64', name='rt,response_obs', length=9504))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (6)created_at :2025-01-22T18:56:21.521791+00:00arviz_version :0.20.0inference_library :pymcinference_library_version :5.17.0modeling_interface :bambimodeling_interface_version :0.13.0</li></ul> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>az.summary(a_interaction.traces, var_names=['a_sex','a_sess','t','z','v'])\n</pre> az.summary(a_interaction.traces, var_names=['a_sex','a_sess','t','z','v']) Out[\u00a0]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat a_sex[M] 0.104 0.052 0.008 0.208 0.001 0.001 1309.0 1706.0 1.0 a_sess[2] -0.006 0.016 -0.035 0.024 0.000 0.000 2970.0 2802.0 1.0 a_sess[3] -0.003 0.016 -0.031 0.027 0.000 0.000 2903.0 2752.0 1.0 a_sess[4] -0.025 0.015 -0.052 0.003 0.000 0.000 2597.0 3033.0 1.0 a_sess[5] -0.010 0.015 -0.039 0.017 0.000 0.000 3018.0 2869.0 1.0 t 0.225 0.002 0.222 0.228 0.000 0.000 4266.0 3069.0 1.0 z 0.517 0.004 0.509 0.525 0.000 0.000 3449.0 2443.0 1.0 v 0.971 0.020 0.934 1.011 0.000 0.000 3915.0 2837.0 1.0 In\u00a0[\u00a0]: Copied! <pre>a_interaction.plot_trace()\n</pre> a_interaction.plot_trace() In\u00a0[\u00a0]: Copied! <pre>hssm.plotting.plot_predictive(\n    a_interaction, # the model which you have fit\n    x_range=(-5,5) # the range on the x-axis, which is the response time\n)\n</pre> hssm.plotting.plot_predictive(     a_interaction, # the model which you have fit     x_range=(-5,5) # the range on the x-axis, which is the response time ) <pre>No posterior predictive samples found. Generating posterior predictive samples using the provided InferenceData object and the original data. This will modify the provided InferenceData object, or if not provided, the traces object stored inside the model.\n</pre> Out[\u00a0]: <pre>''</pre> In\u00a0[\u00a0]: Copied! <pre>hssm.plotting.plot_predictive(\n    a_interaction, # the model which you have fit\n    data = df[df.participant_id=='BG04'], # specifying a specific subject\n    x_range=(-5,5) # the range on the x-axis, which is the response time\n)\n</pre> hssm.plotting.plot_predictive(     a_interaction, # the model which you have fit     data = df[df.participant_id=='BG04'], # specifying a specific subject     x_range=(-5,5) # the range on the x-axis, which is the response time ) Out[\u00a0]: <pre>''</pre> In\u00a0[\u00a0]: Copied! <pre>hssm.plotting.plot_predictive(\n    a_interaction, # the model which you have fit\n    data = df[df.participant_id=='JK03'], # specifying a specific subject\n    x_range=(-5,5) # the range on the x-axis, which is the response time\n)\n;\n</pre> hssm.plotting.plot_predictive(     a_interaction, # the model which you have fit     data = df[df.participant_id=='JK03'], # specifying a specific subject     x_range=(-5,5) # the range on the x-axis, which is the response time ) ; Out[\u00a0]: <pre>''</pre> hssm.plotting.plot_predictive(     a_interaction, # the model which you have fit     col=\"participant_id\", # each column will have a plot for each participant/subject     col_wrap=3,  # each row will have 3 plots     x_range=(-5,5) # the range on the x-axis, which is the response time )  In\u00a0[\u00a0]: Copied! <pre>az.plot_posterior(a_interaction.traces, var_names=('a_sex:sess'))\n</pre> az.plot_posterior(a_interaction.traces, var_names=('a_sex:sess')) Out[\u00a0]: <pre>''</pre> In\u00a0[\u00a0]: Copied! <pre>az.plot_forest(a_interaction.traces, var_names=('a_sex:sess'),combined=True)\n</pre> az.plot_forest(a_interaction.traces, var_names=('a_sex:sess'),combined=True) Out[\u00a0]: <pre>''</pre> In\u00a0[\u00a0]: Copied! <pre>a_interaction_stats = a_interaction.summary()\n</pre> a_interaction_stats = a_interaction.summary() In\u00a0[\u00a0]: Copied! <pre>plt.errorbar([0],a_interaction_stats['mean'].loc['a_sex:sess[M, 2]'], \n             yerr=a_interaction_stats['hdi_97%'].loc['a_sex:sess[M, 2]']-a_interaction_stats['hdi_3%'].loc['a_sex:sess[M, 2]'], \n              marker='o',mew=2.5, ls='',color='C0')\n\nplt.errorbar([1],a_interaction_stats['mean'].loc['a_sex:sess[M, 3]'], \n             yerr=a_interaction_stats['hdi_97%'].loc['a_sex:sess[M, 3]']-a_interaction_stats['hdi_3%'].loc['a_sex:sess[M, 3]'], \n              marker='o',mew=2.5, ls='',color='C1')\n\nplt.errorbar([2],a_interaction_stats['mean'].loc['a_sex:sess[M, 4]'], \n             yerr=a_interaction_stats['hdi_97%'].loc['a_sex:sess[M, 4]']-a_interaction_stats['hdi_3%'].loc['a_sex:sess[M, 4]'], \n              marker='o',mew=2.5, ls='',color='C2')\n\nplt.errorbar([3],a_interaction_stats['mean'].loc['a_sex:sess[M, 5]'], \n             yerr=a_interaction_stats['hdi_97%'].loc['a_sex:sess[M, 5]']-a_interaction_stats['hdi_3%'].loc['a_sex:sess[M, 5]'], \n              marker='o',mew=2.5, ls='',color='C3')\n\nsns.despine()\nlabels = (2,3,4,5)\nx = [0,1,2,3]\nplt.xticks(x,labels)\nplt.xlabel('Session')\n</pre> plt.errorbar([0],a_interaction_stats['mean'].loc['a_sex:sess[M, 2]'],               yerr=a_interaction_stats['hdi_97%'].loc['a_sex:sess[M, 2]']-a_interaction_stats['hdi_3%'].loc['a_sex:sess[M, 2]'],                marker='o',mew=2.5, ls='',color='C0')  plt.errorbar([1],a_interaction_stats['mean'].loc['a_sex:sess[M, 3]'],               yerr=a_interaction_stats['hdi_97%'].loc['a_sex:sess[M, 3]']-a_interaction_stats['hdi_3%'].loc['a_sex:sess[M, 3]'],                marker='o',mew=2.5, ls='',color='C1')  plt.errorbar([2],a_interaction_stats['mean'].loc['a_sex:sess[M, 4]'],               yerr=a_interaction_stats['hdi_97%'].loc['a_sex:sess[M, 4]']-a_interaction_stats['hdi_3%'].loc['a_sex:sess[M, 4]'],                marker='o',mew=2.5, ls='',color='C2')  plt.errorbar([3],a_interaction_stats['mean'].loc['a_sex:sess[M, 5]'],               yerr=a_interaction_stats['hdi_97%'].loc['a_sex:sess[M, 5]']-a_interaction_stats['hdi_3%'].loc['a_sex:sess[M, 5]'],                marker='o',mew=2.5, ls='',color='C3')  sns.despine() labels = (2,3,4,5) x = [0,1,2,3] plt.xticks(x,labels) plt.xlabel('Session') Out[\u00a0]: <pre>''</pre>"},{"location":"tutorials/hssm_tutorial_workshop_2/#getting-started-with-drift-diffusion-models-a-python-tutorial","title":"Getting Started with Drift Diffusion Models: A Python Tutorial\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#outline","title":"Outline:\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#1-explanation-of-behavioral-task","title":"1. Explanation of behavioral task\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#2-behavioral-results-sex-differences-during-learning","title":"2. Behavioral results: Sex differences during learning\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#3-what-is-drift-diffusion-modeling","title":"3. What is Drift Diffusion Modeling?\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#4-hssm-tutorial","title":"4. HSSM Tutorial\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#all-data-is-from-our-recent-publication","title":"All data is from our recent publication:\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#explanation-of-behavioral-task","title":"Explanation of behavioral task\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#behavioral-results-sex-differences-during-learning","title":"Behavioral results: Sex differences during learning\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#latencies-over-learning","title":"Latencies over learning:\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#choice-preferences","title":"Choice preferences.\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#how-do-observable-behavioral-effects-relate-to-latent-cognitive-processes-involved-in-a-decision","title":"How do observable behavioral effects relate to latent cognitive processes involved in a decision?\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#what-is-drift-diffusion-modeling","title":"What is Drift Diffusion Modeling?\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#drift-diffusion-model-ddm","title":"Drift Diffusion Model (DDM):\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#a-model-which-describes-how-decisions-are-made-via-evidence-accumulation-until-a-threshold-is-reached","title":"A model which describes how decisions are made via evidence accumulation until a threshold is reached.\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#parameters","title":"Parameters:\u00b6","text":"<ul> <li>Drift rate: the rate of sensory integration</li> <li>Threshold: the amount of evidence/information needed to trigger a decision</li> <li>Starting point bias: the variability in the starting point of evidence accumulation</li> <li>Non-decision time: time taken to iniate stimulus processing &amp; execute the motor response to make a choice</li> </ul>"},{"location":"tutorials/hssm_tutorial_workshop_2/#hssm-tutorial","title":"HSSM Tutorial\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#data-set-up","title":"Data set up:\u00b6","text":"<ul> <li>Data can be read in as a csv file (similar to an excel spreadsheet) using pandas (pd)</li> <li>Each row is a trial, columns provide information about each trial.</li> <li>Column names that are needed for HSSM:<ul> <li>rt (the response time measurement; in our case, this is the latency variable, which is already saved as rt)</li> <li>response (indicates whether a trial was an error or correct response; errors are coded as -1, and correct is coded as 1; in our case, since we are only using dual offer trials, -1 indicates choosing the low-value cue, and 1 indicates choosing the high value cue)</li> <li>participant_id (this is each subject, or in our case each rat)</li> </ul> </li> </ul>"},{"location":"tutorials/hssm_tutorial_workshop_2/#example-1-effects-of-sex-on-drift-rate","title":"EXAMPLE 1: Effects of sex on drift rate.\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#defining-sampling-model","title":"Defining &amp; sampling model:\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#ddm-parameters","title":"DDM parameters\u00b6","text":"<ul> <li>Drift rate = <code>v</code></li> <li>Threshold = <code>a</code></li> <li>Starting point bias = <code>z</code></li> <li>Non-decision time = <code>t</code></li> </ul>"},{"location":"tutorials/hssm_tutorial_workshop_2/#visualizing-results","title":"Visualizing results:\u00b6","text":"<p>The following cell allows us to visualize the fits from the model we just sampled. On the left, we will see the posterior distributions, and on the right we will see the traces from sampling.</p> <p>Posteriors can give us a first insight into the effects we are interested in (in this example, we can see that the distributions for males and females are pretty much non-overlapping).</p> <p>Traces should look like white-noise, often times described as \"hairy caterpillars.\" We do not want to see any oscillations in the traces.</p>"},{"location":"tutorials/hssm_tutorial_workshop_2/#example-of-a-good-fit","title":"Example of a good fit:\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#example-of-a-bad-fit","title":"Example of a bad fit:\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#back-to-the-stats-hdi","title":"Back to the stats: HDI\u00b6","text":"<p>\"hdi_3%\" and \"hdi_97%\" - indicate the 94% HDI (Highest Density Interval)</p> <ul> <li>HDI expresses the certainty in Bayesian fits. It refers to the interval within the posterior distribution that is most concentrated, and therefore contains the most probable values of the parameter.</li> </ul>"},{"location":"tutorials/hssm_tutorial_workshop_2/#interpretations","title":"Interpretations:\u00b6","text":"<p>Females have a greater drift rate than males. This means that females integrated sensory information about the stimuli faster than males. When linked back to our behavioral effects, this can explain why females chose the higher-value cue more frequently than males.</p>"},{"location":"tutorials/hssm_tutorial_workshop_2/#example-2-effects-of-sex-and-session-on-threshold","title":"EXAMPLE 2: Effects of sex AND session on threshold.\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#our-approach-here-will-be-the-exact-same-as-in-example-1-only-now-we-will-look-at-interaction-effects-of-sex-and-session","title":"Our approach here will be the exact same as in Example 1, only now we will look at interaction effects of sex and session.\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#checking-traces-posteriors","title":"Checking traces &amp; posteriors.\u00b6","text":"<ul> <li><p>We can see in the posteriors that fits are no longer estimated for each level (because we removed the 0 in our formula above).</p> </li> <li><p>There is a single estimate for sex (which has 2 levels), and four for sessions (which has 5 levels). This is because one level from each category serves as a \"reference\" for all others. In this case, females and session 1 are the reference levels.</p> </li> <li><p>The interpretation of the sex posterior (a_sex) is a bit different now that we have an interaction in our model. Since the reference level is females in session 1, we are looking at the posterior of males compared to females when session is 1. This distribution is barely overlapping with zero, meaning there is likely a slight offset by sex in session 1. Referring to the summary table above, we can see the HDI does not contain zero, so the most probable threshold for males is slightly greater than that of females in session 1.</p> </li> <li><p>The same goes for interpreting the session posteriors (a_sess). We are looking at the posteriors of sessions 2-5 compared to session 1 when sex is female. They are all roughly overlapping, and all HDI's contain zero. This means that females did not have a change in threshold over sessions.</p> </li> </ul> <p>The important fit to note here is the interaction between sex and session (a_sex:sess). Here, we are looking at posteriors of males in sessions 2-5, each compared to females in session 1. This reveals the interaction effect, where males have a reduction in threshold over sessions (specifcally sessions 4 and 5, which are depicted in green and red).</p>"},{"location":"tutorials/hssm_tutorial_workshop_2/#checking-fits-to-rt","title":"Checking fits to RT.\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#global","title":"Global:\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#example-of-a-good-fit","title":"Example of a good fit.\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#example-of-a-bad-fit","title":"Example of a bad fit.\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#uncomment-the-below-cell-to-look-at-fits-to-all-participants","title":"Uncomment the below cell to look at fits to all participants:\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#hdi","title":"HDI\u00b6","text":""},{"location":"tutorials/hssm_tutorial_workshop_2/#interpretations","title":"Interpretations:\u00b6","text":"<p>Males have a reduction in threshold over sessions (specifically, sessions 4 and 5), and females do not. This indicates that with learning, males required less information/evidence to make their choice. If we think back to the behavior, this aligns well with the reduction in response latencies over sessions that males demonstrated.</p> <p>Taken together with the effects on drift rate, one interpretation of these results is a reflection of males being more choice impulsive than females - they begin to require less information to make a choice, yet consistently choose the worse option more frequently than females do.</p>"},{"location":"tutorials/hssm_tutorial_workshop_2/#now-youre-ready-to-use-hssm-with-your-own-behavioral-data","title":"Now you're ready to use HSSM with your own behavioral data!\u00b6","text":""},{"location":"tutorials/initial_values/","title":"Initial Values","text":"In\u00a0[1]: Copied! <pre>import warnings\nimport numpy as np\nimport hssm\nwarnings.filterwarnings(\"ignore\")\n</pre> import warnings import numpy as np import hssm warnings.filterwarnings(\"ignore\") <p>Folklore suggests that the initial values of our sampler shouldn't matter for the outcome of the analysis, since MCMC will find the relevant region of the parameter space eventually.</p> <p>Well, we can't trust the elders blindly.. and you will sometimes find that initial value setting need to be corrected for good results.</p> <p>There are multiple reasons, but chief among them is that we are routinely dealing with constrained parameter spaces. At the edges of these parameter spaces, likelihoods can become less well behaved (this is true e.g. with our <code>approx_differentiable</code> likelihoods based on LANs). These edges may be unlikely a priori, but if your sampler takes a path to the parameter space (even if just on the way to a distant mode), that passes through these regions, you may get undesirable results.</p> <p>There are two ways in which you can control the bounds of a parameter:</p> <ol> <li>For an individual parameter, you can specify a <code>Truncated</code> distribution, or simply choose a prior distribution that naturally lives in the desired space (e.g. a <code>Gamma</code> distribution when dealing with positivity constraints).</li> <li>To constrain the outcome of a regression you can always use a <code>Link</code> function that will target the desired output space.</li> </ol> <p>Option 2., while computationally kosher in principle, can produce some downstream headaches, since it changes the interpretation of parameter values from straightforward to e.g. log-odds (Logistic Regression with logit link). This demands careful thinking about priors and can sometimes make reporting of result more difficult.</p> <p>Therefore you may find yourself in the situation that you do not want to use link functions or other a priori constaints, while still needing to respect pathologies concerning regions of the parameter space.</p> <p>At this point, the wisdom of the ancients aside, practical considerations will force you to think about initial values.</p> <p>We are not here to prescribe you how to deal with this, but we try to provide you with options. This short tutorial illustrates how to inspect and adjust the initial value settings of an <code>HSSM</code> model.</p> In\u00a0[2]: Copied! <pre>cav_data = hssm.load_data(\"cavanagh_theta\")\n</pre> cav_data = hssm.load_data(\"cavanagh_theta\") In\u00a0[3]: Copied! <pre>model = hssm.HSSM(\n    data=cav_data,\n    model=\"ddm\",\n    loglik_kind=\"approx_differentiable\",\n    model_config={\"backend\": \"pytensor\"},\n)\n</pre> model = hssm.HSSM(     data=cav_data,     model=\"ddm\",     loglik_kind=\"approx_differentiable\",     model_config={\"backend\": \"pytensor\"}, ) <pre>Model initialized successfully.\n</pre> <p>Now we can inspect the initial value setting.</p> In\u00a0[4]: Copied! <pre>model.initvals\n</pre> model.initvals Out[4]: <pre>{'a': array(1.5), 't': array(0.025), 'z': array(0.5), 'v': array(0.)}</pre> In\u00a0[5]: Copied! <pre>from copy import deepcopy\n\nmy_initvals = deepcopy(model.initvals)\nmy_initvals[\"z\"] = np.array(0.6, dtype=\"float32\")\n</pre> from copy import deepcopy  my_initvals = deepcopy(model.initvals) my_initvals[\"z\"] = np.array(0.6, dtype=\"float32\") In\u00a0[6]: Copied! <pre>idata = model.sample(draws=100,\n                     tune=100,\n                     sampler=\"mcmc\",\n                     initvals = my_initvals)\n</pre> idata = model.sample(draws=100,                      tune=100,                      sampler=\"mcmc\",                      initvals = my_initvals) <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [a, t, z, v]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 100 tune and 100 draw iterations (400 + 400 draws total) took 33 seconds.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 400/400 [00:11&lt;00:00, 34.16it/s]\n</pre> In\u00a0[7]: Copied! <pre>idata.posterior\n</pre> idata.posterior Out[7]: <pre>&lt;xarray.Dataset&gt; Size: 14kB\nDimensions:  (chain: 4, draw: 100)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 800B 0 1 2 3 4 5 6 7 8 ... 91 92 93 94 95 96 97 98 99\nData variables:\n    a        (chain, draw) float64 3kB 1.038 1.042 1.02 ... 1.044 1.044 1.043\n    t        (chain, draw) float64 3kB 0.36 0.357 0.3667 ... 0.3586 0.3572\n    z        (chain, draw) float64 3kB 0.5023 0.5046 0.5073 ... 0.5006 0.501\n    v        (chain, draw) float64 3kB 0.3759 0.3925 0.3731 ... 0.3932 0.4054\nAttributes:\n    created_at:                  2025-09-26T23:56:23.941126+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               32.72891592979431\n    tuning_steps:                100\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 100</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99<pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])</pre></li></ul></li><li>Data variables: (4)<ul><li>a(chain, draw)float641.038 1.042 1.02 ... 1.044 1.043<pre>array([[1.0383923 , 1.0423295 , 1.02020527, 1.05079805, 1.02415603,\n        1.03022323, 1.04465337, 1.03551052, 1.03836428, 1.03123169,\n        1.03867486, 1.04231138, 1.04105561, 1.03953239, 1.04024812,\n        1.04453212, 1.04260133, 1.02981424, 1.03101981, 1.04576419,\n        1.02971091, 1.037936  , 1.04034273, 1.02266256, 1.02125241,\n        1.03318661, 1.03123176, 1.02848015, 1.03955057, 1.03535753,\n        1.04574421, 1.04744784, 1.04553893, 1.03518973, 1.02987992,\n        1.0476744 , 1.04333107, 1.04322598, 1.03304011, 1.0347321 ,\n        1.03562412, 1.03742516, 1.03285144, 1.02376003, 1.0420778 ,\n        1.04131584, 1.04048052, 1.02759839, 1.03072931, 1.03803466,\n        1.02853475, 1.02281395, 1.01945031, 1.04557428, 1.04566927,\n        1.03536615, 1.03265922, 1.03247856, 1.02655765, 1.02728424,\n        1.01866046, 1.06389601, 1.0369807 , 1.04247382, 1.03419222,\n        1.03199319, 1.03768842, 1.04154492, 1.03583793, 1.03438143,\n        1.04192762, 1.04801979, 1.05445234, 1.04265471, 1.05385506,\n        1.02636899, 1.01911295, 1.03267941, 1.02347687, 1.04092408,\n        1.0379158 , 1.0407336 , 1.02886869, 1.02590697, 1.03826823,\n        1.03956086, 1.03453363, 1.03453363, 1.03883181, 1.03512714,\n        1.03667701, 1.03100601, 1.02989969, 1.02595957, 1.04439282,\n        1.02408327, 1.02015741, 1.04361125, 1.04063186, 1.04080262],\n...\n       [1.04103478, 1.04322278, 1.04325148, 1.02520374, 1.0359626 ,\n        1.03472587, 1.03502368, 1.02543084, 1.02533396, 1.02997671,\n        1.04077665, 1.03430304, 1.04302457, 1.03266   , 1.01554408,\n        1.0239759 , 1.02671655, 1.0286871 , 1.04513747, 1.04179966,\n        1.04356959, 1.02680824, 1.03461129, 1.03983388, 1.03351448,\n        1.04746627, 1.02477373, 1.03646658, 1.03788413, 1.03445808,\n        1.03217963, 1.03248744, 1.0365956 , 1.04834016, 1.03465328,\n        1.03758654, 1.03389671, 1.03230712, 1.02279208, 1.04928762,\n        1.03436474, 1.03844631, 1.03786748, 1.02599602, 1.03776602,\n        1.04112556, 1.02636374, 1.02195733, 1.02008185, 1.04620127,\n        1.02458014, 1.03060364, 1.04256305, 1.03797613, 1.02988314,\n        1.04270308, 1.05199891, 1.05300155, 1.02831959, 1.04272192,\n        1.03039981, 1.02977238, 1.03772149, 1.02228558, 1.02452136,\n        1.02765013, 1.03744861, 1.03905901, 1.0394399 , 1.03433761,\n        1.03930644, 1.03913732, 1.04865826, 1.02312903, 1.02830705,\n        1.03959438, 1.0299596 , 1.03621495, 1.03558578, 1.04290707,\n        1.04949933, 1.0340835 , 1.03859983, 1.01940209, 1.01604957,\n        1.06114584, 1.01929441, 1.04205031, 1.0330451 , 1.04419446,\n        1.03741772, 1.03750371, 1.0478201 , 1.0330621 , 1.024399  ,\n        1.03437696, 1.04842742, 1.04447585, 1.04447585, 1.04320541]])</pre></li><li>t(chain, draw)float640.36 0.357 0.3667 ... 0.3586 0.3572<pre>array([[0.36002316, 0.35702316, 0.36668759, 0.36402294, 0.36689874,\n        0.37665296, 0.35769152, 0.36081067, 0.3627508 , 0.36753119,\n        0.3526664 , 0.35771315, 0.35986785, 0.3572413 , 0.36223382,\n        0.36159817, 0.36147198, 0.36712611, 0.36649143, 0.35236587,\n        0.36673733, 0.36520124, 0.35985951, 0.36470028, 0.37044933,\n        0.36769124, 0.35805909, 0.36835551, 0.35981145, 0.36237733,\n        0.36117642, 0.36173311, 0.35431236, 0.35706855, 0.37073132,\n        0.36387229, 0.36055195, 0.3537677 , 0.36833552, 0.35373311,\n        0.37370278, 0.36038028, 0.37078336, 0.36849197, 0.36322323,\n        0.35657757, 0.36155539, 0.36356796, 0.36885433, 0.35827081,\n        0.36748367, 0.36851131, 0.36041486, 0.36352693, 0.35845668,\n        0.36012832, 0.36127154, 0.36549656, 0.3683766 , 0.36402575,\n        0.37231236, 0.34565851, 0.36202479, 0.34346859, 0.37141991,\n        0.35619988, 0.36677675, 0.37035672, 0.36582119, 0.36472186,\n        0.35699155, 0.35564032, 0.35419936, 0.35616359, 0.35653424,\n        0.36758029, 0.36042632, 0.3635686 , 0.36224494, 0.36151394,\n        0.3556078 , 0.35696774, 0.37042437, 0.36441932, 0.36150008,\n        0.36360763, 0.3598627 , 0.3598627 , 0.36541358, 0.36709582,\n        0.35872467, 0.36438473, 0.37107189, 0.36498477, 0.35182173,\n        0.37228162, 0.36558967, 0.36545039, 0.35350045, 0.35393407],\n...\n       [0.36068201, 0.35287809, 0.35800372, 0.36672622, 0.3642544 ,\n        0.36074393, 0.36196605, 0.36088691, 0.36189074, 0.36994147,\n        0.36553697, 0.36100974, 0.36357803, 0.36574991, 0.37355968,\n        0.37155125, 0.37171938, 0.36223318, 0.36527751, 0.36409842,\n        0.370136  , 0.36428903, 0.36606884, 0.35624959, 0.36026603,\n        0.35991051, 0.37347242, 0.35854406, 0.3563188 , 0.3605957 ,\n        0.36062555, 0.36693026, 0.36370894, 0.36157744, 0.36055559,\n        0.3653366 , 0.35561629, 0.36960979, 0.3700614 , 0.35190697,\n        0.36387779, 0.37089758, 0.36264334, 0.36586889, 0.36507969,\n        0.3631176 , 0.3690542 , 0.37153102, 0.36580037, 0.36512512,\n        0.35705987, 0.36742305, 0.36848197, 0.36594811, 0.35908756,\n        0.36494427, 0.34800037, 0.36009073, 0.35784272, 0.35805351,\n        0.37058914, 0.36026319, 0.35928466, 0.36690349, 0.36566931,\n        0.37540608, 0.35440629, 0.36669049, 0.36544568, 0.35666289,\n        0.36456775, 0.35730925, 0.35677242, 0.36415467, 0.36463694,\n        0.3576807 , 0.36715603, 0.3605471 , 0.35867614, 0.35733592,\n        0.36161015, 0.3551033 , 0.36224314, 0.37431198, 0.3798789 ,\n        0.34672903, 0.36584813, 0.36028267, 0.35600484, 0.36008824,\n        0.36243471, 0.37263302, 0.36665192, 0.35365528, 0.35953958,\n        0.35720674, 0.35069454, 0.35862755, 0.35862755, 0.35723115]])</pre></li><li>z(chain, draw)float640.5023 0.5046 ... 0.5006 0.501<pre>array([[0.50233382, 0.50458548, 0.50733958, 0.50665155, 0.51788623,\n        0.51362934, 0.49332001, 0.4994341 , 0.50350464, 0.50515302,\n        0.4969169 , 0.4977531 , 0.50050983, 0.49828953, 0.49981848,\n        0.4999385 , 0.50114025, 0.50560303, 0.5016649 , 0.49961726,\n        0.50843385, 0.50837957, 0.50289055, 0.49777198, 0.5001297 ,\n        0.49391261, 0.50351769, 0.49553933, 0.49591467, 0.50043658,\n        0.49352354, 0.49729404, 0.50235074, 0.4892659 , 0.51617052,\n        0.5036786 , 0.50186173, 0.50171496, 0.49763392, 0.49561556,\n        0.50973209, 0.50208059, 0.51341713, 0.51583445, 0.50846874,\n        0.49942273, 0.50785828, 0.49722289, 0.50114868, 0.50252307,\n        0.49469959, 0.49969866, 0.49089786, 0.50770441, 0.50937446,\n        0.49459954, 0.50245814, 0.49526879, 0.49248856, 0.49235128,\n        0.49463602, 0.49188259, 0.48918549, 0.48375187, 0.49605514,\n        0.50068539, 0.50292964, 0.50293154, 0.50418213, 0.50286567,\n        0.49163045, 0.49779552, 0.50055531, 0.4972943 , 0.4961044 ,\n        0.49294458, 0.49332683, 0.49437714, 0.49992495, 0.49703667,\n        0.49158067, 0.49345588, 0.49637954, 0.51046156, 0.4990523 ,\n        0.49956976, 0.50376754, 0.50376754, 0.51015905, 0.49354233,\n        0.49458828, 0.49746031, 0.49806691, 0.50005986, 0.49383017,\n        0.4983483 , 0.49673834, 0.50351222, 0.50234831, 0.50119036],\n...\n       [0.49633982, 0.49388017, 0.49206615, 0.50006659, 0.49654806,\n        0.50518397, 0.49149454, 0.49147507, 0.49197816, 0.50733534,\n        0.5030148 , 0.50252191, 0.50244386, 0.49413911, 0.48859711,\n        0.49422951, 0.4906043 , 0.50612945, 0.50633269, 0.50645166,\n        0.49953761, 0.50349394, 0.49632808, 0.50299515, 0.50908257,\n        0.50493998, 0.50592128, 0.5011213 , 0.50117779, 0.49223091,\n        0.50937452, 0.51207123, 0.50193362, 0.50021012, 0.50184064,\n        0.50964296, 0.49983281, 0.50104991, 0.50740345, 0.49988471,\n        0.50176293, 0.50407536, 0.50228269, 0.50541796, 0.5037549 ,\n        0.50515954, 0.50108667, 0.5012303 , 0.506339  , 0.50716384,\n        0.50423736, 0.50070616, 0.49730336, 0.49843317, 0.49780373,\n        0.51039321, 0.49209932, 0.49476061, 0.48845773, 0.48493998,\n        0.50665731, 0.49652217, 0.50076941, 0.492187  , 0.49010194,\n        0.49263649, 0.48760069, 0.50304961, 0.50152254, 0.49190829,\n        0.49957273, 0.50304065, 0.50724354, 0.48608888, 0.49557844,\n        0.49376845, 0.50828105, 0.50357368, 0.49985038, 0.4947304 ,\n        0.50039002, 0.50517632, 0.50129078, 0.50830349, 0.5113343 ,\n        0.49853889, 0.49373094, 0.49349021, 0.49239869, 0.49572721,\n        0.4975703 , 0.500381  , 0.49938517, 0.50507508, 0.50855691,\n        0.49795501, 0.50100637, 0.50056452, 0.50056452, 0.50104562]])</pre></li><li>v(chain, draw)float640.3759 0.3925 ... 0.3932 0.4054<pre>array([[0.37589375, 0.39253405, 0.37310232, 0.36511301, 0.35530862,\n        0.35175118, 0.40444321, 0.39116693, 0.36790589, 0.36369098,\n        0.40492991, 0.35246161, 0.42109002, 0.38203025, 0.3919095 ,\n        0.37448356, 0.40163689, 0.37264731, 0.38176226, 0.39555121,\n        0.37465874, 0.35461171, 0.36940893, 0.36678576, 0.41602056,\n        0.41030966, 0.38414056, 0.37853414, 0.37598615, 0.37531079,\n        0.40466999, 0.41171474, 0.37812261, 0.39592325, 0.3255609 ,\n        0.36173514, 0.36467424, 0.38863104, 0.37144594, 0.40778382,\n        0.34250503, 0.39336326, 0.33878342, 0.35361973, 0.35759332,\n        0.37319319, 0.37564084, 0.39921382, 0.3509756 , 0.4055952 ,\n        0.36995836, 0.3841274 , 0.3743439 , 0.35954703, 0.34988689,\n        0.41158355, 0.36246311, 0.40683738, 0.37068226, 0.36749236,\n        0.39203401, 0.38510614, 0.43463693, 0.39798888, 0.38000086,\n        0.40359018, 0.37034659, 0.37902222, 0.38716257, 0.37976117,\n        0.39736393, 0.38822234, 0.38770388, 0.3855561 , 0.40814959,\n        0.40270105, 0.38847429, 0.38311259, 0.39533717, 0.39430782,\n        0.38863902, 0.39629953, 0.41781871, 0.3598848 , 0.37569056,\n        0.38504612, 0.37717697, 0.37717697, 0.35773391, 0.38971661,\n        0.3853076 , 0.38554165, 0.37346983, 0.36193322, 0.38258068,\n        0.38554133, 0.38184663, 0.38104824, 0.3827102 , 0.38839219],\n...\n       [0.35370827, 0.39460775, 0.36913276, 0.40336639, 0.39902424,\n        0.35769589, 0.43289765, 0.36982691, 0.37142862, 0.39723598,\n        0.38522646, 0.36578433, 0.39413473, 0.40736378, 0.39213239,\n        0.42698707, 0.40290035, 0.36236613, 0.38021375, 0.39245614,\n        0.35929571, 0.39408393, 0.35874275, 0.39573233, 0.36322707,\n        0.37011632, 0.37005502, 0.39050643, 0.36311121, 0.40814435,\n        0.36382353, 0.36368029, 0.38566091, 0.39339282, 0.38413635,\n        0.3565246 , 0.3771162 , 0.36504007, 0.3722897 , 0.37708626,\n        0.3777683 , 0.37079233, 0.3990545 , 0.371584  , 0.39556655,\n        0.36943204, 0.38846758, 0.38778262, 0.36959575, 0.38684959,\n        0.37999093, 0.34955043, 0.38327731, 0.38327066, 0.39300016,\n        0.35210426, 0.42802736, 0.4185086 , 0.40047548, 0.42246513,\n        0.36900455, 0.38417206, 0.38794087, 0.40159   , 0.40754125,\n        0.42416478, 0.38887605, 0.37696653, 0.38582169, 0.39553987,\n        0.38381104, 0.38100534, 0.37058439, 0.40834227, 0.39590276,\n        0.40481456, 0.36567249, 0.37907096, 0.39234387, 0.36353667,\n        0.3708446 , 0.38456445, 0.37254365, 0.3669524 , 0.37507727,\n        0.3968363 , 0.39711521, 0.39838464, 0.40198097, 0.39029163,\n        0.39511603, 0.37082478, 0.36667596, 0.37270698, 0.36217544,\n        0.38565219, 0.38633708, 0.39318558, 0.39318558, 0.40543479]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n      dtype='int64', name='draw'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-26T23:56:23.941126+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :32.72891592979431tuning_steps :100modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> <p>We allowed the sampler very little tuning, and and therefore our initial values are still apparent in the chains.</p> In\u00a0[8]: Copied! <pre>model._initvals\n</pre> model._initvals Out[8]: <pre>{'a': array(1.5), 't': array(0.025), 'z': array(0.5), 'v': array(0.)}</pre> In\u00a0[9]: Copied! <pre>model._initvals[\"z\"] = np.array(0.6, dtype=\"float32\")\nidata2 = model.sample(draws=10, tune=1)\n</pre> model._initvals[\"z\"] = np.array(0.6, dtype=\"float32\") idata2 = model.sample(draws=10, tune=1) <pre>Using default initvals. \n\nThe model has already been sampled. Overwriting the previous inference object. Any previous reference to the inference object will still point to the old object.\n</pre> <pre>Only 10 samples per chain. Reliable r-hat and ESS diagnostics require longer chains for accurate estimate.\nInitializing NUTS using adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [a, t, z, v]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 1 tune and 10 draw iterations (4 + 40 draws total) took 1 seconds.\nThere were 40 divergences after tuning. Increase `target_accept` or reparameterize.\nThe number of samples is too small to check convergence reliably.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:00&lt;00:00, 54.07it/s]\n</pre> In\u00a0[10]: Copied! <pre>idata2\n</pre> idata2 Out[10]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 1kB\nDimensions:  (chain: 4, draw: 10)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 80B 0 1 2 3 4 5 6 7 8 9\nData variables:\n    a        (chain, draw) float64 320B 1.5 1.5 1.5 1.5 1.5 ... 1.5 1.5 1.5 1.5\n    t        (chain, draw) float64 320B 0.025 0.025 0.025 ... 0.025 0.025 0.025\n    z        (chain, draw) float64 320B 0.6 0.6 0.6 0.6 0.6 ... 0.6 0.6 0.6 0.6\n    v        (chain, draw) float64 320B 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\nAttributes:\n    created_at:                  2025-09-26T23:56:38.654807+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               0.9042541980743408\n    tuning_steps:                1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 10</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 6 7 8 9<pre>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre></li></ul></li><li>Data variables: (4)<ul><li>a(chain, draw)float641.5 1.5 1.5 1.5 ... 1.5 1.5 1.5 1.5<pre>array([[1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5],\n       [1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5],\n       [1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5],\n       [1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5]])</pre></li><li>t(chain, draw)float640.025 0.025 0.025 ... 0.025 0.025<pre>array([[0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n        0.025],\n       [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n        0.025],\n       [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n        0.025],\n       [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n        0.025]])</pre></li><li>z(chain, draw)float640.6 0.6 0.6 0.6 ... 0.6 0.6 0.6 0.6<pre>array([[0.60000002, 0.60000002, 0.60000002, 0.60000002, 0.60000002,\n        0.60000002, 0.60000002, 0.60000002, 0.60000002, 0.60000002],\n       [0.60000002, 0.60000002, 0.60000002, 0.60000002, 0.60000002,\n        0.60000002, 0.60000002, 0.60000002, 0.60000002, 0.60000002],\n       [0.60000002, 0.60000002, 0.60000002, 0.60000002, 0.60000002,\n        0.60000002, 0.60000002, 0.60000002, 0.60000002, 0.60000002],\n       [0.60000002, 0.60000002, 0.60000002, 0.60000002, 0.60000002,\n        0.60000002, 0.60000002, 0.60000002, 0.60000002, 0.60000002]])</pre></li><li>v(chain, draw)float640.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0<pre>array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64', name='draw'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-26T23:56:38.654807+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :0.9042541980743408tuning_steps :1modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 1MB\nDimensions:      (chain: 4, draw: 10, __obs__: 3988)\nCoordinates:\n  * chain        (chain) int64 32B 0 1 2 3\n  * draw         (draw) int64 80B 0 1 2 3 4 5 6 7 8 9\n  * __obs__      (__obs__) int64 32kB 0 1 2 3 4 5 ... 3983 3984 3985 3986 3987\nData variables:\n    rt,response  (chain, draw, __obs__) float64 1MB -1.634 -1.914 ... -1.662\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 10</li><li>__obs__: 3988</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 6 7 8 9<pre>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre></li><li>__obs__(__obs__)int640 1 2 3 4 ... 3984 3985 3986 3987<pre>array([   0,    1,    2, ..., 3985, 3986, 3987], shape=(3988,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-1.634 -1.914 ... -2.424 -1.662<pre>array([[[-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379],\n        [-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379],\n        [-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379],\n        ...,\n        [-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379],\n        [-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379],\n        [-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379]],\n\n       [[-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379],\n        [-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379],\n        [-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379],\n...\n        [-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379],\n        [-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379],\n        [-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379]],\n\n       [[-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379],\n        [-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379],\n        [-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379],\n        ...,\n        [-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379],\n        [-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379],\n        [-1.63365888, -1.91400975, -1.50112679, ..., -1.32999482,\n         -2.42357901, -1.66225379]]], shape=(4, 10, 3988))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64', name='draw'))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987],\n      dtype='int64', name='__obs__', length=3988))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 5kB\nDimensions:                (chain: 4, draw: 10)\nCoordinates:\n  * chain                  (chain) int64 32B 0 1 2 3\n  * draw                   (draw) int64 80B 0 1 2 3 4 5 6 7 8 9\nData variables: (12/18)\n    reached_max_treedepth  (chain, draw) bool 40B False False ... False False\n    step_size_bar          (chain, draw) float64 320B 0.4128 0.4128 ... 0.4128\n    n_steps                (chain, draw) float64 320B 1.0 1.0 1.0 ... 1.0 1.0\n    smallest_eigval        (chain, draw) float64 320B nan nan nan ... nan nan\n    acceptance_rate        (chain, draw) float64 320B 0.0 0.0 0.0 ... 0.0 0.0\n    divergences            (chain, draw) int64 320B 1 2 3 4 5 6 ... 5 6 7 8 9 10\n    ...                     ...\n    step_size              (chain, draw) float64 320B 0.4128 0.4128 ... 0.4128\n    tree_depth             (chain, draw) int64 320B 1 1 1 1 1 1 ... 1 1 1 1 1 1\n    perf_counter_start     (chain, draw) float64 320B 1.434e+06 ... 1.434e+06\n    largest_eigval         (chain, draw) float64 320B nan nan nan ... nan nan\n    lp                     (chain, draw) float64 320B -7.481e+03 ... -7.481e+03\n    diverging              (chain, draw) bool 40B True True True ... True True\nAttributes:\n    created_at:                  2025-09-26T23:56:38.675816+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               0.9042541980743408\n    tuning_steps:                1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 10</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 6 7 8 9<pre>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre></li></ul></li><li>Data variables: (18)<ul><li>reached_max_treedepth(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, False, False, False, False, False, False,\n        False],\n       [False, False, False, False, False, False, False, False, False,\n        False],\n       [False, False, False, False, False, False, False, False, False,\n        False],\n       [False, False, False, False, False, False, False, False, False,\n        False]])</pre></li><li>step_size_bar(chain, draw)float640.4128 0.4128 ... 0.4128 0.4128<pre>array([[0.41278504, 0.41278504, 0.41278504, 0.41278504, 0.41278504,\n        0.41278504, 0.41278504, 0.41278504, 0.41278504, 0.41278504],\n       [0.41278504, 0.41278504, 0.41278504, 0.41278504, 0.41278504,\n        0.41278504, 0.41278504, 0.41278504, 0.41278504, 0.41278504],\n       [0.41278504, 0.41278504, 0.41278504, 0.41278504, 0.41278504,\n        0.41278504, 0.41278504, 0.41278504, 0.41278504, 0.41278504],\n       [0.41278504, 0.41278504, 0.41278504, 0.41278504, 0.41278504,\n        0.41278504, 0.41278504, 0.41278504, 0.41278504, 0.41278504]])</pre></li><li>n_steps(chain, draw)float641.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0<pre>array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])</pre></li><li>smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])</pre></li><li>acceptance_rate(chain, draw)float640.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0<pre>array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])</pre></li><li>divergences(chain, draw)int641 2 3 4 5 6 7 8 ... 4 5 6 7 8 9 10<pre>array([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n       [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n       [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n       [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])</pre></li><li>energy(chain, draw)float647.485e+03 7.485e+03 ... 7.481e+03<pre>array([[7485.23253296, 7485.02646663, 7481.93902941, 7480.93404957,\n        7482.35913742, 7483.17894291, 7481.54907115, 7485.76510367,\n        7484.14881867, 7481.63102838],\n       [7481.79100856, 7481.03072747, 7483.01265022, 7484.03223961,\n        7483.22140088, 7481.7853693 , 7485.74593843, 7482.59489603,\n        7482.3542718 , 7482.2836188 ],\n       [7487.59991544, 7481.539434  , 7481.80909876, 7481.33255204,\n        7480.57410683, 7481.93696592, 7482.37473515, 7481.06845727,\n        7482.58471509, 7481.15969068],\n       [7481.95843325, 7484.04629209, 7481.69170096, 7480.61514541,\n        7481.40565853, 7483.84164373, 7481.23802368, 7484.14168886,\n        7481.258519  , 7480.82330174]])</pre></li><li>max_energy_error(chain, draw)float647.392e+05 1.118e+06 ... 6.212e+05<pre>array([[ 739192.06236725, 1117933.2470587 , 1173238.56786509,\n         652780.11022922,  279881.61411803,  151296.01230375,\n         627080.02411546,  859654.70548988,  326091.38597291,\n         375880.57052356],\n       [ 674779.38912319,  887848.54247649,  717211.10184111,\n         734223.13151647,  634712.61982022, 1142635.75412388,\n         128848.93435807,  641458.30183052, 1177307.74809145,\n         302430.60011219],\n       [ 257851.78122197,  248069.18469548,  713147.08822688,\n         669344.60017949,  850289.64797179, 1099446.24643068,\n         508219.86934887, 1169266.65598598,  747387.89091576,\n         578923.33975482],\n       [ 613552.80492106,  710375.04344807,  948655.51361823,\n         742326.07247406,  734373.23979327,  102002.6293046 ,\n         501635.61091283,  625947.21010879, 1192097.92959953,\n         621248.5742249 ]])</pre></li><li>process_time_diff(chain, draw)float640.03584 0.03217 ... 0.03303 0.04273<pre>array([[0.03584 , 0.032169, 0.03409 , 0.03172 , 0.031542, 0.032737,\n        0.032919, 0.032352, 0.03208 , 0.038072],\n       [0.03521 , 0.035514, 0.032693, 0.03286 , 0.032003, 0.032712,\n        0.031336, 0.033698, 0.032398, 0.032961],\n       [0.047858, 0.032577, 0.033329, 0.033257, 0.032626, 0.033093,\n        0.031754, 0.032365, 0.031865, 0.032853],\n       [0.292461, 0.032625, 0.033763, 0.032519, 0.032903, 0.032096,\n        0.032735, 0.031145, 0.033035, 0.042732]])</pre></li><li>energy_error(chain, draw)float640.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0<pre>array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])</pre></li><li>index_in_trajectory(chain, draw)int640 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0<pre>array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</pre></li><li>perf_counter_diff(chain, draw)float640.08749 0.03239 ... 0.03423 0.09842<pre>array([[0.08748613, 0.03239379, 0.03431313, 0.03197504, 0.03164546,\n        0.03320592, 0.03346863, 0.03245804, 0.03321004, 0.06870529],\n       [0.10135708, 0.03834317, 0.03277304, 0.03331492, 0.03234342,\n        0.03317387, 0.03147021, 0.03437346, 0.03255738, 0.03399133],\n       [0.13606542, 0.03269958, 0.03342513, 0.03414992, 0.03267175,\n        0.03375533, 0.03195833, 0.03303488, 0.03255946, 0.03416425],\n       [0.11234692, 0.03288675, 0.03421225, 0.03277579, 0.03318575,\n        0.03225338, 0.03359296, 0.03123125, 0.03422683, 0.09841692]])</pre></li><li>step_size(chain, draw)float640.4128 0.4128 ... 0.4128 0.4128<pre>array([[0.41278504, 0.41278504, 0.41278504, 0.41278504, 0.41278504,\n        0.41278504, 0.41278504, 0.41278504, 0.41278504, 0.41278504],\n       [0.41278504, 0.41278504, 0.41278504, 0.41278504, 0.41278504,\n        0.41278504, 0.41278504, 0.41278504, 0.41278504, 0.41278504],\n       [0.41278504, 0.41278504, 0.41278504, 0.41278504, 0.41278504,\n        0.41278504, 0.41278504, 0.41278504, 0.41278504, 0.41278504],\n       [0.41278504, 0.41278504, 0.41278504, 0.41278504, 0.41278504,\n        0.41278504, 0.41278504, 0.41278504, 0.41278504, 0.41278504]])</pre></li><li>tree_depth(chain, draw)int641 1 1 1 1 1 1 1 ... 1 1 1 1 1 1 1 1<pre>array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])</pre></li><li>perf_counter_start(chain, draw)float641.434e+06 1.434e+06 ... 1.434e+06<pre>array([[1434015.94783158, 1434016.03572479, 1434016.06839008,\n        1434016.10289458, 1434016.13510363, 1434016.16697204,\n        1434016.20039758, 1434016.23412058, 1434016.26682342,\n        1434016.30022825],\n       [1434015.90315129, 1434016.00498854, 1434016.04359975,\n        1434016.07657529, 1434016.11007992, 1434016.14261096,\n        1434016.17600513, 1434016.20769879, 1434016.24227279,\n        1434016.27499975],\n       [1434015.88459058, 1434016.02101929, 1434016.05397554,\n        1434016.08766613, 1434016.1220415 , 1434016.15489321,\n        1434016.18885292, 1434016.22100758, 1434016.25424746,\n        1434016.287042  ],\n       [1434015.92745812, 1434016.04019312, 1434016.07328292,\n        1434016.10774792, 1434016.14077379, 1434016.17420925,\n        1434016.20667483, 1434016.24046762, 1434016.27193908,\n        1434016.30637867]])</pre></li><li>largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])</pre></li><li>lp(chain, draw)float64-7.481e+03 ... -7.481e+03<pre>array([[-7480.5081219, -7480.5081219, -7480.5081219, -7480.5081219,\n        -7480.5081219, -7480.5081219, -7480.5081219, -7480.5081219,\n        -7480.5081219, -7480.5081219],\n       [-7480.5081219, -7480.5081219, -7480.5081219, -7480.5081219,\n        -7480.5081219, -7480.5081219, -7480.5081219, -7480.5081219,\n        -7480.5081219, -7480.5081219],\n       [-7480.5081219, -7480.5081219, -7480.5081219, -7480.5081219,\n        -7480.5081219, -7480.5081219, -7480.5081219, -7480.5081219,\n        -7480.5081219, -7480.5081219],\n       [-7480.5081219, -7480.5081219, -7480.5081219, -7480.5081219,\n        -7480.5081219, -7480.5081219, -7480.5081219, -7480.5081219,\n        -7480.5081219, -7480.5081219]])</pre></li><li>diverging(chain, draw)boolTrue True True ... True True True<pre>array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True],\n       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True],\n       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True],\n       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64', name='draw'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-26T23:56:38.675816+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :0.9042541980743408tuning_steps :1modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 96kB\nDimensions:                  (__obs__: 3988, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 32kB 0 1 2 3 ... 3985 3986 3987\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 64kB ...\nAttributes:\n    created_at:                  2025-09-26T23:56:38.678738+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 3988</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 ... 3984 3985 3986 3987<pre>array([   0,    1,    2, ..., 3985, 3986, 3987], shape=(3988,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float641.21 1.0 1.63 1.0 ... -1.0 1.25 1.0<pre>array([[ 1.21 ,  1.   ],\n       [ 1.63 ,  1.   ],\n       [ 1.03 ,  1.   ],\n       ...,\n       [ 0.784,  1.   ],\n       [ 2.35 , -1.   ],\n       [ 1.25 ,  1.   ]], shape=(3988, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987],\n      dtype='int64', name='__obs__', length=3988))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (6)created_at :2025-09-26T23:56:38.678738+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> <p>While models can become a lot more complicated, you will be able to adjust initial values via this process consistently.</p> In\u00a0[11]: Copied! <pre>model = hssm.HSSM(\n    data=cav_data,\n    model=\"ddm\",\n    link_settings=\"log_logit\",\n    loglik_kind=\"approx_differentiable\",\n    include=[{\"name\": \"a\", \"formula\": \"a ~ 1 + theta\"}],\n)\n</pre> model = hssm.HSSM(     data=cav_data,     model=\"ddm\",     link_settings=\"log_logit\",     loglik_kind=\"approx_differentiable\",     include=[{\"name\": \"a\", \"formula\": \"a ~ 1 + theta\"}], ) <pre>Model initialized successfully.\n</pre> In\u00a0[12]: Copied! <pre>model.initvals\n</pre> model.initvals Out[12]: <pre>{'v': array(0.),\n 't': array(0.025),\n 'z': array(0.5),\n 'a_Intercept': array(0.),\n 'a_theta': array(0.)}</pre> <p>Well, <code>a_Intercept</code> gets a default initial value of <code>0</code>?</p> <p><code>a</code> is the boundary separation parameter in the drift diffusion model, a setting of <code>0</code> seems like an extremely bad choice... It would lead to a pointmass <code>rt</code> at <code>0</code>s!</p> <p>What's going on here? Let's expect our model...</p> In\u00a0[13]: Copied! <pre>model\n</pre> model Out[13]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: approx_differentiable\nObservations: 3988\n\nParameters:\n\nv:\n    Prior: Uniform(lower: -3.0, upper: 3.0)\n    Explicit bounds: (-3.0, 3.0)\n (ignored due to link function)\na:\n    Formula: a ~ 1 + theta\n    Priors:\n        a_Intercept ~ Normal(mu: 0.0, sigma: 0.25)\n        a_theta ~ Normal(mu: 0.0, sigma: 0.25)\n    Link: Generalized logit link function with bounds (0.3, 2.5)\n    Explicit bounds: (0.3, 2.5)\n (ignored due to link function)\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n (ignored due to link function)\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, 2.0)\n (ignored due to link function)\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> <p>Ah, we apply the Generalized Logit link to the <code>a</code> parameters. The initial value of the actual parameter <code>a</code> that the likelihood will receive, is the output of the transformation.</p> <p>Our <code>hssm</code> model class includes everything we need to inspect this further.</p> <p>What would we expect here?</p> <p>The generalized logit transformation (link), has the associated generalized sigmoid transformation (inverse link) as the forward transform. We expect that, evaluating this transformation at <code>0</code>, should give back the mean value between the explicit bounds we set for the parameter.</p> <p>So we expect: $(0.3 + 2.5) / 2 = 1.4$... let's check this.</p> <p>Note:</p> <p>The <code>forward</code> link, (from parameter to function output that the likelihood receives), is call inverse link function in GLM lingo. We follow this nomenclature in HSSM.</p> In\u00a0[14]: Copied! <pre>model.params[\"a\"].link.linkinv(model._initvals[\"a_Intercept\"])\n</pre> model.params[\"a\"].link.linkinv(model._initvals[\"a_Intercept\"]) Out[14]: <pre>np.float64(1.4000000000000001)</pre> <p>Voila! This checks out! We note that the <code>linkinv()</code> function can come in handy if you want to play around with initial value setting in the context of using link functions yourself.</p> <p>We try to guide the initial value settings in HSSM, with reasonble defaults that hopefully work in many cases without needed further adjustments. It is however difficulty to find settings that work blindly.</p> <p>We follow the guidelines below:</p> <ol> <li>Avoid known issues near parameter boundaries(especially important for <code>approx_differentiable</code> likelihoods) --&gt; whenever possible, initialize near the center of bounded parameter spaces</li> <li>Starting values for the <code>t</code> parameter should be low to avoid known pathologies of the <code>analytic</code> DDM likelihood when the smallest reaction times (<code>rt</code>) values come close to <code>t</code></li> <li>In a regression setting, minimize the spread of <code>offset</code> parameters to avoid inadvertently running into parameter limits and initialize all but the <code>Intercept</code> parameter to <code>0</code></li> </ol> <p>NOTE:</p> <p>Especially guideline 3. is a very conservative setting, focused solely on avoiding boundary behavior. This will NOT always be a smart idea, and it may sometimes collaterally have a negative impact on convergence.</p> <p>We encourage users to actively play with initial value setting at the moment.</p> In\u00a0[15]: Copied! <pre>cav_data\n</pre> cav_data Out[15]: participant_id stim rt response theta dbs conf 0 0 LL 1.210 1.0 0.656275 1 HC 1 0 WL 1.630 1.0 -0.327889 1 LC 2 0 WW 1.030 1.0 -0.480285 1 HC 3 0 WL 2.770 1.0 1.927427 1 LC 4 0 WW 1.140 -1.0 -0.213236 1 HC ... ... ... ... ... ... ... ... 3983 13 LL 1.450 -1.0 -1.237166 0 HC 3984 13 WL 0.711 1.0 -0.377450 0 LC 3985 13 WL 0.784 1.0 -0.694194 0 LC 3986 13 LL 2.350 -1.0 -0.546536 0 HC 3987 13 WW 1.250 1.0 0.752388 0 HC <p>3988 rows \u00d7 7 columns</p> In\u00a0[16]: Copied! <pre>model_reg = hssm.HSSM(\n    data=cav_data,\n    model=\"ddm\",\n    prior_settings=\"safe\",\n    loglik_kind=\"approx_differentiable\",\n    include=[{\"name\": \"a\", \"formula\": \"a ~ 1 + theta + (1|participant_id)\"}],\n)\n</pre> model_reg = hssm.HSSM(     data=cav_data,     model=\"ddm\",     prior_settings=\"safe\",     loglik_kind=\"approx_differentiable\",     include=[{\"name\": \"a\", \"formula\": \"a ~ 1 + theta + (1|participant_id)\"}], ) <pre>Model initialized successfully.\n</pre> In\u00a0[17]: Copied! <pre>model_reg.initvals\n</pre> model_reg.initvals Out[17]: <pre>{'v': array(0.),\n 't': array(0.025),\n 'z': array(0.5),\n 'a_Intercept': array(1.5),\n 'a_theta': array(0.),\n 'a_1|participant_id_sigma': array(0.27082359),\n 'a_1|participant_id_offset': array([ 0.00542568,  0.00583739,  0.00903185,  0.00035243,  0.00880101,\n         0.0073259 ,  0.00826543, -0.00877374,  0.00663986, -0.00258646,\n        -0.00104824,  0.00168488,  0.00347628,  0.00964669])}</pre> <p>Let's discuss what we see here:</p> <ol> <li>We do NOT apply a link function in this case, so guideline 1 applies without further cognitive effort. By contrast to the previous example, check how <code>a_Intercept</code> is now directly initialized as <code>1.5</code>, near the middle of the parameter range.</li> <li>Covariate betas, in this case <code>a_theta</code> as initialized to <code>0</code></li> <li>The <code>offset</code> parameters associated with individual parameters in group hierarchies are initialized cloe to <code>0</code></li> <li><code>t</code> is initialized close to <code>0</code> as per guideline 2.</li> <li>The remaining parameters are set close to or at the middle of the allowed parameters space (if bounded)</li> </ol> <p>For parameters that we do not actively manipulate, PyMC (BAMBI) defaults are applied unchanged (e.g. here: <code>a_1|participant_id_sigma</code>). These setting may sometimes be sub-optimal for applications in HSSM, hence we again caution the user to take an active approach towards investiging initial values in case of convergence issues.</p> <p>There are two keyword arguments (<code>kwargs</code>) that we can set in the the base <code>HSSM</code> class.</p> <ol> <li><code>process_initvals: bool</code> turns processing of initial values on and off</li> <li><code>initval_jitter: float</code> which applies an uniform jitter around vector values initial values</li> </ol> In\u00a0[18]: Copied! <pre>model_no_initval = hssm.HSSM(\n    data=cav_data,\n    model=\"ddm\",\n    loglik_kind=\"approx_differentiable\",\n    include=[{\"name\": \"a\", \"formula\": \"a ~ 1 + theta + (1|participant_id)\"}],\n    process_initvals=False,\n)\n</pre> model_no_initval = hssm.HSSM(     data=cav_data,     model=\"ddm\",     loglik_kind=\"approx_differentiable\",     include=[{\"name\": \"a\", \"formula\": \"a ~ 1 + theta + (1|participant_id)\"}],     process_initvals=False, ) <pre>Model initialized successfully.\n</pre> <p>We can see the result of turning initial value processing off (NOT recommended if you want to use defaults at all). This applies BAMBI/PyMC defaults, and adds default jitter to vector valued parameters.</p> In\u00a0[19]: Copied! <pre>model_no_initval.initvals\n</pre> model_no_initval.initvals Out[19]: <pre>{'v': array(0.),\n 't': array(2.),\n 'z': array(0.5),\n 'a_Intercept': array(1.4),\n 'a_theta': array(0.),\n 'a_1|participant_id_sigma': array(0.27082359),\n 'a_1|participant_id_offset': array([-0.00379855,  0.00473771, -0.00496006,  0.00898365, -0.00593145,\n         0.00381492, -0.00238199,  0.00792207,  0.00214259,  0.00650719,\n        -0.00650773, -0.00227739, -0.00743215, -0.00412257])}</pre> <p>If you want to change the magnitude of the default jitter, you can manipulate it via the <code>initval_jitter</code> argument.</p> In\u00a0[20]: Copied! <pre>model_jitter = hssm.HSSM(\n    data=cav_data,\n    model=\"ddm\",\n    loglik_kind=\"approx_differentiable\",\n    include=[{\"name\": \"a\",\n              \"formula\": \"a ~ 1 + theta + (1|participant_id)\"}],\n    process_initvals=False,\n    initval_jitter=0.5,\n)\n</pre> model_jitter = hssm.HSSM(     data=cav_data,     model=\"ddm\",     loglik_kind=\"approx_differentiable\",     include=[{\"name\": \"a\",               \"formula\": \"a ~ 1 + theta + (1|participant_id)\"}],     process_initvals=False,     initval_jitter=0.5, ) <pre>Model initialized successfully.\n</pre> In\u00a0[21]: Copied! <pre>model_jitter.initvals\n</pre> model_jitter.initvals Out[21]: <pre>{'v': array(0.),\n 't': array(2.),\n 'z': array(0.5),\n 'a_Intercept': array(1.4),\n 'a_theta': array(0.),\n 'a_1|participant_id_sigma': array(0.27082359),\n 'a_1|participant_id_offset': array([ 0.07527246, -0.08141247, -0.07274412,  0.42149583, -0.34847635,\n         0.27248493,  0.32527581, -0.14320281,  0.2295596 , -0.06499428,\n        -0.37002045, -0.00796513, -0.35770026, -0.40223777])}</pre>"},{"location":"tutorials/initial_values/#initial-values","title":"Initial Values\u00b6","text":""},{"location":"tutorials/initial_values/#load-up-some-data","title":"Load up some data\u00b6","text":""},{"location":"tutorials/initial_values/#simple-model","title":"Simple model\u00b6","text":""},{"location":"tutorials/initial_values/#set-initial-values","title":"Set Initial Values\u00b6","text":"<p>We illustrate the preferred way of setting initial values manually.</p>"},{"location":"tutorials/initial_values/#route-1","title":"Route 1\u00b6","text":"<p>Define a custom dictionary and pass it to the sampler.</p>"},{"location":"tutorials/initial_values/#route-2","title":"Route 2\u00b6","text":"<p>Adjust the <code>._initval</code> attribute. <code>model.initvals</code> is a class property, which serves as an accessor the the underlying <code>._initvals</code> attribute. We can adjust this directly, and it will be used as the default for our sampler.</p>"},{"location":"tutorials/initial_values/#link-function","title":"Link function\u00b6","text":"<p>Setting initial values for parameters when working with link functions becomes a little more tricky. Below is an example.</p> <p>Let's define a simple regression with <code>logit</code> link functions.</p>"},{"location":"tutorials/initial_values/#hssms-initial-value-defaults-logic","title":"HSSM's initial value defaults logic\u00b6","text":""},{"location":"tutorials/initial_values/#example","title":"Example\u00b6","text":""},{"location":"tutorials/initial_values/#parameters","title":"Parameters\u00b6","text":""},{"location":"tutorials/jax_callable_contribution_onnx_example/","title":"Construct Custom Models from simulators and JAX callables","text":"In\u00a0[1]: Copied! <pre>import os\nimport pickle\n\nimport arviz as az\nimport jax.numpy as jnp\nimport lanfactory as lf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc as pm\n\nimport hssm\nfrom hssm.config import ModelConfig\n</pre> import os import pickle  import arviz as az import jax.numpy as jnp import lanfactory as lf import matplotlib.pyplot as plt import numpy as np import pymc as pm  import hssm from hssm.config import ModelConfig <pre>wandb not available\nwandb not available\n</pre> In\u00a0[2]: Copied! <pre># simulate some data from the model\nSEED = 123\nobs_ddm = hssm.simulate_data(\n    theta=dict(v=0.40,\n               a=1.25,\n               t=0.2,\n               z=0.5),\n    model=\"ddm\",\n    size=500,\n    random_state = SEED,\n)\n</pre> # simulate some data from the model SEED = 123 obs_ddm = hssm.simulate_data(     theta=dict(v=0.40,                a=1.25,                t=0.2,                z=0.5),     model=\"ddm\",     size=500,     random_state = SEED, ) In\u00a0[3]: Copied! <pre># Loaded Net\njax_infer = lf.trainers.MLPJaxFactory(\n    network_config=pickle.load(\n        open(\n            os.path.join(\n                \"data\", \"jax_models\", \"ddm\", \"ddm_jax_lan_network_config.pickle\"\n            ),\n            \"rb\",\n        )\n    ),\n    train=False,\n)\n\njax_mlp_forward, _ = jax_infer.make_forward_partial(\n    seed=42,\n    input_dim=4 + 2,  # n-parameters (v,a,z,t) + n-data (rts and choices)\n    state=os.path.join(\"data\", \"jax_models\", \"ddm\", \"ddm_jax_lan_train_state.jax\"),\n    add_jitted=True,\n)\n</pre> # Loaded Net jax_infer = lf.trainers.MLPJaxFactory(     network_config=pickle.load(         open(             os.path.join(                 \"data\", \"jax_models\", \"ddm\", \"ddm_jax_lan_network_config.pickle\"             ),             \"rb\",         )     ),     train=False, )  jax_mlp_forward, _ = jax_infer.make_forward_partial(     seed=42,     input_dim=4 + 2,  # n-parameters (v,a,z,t) + n-data (rts and choices)     state=os.path.join(\"data\", \"jax_models\", \"ddm\", \"ddm_jax_lan_train_state.jax\"),     add_jitted=True, ) In\u00a0[4]: Copied! <pre># Testing the signature of the JAX function 2\nn_dim_model_parameters = 4\nn_dim_data = 2\nn_trials = 1\nin_ = jnp.zeros((n_trials,\n                 n_dim_model_parameters + n_dim_data))\nout = jax_mlp_forward(in_)\nprint(out.shape)\n</pre> # Testing the signature of the JAX function 2 n_dim_model_parameters = 4 n_dim_data = 2 n_trials = 1 in_ = jnp.zeros((n_trials,                  n_dim_model_parameters + n_dim_data)) out = jax_mlp_forward(in_) print(out.shape) <pre>(1, 1)\n</pre> In\u00a0[5]: Copied! <pre>from hssm.distribution_utils.jax import make_jax_single_trial_logp_from_network_forward\nimport jax.numpy as jnp\njax_logp = make_jax_single_trial_logp_from_network_forward(jax_forward_fn = jax_mlp_forward,\n                                                           params_only = False)\n</pre> from hssm.distribution_utils.jax import make_jax_single_trial_logp_from_network_forward import jax.numpy as jnp jax_logp = make_jax_single_trial_logp_from_network_forward(jax_forward_fn = jax_mlp_forward,                                                            params_only = False) In\u00a0[6]: Copied! <pre># Test call\njax_logp(jnp.zeros((2)),\n         jnp.array([1.0]),\n         jnp.array([1.5]),\n         jnp.array([0.5]),\n         jnp.array([0.3]))\n</pre> # Test call jax_logp(jnp.zeros((2)),          jnp.array([1.0]),          jnp.array([1.5]),          jnp.array([0.5]),          jnp.array([0.3])) Out[6]: <pre>Array(-20.01882519, dtype=float64)</pre> In\u00a0[7]: Copied! <pre>from ssms.hssm_support import hssm_sim_wrapper, decorate_atomic_simulator\nfrom functools import partial\nfrom ssms.basic_simulators.simulator import simulator\n\nrv_ready_simulator = partial(hssm_sim_wrapper, \n                             simulator_fun = simulator,\n                             model = \"ddm\", \n                             n_replicas = 1) # AF-TODO: n_replicas should default to 1 instead of being required\n\n# We decorate the simulator to attach some metadata\n# that HSSM can use\ndecorated_simulator = decorate_atomic_simulator(\n    model_name=\"ddm\", choices=[-1, 1], obs_dim=2\n)(rv_ready_simulator)\n</pre> from ssms.hssm_support import hssm_sim_wrapper, decorate_atomic_simulator from functools import partial from ssms.basic_simulators.simulator import simulator  rv_ready_simulator = partial(hssm_sim_wrapper,                               simulator_fun = simulator,                              model = \"ddm\",                               n_replicas = 1) # AF-TODO: n_replicas should default to 1 instead of being required  # We decorate the simulator to attach some metadata # that HSSM can use decorated_simulator = decorate_atomic_simulator(     model_name=\"ddm\", choices=[-1, 1], obs_dim=2 )(rv_ready_simulator) In\u00a0[8]: Copied! <pre>decorated_simulator(\n    theta=np.tile(np.array([1.0, 1.5, 0.5, 0.2]), (10, 1)), random_state=42\n)\n</pre> decorated_simulator(     theta=np.tile(np.array([1.0, 1.5, 0.5, 0.2]), (10, 1)), random_state=42 ) Out[8]: <pre>array([[ 2.02210951,  1.        ],\n       [ 2.58906269,  1.        ],\n       [ 1.1866796 ,  1.        ],\n       [ 1.06390691,  1.        ],\n       [ 1.32591701,  1.        ],\n       [ 1.04748344,  1.        ],\n       [ 1.37392318,  1.        ],\n       [ 0.78049529, -1.        ],\n       [ 1.6851691 ,  1.        ],\n       [ 1.10256469,  1.        ]])</pre> In\u00a0[9]: Copied! <pre>from hssm.distribution_utils.dist import (\n    make_distribution,\n    make_hssm_rv,\n    make_likelihood_callable,\n)\n\n# Step 1: Define a pytensor RandomVariable\nCustomRV = make_hssm_rv(\n    simulator_fun=decorated_simulator,\n    list_params=[\"v\", \"a\", \"z\", \"t\"]\n)\n\n# Step 2: Define a likelihood function\nlogp_jax_op = make_likelihood_callable(\n    loglik=jax_logp,\n    loglik_kind=\"approx_differentiable\",\n    backend=\"jax\",\n    params_is_reg=[True, False, False, False],\n    params_only=False,\n)\n\n# Step 3: Define a distribution\nCustomDistribution = make_distribution(\n    rv=CustomRV,\n    loglik=logp_jax_op,\n    list_params=[\"v\", \"a\", \"z\", \"t\"],\n    bounds=dict(v=(-3, 3), a=(0.5, 3.0), z=(0.1, 0.9), t=(0, 2.0)),\n)\n</pre> from hssm.distribution_utils.dist import (     make_distribution,     make_hssm_rv,     make_likelihood_callable, )  # Step 1: Define a pytensor RandomVariable CustomRV = make_hssm_rv(     simulator_fun=decorated_simulator,     list_params=[\"v\", \"a\", \"z\", \"t\"] )  # Step 2: Define a likelihood function logp_jax_op = make_likelihood_callable(     loglik=jax_logp,     loglik_kind=\"approx_differentiable\",     backend=\"jax\",     params_is_reg=[True, False, False, False],     params_only=False, )  # Step 3: Define a distribution CustomDistribution = make_distribution(     rv=CustomRV,     loglik=logp_jax_op,     list_params=[\"v\", \"a\", \"z\", \"t\"],     bounds=dict(v=(-3, 3), a=(0.5, 3.0), z=(0.1, 0.9), t=(0, 2.0)), ) <pre>passing here: \nparams_only:  False\nparams_is_reg:  [True, False, False, False]\n</pre> <p>We can now test the distribution by passing it to a simple PyMC model.</p> In\u00a0[10]: Copied! <pre>from pytensor import tensor as pt\n# Test via basic pymc model\nwith pm.Model() as model_pymc:\n    v = pm.Normal(\"v\", mu=0, sigma=1)\n    a = pm.Weibull(\"a\", alpha=2.0, beta=1.2)\n    z = pm.Beta(\"z\", alpha=10, beta=10)\n    t = pm.Weibull(\"t\", alpha=1.5, beta=0.5)\n\n    # We define `v` as a vector of length `n_trials`\n    # To conform to the expected signature of the likelihood function\n    v_det = pm.Deterministic(\"v_det\", v * pt.ones(obs_ddm.shape[0]))\n    CustomDistribution(\"custom\", observed=obs_ddm.values, v=v_det, a=a, z=z, t=t)\n</pre> from pytensor import tensor as pt # Test via basic pymc model with pm.Model() as model_pymc:     v = pm.Normal(\"v\", mu=0, sigma=1)     a = pm.Weibull(\"a\", alpha=2.0, beta=1.2)     z = pm.Beta(\"z\", alpha=10, beta=10)     t = pm.Weibull(\"t\", alpha=1.5, beta=0.5)      # We define `v` as a vector of length `n_trials`     # To conform to the expected signature of the likelihood function     v_det = pm.Deterministic(\"v_det\", v * pt.ones(obs_ddm.shape[0]))     CustomDistribution(\"custom\", observed=obs_ddm.values, v=v_det, a=a, z=z, t=t)  In\u00a0[11]: Copied! <pre>pm.model_to_graphviz(model_pymc)\n</pre> pm.model_to_graphviz(model_pymc) <pre>max_shape:  (500,)\nsize:  (np.int64(500),)\n</pre> Out[11]: In\u00a0[12]: Copied! <pre>with model_pymc:\n    idata = pm.sample(draws=500,\n                      tune=500,\n                      chains=2,\n                      nuts_sampler=\"numpyro\")\n</pre> with model_pymc:     idata = pm.sample(draws=500,                       tune=500,                       chains=2,                       nuts_sampler=\"numpyro\") <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/pymc/sampling/jax.py:475: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  pmap_numpyro = MCMC(\nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:12&lt;00:00, 82.15it/s, 3 steps of size 4.10e-01. acc. prob=0.84] \nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:10&lt;00:00, 93.81it/s, 3 steps of size 3.90e-01. acc. prob=0.89] \nThere was 1 divergence after tuning. Increase `target_accept` or reparameterize.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n</pre> In\u00a0[13]: Copied! <pre>az.summary(idata)\n</pre> az.summary(idata) Out[13]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat v 0.508 0.022 0.467 0.549 0.001 0.001 506.0 457.0 1.00 a 1.275 0.028 1.221 1.325 0.001 0.001 668.0 466.0 1.00 z 0.472 0.014 0.447 0.499 0.001 0.001 426.0 327.0 1.01 t 0.184 0.021 0.145 0.222 0.001 0.001 544.0 508.0 1.00 v_det[0] 0.508 0.022 0.467 0.549 0.001 0.001 506.0 457.0 1.00 ... ... ... ... ... ... ... ... ... ... v_det[495] 0.508 0.022 0.467 0.549 0.001 0.001 506.0 457.0 1.00 v_det[496] 0.508 0.022 0.467 0.549 0.001 0.001 506.0 457.0 1.00 v_det[497] 0.508 0.022 0.467 0.549 0.001 0.001 506.0 457.0 1.00 v_det[498] 0.508 0.022 0.467 0.549 0.001 0.001 506.0 457.0 1.00 v_det[499] 0.508 0.022 0.467 0.549 0.001 0.001 506.0 457.0 1.00 <p>504 rows \u00d7 9 columns</p> In\u00a0[14]: Copied! <pre>az.plot_trace(idata)\nplt.tight_layout()\n</pre> az.plot_trace(idata) plt.tight_layout() <p>Next, we will create a custom HSSM model from the simulator and JAX callable. After the work we have done above, this is now very straightforward.</p> <p>The only <code>hssm</code> specific extra step is to define a <code>ModelConfig</code> object, which bundles all information about the model.</p> <p>Then we pass our <code>ModelConfig</code> object to the <code>HSSM</code> class, along with the data and the log-likelihood function, and <code>hssm</code> will take care of the rest.</p> <p>Importantly, <code>hssm</code> will automatically understand how to construct the correct likelihood function for the specified model configuration (parameter-wise regression settings, etc.). A step we have to accomplish manually in the code above.</p> In\u00a0[15]: Copied! <pre># Define model config\nmy_custom_model_config = ModelConfig(\n    response=[\"rt\", \"response\"],\n    list_params=[\"v\", \"a\", \"z\", \"t\"],\n    bounds={\n        \"v\": (-2.5, 2.5),\n        \"a\": (1.0, 3.0),\n        \"z\": (0.0, 0.9),\n        \"t\": (0.001, 2),\n    },\n    rv=decorated_simulator,\n    backend=\"jax\",\n    choices=[-1, 1],\n)\n</pre> # Define model config my_custom_model_config = ModelConfig(     response=[\"rt\", \"response\"],     list_params=[\"v\", \"a\", \"z\", \"t\"],     bounds={         \"v\": (-2.5, 2.5),         \"a\": (1.0, 3.0),         \"z\": (0.0, 0.9),         \"t\": (0.001, 2),     },     rv=decorated_simulator,     backend=\"jax\",     choices=[-1, 1], ) In\u00a0[16]: Copied! <pre># Define the HSSM model\nmodel = hssm.HSSM(\n    data=obs_ddm,\n    model=\"my_new_model\",  # some name for the model\n    model_config=my_custom_model_config,\n    loglik_kind=\"approx_differentiable\",  # use the blackbox loglik\n    loglik=jax_logp,\n    p_outlier=0,\n)\n\nmodel.graph()\n</pre> # Define the HSSM model model = hssm.HSSM(     data=obs_ddm,     model=\"my_new_model\",  # some name for the model     model_config=my_custom_model_config,     loglik_kind=\"approx_differentiable\",  # use the blackbox loglik     loglik=jax_logp,     p_outlier=0, )  model.graph() <pre>You have specified the `lapse` argument to include a lapse distribution, but `p_outlier` is set to either 0 or None. Your lapse distribution will be ignored.\npassing here: \nparams_only:  False\nparams_is_reg:  [True, False, False, False]\nModel initialized successfully.\nmax_shape:  (500,)\nsize:  (np.int64(500),)\n</pre> Out[16]: In\u00a0[17]: Copied! <pre># Test sampling\nmodel.sample(draws=500,\n             tune=200,\n             nuts_sampler=\"numpyro\",\n             chains = 2,\n             cores = 2,\n             discard_tuned_samples=False)\n</pre> # Test sampling model.sample(draws=500,              tune=200,              nuts_sampler=\"numpyro\",              chains = 2,              cores = 2,              discard_tuned_samples=False) <pre>Using default initvals. \n\n</pre> <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/pymc/sampling/jax.py:475: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  pmap_numpyro = MCMC(\nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 700/700 [00:08&lt;00:00, 84.96it/s, 7 steps of size 4.33e-01. acc. prob=0.88]  \nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 700/700 [00:07&lt;00:00, 97.64it/s, 3 steps of size 4.85e-01. acc. prob=0.90]  \nThere was 1 divergence after tuning. Increase `target_accept` or reparameterize.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00&lt;00:00, 1287.66it/s]\n</pre> Out[17]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 36kB\nDimensions:  (chain: 2, draw: 500)\nCoordinates:\n  * chain    (chain) int64 16B 0 1\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    v        (chain, draw) float64 8kB 0.5065 0.5218 0.5144 ... 0.4876 0.5138\n    z        (chain, draw) float64 8kB 0.4671 0.4671 0.4669 ... 0.4706 0.4799\n    t        (chain, draw) float64 8kB 0.1846 0.1945 0.1819 ... 0.1861 0.1711\n    a        (chain, draw) float64 8kB 1.203 1.215 1.282 ... 1.304 1.275 1.259\nAttributes:\n    created_at:                  2025-09-29T20:06:54.606700+00:00\n    arviz_version:               0.22.0\n    inference_library:           numpyro\n    inference_library_version:   0.19.0\n    sampling_time:               15.733445\n    tuning_steps:                200\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (4)<ul><li>v(chain, draw)float640.5065 0.5218 ... 0.4876 0.5138<pre>array([[0.50652225, 0.52177754, 0.51440562, 0.50536979, 0.51314912,\n        0.54001204, 0.54985591, 0.5046882 , 0.51807674, 0.51625702,\n        0.54692524, 0.51421264, 0.51710899, 0.49378862, 0.4947128 ,\n        0.53235415, 0.51005456, 0.53332947, 0.52632567, 0.52684755,\n        0.52779519, 0.5294698 , 0.4964419 , 0.53699494, 0.51500358,\n        0.5055484 , 0.52071999, 0.5045647 , 0.51804783, 0.50986454,\n        0.50864139, 0.50109367, 0.4881502 , 0.48044392, 0.50691332,\n        0.51683577, 0.54273262, 0.51347458, 0.49269567, 0.47884544,\n        0.48654871, 0.51413014, 0.48880319, 0.53460902, 0.50877805,\n        0.52585213, 0.53724231, 0.48906722, 0.53126491, 0.50541683,\n        0.51971993, 0.48543819, 0.49915962, 0.50343884, 0.49394213,\n        0.5238966 , 0.53831292, 0.48980574, 0.49413236, 0.48285012,\n        0.52668845, 0.5065933 , 0.53272115, 0.4759908 , 0.54927688,\n        0.50804959, 0.51941195, 0.51273276, 0.50634931, 0.53831005,\n        0.46902599, 0.53549095, 0.51559344, 0.49458857, 0.52373304,\n        0.51676471, 0.51585202, 0.4971989 , 0.50324829, 0.52659986,\n        0.48776029, 0.49583361, 0.51642685, 0.50864799, 0.52048169,\n        0.50799867, 0.52166282, 0.51429888, 0.5165577 , 0.5260447 ,\n        0.52355933, 0.50797142, 0.49952107, 0.50913998, 0.52369284,\n        0.53037348, 0.51470877, 0.49160171, 0.51312112, 0.51316648,\n...\n        0.52179346, 0.52146549, 0.4975658 , 0.51490137, 0.49408913,\n        0.49357823, 0.4903704 , 0.54105208, 0.53586734, 0.52674522,\n        0.52276486, 0.51645444, 0.52549249, 0.51279359, 0.49941194,\n        0.50048403, 0.54235656, 0.52382056, 0.53626012, 0.50025861,\n        0.50750055, 0.51241461, 0.49984952, 0.52608315, 0.51588354,\n        0.50221473, 0.50796516, 0.50973547, 0.50397186, 0.50601964,\n        0.51215477, 0.5056883 , 0.56627884, 0.54770328, 0.53363162,\n        0.51231985, 0.52510808, 0.51069525, 0.52613202, 0.5189598 ,\n        0.53080553, 0.49671126, 0.48387644, 0.54014847, 0.49591714,\n        0.51315711, 0.51697004, 0.4746685 , 0.45192018, 0.48479673,\n        0.47378715, 0.5070617 , 0.51941783, 0.52041026, 0.51621638,\n        0.51840835, 0.5376444 , 0.5480531 , 0.47862457, 0.55005902,\n        0.48254969, 0.53249948, 0.49245901, 0.52639586, 0.51866585,\n        0.52917051, 0.47613987, 0.52214713, 0.50733743, 0.5031042 ,\n        0.50084576, 0.50611646, 0.52997496, 0.53510888, 0.4896497 ,\n        0.49678009, 0.51052401, 0.49369998, 0.47115982, 0.48818947,\n        0.51175901, 0.50525546, 0.50490745, 0.52387144, 0.47844011,\n        0.5524566 , 0.47731821, 0.54346315, 0.47564284, 0.52586607,\n        0.47151783, 0.50987316, 0.51923989, 0.50236927, 0.53014138,\n        0.50434145, 0.53301445, 0.48701781, 0.48762538, 0.51375528]])</pre></li><li>z(chain, draw)float640.4671 0.4671 ... 0.4706 0.4799<pre>array([[0.46707289, 0.46705514, 0.46692803, 0.47987911, 0.45962194,\n        0.46317014, 0.45906122, 0.46282049, 0.4644378 , 0.44430251,\n        0.48033406, 0.46000606, 0.48352326, 0.47605609, 0.47954661,\n        0.46501301, 0.45971501, 0.4553757 , 0.45790673, 0.45624751,\n        0.46850379, 0.4596723 , 0.490406  , 0.46825442, 0.46342307,\n        0.47528961, 0.49005823, 0.45315351, 0.47394187, 0.47479753,\n        0.46634729, 0.4548491 , 0.46783867, 0.48156271, 0.47026937,\n        0.47505446, 0.47346747, 0.45719763, 0.50011816, 0.46522773,\n        0.48447175, 0.47669342, 0.46732189, 0.45849133, 0.47834132,\n        0.45868229, 0.48453105, 0.47224073, 0.47483256, 0.45105506,\n        0.45361608, 0.49861481, 0.46291452, 0.47087862, 0.46867769,\n        0.44926645, 0.45506913, 0.47721   , 0.44919516, 0.44265885,\n        0.47237607, 0.47273957, 0.4595531 , 0.4970589 , 0.44651148,\n        0.47823903, 0.47181496, 0.46244842, 0.47561841, 0.48189254,\n        0.48110401, 0.49584675, 0.49425351, 0.46092834, 0.48234143,\n        0.45123694, 0.47135874, 0.4834707 , 0.47291848, 0.47194059,\n        0.47459032, 0.47560924, 0.47769324, 0.46442091, 0.47322174,\n        0.48696376, 0.49363185, 0.49298928, 0.46981199, 0.47354827,\n        0.45761414, 0.47514897, 0.47331598, 0.48249721, 0.47704065,\n        0.4615957 , 0.48299653, 0.48384578, 0.47130569, 0.456024  ,\n...\n        0.45540446, 0.49545631, 0.49123246, 0.4697623 , 0.47844338,\n        0.48505791, 0.47279213, 0.47322986, 0.47536561, 0.48097117,\n        0.46548548, 0.46817435, 0.47038046, 0.476976  , 0.48116558,\n        0.47031945, 0.44471027, 0.43314817, 0.46103846, 0.47088118,\n        0.47016685, 0.47509805, 0.47031555, 0.47021288, 0.45339514,\n        0.47489114, 0.47029873, 0.48359386, 0.48268777, 0.48784088,\n        0.49077189, 0.43836746, 0.43322178, 0.43823064, 0.44973226,\n        0.48783122, 0.45595254, 0.45344318, 0.45896935, 0.45753584,\n        0.46321131, 0.47667537, 0.45512941, 0.46965716, 0.47920567,\n        0.47711878, 0.46174016, 0.49011458, 0.50081173, 0.51565168,\n        0.49756786, 0.47295238, 0.48417192, 0.47976807, 0.47978892,\n        0.47472801, 0.47449449, 0.47090881, 0.46405832, 0.47460208,\n        0.45743532, 0.48150159, 0.47477507, 0.49362611, 0.48366951,\n        0.48506502, 0.45938746, 0.4683823 , 0.47814166, 0.47370846,\n        0.47876426, 0.47953095, 0.47232635, 0.46383063, 0.44893052,\n        0.47465338, 0.46168378, 0.48665151, 0.48094522, 0.48359071,\n        0.47340486, 0.5037483 , 0.44489693, 0.48619151, 0.47417816,\n        0.4667634 , 0.44728717, 0.47600679, 0.45439402, 0.48325295,\n        0.46113067, 0.47756224, 0.46874161, 0.45013817, 0.46071212,\n        0.45152349, 0.45267524, 0.47384116, 0.47057068, 0.4799169 ]])</pre></li><li>t(chain, draw)float640.1846 0.1945 ... 0.1861 0.1711<pre>array([[0.18456158, 0.19451824, 0.18187668, 0.19215992, 0.18040708,\n        0.12422972, 0.12987815, 0.17223773, 0.18662542, 0.17583724,\n        0.20090795, 0.18522286, 0.18899613, 0.17754081, 0.18308799,\n        0.20097747, 0.16911695, 0.17705788, 0.18244548, 0.17024359,\n        0.19510386, 0.17679574, 0.16724145, 0.19150161, 0.17713633,\n        0.1744337 , 0.16516208, 0.18628509, 0.1881942 , 0.16294873,\n        0.18868426, 0.14776777, 0.14421856, 0.14563122, 0.19885959,\n        0.18502666, 0.18313126, 0.20998347, 0.17602116, 0.20520314,\n        0.17027401, 0.19856656, 0.16295118, 0.16638263, 0.16790246,\n        0.18074459, 0.17463519, 0.15856979, 0.21700053, 0.16139955,\n        0.17217231, 0.22924561, 0.19018002, 0.17018554, 0.21247883,\n        0.180579  , 0.1880965 , 0.16739468, 0.15096576, 0.15606402,\n        0.18655344, 0.17892445, 0.17249962, 0.18054001, 0.19018294,\n        0.19184777, 0.18263277, 0.19645286, 0.19156914, 0.20134055,\n        0.17557041, 0.20403268, 0.19747363, 0.18045437, 0.17238034,\n        0.16074271, 0.18261818, 0.1944816 , 0.19033238, 0.20844295,\n        0.17417366, 0.19716517, 0.18083023, 0.17025333, 0.16726881,\n        0.18727384, 0.18311117, 0.1833653 , 0.20115333, 0.1861249 ,\n        0.19073879, 0.18817095, 0.20039477, 0.21020831, 0.21336497,\n        0.2124005 , 0.18041996, 0.1952306 , 0.16834383, 0.18055734,\n...\n        0.15711347, 0.21299212, 0.19809229, 0.19636827, 0.20976128,\n        0.20781172, 0.20064402, 0.16720983, 0.16467209, 0.17013383,\n        0.19335899, 0.16243467, 0.1709768 , 0.17978643, 0.20401343,\n        0.17196931, 0.16499063, 0.16200092, 0.14032082, 0.19433783,\n        0.20421419, 0.17053999, 0.17132096, 0.17421094, 0.20411631,\n        0.18975589, 0.19629324, 0.17071886, 0.21954638, 0.18791472,\n        0.17990066, 0.17335143, 0.16523443, 0.15891853, 0.16167262,\n        0.17424785, 0.17920048, 0.16052326, 0.15164332, 0.13682306,\n        0.13114856, 0.20541622, 0.16342143, 0.20366233, 0.19828323,\n        0.19262285, 0.1797596 , 0.1923774 , 0.18446856, 0.15691725,\n        0.16060145, 0.1906133 , 0.19141024, 0.20975387, 0.19775923,\n        0.20634758, 0.16942101, 0.18197607, 0.20001394, 0.16637003,\n        0.15506095, 0.20135218, 0.1945418 , 0.19857911, 0.20059121,\n        0.19653459, 0.1787149 , 0.15941823, 0.147958  , 0.15013485,\n        0.1802808 , 0.17507775, 0.17481469, 0.15813237, 0.19462434,\n        0.19565736, 0.1610906 , 0.20360879, 0.1998874 , 0.201282  ,\n        0.19814574, 0.18558622, 0.17281304, 0.16678042, 0.16033091,\n        0.18324388, 0.13495458, 0.13226886, 0.14767839, 0.20636781,\n        0.16629921, 0.19763679, 0.19788505, 0.18946303, 0.18261652,\n        0.1677317 , 0.15499104, 0.20004703, 0.1861162 , 0.17112339]])</pre></li><li>a(chain, draw)float641.203 1.215 1.282 ... 1.275 1.259<pre>array([[1.20326242, 1.21547486, 1.28196939, 1.26824224, 1.27868017,\n        1.30775333, 1.31356188, 1.29482078, 1.27108465, 1.29994867,\n        1.26005882, 1.27176033, 1.29019972, 1.30064045, 1.29125096,\n        1.25399686, 1.29259317, 1.2601696 , 1.26543079, 1.29476289,\n        1.28216191, 1.24519897, 1.29597018, 1.3018195 , 1.24366272,\n        1.30443081, 1.31850478, 1.25406027, 1.27450707, 1.25190875,\n        1.29008927, 1.29719632, 1.29591881, 1.28027005, 1.30248993,\n        1.28426117, 1.27737017, 1.21993823, 1.30872926, 1.25389113,\n        1.26727956, 1.26487547, 1.31002092, 1.32795031, 1.30623199,\n        1.27422322, 1.31676113, 1.30595967, 1.23553664, 1.30046996,\n        1.30225591, 1.26614353, 1.26839809, 1.26863994, 1.26868928,\n        1.27788577, 1.26496472, 1.30559604, 1.26260209, 1.2766912 ,\n        1.25675627, 1.30552605, 1.26219227, 1.29302994, 1.27901441,\n        1.26583056, 1.29682506, 1.2845927 , 1.24105554, 1.25024567,\n        1.31194613, 1.25328479, 1.25666717, 1.27529196, 1.29527339,\n        1.25951093, 1.27436495, 1.27437558, 1.28395351, 1.25792395,\n        1.2997523 , 1.28692682, 1.28204978, 1.29471661, 1.2712169 ,\n        1.34678947, 1.31455788, 1.32730276, 1.29021304, 1.22595541,\n        1.27639303, 1.25500251, 1.25320671, 1.2656999 , 1.27100652,\n        1.23800855, 1.25475011, 1.26935107, 1.27733733, 1.27980021,\n...\n        1.27379357, 1.23233555, 1.21868056, 1.26735702, 1.21958769,\n        1.24634098, 1.24813669, 1.28999096, 1.30438674, 1.31657242,\n        1.2849697 , 1.27378044, 1.27769661, 1.27559349, 1.27533641,\n        1.28208649, 1.28115603, 1.28604052, 1.29983571, 1.28544507,\n        1.25815635, 1.29233857, 1.32336619, 1.29625323, 1.23553288,\n        1.2905234 , 1.25885322, 1.29361631, 1.26489186, 1.25603513,\n        1.25230589, 1.30001   , 1.29138987, 1.30278022, 1.29795554,\n        1.28683047, 1.27622095, 1.28748055, 1.29358678, 1.300482  ,\n        1.29138781, 1.2969677 , 1.310484  , 1.22659953, 1.26222605,\n        1.26211137, 1.28587821, 1.2690419 , 1.27666632, 1.29772072,\n        1.28817393, 1.30744993, 1.27408361, 1.28353668, 1.27233579,\n        1.27991762, 1.25252658, 1.25243262, 1.26710836, 1.30641241,\n        1.306253  , 1.28083182, 1.27312591, 1.30061871, 1.30713919,\n        1.29103343, 1.28194207, 1.28620973, 1.25526126, 1.27399556,\n        1.29309899, 1.28797268, 1.31404751, 1.27042055, 1.25840402,\n        1.28255085, 1.27111364, 1.29644186, 1.20631779, 1.2146087 ,\n        1.28895929, 1.29617733, 1.2845508 , 1.28515429, 1.29614503,\n        1.30256947, 1.27369114, 1.32740797, 1.28741486, 1.29841707,\n        1.27417442, 1.24022551, 1.24035251, 1.2631388 , 1.230291  ,\n        1.31636072, 1.25485288, 1.30445601, 1.27485287, 1.25944683]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-29T20:06:54.606700+00:00arviz_version :0.22.0inference_library :numpyroinference_library_version :0.19.0sampling_time :15.733445tuning_steps :200modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 4MB\nDimensions:      (chain: 2, draw: 500, __obs__: 500)\nCoordinates:\n  * chain        (chain) int64 16B 0 1\n  * draw         (draw) int64 4kB 0 1 2 3 4 5 6 ... 493 494 495 496 497 498 499\n  * __obs__      (__obs__) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\nData variables:\n    rt,response  (chain, draw, __obs__) float64 4MB -0.6224 -2.021 ... -1.468\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li><li>__obs__: 500</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-0.6224 -2.021 ... -2.053 -1.468<pre>array([[[-0.62237211, -2.02123135, -0.65461652, ..., -2.02499833,\n         -1.95394559, -1.4575279 ],\n        [-0.6157066 , -2.03435519, -0.64863427, ..., -1.99133337,\n         -1.96757512, -1.58214591],\n        [-0.69634185, -2.09189441, -0.7211599 , ..., -1.91077587,\n         -2.02821286, -1.77029471],\n        ...,\n        [-0.77577145, -2.24709649, -0.78745507, ..., -1.86565394,\n         -2.14412899, -1.62868603],\n        [-0.79030502, -2.20325618, -0.80532601, ..., -1.87062328,\n         -2.10726798, -1.6703428 ],\n        [-0.71591563, -2.13398685, -0.74138719, ..., -1.9236206 ,\n         -2.06418349, -1.55125517]],\n\n       [[-0.68506456, -2.10268973, -0.71375864, ..., -2.00611148,\n         -2.02687599, -1.27847818],\n        [-0.7164939 , -2.1873615 , -0.7444462 , ..., -1.88696752,\n         -2.11639722, -1.64060232],\n        [-0.63470474, -2.0338495 , -0.66775462, ..., -1.97647035,\n         -1.96845704, -1.6417243 ],\n        ...,\n        [-0.73608402, -2.0941854 , -0.743669  , ..., -1.881587  ,\n         -2.04065418, -2.05846558],\n        [-0.6961844 , -2.06422003, -0.72446526, ..., -1.92972615,\n         -2.00719751, -1.76601448],\n        [-0.64641981, -2.11335535, -0.67403786, ..., -1.9571649 ,\n         -2.05325828, -1.46773047]]], shape=(2, 500, 500))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='__obs__', length=500))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 53kB\nDimensions:          (chain: 2, draw: 500)\nCoordinates:\n  * chain            (chain) int64 16B 0 1\n  * draw             (draw) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\nData variables:\n    acceptance_rate  (chain, draw) float64 8kB 0.5239 0.7118 ... 1.0 0.7309\n    step_size        (chain, draw) float64 8kB 0.4328 0.4328 ... 0.4848 0.4848\n    diverging        (chain, draw) bool 1kB False False False ... False False\n    energy           (chain, draw) float64 8kB 908.3 907.6 907.0 ... 903.9 903.8\n    n_steps          (chain, draw) int64 8kB 15 7 7 7 7 7 7 7 ... 7 7 7 7 7 7 3\n    tree_depth       (chain, draw) int64 8kB 4 3 3 3 3 3 3 3 ... 3 3 3 3 3 3 3 2\n    lp               (chain, draw) float64 8kB 906.8 904.2 900.9 ... 901.5 903.1\nAttributes:\n    created_at:                  2025-09-29T20:06:54.612280+00:00\n    arviz_version:               0.22.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (7)<ul><li>acceptance_rate(chain, draw)float640.5239 0.7118 0.9705 ... 1.0 0.7309<pre>array([[0.52393343, 0.7118161 , 0.97046066, 0.99047327, 0.95275798,\n        0.84114153, 0.98280665, 0.97711775, 0.9792688 , 0.83610171,\n        1.        , 0.92483341, 0.96295498, 0.82696203, 0.99452692,\n        0.86324609, 0.77493171, 0.9808109 , 1.        , 0.86130655,\n        0.79840878, 0.7806092 , 1.        , 0.9628028 , 0.82035009,\n        0.85751029, 0.85454662, 1.        , 0.98873153, 0.34775265,\n        0.7708082 , 0.89134292, 0.99022155, 0.79857568, 1.        ,\n        0.78696712, 0.79584343, 0.81427266, 0.98239214, 0.9634928 ,\n        0.40036365, 0.90296017, 0.98829238, 0.86769189, 0.88399572,\n        0.98852822, 0.85450386, 0.99518107, 0.98527684, 0.39970448,\n        0.89543664, 0.89073655, 0.98015966, 1.        , 0.95988642,\n        0.59705642, 0.93709688, 0.99689801, 0.92237694, 0.93106675,\n        0.97379331, 0.99480084, 0.92797063, 0.97461149, 0.95558046,\n        0.97000395, 0.86045566, 0.81264795, 0.87548354, 0.87680889,\n        0.97712315, 0.88399498, 0.9743201 , 0.86514813, 0.9624663 ,\n        0.94834954, 0.90691949, 0.98379004, 0.87732389, 0.96017077,\n        0.71762895, 0.92300329, 0.95958699, 0.87619106, 0.71166408,\n        0.76419653, 0.98761606, 0.98748984, 0.72512779, 0.49136033,\n        0.96735357, 0.98173797, 0.88189485, 0.95763691, 0.62690973,\n        0.5129819 , 0.40850283, 0.93313876, 0.9012123 , 0.94945998,\n...\n        0.95583748, 0.84951772, 1.        , 0.53814874, 0.83945149,\n        0.8862674 , 0.97614181, 0.99198861, 1.        , 1.        ,\n        0.98723618, 0.96339296, 0.98080841, 0.99811316, 0.93857612,\n        0.95841922, 0.96749002, 0.87690711, 1.        , 1.        ,\n        0.98337577, 0.89790626, 0.88550373, 1.        , 0.86407164,\n        0.99028176, 0.9901891 , 0.9398204 , 0.99126409, 0.99424472,\n        0.78186373, 0.88671442, 0.97604768, 0.99848435, 1.        ,\n        0.92315637, 1.        , 0.96837409, 0.99873499, 0.97747662,\n        0.92572133, 0.79628691, 0.50545273, 0.98748658, 0.8937786 ,\n        0.95925382, 0.73115822, 0.77018945, 0.85131121, 0.76928097,\n        0.99731288, 0.95791786, 0.99571711, 0.63428965, 0.93768797,\n        1.        , 0.36302055, 0.97678579, 0.99749454, 0.97775436,\n        1.        , 0.92395086, 1.        , 0.90329417, 1.        ,\n        0.99085133, 1.        , 1.        , 0.76740404, 1.        ,\n        0.91583443, 0.96247439, 0.96107379, 0.72816971, 0.7804471 ,\n        0.9786633 , 0.95050823, 0.99389834, 0.82813873, 1.        ,\n        0.99803553, 0.86036999, 0.9908861 , 0.94057096, 0.7469984 ,\n        0.97565615, 0.84282805, 1.        , 0.99239291, 0.99662151,\n        0.69987799, 0.98116611, 0.98346206, 0.9341981 , 0.98345025,\n        0.69671508, 1.        , 0.95375352, 1.        , 0.73092882]])</pre></li><li>step_size(chain, draw)float640.4328 0.4328 ... 0.4848 0.4848<pre>array([[0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n        0.43279338, 0.43279338, 0.43279338, 0.43279338, 0.43279338,\n...\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ,\n        0.484808  , 0.484808  , 0.484808  , 0.484808  , 0.484808  ]])</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False]])</pre></li><li>energy(chain, draw)float64908.3 907.6 907.0 ... 903.9 903.8<pre>array([[908.34076236, 907.62818975, 907.00864962, 901.35579791,\n        902.80754054, 906.28132199, 906.91650315, 908.20481244,\n        902.88065434, 905.05649815, 907.26845232, 905.9481659 ,\n        902.40790906, 905.44199188, 901.75302691, 902.52820246,\n        903.25661073, 902.81494617, 902.27455634, 903.63143594,\n        903.40827034, 903.70816852, 904.16958751, 905.81101362,\n        903.74080265, 904.67782595, 904.06084592, 904.56506076,\n        903.26585246, 905.91748583, 904.29799677, 905.05127697,\n        904.75932698, 908.33434615, 905.35257701, 903.32336945,\n        904.80198196, 906.50554129, 904.95056794, 905.08032574,\n        905.83160879, 905.16468963, 903.68855069, 906.53514962,\n        904.97303991, 902.63570285, 904.95474201, 905.80005178,\n        903.34552045, 905.63439099, 904.17673769, 906.4278951 ,\n        905.18265345, 902.31126547, 903.22169666, 908.17350038,\n        904.61969002, 903.36386108, 906.5133706 , 907.58213331,\n        908.29381492, 902.1674065 , 902.88090852, 904.51104245,\n        908.41751648, 905.08438282, 902.3877344 , 903.92753529,\n        903.14900719, 904.63486455, 906.41457957, 907.53837331,\n        906.1297282 , 903.67033672, 902.82567269, 904.09195511,\n        904.25854434, 902.08692238, 902.51592422, 903.37989244,\n...\n        902.18649039, 903.64735183, 905.05340912, 903.00820888,\n        904.97297878, 904.27886243, 902.20837809, 903.08408358,\n        904.47749201, 904.76886016, 904.16339119, 907.69907289,\n        910.73706653, 908.96088915, 905.21769741, 905.09676831,\n        902.77211512, 903.05040847, 902.89269536, 903.97154871,\n        904.97655557, 906.4496457 , 906.43807476, 907.10981224,\n        907.82776893, 902.32447187, 903.2137481 , 906.37616979,\n        905.49922728, 910.55362341, 909.30308597, 906.00377764,\n        903.13163867, 903.99197895, 903.16098447, 902.35344091,\n        907.12841664, 906.49182135, 904.18277083, 904.58565177,\n        904.88463539, 904.67628716, 902.89334654, 905.04650418,\n        905.42562531, 903.99547013, 904.63364435, 903.93756645,\n        906.50298206, 906.15884341, 904.722615  , 901.46418653,\n        902.65072502, 905.35461497, 906.30710108, 906.00293486,\n        903.1248005 , 904.07276554, 908.33580426, 906.91402628,\n        904.44712909, 905.07256623, 906.30424013, 905.58901253,\n        905.44267737, 904.6776849 , 909.0812336 , 909.45838514,\n        907.4972217 , 908.31725365, 906.83006527, 905.75322667,\n        903.41577561, 905.37028187, 905.27670818, 906.9604372 ,\n        905.88113978, 905.61972084, 903.8785336 , 903.82843125]])</pre></li><li>n_steps(chain, draw)int6415 7 7 7 7 7 7 7 ... 7 7 7 7 7 7 3<pre>array([[15,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  3,  7,  7,  3,  7,\n         7,  7,  7,  7,  7,  7,  7,  7, 15,  7,  3,  7,  7,  3,  7,  7,\n        15, 15,  7,  3,  3,  7, 15, 15,  7,  7,  7,  7,  7,  7,  7,  7,\n         7,  7,  3, 11,  7,  7,  7, 15,  7,  7,  7,  3,  7, 11,  7,  7,\n        15,  7,  7,  7,  7,  7,  7,  7,  3,  7,  7,  7,  7,  7,  7,  7,\n         7,  7, 15,  7,  7,  3,  7,  7,  7,  3,  3,  7,  7,  7,  7,  7,\n         7,  7, 15,  7,  7,  3,  7,  7,  7,  7,  7, 15,  7, 15,  7,  7,\n        11, 15,  7,  7, 15, 15,  7,  7,  3,  7,  7, 11,  7,  3,  3, 11,\n         3,  7,  7,  5,  7,  3,  7, 15, 15,  7,  3,  7,  7,  7,  7, 15,\n         7, 15, 15,  3, 15,  7,  7,  7,  7, 15,  7,  7,  3,  3,  7,  7,\n         7,  7,  7,  7,  3,  7,  3,  3, 15,  7,  7,  7,  3, 15,  3, 15,\n         7,  7, 15, 15,  7,  7,  7,  7,  3,  7,  3, 11, 15,  3, 19, 15,\n         7,  7,  7,  7,  7,  3, 11, 15,  7,  7,  7,  3,  7,  7,  7,  7,\n         7,  7,  7,  7,  7,  7,  7,  7,  7,  3,  7,  7,  7,  7,  3,  3,\n         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  3,  7,  7, 11,  7,  7,\n         7,  7,  7,  7,  7,  7,  7,  7,  3,  7,  3,  7,  7,  7,  7,  7,\n         7,  7,  7, 15, 15,  3,  3,  7,  3,  7,  7,  5,  7,  3,  7,  3,\n         7, 15,  7,  7,  3,  7,  7,  7,  7,  7,  3,  7,  7, 39, 15,  3,\n         7,  3,  3,  7,  7, 15,  7,  7,  3,  3, 15,  7,  3,  3,  7,  7,\n         3,  7,  7,  7, 11,  1, 15,  7,  3,  7,  7,  6,  3,  7,  7, 23,\n...\n         7,  7,  7,  7,  3,  7,  7, 15,  3, 15,  7, 15,  7, 15, 15, 15,\n         3,  7,  7,  7,  3,  7, 15,  7, 15,  3,  7, 15,  7,  7,  7,  7,\n        15,  7, 15,  3, 15, 15,  3, 11,  7,  7, 15, 15, 15, 11, 15,  3,\n         7, 15,  7,  7,  7,  7, 15,  7,  7,  7, 11,  3, 11, 15, 15,  7,\n         7,  3, 15,  7,  7,  7,  7, 15,  7, 15,  7,  3,  7,  7, 15,  7,\n         7,  7,  7,  7,  7,  3,  7,  7,  7,  7,  7,  7,  3,  7,  3,  7,\n        15, 15,  7,  3,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  3,  7,\n         7,  7,  7,  7,  7,  7, 15,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n         7,  3, 15,  7,  7, 15,  7,  3,  3,  7,  7,  7,  3,  7,  7,  7,\n        15,  7,  3,  7,  7,  3, 15,  7,  7,  7,  7,  7,  7,  3,  7,  7,\n         7,  7,  7,  7,  7,  7,  7, 11,  7,  7,  7,  7,  7,  7,  7,  7,\n         7,  7,  7,  7,  7,  7,  7, 15, 15,  7,  3,  7,  7,  7,  7,  7,\n         7,  7,  7,  7,  7,  7,  7, 15,  7,  7,  7,  7,  7,  7,  7, 15,\n         7,  3,  7,  7,  7, 15,  7,  3,  7,  7,  7,  7, 15,  7,  3, 15,\n         7,  3,  3, 15,  7,  7,  7,  7,  3, 15,  7,  7,  7,  5, 11,  7,\n         3,  7,  3,  7, 15,  7,  3,  3,  3,  7,  7, 11,  7,  7,  7,  7,\n         7,  7,  7,  7,  3,  7,  7,  3,  7,  7, 15,  7,  7,  7,  7,  3,\n        15,  7,  7,  7,  7,  7,  7,  7,  7, 15,  7,  7,  7,  7,  7,  7,\n         7,  7,  7,  3]])</pre></li><li>tree_depth(chain, draw)int644 3 3 3 3 3 3 3 ... 3 3 3 3 3 3 3 2<pre>array([[4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 4, 3, 2, 3, 3, 2, 3, 3, 4, 4, 3, 2, 2, 3, 4, 4, 3, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 2, 4, 3, 3, 3, 4, 3, 3, 3, 2, 3, 4, 3, 3, 4, 3,\n        3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 2, 3, 3,\n        3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 2, 3, 3, 3, 3, 3, 4, 3, 4,\n        3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 2, 3, 3, 4, 3, 2, 2, 4, 2, 3, 3, 3,\n        3, 2, 3, 4, 4, 3, 2, 3, 3, 3, 3, 4, 3, 4, 4, 2, 4, 3, 3, 3, 3, 4,\n        3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 4, 3, 3, 3, 2, 4, 2, 4,\n        3, 3, 4, 4, 3, 3, 3, 3, 2, 3, 2, 4, 4, 2, 5, 4, 3, 3, 3, 3, 3, 2,\n        4, 4, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3,\n        3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 4, 3, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 2, 2, 3,\n        2, 3, 3, 3, 3, 2, 3, 2, 3, 4, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 6,\n        4, 2, 3, 2, 2, 3, 3, 4, 3, 3, 2, 2, 4, 3, 2, 2, 3, 3, 2, 3, 3, 3,\n        4, 1, 4, 3, 2, 3, 3, 3, 2, 3, 3, 5, 3, 3, 3, 3, 3, 3, 4, 3, 4, 3,\n        3, 3, 3, 3, 3, 2, 3, 3, 3, 4, 4, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2,\n        2, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4,\n        3, 3, 3, 3, 4, 3, 5, 3, 3, 3, 2, 4, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3,\n        3, 3, 3, 4, 3, 3, 3, 3, 3, 2, 3, 2, 3, 4, 3, 4, 3, 2, 2, 3, 2, 3,\n        3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 2, 3, 3, 4, 3, 3, 3, 3, 4, 3, 3, 3,\n...\n        2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4,\n        3, 4, 4, 3, 2, 3, 3, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 3, 3, 4, 3, 2,\n        2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3,\n        3, 4, 3, 3, 3, 3, 3, 4, 2, 2, 3, 3, 4, 3, 3, 3, 3, 4, 3, 3, 2, 3,\n        2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 2, 3, 3, 3,\n        3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3,\n        3, 4, 2, 4, 3, 4, 3, 4, 4, 4, 2, 3, 3, 3, 2, 3, 4, 3, 4, 2, 3, 4,\n        3, 3, 3, 3, 4, 3, 4, 2, 4, 4, 2, 4, 3, 3, 4, 4, 4, 4, 4, 2, 3, 4,\n        3, 3, 3, 3, 4, 3, 3, 3, 4, 2, 4, 4, 4, 3, 3, 2, 4, 3, 3, 3, 3, 4,\n        3, 4, 3, 2, 3, 3, 4, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3,\n        2, 3, 4, 4, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3,\n        3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 2, 4, 3, 3, 4, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3,\n        4, 3, 2, 3, 3, 2, 4, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 2, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 4, 3, 2,\n        3, 3, 3, 4, 3, 2, 3, 3, 3, 3, 4, 3, 2, 4, 3, 2, 2, 4, 3, 3, 3, 3,\n        2, 4, 3, 3, 3, 3, 4, 3, 2, 3, 2, 3, 4, 3, 2, 2, 2, 3, 3, 4, 3, 3,\n        3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 4, 3, 3, 3, 3, 2, 4, 3, 3, 3,\n        3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2]])</pre></li><li>lp(chain, draw)float64906.8 904.2 900.9 ... 901.5 903.1<pre>array([[906.769642  , 904.23236474, 900.912389  , 901.01483087,\n        901.33862262, 905.66255571, 905.64204048, 901.50760461,\n        901.08629863, 904.32974108, 904.14608215, 901.3561925 ,\n        901.57268067, 901.57759171, 901.2872726 , 901.86252107,\n        901.54493053, 902.25950593, 901.6493578 , 901.828825  ,\n        901.41447177, 902.82858124, 903.0978933 , 902.50146752,\n        902.51758929, 901.39885246, 903.84967164, 902.47708703,\n        900.91985112, 904.03128632, 901.34611998, 903.31081742,\n        903.86524136, 905.47079198, 902.55635645, 900.9160106 ,\n        902.52704921, 903.49258855, 904.08282695, 902.8293488 ,\n        903.31803294, 901.15676437, 902.78414041, 903.50487934,\n        901.80416287, 901.49115873, 903.73848507, 902.7683851 ,\n        903.12857773, 902.99475006, 902.48545246, 905.39801252,\n        901.53626592, 901.79704072, 902.87299782, 902.58055751,\n        902.54727812, 902.27568028, 905.06750364, 906.27412812,\n        901.69288249, 901.45597628, 902.23231573, 903.55203981,\n        904.38935788, 900.98523569, 901.19965523, 901.60991687,\n        901.96896919, 903.5373384 , 903.68031709, 905.27727669,\n        902.99540891, 901.80496423, 902.18384642, 903.19167336,\n        900.86660825, 901.32505022, 901.06458487, 901.72072188,\n...\n        901.1013042 , 901.35794805, 902.73267809, 901.34014202,\n        902.92390053, 901.24445666, 901.09963531, 901.99834813,\n        902.98877348, 902.24871146, 904.01122092, 906.30900509,\n        907.93102397, 905.30460068, 902.72480678, 902.37620249,\n        901.68143736, 902.24651562, 902.43323965, 903.59777545,\n        904.87389186, 903.09313592, 904.58988073, 904.14191923,\n        901.30761239, 901.08069641, 901.24521092, 902.45041688,\n        905.30687745, 908.63752385, 905.61766221, 902.08934437,\n        901.58340512, 902.59686668, 901.34756825, 901.81222061,\n        904.6614735 , 904.10850413, 902.82907342, 904.09437442,\n        904.24065091, 902.63624297, 901.29861631, 904.17865003,\n        903.15176579, 902.83993826, 903.53033349, 902.08936653,\n        905.99555742, 903.7742865 , 901.16508933, 901.3465417 ,\n        902.10566217, 903.27606066, 904.11437364, 901.32712961,\n        902.1602544 , 902.82144427, 906.23247844, 904.42359527,\n        901.37972397, 904.11587076, 903.81616055, 903.46804695,\n        903.25094815, 903.75040964, 908.20420352, 906.34512064,\n        905.4865916 , 903.43538028, 903.93688758, 901.91201239,\n        901.95622135, 902.95834748, 903.74994562, 904.222996  ,\n        904.50401344, 903.59998306, 901.52795925, 903.07495726]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (4)created_at :2025-09-29T20:06:54.612280+00:00arviz_version :0.22.0modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 12kB\nDimensions:                  (__obs__: 500, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 4kB 0 1 2 3 4 ... 496 497 498 499\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 8kB 0...\nAttributes:\n    created_at:                  2025-09-29T20:06:54.613262+00:00\n    arviz_version:               0.22.0\n    inference_library:           numpyro\n    inference_library_version:   0.19.0\n    sampling_time:               15.733445\n    tuning_steps:                200\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 500</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float640.7336 1.0 1.114 ... 0.4049 1.0<pre>array([[ 0.73362625,  1.        ],\n       [ 1.11384964, -1.        ],\n       [ 0.80583185,  1.        ],\n       [ 0.80243492,  1.        ],\n       [ 2.19640899,  1.        ],\n       [ 0.65593803,  1.        ],\n       [ 0.90178442,  1.        ],\n       [ 4.80741978,  1.        ],\n       [ 0.59347796,  1.        ],\n       [ 1.13075411,  1.        ],\n       [ 1.92198789,  1.        ],\n       [ 0.90965253,  1.        ],\n       [ 2.11170959,  1.        ],\n       [ 2.21477485,  1.        ],\n       [ 2.17002368, -1.        ],\n       [ 2.209167  ,  1.        ],\n       [ 0.86345994,  1.        ],\n       [ 1.64354908, -1.        ],\n       [ 1.05286789,  1.        ],\n       [ 3.11714959, -1.        ],\n...\n       [ 1.5871619 , -1.        ],\n       [ 0.52945417,  1.        ],\n       [ 0.6030978 ,  1.        ],\n       [ 0.80487525,  1.        ],\n       [ 0.8251211 ,  1.        ],\n       [ 2.62559605, -1.        ],\n       [ 1.10837114,  1.        ],\n       [ 3.04465795,  1.        ],\n       [ 1.80810845, -1.        ],\n       [ 1.22626626,  1.        ],\n       [ 1.82505405,  1.        ],\n       [ 0.54845285,  1.        ],\n       [ 1.01013482,  1.        ],\n       [ 1.86507869,  1.        ],\n       [ 0.94010693,  1.        ],\n       [ 1.73986757,  1.        ],\n       [ 2.15558338,  1.        ],\n       [ 2.28776789,  1.        ],\n       [ 1.02572453, -1.        ],\n       [ 0.40494695,  1.        ]])</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='__obs__', length=500))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-29T20:06:54.613262+00:00arviz_version :0.22.0inference_library :numpyroinference_library_version :0.19.0sampling_time :15.733445tuning_steps :200modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[18]: Copied! <pre>az.plot_trace(model.traces)\nplt.tight_layout()\n\naz.plot_pair(model.traces)\nplt.tight_layout()\n</pre> az.plot_trace(model.traces) plt.tight_layout()  az.plot_pair(model.traces) plt.tight_layout() <p>We hope you find it easy to use the above example to leverage <code>hssm</code> to fit your own custom models.</p>"},{"location":"tutorials/jax_callable_contribution_onnx_example/#construct-custom-models-from-simulators-and-jax-callables","title":"Construct Custom Models from simulators and JAX callables\u00b6","text":""},{"location":"tutorials/jax_callable_contribution_onnx_example/#simulate-data","title":"Simulate Data\u00b6","text":"<p>As a pre-amble, we will simulate a simple dataset from the DDM model to use through the example.</p>"},{"location":"tutorials/jax_callable_contribution_onnx_example/#construct-pymc-distribution-from-simulator-and-jax-callable","title":"Construct PyMC distribution from simulator and JAX callable\u00b6","text":""},{"location":"tutorials/jax_callable_contribution_onnx_example/#create-jax-log-likelihood-callable","title":"Create JAX Log-likelihood Callable\u00b6","text":"<p>What we need is a callable that takes in a matrix that stacks model parameters and data trialwise, hence with input dimensions: $trials \\times (dim-parameters + dim-data)$. The function should return a vector of length $trials$ that contains the log-likelihood for each trial.</p> <p>If we have a JAX function with this signature, we will be able to proceed to create a valid PyMC distribution form helper functions provided by <code>hssm</code>.</p> <p>In this particular example, we will reinstantiate a pretrained LAN via utilities from the <code>lanfactory</code> package. This is not necessary, but illustrates how to make use of the braoder ecosystem around HSSM.</p> <p>Note: <code>lanfactory</code> is an optional dependency of <code>hssm</code> and can be installed using the command <code>pip install hssm[notebook]</code>. Alternatively, you can install it directly with <code>pip install lanfactory</code>.</p>"},{"location":"tutorials/jax_callable_contribution_onnx_example/#checking-the-signature-of-the-jax-callable","title":"Checking the signature of the JAX callable\u00b6","text":"<p>We can test the signature of the JAX callable by passing in a batch of inputs and checking the output shape.</p>"},{"location":"tutorials/jax_callable_contribution_onnx_example/#decorate-and-wrap-a-simulator","title":"Decorate and wrap a simulator\u00b6","text":"<p>The simulator-signature expected by <code>hssm</code> is the following:</p> <p>A simulator is a callable that takes in a matrix that stacks model parameters and data trialwise, hence with input dimensions: $trials \\times (|paramters))$. The function should return a matrix of shape $trials \\times |data|$.</p> <p>We will use the <code>decorate_atomic_simulator()</code> utility to annotate the simulator with necessary metadata we use the <code>hssm_sim_wrapper()</code> function from the <code>ssm-simulators</code> package to make our simulator ready for usage inside a <code>PyTensor</code> <code>RandomVariable</code> later.</p> <p>If you check the signature of the resulting <code>rv_ready_simulator</code>, you should find no problems shoe-horning your own custon simulator into the corresponding behavior.</p> <p><code>decorate_atomic_simulator()</code> is just a <code>Python</code> <code>decorator</code> that you can use around any function.</p> <p>You can start from any simulator you like, we use the one from the <code>ssm-simulators</code> package for convenience.</p>"},{"location":"tutorials/jax_callable_contribution_onnx_example/#checking-the-signature-of-the-decorated-simulator","title":"Checking the signature of the decorated simulator\u00b6","text":"<p>We can check the signature of the decorated simulator by passing in a batch of parameters and checking the output shape.</p>"},{"location":"tutorials/jax_callable_contribution_onnx_example/#create-valid-pymc-distribution","title":"Create valid PyMC distribution\u00b6","text":"<p>We have all ingredients to create a valid PyMC distribution, form a few helper functions provided by <code>hssm</code> at this point.</p> <p>A valid Distribution will need two ingredients:</p> <ol> <li>A <code>RandomVariable</code> (RV) that encodes the simulator and parameter names.</li> <li>A likelihood function, which is a valid PyTensor <code>Op</code> that encodes the log-likelihood of the model.</li> </ol> <p>We will use the <code>make_hssm_rv()</code> helper function to create the <code>RandomVariable</code> and the <code>make_likelihood_callable()</code> helper function to create the likelihood <code>Op</code>.</p> <p>Finally, the <code>make_distribution()</code> helper function will package everything into a valid PyMC distribution.</p> <p>NOTE:</p> <p>There are a few helpful settings which we can use to customize our distribution. We will not cover all details here, however it is worth highlighting the <code>params_is_reg</code> argument in <code>make_likelihood_callable()</code>. This argument is used to tell PyMC whether the parameter is a regression parameter or not. Specifically, if a parameter is treated as a regression, the likelihood function is built to assume that the parameter is passed trial-wise, i.e. as a vector of length <code>n_trials</code>.</p> <p>Note here, we set all parameters to be non-regression parameters, as we expect the parameters to be passed as single values in the simple <code>PyMC</code> model below.</p>"},{"location":"tutorials/jax_callable_contribution_onnx_example/#custom-hssm-model-from-simulator-and-jax-callable","title":"Custom HSSM model from simulator and JAX callable\u00b6","text":""},{"location":"tutorials/lapse_prob_and_dist/","title":"Using lapse probabilities and distributions in HSSM","text":"In\u00a0[1]: Copied! <pre># If running this on Colab, please uncomment the next line\n# !pip install hssm\n</pre> # If running this on Colab, please uncomment the next line # !pip install hssm In\u00a0[2]: Copied! <pre>import bambi as bmb\nimport matplotlib.pyplot as plt\nimport hssm\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n</pre> import bambi as bmb import matplotlib.pyplot as plt import hssm import warnings  warnings.filterwarnings(\"ignore\") In\u00a0[3]: Copied! <pre># Simulate some data\nddm_data = hssm.simulate_data(\n    model=\"ddm\", theta=dict(v=0.5, a=1.5, z=0.5, t=0.1), size=1000\n)\nddm_data.head()\n</pre> # Simulate some data ddm_data = hssm.simulate_data(     model=\"ddm\", theta=dict(v=0.5, a=1.5, z=0.5, t=0.1), size=1000 ) ddm_data.head() Out[3]: rt response 0 1.591710 -1.0 1 1.523280 1.0 2 0.693384 1.0 3 1.160937 1.0 4 2.355452 1.0 In\u00a0[4]: Copied! <pre># Build the simplest model specifying only data\n# Note the model output\n\nddm_model_default = hssm.HSSM(data=ddm_data)\nddm_model_default\n</pre> # Build the simplest model specifying only data # Note the model output  ddm_model_default = hssm.HSSM(data=ddm_data) ddm_model_default <pre>Model initialized successfully.\n</pre> Out[4]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 1000\n\nParameters:\n\nv:\n    Prior: Normal(mu: 0.0, sigma: 2.0)\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> <p>Note</p> <p>     Note in the above output that lapse probability and lapse distributions are at default values.   </p> In\u00a0[5]: Copied! <pre>ddm_model_lapse = hssm.HSSM(\n    data=ddm_data,\n    p_outlier={\"name\": \"Uniform\", \"lower\": 0.0001, \"upper\": 0.5},\n    lapse=bmb.Prior(\"Uniform\", lower=0.0, upper=20.0),\n)\n\nddm_model_lapse\n</pre> ddm_model_lapse = hssm.HSSM(     data=ddm_data,     p_outlier={\"name\": \"Uniform\", \"lower\": 0.0001, \"upper\": 0.5},     lapse=bmb.Prior(\"Uniform\", lower=0.0, upper=20.0), )  ddm_model_lapse <pre>Model initialized successfully.\n</pre> Out[5]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 1000\n\nParameters:\n\nv:\n    Prior: Normal(mu: 0.0, sigma: 2.0)\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: Uniform(lower: 0.0001, upper: 0.5)\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[6]: Copied! <pre>lapse_trace = ddm_model_lapse.sample()\nlapse_trace\n</pre> lapse_trace = ddm_model_lapse.sample() lapse_trace <pre>Using default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [a, p_outlier, z, t, v]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 25 seconds.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4000/4000 [00:01&lt;00:00, 2777.28it/s]\n</pre> Out[6]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 168kB\nDimensions:    (chain: 4, draw: 1000)\nCoordinates:\n  * chain      (chain) int64 32B 0 1 2 3\n  * draw       (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\nData variables:\n    a          (chain, draw) float64 32kB 1.523 1.575 1.554 ... 1.568 1.557\n    z          (chain, draw) float64 32kB 0.4973 0.516 0.5021 ... 0.4984 0.5098\n    v          (chain, draw) float64 32kB 0.4847 0.4726 0.4935 ... 0.5084 0.501\n    t          (chain, draw) float64 32kB 0.1322 0.0895 ... 0.09385 0.09138\n    p_outlier  (chain, draw) float64 32kB 0.01521 0.01816 ... 0.02173 0.01221\nAttributes:\n    created_at:                  2025-09-27T00:07:40.567292+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               25.002630949020386\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 1000</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li></ul></li><li>Data variables: (5)<ul><li>a(chain, draw)float641.523 1.575 1.554 ... 1.568 1.557<pre>array([[1.52314357, 1.57452956, 1.55421417, ..., 1.56569849, 1.56495169,\n        1.56190568],\n       [1.57188493, 1.57815679, 1.57142391, ..., 1.57923988, 1.57700279,\n        1.51723316],\n       [1.54191145, 1.57236255, 1.51993869, ..., 1.54971192, 1.59710206,\n        1.55262418],\n       [1.5445445 , 1.54911547, 1.56403527, ..., 1.56727621, 1.56805427,\n        1.55661255]], shape=(4, 1000))</pre></li><li>z(chain, draw)float640.4973 0.516 ... 0.4984 0.5098<pre>array([[0.49726932, 0.51600105, 0.50210366, ..., 0.52555134, 0.51810769,\n        0.52588314],\n       [0.47886305, 0.4750029 , 0.48898778, ..., 0.49449442, 0.48527129,\n        0.49885079],\n       [0.48851774, 0.4938121 , 0.49487672, ..., 0.47098775, 0.49122775,\n        0.49021351],\n       [0.49794485, 0.49245697, 0.50058464, ..., 0.48448787, 0.498431  ,\n        0.50979066]], shape=(4, 1000))</pre></li><li>v(chain, draw)float640.4847 0.4726 ... 0.5084 0.501<pre>array([[0.48465195, 0.47257135, 0.49353992, ..., 0.43146644, 0.45292289,\n        0.45647995],\n       [0.57863062, 0.52697886, 0.47064191, ..., 0.48513711, 0.551452  ,\n        0.50347062],\n       [0.50756341, 0.52188299, 0.48121836, ..., 0.55549091, 0.52785551,\n        0.52844225],\n       [0.49973178, 0.50001719, 0.51350322, ..., 0.50810648, 0.50841804,\n        0.50101713]], shape=(4, 1000))</pre></li><li>t(chain, draw)float640.1322 0.0895 ... 0.09385 0.09138<pre>array([[0.13221333, 0.08950309, 0.12708106, ..., 0.13991528, 0.10383198,\n        0.10382255],\n       [0.07243869, 0.06992831, 0.12651647, ..., 0.08159657, 0.08998947,\n        0.11818298],\n       [0.11388802, 0.10160968, 0.11798979, ..., 0.09274831, 0.06791273,\n        0.06737281],\n       [0.08650556, 0.0972482 , 0.09466159, ..., 0.06463118, 0.09384617,\n        0.09138313]], shape=(4, 1000))</pre></li><li>p_outlier(chain, draw)float640.01521 0.01816 ... 0.02173 0.01221<pre>array([[0.01520886, 0.01815826, 0.01170003, ..., 0.00276018, 0.02422434,\n        0.01792678],\n       [0.0092543 , 0.04000709, 0.0044583 , ..., 0.01581544, 0.01988415,\n        0.0246154 ],\n       [0.0084104 , 0.00982768, 0.00984813, ..., 0.02435549, 0.01300533,\n        0.0144027 ],\n       [0.02107058, 0.02088479, 0.02552082, ..., 0.02539893, 0.02172585,\n        0.01221054]], shape=(4, 1000))</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T00:07:40.567292+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :25.002630949020386tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 32MB\nDimensions:      (chain: 4, draw: 1000, __obs__: 1000)\nCoordinates:\n  * chain        (chain) int64 32B 0 1 2 3\n  * draw         (draw) int64 8kB 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\n  * __obs__      (__obs__) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    rt,response  (chain, draw, __obs__) float64 32MB -2.771 -1.269 ... -2.626\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 1000</li><li>__obs__: 1000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-2.771 -1.269 ... -1.091 -2.626<pre>array([[[-2.77065063, -1.26852132, -1.0526624 , ..., -0.98378863,\n         -1.09012044, -2.47293758],\n        [-2.86543427, -1.30537095, -1.02728893, ..., -0.99748636,\n         -1.12443585, -2.62814794],\n        [-2.83241422, -1.25941578, -1.05931306, ..., -0.98524848,\n         -1.08512909, -2.55972219],\n        ...,\n        [-2.76313952, -1.29949833, -1.04263219, ..., -0.99266048,\n         -1.11610165, -2.56313678],\n        [-2.8115164 , -1.31961643, -1.04314335, ..., -1.01046343,\n         -1.13794062, -2.58191302],\n        [-2.83862685, -1.31316107, -0.99493131, ..., -0.97880386,\n         -1.1227276 , -2.61976454]],\n\n       [[-3.01195482, -1.21530237, -1.03577757, ..., -0.96199713,\n         -1.04858293, -2.67177084],\n        [-2.89116313, -1.28567978, -1.15508358, ..., -1.06745218,\n         -1.13433204, -2.57047577],\n        [-2.75783662, -1.27512848, -1.17947024, ..., -1.06433244,\n         -1.12326098, -2.47557728],\n...\n        [-2.9066139 , -1.23653726, -1.10063413, ..., -1.00558388,\n         -1.07655852, -2.55503523],\n        [-2.95258765, -1.26229328, -1.08118735, ..., -1.01390566,\n         -1.10085907, -2.66318927],\n        [-2.91443821, -1.266793  , -1.02191653, ..., -0.97946568,\n         -1.09130144, -2.59752579]],\n\n       [[-2.85238191, -1.2866037 , -1.03172429, ..., -0.99088246,\n         -1.10819206, -2.55665242],\n        [-2.8353439 , -1.27884919, -1.07060808, ..., -1.00813488,\n         -1.1080418 , -2.53497264],\n        [-2.90282447, -1.2726508 , -1.04045747, ..., -0.98770145,\n         -1.09698287, -2.62306052],\n        ...,\n        [-2.86730717, -1.29291875, -1.10147275, ..., -1.04042498,\n         -1.13091014, -2.55687274],\n        [-2.88866008, -1.27358818, -1.05862781, ..., -0.99948467,\n         -1.10190632, -2.60778477],\n        [-2.89485301, -1.27628486, -0.990984  , ..., -0.96113262,\n         -1.09102321, -2.62647986]]], shape=(4, 1000, 1000))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 528kB\nDimensions:                (chain: 4, draw: 1000)\nCoordinates:\n  * chain                  (chain) int64 32B 0 1 2 3\n  * draw                   (draw) int64 8kB 0 1 2 3 4 5 ... 995 996 997 998 999\nData variables: (12/18)\n    perf_counter_start     (chain, draw) float64 32kB 1.435e+06 ... 1.435e+06\n    perf_counter_diff      (chain, draw) float64 32kB 0.007204 ... 0.006451\n    acceptance_rate        (chain, draw) float64 32kB 0.9353 0.7084 ... 0.713\n    lp                     (chain, draw) float64 32kB -2.064e+03 ... -2.064e+03\n    process_time_diff      (chain, draw) float64 32kB 0.00479 ... 0.006411\n    largest_eigval         (chain, draw) float64 32kB nan nan nan ... nan nan\n    ...                     ...\n    step_size              (chain, draw) float64 32kB 0.5325 0.5325 ... 0.6993\n    index_in_trajectory    (chain, draw) int64 32kB -1 -4 2 -4 5 ... 2 -2 -2 -3\n    n_steps                (chain, draw) float64 32kB 3.0 7.0 7.0 ... 3.0 7.0\n    step_size_bar          (chain, draw) float64 32kB 0.549 0.549 ... 0.5765\n    energy_error           (chain, draw) float64 32kB -0.01388 0.1417 ... 0.5025\n    energy                 (chain, draw) float64 32kB 2.066e+03 ... 2.065e+03\nAttributes:\n    created_at:                  2025-09-27T00:07:40.581310+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               25.002630949020386\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 1000</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li></ul></li><li>Data variables: (18)<ul><li>perf_counter_start(chain, draw)float641.435e+06 1.435e+06 ... 1.435e+06<pre>array([[1434670.20490042, 1434670.21225771, 1434670.22389008, ...,\n        1434676.54739233, 1434676.55074354, 1434676.55709283],\n       [1434670.26999404, 1434670.27745929, 1434670.28144671, ...,\n        1434677.33555133, 1434677.34188221, 1434677.3483495 ],\n       [1434671.54635033, 1434671.55036692, 1434671.55698083, ...,\n        1434678.29345158, 1434678.29667463, 1434678.30283646],\n       [1434670.56222175, 1434670.56877546, 1434670.5721575 , ...,\n        1434676.93093292, 1434676.933968  , 1434676.937252  ]],\n      shape=(4, 1000))</pre></li><li>perf_counter_diff(chain, draw)float640.007204 0.01153 ... 0.006451<pre>array([[0.00720387, 0.01153388, 0.00632567, ..., 0.00329008, 0.00628312,\n        0.00631796],\n       [0.00735167, 0.00391325, 0.007101  , ..., 0.00623467, 0.00640825,\n        0.01245925],\n       [0.00391967, 0.00654038, 0.00621542, ..., 0.00315571, 0.00611567,\n        0.00672713],\n       [0.00645287, 0.00332967, 0.00640442, ..., 0.00299792, 0.00320079,\n        0.00645079]], shape=(4, 1000))</pre></li><li>acceptance_rate(chain, draw)float640.9353 0.7084 ... 0.6576 0.713<pre>array([[9.35326062e-01, 7.08430333e-01, 6.17577851e-01, ...,\n        3.54859188e-01, 1.00000000e+00, 1.00000000e+00],\n       [3.77735819e-01, 9.37621428e-01, 4.87904552e-01, ...,\n        9.53047657e-01, 9.99110167e-01, 8.47429528e-01],\n       [3.84443298e-04, 4.39508847e-01, 9.37112690e-01, ...,\n        9.72085300e-01, 8.44883292e-01, 8.32510536e-01],\n       [8.42235839e-01, 9.96626631e-01, 9.41814751e-01, ...,\n        8.14725443e-01, 6.57576914e-01, 7.13032900e-01]], shape=(4, 1000))</pre></li><li>lp(chain, draw)float64-2.064e+03 ... -2.064e+03<pre>array([[-2064.21151259, -2064.58897016, -2063.94738602, ...,\n        -2069.65914356, -2065.20901333, -2065.60931566],\n       [-2067.28123066, -2067.18960184, -2070.33331599, ...,\n        -2063.76736043, -2063.82749454, -2063.48760964],\n       [-2064.10534166, -2063.47928785, -2064.26257091, ...,\n        -2064.94307869, -2064.22796055, -2065.21572415],\n       [-2063.53767251, -2062.87396856, -2063.14180599, ...,\n        -2064.90400196, -2062.84232826, -2064.21004612]], shape=(4, 1000))</pre></li><li>process_time_diff(chain, draw)float640.00479 0.00798 ... 0.006411<pre>array([[0.00479 , 0.00798 , 0.006295, ..., 0.00326 , 0.006276, 0.006306],\n       [0.00714 , 0.003326, 0.006702, ..., 0.006228, 0.00638 , 0.012441],\n       [0.003624, 0.006494, 0.006193, ..., 0.003132, 0.006116, 0.006695],\n       [0.006347, 0.003319, 0.006372, ..., 0.002998, 0.003179, 0.006411]],\n      shape=(4, 1000))</pre></li><li>largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], shape=(4, 1000))</pre></li><li>smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], shape=(4, 1000))</pre></li><li>divergences(chain, draw)int640 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0<pre>array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], shape=(4, 1000))</pre></li><li>reached_max_treedepth(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]], shape=(4, 1000))</pre></li><li>max_energy_error(chain, draw)float640.1176 0.7032 ... 1.11 0.7467<pre>array([[ 0.11762532,  0.70321539,  0.86892605, ...,  1.47668238,\n        -1.28647284, -0.28740349],\n       [ 1.72706868, -0.67379427,  1.86785638, ..., -0.43470336,\n        -0.10931566,  0.38746344],\n       [11.52334533,  1.75462848,  0.12978618, ...,  0.07714086,\n         0.41883162,  0.39587829],\n       [ 0.51330051, -0.22628616,  0.13109067, ...,  0.35128792,\n         1.11000133,  0.74669412]], shape=(4, 1000))</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]], shape=(4, 1000))</pre></li><li>tree_depth(chain, draw)int642 3 3 3 3 3 3 3 ... 3 2 3 3 2 2 2 3<pre>array([[2, 3, 3, ..., 2, 3, 3],\n       [3, 2, 3, ..., 3, 3, 4],\n       [2, 3, 3, ..., 2, 3, 3],\n       [3, 2, 3, ..., 2, 2, 3]], shape=(4, 1000))</pre></li><li>step_size(chain, draw)float640.5325 0.5325 ... 0.6993 0.6993<pre>array([[0.53245157, 0.53245157, 0.53245157, ..., 0.53245157, 0.53245157,\n        0.53245157],\n       [0.51908482, 0.51908482, 0.51908482, ..., 0.51908482, 0.51908482,\n        0.51908482],\n       [0.64629515, 0.64629515, 0.64629515, ..., 0.64629515, 0.64629515,\n        0.64629515],\n       [0.69934421, 0.69934421, 0.69934421, ..., 0.69934421, 0.69934421,\n        0.69934421]], shape=(4, 1000))</pre></li><li>index_in_trajectory(chain, draw)int64-1 -4 2 -4 5 2 ... 2 -2 2 -2 -2 -3<pre>array([[-1, -4,  2, ...,  0,  3, -1],\n       [ 3, -2,  4, ...,  4, -4,  5],\n       [ 0,  5, -3, ..., -2,  5, -4],\n       [ 6,  1,  2, ..., -2, -2, -3]], shape=(4, 1000))</pre></li><li>n_steps(chain, draw)float643.0 7.0 7.0 7.0 ... 3.0 3.0 3.0 7.0<pre>array([[ 3.,  7.,  7., ...,  3.,  7.,  7.],\n       [ 7.,  3.,  7., ...,  7.,  7., 15.],\n       [ 3.,  7.,  7., ...,  3.,  7.,  7.],\n       [ 7.,  3.,  7., ...,  3.,  3.,  7.]], shape=(4, 1000))</pre></li><li>step_size_bar(chain, draw)float640.549 0.549 0.549 ... 0.5765 0.5765<pre>array([[0.54903299, 0.54903299, 0.54903299, ..., 0.54903299, 0.54903299,\n        0.54903299],\n       [0.58075397, 0.58075397, 0.58075397, ..., 0.58075397, 0.58075397,\n        0.58075397],\n       [0.57480753, 0.57480753, 0.57480753, ..., 0.57480753, 0.57480753,\n        0.57480753],\n       [0.57649022, 0.57649022, 0.57649022, ..., 0.57649022, 0.57649022,\n        0.57649022]], shape=(4, 1000))</pre></li><li>energy_error(chain, draw)float64-0.01388 0.1417 ... -0.1328 0.5025<pre>array([[-0.01387719,  0.14167478,  0.50808202, ...,  0.        ,\n        -1.28647284, -0.10547295],\n       [ 1.23347612,  0.20719112,  1.2599826 , ..., -0.34869527,\n        -0.09310809, -0.06293857],\n       [ 0.        ,  0.37698196,  0.02943677, ..., -0.02187666,\n        -0.02117295,  0.37364854],\n       [-0.06511485, -0.22628616,  0.02255801, ...,  0.15049056,\n        -0.1327736 ,  0.50252986]], shape=(4, 1000))</pre></li><li>energy(chain, draw)float642.066e+03 2.067e+03 ... 2.065e+03<pre>array([[2065.74502261, 2067.35970786, 2066.94996086, ..., 2073.07503284,\n        2069.79664289, 2066.08710255],\n       [2071.754516  , 2068.4405913 , 2073.28786687, ..., 2066.72290263,\n        2064.47013289, 2066.32135521],\n       [2071.17083392, 2067.5730264 , 2065.44118054, ..., 2066.05566885,\n        2067.24026231, 2066.89428027],\n       [2068.74848913, 2063.45879039, 2063.6741942 , ..., 2065.59488064,\n        2067.04153419, 2064.84423852]], shape=(4, 1000))</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T00:07:40.581310+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :25.002630949020386tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 24kB\nDimensions:                  (__obs__: 1000, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 8kB 0 1 2 3 4 ... 996 997 998 999\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 16kB ...\nAttributes:\n    created_at:                  2025-09-27T00:07:40.584224+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 1000</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float641.592 -1.0 1.523 ... 1.0 1.049 -1.0<pre>array([[ 1.59171045, -1.        ],\n       [ 1.52328038,  1.        ],\n       [ 0.69338369,  1.        ],\n       ...,\n       [ 0.87599444,  1.        ],\n       [ 1.20802402,  1.        ],\n       [ 1.04878163, -1.        ]], shape=(1000, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (6)created_at :2025-09-27T00:07:40.584224+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[7]: Copied! <pre>ddm_model_lapse.summary(lapse_trace)\n</pre> ddm_model_lapse.summary(lapse_trace) Out[7]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat z 0.496 0.013 0.471 0.519 0.000 0.000 2317.0 2328.0 1.0 a 1.559 0.028 1.505 1.613 0.001 0.001 2026.0 1811.0 1.0 t 0.096 0.022 0.053 0.137 0.001 0.000 1841.0 1878.0 1.0 v 0.506 0.031 0.451 0.567 0.001 0.000 2402.0 2837.0 1.0 p_outlier 0.016 0.009 0.000 0.033 0.000 0.000 2629.0 1797.0 1.0 In\u00a0[8]: Copied! <pre>ddm_model_lapse.plot_trace(lapse_trace)\nplt.tight_layout()\n</pre> ddm_model_lapse.plot_trace(lapse_trace) plt.tight_layout() In\u00a0[9]: Copied! <pre>ddm_model_no_lapse = hssm.HSSM(data=ddm_data, p_outlier=None, lapse=None)\nddm_model_no_lapse\n</pre> ddm_model_no_lapse = hssm.HSSM(data=ddm_data, p_outlier=None, lapse=None) ddm_model_no_lapse <pre>Model initialized successfully.\n</pre> Out[9]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 1000\n\nParameters:\n\nv:\n    Prior: Normal(mu: 0.0, sigma: 2.0)\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)</pre> In\u00a0[10]: Copied! <pre>ddm_model_no_lapse_warn = hssm.HSSM(data=ddm_data, p_outlier=0)\nddm_model_no_lapse_warn\n</pre> ddm_model_no_lapse_warn = hssm.HSSM(data=ddm_data, p_outlier=0) ddm_model_no_lapse_warn <pre>You have specified the `lapse` argument to include a lapse distribution, but `p_outlier` is set to either 0 or None. Your lapse distribution will be ignored.\nModel initialized successfully.\n</pre> Out[10]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 1000\n\nParameters:\n\nv:\n    Prior: Normal(mu: 0.0, sigma: 2.0)\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)</pre>"},{"location":"tutorials/lapse_prob_and_dist/#colab-instructions","title":"Colab Instructions\u00b6","text":"<p>If you would like to run this tutorial on Google colab, please click this link.</p> <p>Once you are in the colab, follow the installation instructions below and then restart your runtime.</p> <p>Just uncomment the code in the next code cell and run it!</p> <p>NOTE:</p> <p>You may want to switch your runtime to have a GPU or TPU. To do so, go to Runtime &gt; Change runtime type and select the desired hardware accelerator.</p> <p>Note that if you switch your runtime you have to follow the installation instructions again.</p>"},{"location":"tutorials/lapse_prob_and_dist/#using-lapse-probabilities-and-distributions-in-hssm","title":"Using lapse probabilities and distributions in HSSM\u00b6","text":"<p>Since v0.1.2, HSSM has added the ability to model outliers in the distribution with lapse probabilities and distributions. Let's see how it works.</p>"},{"location":"tutorials/lapse_prob_and_dist/#lapse-probabilities-and-distributions-are-enabled-by-default","title":"Lapse probabilities and distributions are enabled by default.\u00b6","text":"<p>From v0.1.2 on, lapse probabilities and distributions are enabled by default. If left unspecified, the probability for outliers will be a fixed value of 0.05 and the distribution will be specified as <code>Uniform(0, 10)</code>.</p>"},{"location":"tutorials/lapse_prob_and_dist/#specifying-lapse-probability-and-distribution","title":"Specifying lapse probability and distribution\u00b6","text":"<p>It is easy to change the lapse probability and distribution. HSSM has added two arguments, <code>p_outlier</code> and <code>lapse</code> to allow the lapse probability and distribution to be specified.</p> <p>The optional <code>p_outlier</code> accepts a <code>float</code>, a <code>dict</code>, or a <code>bmb.Prior</code> object. When <code>p_outlier</code> is specified as a single <code>float</code> value, it will be considered \"fixed\". You can also specify a prior distribution for <code>p_outlier</code> through a <code>dict</code> or a <code>bmb.Prior</code> object, the same way as you would when specifying priors for any other parameter. That way, the lapse probability will be considered another parameter and will be estimated during MCMC sampling.</p> <p>Likewise, the <code>lapse</code> argument accepts a <code>dict</code> or a <code>bmb.Prior</code> object to specify a fixed lapse distribution. This distribution will be considered as the one that outliers are generated from.</p>"},{"location":"tutorials/lapse_prob_and_dist/#disable-lapse-probabilities-and-distributions","title":"Disable lapse probabilities and distributions\u00b6","text":"<p>When <code>p_outliers</code> is set to <code>None</code> or 0, lapse probability and distributions will be ignored. They will not be included in the model output.</p> <p>Note</p> <p>     If `p_outlier` is set to `None` or 0 but `lapse` is not set to None, you will receive a warning. Please remember to also set `lapse` to `None`.   </p>"},{"location":"tutorials/likelihoods/","title":"Likelihood functions in HSSM explained","text":"In\u00a0[1]: Copied! <pre># If running this on Colab, please uncomment the next line\n# !pip install hssm\n</pre> # If running this on Colab, please uncomment the next line # !pip install hssm In\u00a0[2]: Copied! <pre>import numpy as np\nimport pytensor\nimport hssm\n</pre> import numpy as np import pytensor import hssm In\u00a0[3]: Copied! <pre>help(hssm.simulate_data)\n</pre> help(hssm.simulate_data) <pre>Help on function simulate_data in module hssm.simulator:\n\nsimulate_data(model: str, theta: Union[dict[str, Union[numpy._typing._array_like._Buffer, numpy._typing._array_like._SupportsArray[numpy.dtype[Any]], numpy._typing._nested_sequence._NestedSequence[numpy._typing._array_like._SupportsArray[numpy.dtype[Any]]], complex, bytes, str, numpy._typing._nested_sequence._NestedSequence[complex | bytes | str]]], list[float], numpy._typing._array_like._Buffer, numpy._typing._array_like._SupportsArray[numpy.dtype[Any]], numpy._typing._nested_sequence._NestedSequence[numpy._typing._array_like._SupportsArray[numpy.dtype[Any]]], complex, bytes, str, numpy._typing._nested_sequence._NestedSequence[complex | bytes | str]], size: int, random_state: int | None = None, output_df: bool = True, **kwargs) -&gt; numpy.ndarray | pandas.core.frame.DataFrame\n    Sample simulated data from specified distributions.\n    \n    Parameters\n    ----------\n    model\n        A model name that must be supported in `ssm_simulators`. For a detailed list of\n        supported models, please see all fields in the `model_config` dict\n        [here](https://github.com/AlexanderFengler/ssm-simulators/blob\n        /e09eb2528d885c7b3340516597849fff4d9a5bf8/ssms/config/config.py#L6)\n    theta\n        Parameters of the process. Can be supplied as dictionary with parameter names as\n        key and np.array or float as values. Can also be supplied as a list or 1D-array,\n        however in this case the order of parameters is important and must match\n        specifications [here](https://github.com/AlexanderFengler/\n        ssm-simulators/blob/e09eb2528d885c7b3340516597849fff4d9a5bf8/ssms/config/config.py#L6).\n        Parameters can be specificed 'trial-wise', by supplying 1D arrays of shape\n        `size` to the dictionary, or by supplying a 2D array\n        of shape `(size, n_parameters)` dicrectly.\n    size\n        The size of the data to be simulated. If `theta` is a 2D ArrayLike, this\n        parameter indicates the size of data to be simulated for each trial.\n    random_state : optional\n        A random seed for reproducibility.\n    output_df : optional\n        If True, outputs a DataFrame with column names \"rt\", \"response\". Otherwise a\n        2-column numpy array, by default True.\n    kwargs : optional\n        Other arguments passed to ssms.basic_simulators.simulator.\n    \n    Returns\n    -------\n    np.ndarray | pd.DataFrame\n        An array or DataFrame with simulated data.\n\n</pre> In\u00a0[4]: Copied! <pre># Simulate some data\ndata = hssm.simulate_data(\n    model=\"ddm\", theta=dict(v=0.5, a=1.5, z=0.5, t=0.3), size=1000\n)\ndata\n</pre> # Simulate some data data = hssm.simulate_data(     model=\"ddm\", theta=dict(v=0.5, a=1.5, z=0.5, t=0.3), size=1000 ) data Out[4]: rt response 0 1.045665 1.0 1 2.170658 1.0 2 1.184247 1.0 3 4.467735 1.0 4 1.289009 1.0 ... ... ... 995 1.803433 1.0 996 2.017609 1.0 997 3.286063 1.0 998 1.840125 1.0 999 2.363211 1.0 <p>1000 rows \u00d7 2 columns</p> In\u00a0[5]: Copied! <pre>ddm_model_analytical = hssm.HSSM(data, model=\"ddm\")\n</pre> ddm_model_analytical = hssm.HSSM(data, model=\"ddm\") <pre>Model initialized successfully.\n</pre> In\u00a0[6]: Copied! <pre>ddm_model_analytical\n</pre> ddm_model_analytical Out[6]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 1000\n\nParameters:\n\nv:\n    Prior: Normal(mu: 0.0, sigma: 2.0)\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[7]: Copied! <pre>ddm_model_analytical.graph()\n</pre> ddm_model_analytical.graph() Out[7]: <p>The <code>ddm</code> and <code>ddm_sdv</code> models have <code>analytical</code> and <code>approx_differentiable</code> likelihoods. If <code>loglik_kind</code> is not specified, the <code>analytical</code> likelihood will be used. We can however directly specify the <code>loglik_kind</code> argument for a given model, and if available, the likelihood backend will be switched automatically.</p> In\u00a0[8]: Copied! <pre>ddm_model_approx_diff = hssm.HSSM(\n    data, model=\"ddm\", loglik_kind=\"approx_differentiable\"\n)\n</pre> ddm_model_approx_diff = hssm.HSSM(     data, model=\"ddm\", loglik_kind=\"approx_differentiable\" ) <pre>Model initialized successfully.\n</pre> <p>While the model graph looks the same:</p> In\u00a0[9]: Copied! <pre>ddm_model_approx_diff.graph()\n</pre> ddm_model_approx_diff.graph() Out[9]: <p>We can check that the likelihood is now coming from a different backend by printing the model string:</p> In\u00a0[10]: Copied! <pre>ddm_model_approx_diff\n</pre> ddm_model_approx_diff Out[10]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: approx_differentiable\nObservations: 1000\n\nParameters:\n\nv:\n    Prior: Uniform(lower: -3.0, upper: 3.0)\n    Explicit bounds: (-3.0, 3.0)\n\na:\n    Prior: Uniform(lower: 0.3, upper: 2.5)\n    Explicit bounds: (0.3, 2.5)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, 2.0)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> <p>Note how under the Likelihood rubric, it now says \"approx_differentiable\". Another simple way to check this is to access the <code>loglik_kind</code> attribute of our HSSM model.</p> In\u00a0[11]: Copied! <pre>ddm_model_approx_diff.loglik_kind\n</pre> ddm_model_approx_diff.loglik_kind Out[11]: <pre>'approx_differentiable'</pre> In\u00a0[12]: Copied! <pre>from hssm.likelihoods.analytical import logp_ddm\n</pre> from hssm.likelihoods.analytical import logp_ddm In\u00a0[13]: Copied! <pre>ddm_model_analytical_override = hssm.HSSM(\n    data, model=\"ddm\", loglik_kind=\"analytical\", loglik=logp_ddm\n)\n</pre> ddm_model_analytical_override = hssm.HSSM(     data, model=\"ddm\", loglik_kind=\"analytical\", loglik=logp_ddm ) <pre>Model initialized successfully.\n</pre> <p>HSSM automatically constructed our model with the likelihood function we provided. We can now take posterior samples as usual.</p> In\u00a0[14]: Copied! <pre>idata = ddm_model_analytical_override.sample(draws=500, tune=500, chains=2)\n</pre> idata = ddm_model_analytical_override.sample(draws=500, tune=500, chains=2) <pre>Using default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [z, a, t, v]\n/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n</pre> <pre>Output()</pre> <pre>/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n</pre> <pre></pre> <pre>Sampling 2 chains for 500 tune and 500 draw iterations (1_000 + 1_000 draws total) took 9 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00&lt;00:00, 2803.20it/s]\n</pre> In\u00a0[15]: Copied! <pre>ddm_model_analytical_override._inference_obj\n</pre> ddm_model_analytical_override._inference_obj Out[15]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 36kB\nDimensions:  (chain: 2, draw: 500)\nCoordinates:\n  * chain    (chain) int64 16B 0 1\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    z        (chain, draw) float64 8kB 0.4924 0.5124 0.5089 ... 0.5106 0.5249\n    a        (chain, draw) float64 8kB 1.449 1.497 1.541 ... 1.475 1.461 1.431\n    v        (chain, draw) float64 8kB 0.5545 0.5242 0.5237 ... 0.566 0.436\n    t        (chain, draw) float64 8kB 0.3371 0.2973 0.273 ... 0.3674 0.3316\nAttributes:\n    created_at:                  2025-09-27T00:09:25.419589+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               8.755553007125854\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (4)<ul><li>z(chain, draw)float640.4924 0.5124 ... 0.5106 0.5249<pre>array([[0.49243345, 0.51236393, 0.50891713, 0.51087844, 0.50332314,\n        0.50536913, 0.53127203, 0.54142695, 0.52497121, 0.50263863,\n        0.51132169, 0.52616073, 0.52430475, 0.5025647 , 0.5105813 ,\n        0.51787456, 0.50838738, 0.48610438, 0.50760936, 0.51498147,\n        0.50022486, 0.51782462, 0.51238405, 0.52127121, 0.52127121,\n        0.50428737, 0.52340571, 0.53186518, 0.49857699, 0.50108759,\n        0.50002824, 0.50417014, 0.5274271 , 0.504018  , 0.50438098,\n        0.50215777, 0.50809349, 0.50531183, 0.53173257, 0.49055555,\n        0.51176056, 0.52886147, 0.50237419, 0.51083954, 0.50126649,\n        0.49919408, 0.52160916, 0.50683851, 0.51894548, 0.54554142,\n        0.50011813, 0.50652776, 0.50678721, 0.50001282, 0.49553918,\n        0.5123342 , 0.49903622, 0.50086456, 0.48991204, 0.52081228,\n        0.50113025, 0.48743094, 0.48694221, 0.49184906, 0.51163276,\n        0.49048251, 0.51943115, 0.51402406, 0.51216899, 0.51393165,\n        0.5192807 , 0.52108431, 0.50749438, 0.52307297, 0.51912528,\n        0.51946435, 0.49431428, 0.49661471, 0.50358285, 0.48931415,\n        0.48863858, 0.49355831, 0.49037966, 0.48345771, 0.48499283,\n        0.52556561, 0.51770949, 0.50112176, 0.52750868, 0.50221208,\n        0.50381788, 0.49702434, 0.51295046, 0.50897577, 0.50950445,\n        0.52278529, 0.52147411, 0.5163573 , 0.49670659, 0.49677672,\n...\n        0.49728941, 0.49239765, 0.50221636, 0.50881071, 0.51607025,\n        0.51607025, 0.51808396, 0.49560629, 0.50194796, 0.50109183,\n        0.49977829, 0.50000765, 0.53249526, 0.5242933 , 0.52406284,\n        0.51456574, 0.50412459, 0.51751914, 0.4968145 , 0.52370543,\n        0.50853756, 0.51526226, 0.49068445, 0.54004313, 0.50029918,\n        0.50123244, 0.49537602, 0.50570584, 0.51383863, 0.49999752,\n        0.49999752, 0.50844734, 0.50039133, 0.49537739, 0.53341275,\n        0.50030426, 0.49747511, 0.52602864, 0.50822399, 0.49507808,\n        0.52364736, 0.52218558, 0.50915414, 0.53612612, 0.52069466,\n        0.53515988, 0.51759241, 0.49672341, 0.52980956, 0.52678497,\n        0.51107759, 0.50547751, 0.51146432, 0.51146432, 0.50374982,\n        0.499798  , 0.49433   , 0.49301593, 0.49059601, 0.48771879,\n        0.49726084, 0.49643344, 0.48467663, 0.51366668, 0.50490524,\n        0.5163574 , 0.52264488, 0.48184485, 0.48684256, 0.50995362,\n        0.50995362, 0.52526349, 0.49583191, 0.5161257 , 0.52698808,\n        0.51168195, 0.50824509, 0.50975924, 0.50730098, 0.50989791,\n        0.50446001, 0.51170942, 0.51316679, 0.4870059 , 0.52194468,\n        0.53334562, 0.53016739, 0.51024704, 0.51458635, 0.50401215,\n        0.53082742, 0.50467325, 0.49866031, 0.51317509, 0.52078817,\n        0.51751102, 0.52106286, 0.50798476, 0.51064596, 0.52486787]])</pre></li><li>a(chain, draw)float641.449 1.497 1.541 ... 1.461 1.431<pre>array([[1.44867982, 1.49700273, 1.5410275 , 1.55489601, 1.48437747,\n        1.51480429, 1.41244399, 1.4915839 , 1.48822217, 1.44954226,\n        1.44551737, 1.52826705, 1.47109532, 1.39434803, 1.44820285,\n        1.43200872, 1.46632276, 1.49730632, 1.46926429, 1.43928856,\n        1.45474956, 1.45701327, 1.42147138, 1.46787715, 1.46787715,\n        1.43636908, 1.45811338, 1.44560291, 1.43364302, 1.46023771,\n        1.46266983, 1.52492736, 1.43502221, 1.45316082, 1.45499197,\n        1.43208844, 1.4287162 , 1.47836995, 1.45115994, 1.43556119,\n        1.4996105 , 1.44010175, 1.49659935, 1.46365789, 1.48036033,\n        1.48660973, 1.47671309, 1.51953065, 1.41754635, 1.47771435,\n        1.46602976, 1.50902303, 1.48075424, 1.48422512, 1.48218761,\n        1.48929057, 1.52306965, 1.5186279 , 1.46696719, 1.44719504,\n        1.46661793, 1.47745217, 1.47482363, 1.44649759, 1.47768524,\n        1.49907188, 1.47493489, 1.46992287, 1.47924519, 1.43020579,\n        1.40324938, 1.42574518, 1.43992557, 1.46849852, 1.45773062,\n        1.44245289, 1.49246273, 1.46939566, 1.50468937, 1.44129872,\n        1.43380063, 1.4559823 , 1.4638097 , 1.51146987, 1.44066187,\n        1.44275607, 1.44372792, 1.48082502, 1.46158282, 1.4708167 ,\n        1.4820371 , 1.48345705, 1.50899871, 1.4389028 , 1.45223053,\n        1.45933432, 1.4653569 , 1.45638969, 1.49390359, 1.48326865,\n...\n        1.44785446, 1.44898874, 1.43638268, 1.47488904, 1.42646583,\n        1.42646583, 1.47022665, 1.46469585, 1.4443746 , 1.46628456,\n        1.45781405, 1.44782734, 1.46312181, 1.4519495 , 1.45858394,\n        1.45202634, 1.46694202, 1.45791486, 1.43568985, 1.51022449,\n        1.41424548, 1.47730177, 1.45961034, 1.44655836, 1.4577226 ,\n        1.45554034, 1.46444989, 1.4541575 , 1.45337517, 1.4629003 ,\n        1.4629003 , 1.47037765, 1.46200506, 1.45744361, 1.46902533,\n        1.41512501, 1.4630677 , 1.50329652, 1.43310078, 1.43974022,\n        1.49546976, 1.46184831, 1.43550422, 1.47331612, 1.44616782,\n        1.50136561, 1.48164565, 1.49040819, 1.49053946, 1.48725345,\n        1.45943468, 1.44062516, 1.46507754, 1.46507754, 1.49223111,\n        1.44635032, 1.45160525, 1.50101377, 1.49883085, 1.5077889 ,\n        1.45943757, 1.44069028, 1.43699402, 1.47588931, 1.48261039,\n        1.45231537, 1.45211539, 1.49504206, 1.45851018, 1.45177964,\n        1.45177964, 1.50318125, 1.46743101, 1.45925477, 1.44040475,\n        1.49913335, 1.4532922 , 1.46143806, 1.43948954, 1.42335122,\n        1.4436351 , 1.47180994, 1.43712233, 1.48997784, 1.42561656,\n        1.5068633 , 1.45626344, 1.46210231, 1.48420454, 1.43515342,\n        1.47903115, 1.45130005, 1.46731754, 1.46620282, 1.45899566,\n        1.42334476, 1.41156369, 1.47537519, 1.46062467, 1.43113854]])</pre></li><li>v(chain, draw)float640.5545 0.5242 ... 0.566 0.436<pre>array([[0.55452166, 0.52416316, 0.52370448, 0.51023587, 0.51401196,\n        0.51295639, 0.50128531, 0.44759525, 0.51758   , 0.4926205 ,\n        0.51656425, 0.47671551, 0.45174294, 0.53458142, 0.49697756,\n        0.4584301 , 0.4995116 , 0.56280055, 0.5127314 , 0.50726046,\n        0.50677881, 0.4796632 , 0.46865171, 0.50809717, 0.50809717,\n        0.50529794, 0.50451996, 0.49519167, 0.54477793, 0.51487899,\n        0.54559668, 0.56099691, 0.48722579, 0.46994259, 0.5514804 ,\n        0.53647089, 0.53621408, 0.51723726, 0.47939132, 0.53255002,\n        0.51025428, 0.50889208, 0.54818149, 0.55405085, 0.50881741,\n        0.55305611, 0.51179232, 0.52525023, 0.47557611, 0.45839321,\n        0.56095293, 0.5480174 , 0.53395367, 0.52101386, 0.54010537,\n        0.57632297, 0.56474224, 0.55270073, 0.53273394, 0.46416946,\n        0.53016804, 0.57233949, 0.56877315, 0.59739085, 0.51635193,\n        0.55063859, 0.48104177, 0.50377806, 0.49914721, 0.5300313 ,\n        0.51908188, 0.51267498, 0.52358792, 0.51230633, 0.47549468,\n        0.54485151, 0.52838015, 0.5469161 , 0.52220159, 0.58956543,\n        0.53023456, 0.53505836, 0.52538901, 0.5874495 , 0.56487426,\n        0.51894053, 0.47548597, 0.46484688, 0.48585999, 0.54373533,\n        0.54665337, 0.53561404, 0.50608959, 0.54885905, 0.53424172,\n        0.46651764, 0.44342754, 0.52126385, 0.51730385, 0.54642727,\n...\n        0.51485022, 0.54526266, 0.56452886, 0.53210093, 0.48178544,\n        0.48178544, 0.50612891, 0.56691244, 0.51990944, 0.52502164,\n        0.50958216, 0.55793676, 0.44404   , 0.52448209, 0.48686913,\n        0.56332066, 0.52521051, 0.51814259, 0.46920512, 0.51603195,\n        0.53072562, 0.51595048, 0.53630404, 0.43181477, 0.55391178,\n        0.57070506, 0.56903199, 0.51904248, 0.52945788, 0.53952289,\n        0.53952289, 0.51812701, 0.49923604, 0.52906691, 0.4671826 ,\n        0.52698377, 0.51726112, 0.46513556, 0.49692648, 0.46807381,\n        0.52820218, 0.49967144, 0.48902506, 0.50575518, 0.47085906,\n        0.4851852 , 0.45673248, 0.57773131, 0.48480077, 0.46996168,\n        0.53748339, 0.53182171, 0.5103147 , 0.5103147 , 0.49133474,\n        0.54554063, 0.52673208, 0.5511138 , 0.58998622, 0.53771462,\n        0.55418171, 0.5674853 , 0.520238  , 0.50914638, 0.52098799,\n        0.46892154, 0.52379926, 0.53224398, 0.515661  , 0.55209096,\n        0.55209096, 0.49442448, 0.58035507, 0.4814399 , 0.44427651,\n        0.54220555, 0.49514774, 0.52382353, 0.51085379, 0.52001323,\n        0.49903082, 0.50654949, 0.53196033, 0.56042939, 0.51704828,\n        0.42743097, 0.49643753, 0.49888369, 0.47494508, 0.49874618,\n        0.51072927, 0.55398116, 0.55522295, 0.51050389, 0.49177326,\n        0.51430143, 0.49637886, 0.50751856, 0.56604999, 0.43600677]])</pre></li><li>t(chain, draw)float640.3371 0.2973 ... 0.3674 0.3316<pre>array([[0.3371054 , 0.29726915, 0.27300573, 0.26905926, 0.29280101,\n        0.31796648, 0.36930615, 0.36321222, 0.33618962, 0.33206385,\n        0.34124073, 0.29188982, 0.30256364, 0.359091  , 0.3472768 ,\n        0.33312179, 0.34042158, 0.28456985, 0.3324904 , 0.33625414,\n        0.33447625, 0.35689088, 0.3234821 , 0.3461572 , 0.3461572 ,\n        0.349096  , 0.33404081, 0.33387092, 0.32761221, 0.32332667,\n        0.30414051, 0.33409562, 0.33414926, 0.34491438, 0.32439538,\n        0.36051929, 0.35577437, 0.30562688, 0.36866062, 0.32633219,\n        0.29864861, 0.34749579, 0.36031319, 0.34675111, 0.30508521,\n        0.31772582, 0.30882574, 0.32210782, 0.36045364, 0.34401912,\n        0.31430795, 0.34463471, 0.31443756, 0.30549857, 0.30361372,\n        0.32156851, 0.31809165, 0.31842198, 0.31134767, 0.36448562,\n        0.31053682, 0.33082355, 0.32491828, 0.33644779, 0.29188293,\n        0.29346104, 0.34160531, 0.34821823, 0.30738338, 0.34464817,\n        0.35125178, 0.35060085, 0.34272455, 0.33871765, 0.3332869 ,\n        0.36364544, 0.27548054, 0.28566518, 0.31503568, 0.30554297,\n        0.32037471, 0.32374527, 0.2776344 , 0.30872587, 0.30335972,\n        0.36724692, 0.35807714, 0.35267409, 0.33865955, 0.32376617,\n        0.30147566, 0.30181834, 0.32629891, 0.34447501, 0.34597762,\n        0.32799991, 0.33506837, 0.32746062, 0.3056614 , 0.32189753,\n...\n        0.37762222, 0.37412116, 0.37029047, 0.30903563, 0.35683222,\n        0.35683222, 0.33277175, 0.3372519 , 0.32640248, 0.32379131,\n        0.32107406, 0.33894035, 0.35050873, 0.32708453, 0.31445256,\n        0.33719462, 0.35198159, 0.32350235, 0.32607305, 0.34696188,\n        0.34581126, 0.3497987 , 0.31581188, 0.35619222, 0.33685416,\n        0.3197483 , 0.32126088, 0.34267755, 0.3410377 , 0.33619267,\n        0.33619267, 0.3067468 , 0.29826433, 0.32215354, 0.33495338,\n        0.32046477, 0.34382576, 0.323156  , 0.3271871 , 0.32382085,\n        0.36405381, 0.36485683, 0.32970967, 0.37547317, 0.33306027,\n        0.34497091, 0.31576963, 0.33308696, 0.31994795, 0.3332607 ,\n        0.31861839, 0.35857302, 0.31716   , 0.31716   , 0.29825518,\n        0.33136652, 0.323331  , 0.30779721, 0.27731832, 0.30502179,\n        0.32011133, 0.32470958, 0.32102514, 0.31217172, 0.33113755,\n        0.34321255, 0.31447402, 0.32041614, 0.31592858, 0.33580202,\n        0.33580202, 0.33405093, 0.32951004, 0.33529236, 0.35059326,\n        0.31923759, 0.32828531, 0.33286343, 0.34424114, 0.3480115 ,\n        0.34028048, 0.31357993, 0.34830092, 0.28150374, 0.36272445,\n        0.32182007, 0.34760738, 0.34162037, 0.32972294, 0.34826573,\n        0.3566816 , 0.3227924 , 0.30973684, 0.31112808, 0.29893037,\n        0.3709809 , 0.36144851, 0.31023859, 0.36736403, 0.33155846]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T00:09:25.419589+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :8.755553007125854tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 8MB\nDimensions:      (chain: 2, draw: 500, __obs__: 1000)\nCoordinates:\n  * chain        (chain) int64 16B 0 1\n  * draw         (draw) int64 4kB 0 1 2 3 4 5 6 ... 493 494 495 496 497 498 499\n  * __obs__      (__obs__) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    rt,response  (chain, draw, __obs__) float64 8MB -0.8813 -1.571 ... -1.814\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li><li>__obs__: 1000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-0.8813 -1.571 ... -1.443 -1.814<pre>array([[[-0.88131232, -1.57091297, -0.92081727, ..., -2.37906631,\n         -1.3333713 , -1.7104794 ],\n        [-0.91246246, -1.605177  , -0.96358766, ..., -2.36318145,\n         -1.37783749, -1.73731322],\n        [-0.95810583, -1.59449177, -0.99890795, ..., -2.31727589,\n         -1.37926059, -1.72014473],\n        ...,\n        [-0.8222712 , -1.58990299, -0.88157132, ..., -2.41704759,\n         -1.34134907, -1.73415112],\n        [-0.91515873, -1.62088698, -0.96517151, ..., -2.40553668,\n         -1.38732278, -1.75713453],\n        [-0.90795205, -1.59164615, -0.95297074, ..., -2.34395665,\n         -1.36460642, -1.72321763]],\n\n       [[-0.87558988, -1.5958391 , -0.92759909, ..., -2.3938555 ,\n         -1.35805634, -1.73448709],\n        [-0.87934   , -1.61732529, -0.93549736, ..., -2.39604588,\n         -1.38029891, -1.75395544],\n        [-0.85579367, -1.56328928, -0.89821972, ..., -2.3937704 ,\n         -1.32007925, -1.70646711],\n        ...,\n        [-0.92109209, -1.61200043, -0.97046542, ..., -2.37611532,\n         -1.38374591, -1.74492945],\n        [-0.83075084, -1.55659744, -0.87605453, ..., -2.36942703,\n         -1.31314487, -1.69820485],\n        [-0.92130265, -1.67822855, -0.9897226 , ..., -2.44948656,\n         -1.44309777, -1.81357239]]], shape=(2, 500, 1000))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 134kB\nDimensions:                (chain: 2, draw: 500)\nCoordinates:\n  * chain                  (chain) int64 16B 0 1\n  * draw                   (draw) int64 4kB 0 1 2 3 4 5 ... 495 496 497 498 499\nData variables: (12/18)\n    step_size              (chain, draw) float64 8kB 0.5059 0.5059 ... 0.5671\n    n_steps                (chain, draw) float64 8kB 7.0 7.0 3.0 ... 7.0 3.0 3.0\n    energy                 (chain, draw) float64 8kB 1.983e+03 ... 1.987e+03\n    reached_max_treedepth  (chain, draw) bool 1kB False False ... False False\n    max_energy_error       (chain, draw) float64 8kB 0.5403 0.4229 ... -0.5541\n    tree_depth             (chain, draw) int64 8kB 3 3 2 2 2 3 3 ... 2 3 3 3 2 2\n    ...                     ...\n    lp                     (chain, draw) float64 8kB -1.981e+03 ... -1.984e+03\n    perf_counter_start     (chain, draw) float64 8kB 1.435e+06 ... 1.435e+06\n    step_size_bar          (chain, draw) float64 8kB 0.625 0.625 ... 0.6339\n    perf_counter_diff      (chain, draw) float64 8kB 0.006602 ... 0.003229\n    divergences            (chain, draw) int64 8kB 0 0 0 0 0 0 0 ... 0 0 0 0 0 0\n    diverging              (chain, draw) bool 1kB False False ... False False\nAttributes:\n    created_at:                  2025-09-27T00:09:25.454929+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               8.755553007125854\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (18)<ul><li>step_size(chain, draw)float640.5059 0.5059 ... 0.5671 0.5671<pre>array([[0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n        0.50588975, 0.50588975, 0.50588975, 0.50588975, 0.50588975,\n...\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974,\n        0.56706974, 0.56706974, 0.56706974, 0.56706974, 0.56706974]])</pre></li><li>n_steps(chain, draw)float647.0 7.0 3.0 3.0 ... 7.0 7.0 3.0 3.0<pre>array([[ 7.,  7.,  3.,  3.,  3.,  7.,  5.,  7.,  7.,  3.,  7.,  7.,  7.,\n         7.,  3.,  3.,  3.,  5.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,  7.,\n         7.,  7.,  7.,  3.,  3.,  3.,  7.,  3.,  7.,  3.,  7.,  7.,  7.,\n        15.,  7., 15.,  7.,  3., 15.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n         3.,  7.,  7.,  7.,  3.,  1.,  7.,  7.,  7.,  3.,  3.,  3.,  7.,\n         7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n         3.,  7.,  7.,  3.,  7.,  7.,  7.,  7.,  3.,  7.,  3.,  7.,  7.,\n         3.,  7.,  7.,  3.,  7.,  3.,  7.,  7.,  7.,  7.,  3.,  7.,  7.,\n         7.,  3.,  3.,  3.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,  3.,  3.,\n         3.,  7.,  3.,  7.,  7.,  7.,  7.,  3.,  7.,  7.,  7.,  3.,  3.,\n         7.,  3.,  7.,  7.,  3.,  3.,  7.,  7.,  3.,  7.,  3.,  1.,  7.,\n         7.,  7.,  7.,  7.,  3.,  7.,  7.,  7.,  3.,  7.,  7.,  7.,  7.,\n         7.,  3.,  7.,  5.,  7.,  7.,  3.,  3.,  3.,  3.,  7.,  7.,  7.,\n         3., 11.,  7.,  3.,  7.,  7.,  7.,  3.,  7.,  7.,  3.,  3.,  7.,\n         3.,  7.,  7.,  7.,  7.,  3.,  7.,  7.,  3.,  7.,  3.,  7.,  3.,\n         3.,  7.,  7.,  3.,  7.,  7.,  3.,  3.,  7.,  7.,  3.,  3.,  7.,\n         7.,  7.,  7.,  3.,  7.,  3.,  3.,  7.,  3.,  3.,  7.,  7., 15.,\n         3.,  3.,  7.,  1.,  3.,  3.,  7.,  3.,  7.,  3.,  7.,  7.,  3.,\n         7.,  7., 15.,  7.,  7.,  7.,  3.,  7.,  5.,  3.,  3.,  3.,  7.,\n         3.,  7.,  7.,  7.,  7.,  3.,  3.,  7.,  7.,  3.,  3.,  7.,  3.,\n...\n         7.,  7., 11.,  7.,  7.,  7.,  7.,  7.,  3.,  5.,  3.,  7.,  7.,\n         3.,  3.,  3.,  3.,  7.,  3.,  7.,  3.,  7.,  3.,  7.,  7.,  3.,\n         3.,  7.,  7.,  7.,  7.,  3.,  3.,  7.,  3.,  7.,  7.,  7.,  3.,\n         7.,  3.,  7.,  7.,  7.,  7.,  7.,  1.,  3.,  7.,  7.,  3.,  7.,\n         7.,  7.,  7.,  7.,  3.,  3.,  3.,  3.,  3.,  3.,  7.,  7.,  7.,\n         7.,  7.,  3.,  7.,  7.,  7.,  7.,  3.,  7.,  3.,  7.,  7.,  3.,\n         7.,  7.,  3.,  7.,  7.,  7.,  7.,  1.,  7.,  3.,  3.,  5.,  3.,\n         3.,  3.,  7.,  7.,  7.,  3.,  3.,  7.,  7.,  7.,  3.,  7.,  7.,\n         7.,  3.,  3.,  3., 15.,  3.,  3.,  3.,  7.,  7.,  7.,  7.,  7.,\n         7.,  7.,  3.,  3.,  3.,  7.,  3.,  3.,  3.,  7.,  7.,  3.,  3.,\n         3.,  7.,  7.,  3.,  3.,  7.,  7.,  3.,  3.,  7.,  3.,  3.,  7.,\n         3.,  7.,  3.,  3.,  7.,  7.,  7.,  7.,  3.,  3.,  7.,  3.,  3.,\n         7.,  7.,  3.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,  3.,  3.,  7.,\n         3.,  3.,  3.,  7.,  7.,  3.,  7.,  7.,  7.,  7.,  3.,  7.,  3.,\n         7.,  3.,  3.,  3.,  3.,  7.,  7.,  3.,  3.,  3.,  3.,  3.,  7.,\n         7.,  7.,  3.,  7.,  3.,  7.,  7.,  7.,  7., 11.,  7.,  3.,  7.,\n         7.,  7.,  7.,  3.,  3.,  3.,  3.,  7.,  7.,  3.,  7.,  7.,  7.,\n         3.,  3.,  3.,  7.,  7.,  7.,  7.,  7.,  3.,  3.,  7.,  3.,  7.,\n         7.,  7.,  7.,  7.,  3.,  7.,  3.,  7., 11.,  7.,  7.,  3.,  3.,\n         3.,  7.,  7.,  7.,  3.,  3.]])</pre></li><li>energy(chain, draw)float641.983e+03 1.983e+03 ... 1.987e+03<pre>array([[1983.36680488, 1983.45517794, 1987.16413217, 1988.0424661 ,\n        1987.85841585, 1983.92670195, 1987.01271577, 1987.73339336,\n        1987.74307005, 1981.69534843, 1981.37294495, 1986.37338339,\n        1988.80460421, 1986.95417806, 1984.1465934 , 1983.17769142,\n        1984.65050763, 1984.33824548, 1985.84059915, 1980.63932281,\n        1981.13243035, 1981.59485016, 1984.64010467, 1983.91656421,\n        1985.49273459, 1982.51294439, 1981.61495589, 1983.22076969,\n        1985.07936589, 1982.61366653, 1983.38107303, 1985.63958428,\n        1987.99110027, 1985.2942213 , 1983.00720474, 1983.77903513,\n        1981.77258585, 1985.43020152, 1983.52364062, 1983.07892609,\n        1983.66106811, 1985.43736253, 1988.35976983, 1985.16320137,\n        1982.26567384, 1981.9369534 , 1983.43163334, 1983.73323483,\n        1984.62602654, 1985.00455649, 1986.38243185, 1985.29581922,\n        1985.63432618, 1981.13121702, 1981.55441865, 1986.3238459 ,\n        1986.91177851, 1984.02437534, 1984.74268266, 1983.33216722,\n        1982.26069204, 1983.11981541, 1982.70791637, 1984.61042557,\n        1985.09689076, 1986.76340108, 1983.49721245, 1983.09410415,\n        1981.75106649, 1982.18577474, 1984.96461845, 1986.46671678,\n        1981.96026844, 1981.39401506, 1981.12373032, 1984.0208622 ,\n        1984.93773062, 1985.85017559, 1984.03693733, 1987.90328358,\n...\n        1985.51590352, 1984.02203398, 1982.4682701 , 1985.16522373,\n        1983.49644957, 1982.54159196, 1982.00871494, 1982.71306862,\n        1980.73005797, 1981.05641105, 1982.0026249 , 1981.68864825,\n        1982.8669357 , 1983.29182978, 1986.89488565, 1985.37249151,\n        1983.54442568, 1983.65701767, 1984.4194252 , 1984.41431246,\n        1984.96380213, 1985.35271319, 1981.9339256 , 1985.46248416,\n        1984.092747  , 1983.57313549, 1986.4301308 , 1984.84687912,\n        1986.11379248, 1982.46426687, 1983.14556465, 1982.84016104,\n        1981.81843528, 1983.62630347, 1983.5379381 , 1984.08856977,\n        1981.27645371, 1982.96171324, 1986.8108539 , 1986.59155574,\n        1982.9874751 , 1982.53687692, 1984.28429585, 1983.8176742 ,\n        1981.41169199, 1982.25623718, 1986.17853666, 1985.84912595,\n        1985.62289075, 1982.46089824, 1987.98714126, 1984.36275266,\n        1984.66889927, 1983.49336951, 1984.27787047, 1986.56001723,\n        1981.52659556, 1980.79233912, 1981.46409902, 1981.15621903,\n        1981.36220629, 1980.87980217, 1981.00436398, 1984.43259228,\n        1984.4972168 , 1987.79354607, 1985.74928324, 1981.61684488,\n        1982.76184928, 1982.95490526, 1984.9873915 , 1984.04951486,\n        1982.4331174 , 1984.37446504, 1984.99197817, 1985.39563898,\n        1982.52872334, 1982.8947753 , 1984.9118686 , 1986.79281329]])</pre></li><li>reached_max_treedepth(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False]])</pre></li><li>max_energy_error(chain, draw)float640.5403 0.4229 ... 0.7061 -0.5541<pre>array([[ 0.54032709,  0.42292156,  0.37130737,  0.14003838, -0.21655258,\n         0.45917244,  0.49897974,  0.37173155,  0.77615875, -0.22635033,\n        -0.23939437,  0.49505852,  0.64314423, -0.67497794, -0.29467444,\n         0.64139351,  1.35128639,  0.20044065,  0.18174715,  0.10611066,\n        -0.15226593, -0.17463173,  0.72149303, -0.60831754,  0.55363374,\n         0.81474755, -0.16089259,  0.71542909, -0.63783105, -0.21270618,\n         0.56943498,  0.62316673, -0.78899253,  1.01570748, -0.84969873,\n         0.32967848, -0.1669726 ,  0.91963953, -0.07267113, -0.11134046,\n         0.26140178,  1.02206593,  1.13748541, -1.40274745, -0.22109983,\n        -0.07005467,  0.78865581, -0.53725767, -0.49275061,  0.27982851,\n        -0.27432767,  0.65546175, -0.82635844, -0.05267339,  0.10269437,\n         0.99069364,  0.70006022, -0.08962688, -0.36602889,  0.71597871,\n        -0.36964176,  0.28959811, -0.27000722,  0.24785189, -0.31757339,\n        -0.50459828,  0.15660753,  0.64239225, -0.19859674,  0.35537127,\n         0.66487772,  1.77884129, -0.34005625,  0.16625907, -0.10789612,\n         0.83826113,  0.23042853,  0.36633437, -0.39690744,  1.905664  ,\n        -0.96019363,  0.27968791,  0.69889138, -0.53574035, -0.43596177,\n        -0.33474943,  0.39227096,  2.84401979, -2.99689472,  0.09954244,\n         0.63502079, -0.23297901,  0.42615283, -0.33418081, -0.23358745,\n         0.11665371,  0.33791257, -0.38770186, -0.30048289, -0.213322  ,\n...\n         2.53921244,  0.77227896, -1.69777965,  0.75528719, -0.2987475 ,\n         1.04271107,  3.63774365,  0.91327439,  0.16336156,  0.74863635,\n         0.3727459 , -0.0804828 ,  0.16401928,  1.33760856, -0.57211598,\n        -0.90606671, -0.60706294,  1.81923386,  0.86205017, -0.97404912,\n        -0.87332636,  1.10511569, -0.35203685,  0.85608939, -0.16817785,\n         0.57993863, -0.57786087,  0.87040419,  0.19378412,  0.08006577,\n         1.43341702,  0.22429016, -0.21207736, -0.4666218 ,  1.79738742,\n         1.01239903, -0.90468814, -0.64885649,  0.27413519,  0.82457227,\n        -0.73330827, -0.8500367 , -0.38693952,  1.3620342 , -0.66954499,\n         0.27644905,  0.20130873, -0.30042842,  0.25153116, -0.27280452,\n         0.60207017, -0.33645181,  0.16838267,  2.23314315,  0.80119369,\n         0.67989557, -0.09533088,  0.35594064,  0.85064282,  0.54816226,\n        -0.72152251,  0.38013172, -0.42156544, -0.37754971,  0.08960861,\n         0.34833165,  1.84101389, -1.91166415, -1.40316233, -0.50836533,\n         1.35374937, -0.15997217,  0.37784639,  0.242621  ,  0.21151944,\n         1.61175217, -0.16027124,  0.44156885,  0.20020154,  0.14411116,\n        -0.1548125 , -0.15546657, -0.0719752 ,  0.1433957 , -0.20477436,\n         0.46153847, -0.57002159,  0.22916229,  0.37258532,  0.45948015,\n         0.5848006 ,  0.54184331, -0.20655844,  0.53334025,  1.08404065,\n        -1.45590242, -0.18819118,  0.60069772,  0.70613935, -0.55405015]])</pre></li><li>tree_depth(chain, draw)int643 3 2 2 2 3 3 3 ... 2 2 2 3 3 3 2 2<pre>array([[3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3,\n        3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 4, 3, 4, 3, 2,\n        4, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 1, 3, 3, 3, 2, 2, 2, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3,\n        2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3,\n        3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2,\n        3, 3, 2, 2, 3, 3, 2, 3, 2, 1, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3,\n        3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 4, 3, 2, 3, 3, 3,\n        2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3,\n        2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3,\n        4, 2, 2, 3, 1, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 4, 3, 3, 3, 2, 3,\n        3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3,\n        2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3,\n        3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3,\n        3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3,\n        3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3,\n        2, 3, 3, 4, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3,\n        3, 3, 2, 4, 3, 3, 3, 2, 3, 4, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3,\n        2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2,\n        2, 2, 2, 3, 2, 4, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2,\n...\n        2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 1, 3, 2, 3, 3, 2, 3, 3, 3, 3,\n        2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3,\n        3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 1, 1, 3, 3, 4,\n        3, 3, 2, 3, 3, 1, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2,\n        3, 1, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3,\n        3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 2,\n        2, 2, 4, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3,\n        2, 2, 3, 2, 3, 3, 3, 4, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2,\n        3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2,\n        3, 2, 3, 3, 3, 3, 3, 1, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2,\n        2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3,\n        3, 3, 1, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3,\n        2, 2, 2, 4, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3,\n        3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3,\n        3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2,\n        2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2,\n        2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 4, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2,\n        3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3,\n        3, 2, 3, 2, 3, 4, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2]])</pre></li><li>process_time_diff(chain, draw)float640.006595 0.006667 ... 0.003229<pre>array([[0.006595, 0.006667, 0.003391, 0.003363, 0.003295, 0.006182,\n        0.005122, 0.006328, 0.006654, 0.003153, 0.006478, 0.006533,\n        0.006462, 0.006609, 0.003179, 0.003267, 0.003139, 0.004963,\n        0.006675, 0.006678, 0.006564, 0.006351, 0.006427, 0.003328,\n        0.006431, 0.006584, 0.006592, 0.00648 , 0.006409, 0.003142,\n        0.003267, 0.007366, 0.016362, 0.006452, 0.010035, 0.00357 ,\n        0.013412, 0.006935, 0.007046, 0.013634, 0.006575, 0.013504,\n        0.006437, 0.003263, 0.013247, 0.006428, 0.00645 , 0.007014,\n        0.006363, 0.006228, 0.006882, 0.006686, 0.003415, 0.006824,\n        0.006883, 0.006364, 0.003091, 0.001705, 0.006335, 0.006389,\n        0.006418, 0.003301, 0.003211, 0.003094, 0.0063  , 0.006481,\n        0.006324, 0.006583, 0.006562, 0.006591, 0.006664, 0.006547,\n        0.007003, 0.006709, 0.006496, 0.006658, 0.006573, 0.006586,\n        0.003195, 0.006554, 0.006491, 0.003362, 0.006511, 0.006706,\n        0.006502, 0.00638 , 0.003177, 0.006567, 0.003125, 0.006559,\n        0.006589, 0.003359, 0.006397, 0.006223, 0.003377, 0.006394,\n        0.003283, 0.006528, 0.006452, 0.006402, 0.006165, 0.003334,\n        0.006427, 0.006527, 0.006424, 0.003314, 0.003298, 0.003278,\n        0.006526, 0.007058, 0.006653, 0.006534, 0.006403, 0.003095,\n        0.006448, 0.003265, 0.003227, 0.003369, 0.006321, 0.003382,\n...\n        0.00324 , 0.003262, 0.006376, 0.003103, 0.003362, 0.006437,\n        0.003282, 0.006597, 0.003385, 0.00324 , 0.006488, 0.006558,\n        0.006846, 0.006848, 0.003271, 0.003217, 0.006663, 0.003267,\n        0.003314, 0.00672 , 0.006445, 0.003173, 0.006473, 0.007161,\n        0.006859, 0.006631, 0.006493, 0.003315, 0.006475, 0.00331 ,\n        0.003254, 0.006325, 0.003129, 0.003151, 0.003242, 0.006491,\n        0.006753, 0.003164, 0.006481, 0.006868, 0.006622, 0.00655 ,\n        0.003239, 0.006618, 0.003367, 0.006616, 0.003303, 0.003155,\n        0.003368, 0.003354, 0.00661 , 0.006437, 0.003144, 0.003318,\n        0.003175, 0.003209, 0.003161, 0.006423, 0.006504, 0.006446,\n        0.003286, 0.006509, 0.003296, 0.006451, 0.006511, 0.006639,\n        0.006351, 0.009556, 0.00636 , 0.003197, 0.006418, 0.006454,\n        0.006335, 0.006349, 0.003158, 0.003132, 0.003169, 0.003259,\n        0.006251, 0.006409, 0.003229, 0.006555, 0.006578, 0.006486,\n        0.003225, 0.003131, 0.003121, 0.006398, 0.00635 , 0.006511,\n        0.006247, 0.006451, 0.003253, 0.003191, 0.006577, 0.003139,\n        0.006524, 0.006406, 0.006497, 0.006615, 0.006783, 0.003332,\n        0.006961, 0.003271, 0.006636, 0.009971, 0.006503, 0.006506,\n        0.003242, 0.003117, 0.003121, 0.006648, 0.006467, 0.006624,\n        0.003211, 0.003229]])</pre></li><li>acceptance_rate(chain, draw)float640.7388 0.7879 ... 0.6484 0.8861<pre>array([[0.73877818, 0.78794849, 0.87725131, 0.92277129, 0.99215265,\n        0.83534245, 0.92142996, 0.93282092, 0.8109636 , 0.98553287,\n        0.97859453, 0.71169813, 0.81990299, 0.98652888, 1.        ,\n        0.65660677, 0.51185889, 0.92779616, 0.93631065, 0.95537546,\n        0.97234424, 0.95802381, 0.73389503, 0.9085608 , 0.75257504,\n        0.62393575, 0.95806502, 0.77757245, 0.89088854, 1.        ,\n        0.6903967 , 0.84541454, 0.96922177, 0.53281748, 1.        ,\n        0.90638497, 1.        , 0.57380865, 0.98552699, 0.98753805,\n        0.91450275, 0.63024878, 0.61940595, 1.        , 0.99639765,\n        0.99310158, 0.64635819, 0.98513214, 0.93732016, 0.82687006,\n        0.95616304, 0.77220443, 0.97532535, 0.98395806, 0.96632856,\n        0.51738419, 0.73024468, 1.        , 0.99690688, 0.73992763,\n        1.        , 0.77676407, 1.        , 0.86994809, 0.97177134,\n        0.95807301, 0.93838395, 0.74067739, 0.99459423, 0.86851592,\n        0.80007465, 0.62705433, 0.89697356, 0.89359017, 0.99480746,\n        0.62866343, 0.96554508, 0.84972191, 1.        , 0.38864973,\n        0.9923881 , 0.90925842, 0.62024794, 0.96980919, 0.99636268,\n        0.9727909 , 0.842372  , 0.2984129 , 0.90139541, 0.97351551,\n        0.73072102, 0.99946011, 0.78188423, 0.99306624, 1.        ,\n        0.94951419, 0.85523361, 1.        , 0.98330909, 0.95081178,\n...\n        0.45236589, 0.78801154, 1.        , 0.72200406, 0.97655367,\n        0.53096828, 0.40570437, 0.63130049, 0.95314804, 0.71063475,\n        0.7883609 , 0.97892219, 0.92590712, 0.51488755, 0.91552163,\n        0.98334027, 1.        , 0.45092833, 0.55960773, 1.        ,\n        0.98222085, 0.56303644, 0.97082284, 0.69330636, 0.9886324 ,\n        0.75680598, 1.        , 0.66684935, 0.88985487, 0.96120677,\n        0.49713142, 0.84241229, 0.91140285, 0.89435815, 0.39278727,\n        0.55442639, 1.        , 0.96459869, 0.89347146, 0.75075267,\n        0.9706899 , 1.        , 0.97885136, 0.51860951, 1.        ,\n        0.91616101, 0.89750516, 0.91407191, 0.90273082, 0.99938563,\n        0.76907466, 0.90854991, 0.93887141, 0.40635213, 0.63390909,\n        0.73565395, 0.95476775, 0.83012156, 0.56143293, 0.74363754,\n        1.        , 0.7816513 , 0.87220318, 0.9740804 , 0.9615085 ,\n        0.83017296, 0.47474634, 0.9839341 , 0.93428916, 1.        ,\n        0.39318001, 0.97081575, 0.86012149, 0.94287924, 0.86844665,\n        0.53033468, 1.        , 0.76689595, 0.87460072, 0.9193701 ,\n        0.99207713, 1.        , 0.99608494, 0.88604212, 0.99532747,\n        0.76289805, 1.        , 0.87043659, 0.79125347, 0.78672505,\n        0.75580922, 0.85896321, 0.98960149, 0.7014248 , 0.56897639,\n        0.9398669 , 0.95599454, 0.76566207, 0.64841938, 0.8861194 ]])</pre></li><li>largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n...\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan]])</pre></li><li>smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n...\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan]])</pre></li><li>index_in_trajectory(chain, draw)int64-7 -4 -3 -2 -2 3 ... 1 5 -1 4 -2 2<pre>array([[ -7,  -4,  -3,  -2,  -2,   3,   3,  -2,  -2,   3,   1,  -3,  -2,\n          5,  -3,  -2,   2,   3,   2,  -6,  -2,  -4,   2,   2,   0,   2,\n         -2,  -1,  -5,   1,  -3,  -2,  -2,  -1,   3,  -3,  -1,  -2,   3,\n         -4,  -5,  -3,  -2,  -1,  -3,  -4,  -3,   2,   4,   3,   4,   2,\n          1,  -2,  -2,  -1,  -2,  -1,   5,   6,   6,  -2,   1,  -2,   4,\n          1,  -4,   3,  -3,   6,   1,   3,  -3,  -6,  -3,  -5,   5,   1,\n         -2,  -2,  -1,  -2,  -2,  -2,  -2,  -4,   1,  -1,  -1,  -4,  -5,\n          1,   3,   6,  -1,  -3,  -1,  -2,   2,   6,  -3,  -2,   2,  -2,\n          3,  -1,   1,   2,  -1,   1,  -6,  -3,   6,   2,  -1,  -1,   3,\n          1,  -7,  -2,  -1,  -6,  -1,  -4,  -2,   1,  -4,   2,   2,  -2,\n          1,   3,   3,   6,  -3,  -2,  -1,   5,  -1,   1,   3,  -1,   1,\n         -5,  -1,  -3,   3,   2,   4,   6,  -2,   2,   2,  -4,   2,   3,\n         -2,   2,  -3,  -2,   1,  -1,  -1,   1,  -2,  -2,  -1,   3,  -3,\n          2,   1,  -6,   3,   6,  -6,   3,  -2,   1,  -2,   2,   3,  -1,\n         -2,   1,  -4,   3,   1,  -1,   3,  -6,  -2,   6,  -2,  -6,  -1,\n          3,  -6,  -6,  -3,  -3,   2,  -1,  -2,  -5,   4,  -2,  -2,  -1,\n         -6,  -2,   1,   0,   6,   2,   2,   2,   1,  -3,  -2,   3, -10,\n         -1,  -2,   2,   1,   1,  -3,  -6,  -1,  -2,  -2,  -2,  -5,   2,\n          1,   5, -10,   2,   2,   3,   3,   2,   2,   1,   0,  -2,  -5,\n          3,  -4,   7,   3,  -4,  -3,  -3,  -1,   2,  -1,  -1,   1,  -2,\n...\n         -3,   3,  -4,  -6,  -1,   2,  -1,  -2,  -1,  -1,  -2,   5,  -4,\n         -2,   2,   3,  -2,  -1,  -2,  -1,   2,  -4,   3,   3,  -3,  -1,\n         -2,   1,  -5,  -3,  -3,   2,  -1,   5,  -3,   4,   2,  -3,  -2,\n         -2,  -2,  -4,  -3,   3,  -2,   2,   0,   3,   2,  -1,   2,   3,\n          3,  -6,   2,   4,   2,  -2,   1,  -2,   1,  -1,  -6,   1,  -5,\n          6,   2,  -2,  -4,   4,   1,   3,  -2,  -2,   2,   4,   6,  -3,\n          3,  -2,   3,   1,  -2,  -6,  -2,  -1,  -6,   0,  -2,  -2,   1,\n         -1,  -1,   2,  -6,  -2,  -2,  -3,  -1,   2,   2,   1,   2,  -2,\n          1,   3,  -2,   2,  -2,  -2,  -1,  -2,   2,  -3,  -1,   1,   4,\n          4,   2,  -2,  -2,   2,   6,   2,  -2,  -2,  -4,   3,   3,  -1,\n         -3,   5,  -4,   1,   2,  -3,  -4,   3,  -3,  -3,   2,   3,   6,\n          2,  -4,  -2,  -1,  -4,  -2,  -7,  -1,  -2,   2,  -3,   1,  -1,\n         -6,   5,   0,   2,   6,   3,   2,  -4,  -2,  -5,  -3,  -1,  -4,\n         -2,  -2,   1,  -4,   2,   2,  -3,   4,  -4,  -1,  -1,   2,   2,\n          2,   0,  -3,  -1,   2,   6,  -5,   2,  -3,  -2,  -1,  -3,  -1,\n          3,  -2,  -2,  -2,  -3,  -3,  -2,  -1,  -3,   7,  -3,   0,  -2,\n          2,  -1,  -3,   3,   2,  -3,  -1,   1,   2,  -3,   4,  -1,  -2,\n         -1,  -2,   0,   7,   6,   2,   1,  -2,  -3,  -2,  -6,  -1,   2,\n         -6,   4,  -4,   4,   2,   2,   2,   1,  -4,   5,  -5,  -3,   3,\n          1,   5,  -1,   4,  -2,   2]])</pre></li><li>energy_error(chain, draw)float640.04474 0.1848 ... 0.7061 -0.1741<pre>array([[ 4.47364212e-02,  1.84813480e-01, -3.17455507e-02,\n         1.06484483e-01, -2.16552584e-01,  3.65119604e-01,\n        -3.21891375e-02,  1.43058433e-01, -4.71103277e-01,\n         2.97912105e-02, -2.39394369e-01,  4.10616490e-01,\n         1.89929414e-01, -3.31227041e-01, -2.94674435e-01,\n         2.38006301e-01, -1.03134124e-01, -5.67408405e-02,\n        -1.22000734e-01,  1.06110656e-01,  8.29296401e-02,\n         1.07335573e-01,  3.22612375e-01, -6.08317539e-01,\n         0.00000000e+00,  9.85568476e-02,  7.08559522e-02,\n         6.40660730e-01, -5.79990378e-01, -2.12706175e-01,\n         3.03505924e-01,  6.23166734e-01, -3.52207978e-01,\n         3.48702950e-01, -7.35799881e-01, -1.22393922e-01,\n        -5.57763373e-02, -1.86216484e-02,  2.65227942e-02,\n         2.20990154e-02, -3.06798153e-02,  4.41373435e-01,\n         1.07907806e+00, -1.40274745e+00, -1.20685198e-01,\n        -5.29961458e-02,  5.78047116e-01, -4.85747126e-02,\n        -4.92750615e-01,  2.79828514e-01, -8.86095750e-02,\n         6.55461752e-01, -8.26358444e-01,  2.65711744e-03,\n        -2.27956959e-02,  7.85028184e-01, -2.95377328e-01,\n        -8.96268823e-02, -2.93874523e-01,  3.00828156e-01,\n...\n        -8.35037521e-01, -1.97054763e-01,  6.38054963e-01,\n        -6.69544989e-01,  2.76449054e-01,  1.57909820e-02,\n         9.52535159e-02, -1.43164991e-01, -1.69077872e-01,\n         3.84788686e-01, -3.09643223e-01, -5.32599138e-02,\n         0.00000000e+00,  2.34467608e-01, -2.42585303e-01,\n         2.86274113e-02,  1.42203236e-01,  6.15856743e-01,\n        -7.43604908e-02, -7.21522512e-01,  3.80131717e-01,\n         4.16222500e-02, -3.30658221e-01,  3.02153966e-02,\n        -3.12785656e-02,  1.75258990e+00,  1.10518170e-01,\n        -1.40316233e+00, -1.97842233e-01,  0.00000000e+00,\n        -6.74270871e-02, -8.75537200e-02, -1.87653953e-01,\n         1.59659914e-01,  2.67073557e-02, -1.16350067e-01,\n        -8.06884380e-02,  3.00132731e-02,  1.44111157e-01,\n         1.96618807e-03, -1.92296191e-02,  2.77879639e-02,\n         1.40508849e-01, -6.53430636e-02,  4.61538466e-01,\n        -5.70021587e-01, -6.39884970e-02,  2.77598186e-01,\n        -1.76739055e-01,  1.79374806e-01,  5.50125172e-02,\n        -2.06558438e-01,  1.06193376e-01,  1.08404065e+00,\n        -1.41824727e+00,  8.03869512e-02, -1.66158151e-01,\n         7.06139353e-01, -1.74058428e-01]])</pre></li><li>lp(chain, draw)float64-1.981e+03 ... -1.984e+03<pre>array([[-1981.32730519, -1982.20541617, -1985.64308547, -1987.27095758,\n        -1982.09971214, -1982.64615014, -1983.30318537, -1985.32065896,\n        -1981.47230987, -1980.90666709, -1980.10050324, -1985.57932622,\n        -1984.51463484, -1983.69014291, -1980.37657549, -1982.18645157,\n        -1980.57742976, -1983.1142378 , -1980.10348439, -1980.50441529,\n        -1980.7079148 , -1981.28586047, -1983.40567783, -1980.52323118,\n        -1980.52323118, -1980.81200368, -1980.74092151, -1982.69067291,\n        -1981.44575189, -1980.33709087, -1981.74864795, -1985.25496117,\n        -1982.27545392, -1982.81398405, -1980.98252871, -1981.68979057,\n        -1981.26370905, -1980.95116385, -1981.96113549, -1981.63185534,\n        -1981.94396221, -1981.87992525, -1986.085186  , -1981.42329717,\n        -1981.20882055, -1980.95270523, -1982.3609522 , -1982.95170641,\n        -1981.60711834, -1983.86071716, -1981.37549422, -1984.2838639 ,\n        -1980.56881774, -1981.01275769, -1981.15885614, -1983.3456432 ,\n        -1983.77791941, -1983.00383209, -1981.34845386, -1982.08397927,\n        -1980.73168146, -1982.59795064, -1982.07886093, -1983.60659796,\n        -1983.15950609, -1982.2336326 , -1980.92216842, -1980.73455256,\n        -1981.21592255, -1981.24136336, -1983.61951557, -1981.48986528,\n        -1980.30518953, -1980.6507314 , -1980.64561026, -1982.47351564,\n        -1983.8488545 , -1983.65457855, -1981.552536  , -1984.99994203,\n...\n        -1982.02214753, -1981.14996026, -1981.14158704, -1983.38616862,\n        -1980.79768041, -1982.13348037, -1981.36177793, -1980.2260038 ,\n        -1980.35666439, -1980.51922554, -1980.51922554, -1981.14963446,\n        -1982.47343254, -1980.64037103, -1981.85257079, -1983.52343662,\n        -1981.69083079, -1982.84101622, -1981.19338622, -1984.16119575,\n        -1984.63715769, -1981.67009681, -1981.04301496, -1984.45565598,\n        -1981.0796908 , -1983.29113785, -1982.60557055, -1982.78038967,\n        -1982.22282175, -1981.70449465, -1981.22335778, -1981.21480482,\n        -1980.52220568, -1980.52220568, -1982.41342991, -1980.67869142,\n        -1980.82986095, -1981.84945853, -1985.15833723, -1983.26968713,\n        -1980.8802455 , -1982.12065069, -1982.89907693, -1980.870921  ,\n        -1980.66435272, -1980.98854112, -1983.8325949 , -1985.03834363,\n        -1982.38488108, -1981.19173036, -1981.19173036, -1982.09786788,\n        -1981.97209549, -1980.42019549, -1982.13628709, -1981.38308267,\n        -1980.35380135, -1979.98752827, -1980.29333071, -1981.01195597,\n        -1980.595139  , -1980.68377166, -1980.85090857, -1983.36180551,\n        -1981.71834973, -1985.90530061, -1981.18859539, -1980.34869512,\n        -1981.55815136, -1981.00184279, -1982.45063223, -1981.40791472,\n        -1981.31226846, -1981.17405878, -1984.45102581, -1981.77264985,\n        -1981.91238861, -1980.76173138, -1983.93252889, -1983.87611127]])</pre></li><li>perf_counter_start(chain, draw)float641.435e+06 1.435e+06 ... 1.435e+06<pre>array([[1434779.00674508, 1434779.01341996, 1434779.02018292,\n        1434779.02365375, 1434779.02707162, 1434779.03043712,\n        1434779.03665417, 1434779.04184938, 1434779.04825033,\n        1434779.05497263, 1434779.05822671, 1434779.06475158,\n        1434779.07137446, 1434779.077871  , 1434779.08453437,\n        1434779.08774358, 1434779.09106287, 1434779.09423571,\n        1434779.09929871, 1434779.10604446, 1434779.11279883,\n        1434779.11942042, 1434779.12582783, 1434779.13233308,\n        1434779.13572829, 1434779.14223767, 1434779.14890221,\n        1434779.155555  , 1434779.16209913, 1434779.16858129,\n        1434779.17176871, 1434779.17510029, 1434779.19039379,\n        1434779.216219  , 1434779.22942517, 1434779.24600608,\n        1434779.24968808, 1434779.26557358, 1434779.27264646,\n        1434779.27980242, 1434779.29371879, 1434779.30039746,\n        1434779.31403571, 1434779.32054983, 1434779.32388188,\n        1434779.33721283, 1434779.34370833, 1434779.35019825,\n        1434779.35728958, 1434779.36369337, 1434779.36996571,\n        1434779.37692179, 1434779.38369458, 1434779.38717221,\n        1434779.39409329, 1434779.40103746, 1434779.40747933,\n        1434779.41060313, 1434779.41237188, 1434779.41876396,\n...\n        1434782.16790346, 1434782.17436437, 1434782.18092213,\n        1434782.18742821, 1434782.19080921, 1434782.19745246,\n        1434782.20081854, 1434782.20733446, 1434782.21394179,\n        1434782.22066971, 1434782.22706421, 1434782.23668942,\n        1434782.24312287, 1434782.24636225, 1434782.25285125,\n        1434782.25936158, 1434782.26575517, 1434782.27216263,\n        1434782.27535288, 1434782.27851654, 1434782.28171633,\n        1434782.28503754, 1434782.29133038, 1434782.29779762,\n        1434782.30108333, 1434782.30771246, 1434782.31438092,\n        1434782.32090904, 1434782.32420408, 1434782.32737188,\n        1434782.33052542, 1434782.33697008, 1434782.34335788,\n        1434782.349932  , 1434782.35621312, 1434782.36273779,\n        1434782.36603054, 1434782.36925742, 1434782.3758965 ,\n        1434782.37907013, 1434782.38565125, 1434782.39214704,\n        1434782.39869238, 1434782.40538367, 1434782.41231625,\n        1434782.41572562, 1434782.42295504, 1434782.42629958,\n        1434782.4330195 , 1434782.44305871, 1434782.44960838,\n        1434782.45620858, 1434782.45950279, 1434782.46265529,\n        1434782.46580792, 1434782.47252083, 1434782.47905721,\n        1434782.48575921, 1434782.48900983]])</pre></li><li>step_size_bar(chain, draw)float640.625 0.625 0.625 ... 0.6339 0.6339<pre>array([[0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n        0.62500402, 0.62500402, 0.62500402, 0.62500402, 0.62500402,\n...\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095,\n        0.63385095, 0.63385095, 0.63385095, 0.63385095, 0.63385095]])</pre></li><li>perf_counter_diff(chain, draw)float640.006602 0.006692 ... 0.003229<pre>array([[0.00660246, 0.00669242, 0.003408  , 0.00337004, 0.00331475,\n        0.00618221, 0.00514283, 0.00633388, 0.0066745 , 0.00316037,\n        0.00647867, 0.00656921, 0.00646179, 0.00662258, 0.00317838,\n        0.00328192, 0.00313871, 0.00497042, 0.00667892, 0.00669562,\n        0.0065655 , 0.00635979, 0.00643446, 0.00333917, 0.00644087,\n        0.00660129, 0.00659704, 0.00649337, 0.006416  , 0.00314508,\n        0.00328771, 0.01518146, 0.02549542, 0.01247837, 0.01646025,\n        0.00357975, 0.01566679, 0.00694696, 0.00707229, 0.013822  ,\n        0.00658446, 0.01353421, 0.00644042, 0.003277  , 0.01326387,\n        0.00644533, 0.00645025, 0.00703579, 0.00636342, 0.00622792,\n        0.00691321, 0.006699  , 0.00342842, 0.00684517, 0.00689054,\n        0.00639017, 0.00309008, 0.001711  , 0.00634271, 0.00639254,\n        0.00642313, 0.00330071, 0.00322458, 0.00309317, 0.00629987,\n        0.00648817, 0.00633063, 0.00659367, 0.00657692, 0.006606  ,\n        0.00668954, 0.00659683, 0.00774146, 0.00672921, 0.00650108,\n        0.00673017, 0.006586  , 0.00662933, 0.00319508, 0.00657108,\n        0.00650238, 0.00336858, 0.00652129, 0.00671658, 0.0065225 ,\n        0.00638096, 0.00318454, 0.00658183, 0.00313204, 0.0065705 ,\n        0.00662538, 0.00336346, 0.00640579, 0.00622387, 0.00339221,\n        0.00640254, 0.00329046, 0.00653929, 0.00645821, 0.00641387,\n...\n        0.00667642, 0.00326729, 0.00331458, 0.00676079, 0.00644496,\n        0.00317283, 0.00648138, 0.00726446, 0.00696408, 0.00664546,\n        0.00649658, 0.00332154, 0.00648992, 0.00333154, 0.00325367,\n        0.00632567, 0.00312837, 0.00315133, 0.00324854, 0.00649129,\n        0.00678033, 0.00316417, 0.00649842, 0.00695213, 0.00663475,\n        0.00655846, 0.00323908, 0.00665137, 0.00336654, 0.00663217,\n        0.00331879, 0.00315467, 0.00337217, 0.00336258, 0.00662454,\n        0.00644433, 0.00314358, 0.00332508, 0.00317437, 0.00321875,\n        0.00316092, 0.00642317, 0.00650854, 0.00646225, 0.00329483,\n        0.00657729, 0.00330092, 0.00646075, 0.006518  , 0.00666442,\n        0.00635117, 0.00957138, 0.00637375, 0.00319708, 0.0064245 ,\n        0.00646542, 0.00634358, 0.00635162, 0.00315846, 0.00313233,\n        0.00316875, 0.00327588, 0.00625037, 0.00643058, 0.0032395 ,\n        0.00656042, 0.00659862, 0.00648596, 0.00325   , 0.00313096,\n        0.00312158, 0.00640088, 0.00635017, 0.00652704, 0.00624688,\n        0.00645796, 0.00325371, 0.00319117, 0.00658883, 0.00313908,\n        0.00654292, 0.00641304, 0.0064975 , 0.00663121, 0.00685608,\n        0.00334013, 0.00715058, 0.00327429, 0.00666092, 0.00997996,\n        0.00650296, 0.00650871, 0.00324167, 0.00311692, 0.00312075,\n        0.00666488, 0.00647513, 0.00664054, 0.00321121, 0.00322871]])</pre></li><li>divergences(chain, draw)int640 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0<pre>array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n...\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T00:09:25.454929+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :8.755553007125854tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 24kB\nDimensions:                  (__obs__: 1000, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 8kB 0 1 2 3 4 ... 996 997 998 999\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 16kB ...\nAttributes:\n    created_at:                  2025-09-27T00:09:25.458147+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 1000</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float641.046 1.0 2.171 ... 1.0 2.363 1.0<pre>array([[1.04566455, 1.        ],\n       [2.17065811, 1.        ],\n       [1.1842469 , 1.        ],\n       ...,\n       [3.28606343, 1.        ],\n       [1.84012544, 1.        ],\n       [2.36321115, 1.        ]], shape=(1000, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (6)created_at :2025-09-27T00:09:25.458147+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> <pre># An angle model with an analytical likelihood function.\n# Because `model` is known, no `list_params` needs to be provided.\n\ncustom_angle_model = hssm.HSSM(\n    data,\n    model=\"angle\",\n    model_config={\n        \"bounds\": {\n            \"v\": (-3.0, 3.0),\n            \"a\": (0.3, 3.0),\n            \"z\": (0.1, 0.9),\n            \"t\": (0.001, 2.0),\n            \"theta\": (-0.1, 1.3),\n        }  # bounds will be used to create Uniform (uninformative) priors by default\n        # if priors are not supplied in `include`.\n    },\n    loglik=custom_angle_logp,\n    loglik_kind=\"analytical\",\n)\n\n# A fully customized model with a custom likelihood function.\n# Because `model` is not known, a `list_params` needs to be provided.\n\nmy_custom_model = hssm.HSSM(\n    data,\n    model=\"my_model\",\n    model_config={\n        \"list_params\": [\"v\", \"a\", \"z\", \"t\", \"theta\"],\n        \"bounds\": {\n            \"v\": (-3.0, 3.0),\n            \"a\": (0.3, 3.0),\n            \"z\": (0.1, 0.9),\n            \"t\": (0.001, 2.0),\n            \"theta\": (-0.1, 1.3),\n        } # bounds will be used to create Uniform (uninformative) priors by default\n          # if priors are not supplied in `include`.\n        \"default_priors\": ... # usually no need to supply this.\n        \"rv\": MyRV # provide a RandomVariable class if pps is needed.\n    },\n    loglik=\"my_model.onnx\", # Can be a path to an onnx model.\n    loglik_kind=\"approx_differentiable\",\n    include=[...]\n)\n</pre> In\u00a0[16]: Copied! <pre>import bambi as bmb\nimport hddm_wfpt\n\n\n# Define a function with fun(data, *) signature\ndef my_blackbox_loglik(data, v, a, z, t, err=1e-8):\n    \"\"\"Create a blackbox log-likelihood function for the DDM model.\n\n    Note the function signature: the first argument must be the data, and the\n    remaining arguments are the parameters to be estimated. The function must\n    return the log-likelihood of the data given the parameters.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2D array with columns for the RT and choice of each trial.\n    \"\"\"\n    data = data[:, 0] * data[:, 1]\n    data_nrows = data.shape[0]\n    # Our function expects inputs as float64, but they are not guaranteed to\n    # come in as such --&gt; we type convert\n    return hddm_wfpt.wfpt.wiener_logp_array(\n        np.float64(data),\n        (np.ones(data_nrows) * v).astype(np.float64),\n        np.ones(data_nrows) * 0,\n        (np.ones(data_nrows) * 2 * a).astype(np.float64),\n        (np.ones(data_nrows) * z).astype(np.float64),\n        np.ones(data_nrows) * 0,\n        (np.ones(data_nrows) * t).astype(np.float64),\n        np.ones(data_nrows) * 0,\n        err,\n        1,\n    )\n\n\n# Create the model with pdf_ddm_blackbox\nmodel = hssm.HSSM(\n    data=data,\n    model=\"ddm\",\n    loglik=my_blackbox_loglik,\n    loglik_kind=\"blackbox\",\n    model_config={\n        \"bounds\": {\n            \"v\": (-10.0, 10.0),\n            \"a\": (0.0, 4.0),\n            \"z\": (0.0, 1.0),\n            \"t\": (0.0, 2.0),\n        }\n    },\n    t=bmb.Prior(\"Uniform\", lower=0.0, upper=2.0),\n)\n</pre> import bambi as bmb import hddm_wfpt   # Define a function with fun(data, *) signature def my_blackbox_loglik(data, v, a, z, t, err=1e-8):     \"\"\"Create a blackbox log-likelihood function for the DDM model.      Note the function signature: the first argument must be the data, and the     remaining arguments are the parameters to be estimated. The function must     return the log-likelihood of the data given the parameters.      Parameters     ----------     data : np.ndarray         A 2D array with columns for the RT and choice of each trial.     \"\"\"     data = data[:, 0] * data[:, 1]     data_nrows = data.shape[0]     # Our function expects inputs as float64, but they are not guaranteed to     # come in as such --&gt; we type convert     return hddm_wfpt.wfpt.wiener_logp_array(         np.float64(data),         (np.ones(data_nrows) * v).astype(np.float64),         np.ones(data_nrows) * 0,         (np.ones(data_nrows) * 2 * a).astype(np.float64),         (np.ones(data_nrows) * z).astype(np.float64),         np.ones(data_nrows) * 0,         (np.ones(data_nrows) * t).astype(np.float64),         np.ones(data_nrows) * 0,         err,         1,     )   # Create the model with pdf_ddm_blackbox model = hssm.HSSM(     data=data,     model=\"ddm\",     loglik=my_blackbox_loglik,     loglik_kind=\"blackbox\",     model_config={         \"bounds\": {             \"v\": (-10.0, 10.0),             \"a\": (0.0, 4.0),             \"z\": (0.0, 1.0),             \"t\": (0.0, 2.0),         }     },     t=bmb.Prior(\"Uniform\", lower=0.0, upper=2.0), ) <pre>Model initialized successfully.\n</pre> In\u00a0[17]: Copied! <pre>model.graph()\n</pre> model.graph() Out[17]: In\u00a0[18]: Copied! <pre>sample = model.sample()\n</pre> sample = model.sample() <pre>Using default initvals. \n\n</pre> <pre>Multiprocess sampling (4 chains in 4 jobs)\nCompoundStep\n&gt;Slice: [z]\n&gt;Slice: [a]\n&gt;Slice: [t]\n&gt;Slice: [v]\n/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n</pre> <pre>Output()</pre> <pre>/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n</pre> <pre></pre> <pre>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 25 seconds.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4000/4000 [00:01&lt;00:00, 2103.47it/s]\n</pre> In\u00a0[19]: Copied! <pre>model.summary()\n</pre> model.summary() Out[19]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat z 0.510 0.013 0.485 0.534 0.000 0.0 1182.0 2018.0 1.0 a 1.460 0.027 1.410 1.510 0.001 0.0 1812.0 2348.0 1.0 t 0.334 0.021 0.297 0.375 0.001 0.0 1363.0 2095.0 1.0 v 0.515 0.033 0.452 0.574 0.001 0.0 1368.0 2141.0 1.0 In\u00a0[20]: Copied! <pre>model.plot_trace()\n</pre> model.plot_trace() <p>Using the blackbox interface provides maximum flexibility on the user side. We hope you will find it useful!</p>"},{"location":"tutorials/likelihoods/#likelihood-functions-in-hssm-explained","title":"Likelihood functions in HSSM explained\u00b6","text":"<p>One of the design goals of HSSM is its flexibility. It is built from ground up to support many types of likelihood functions out-of-the-box. For more tailored applications, HSSM provides a convenient toolbox. This allows users to create their own likelihood functions, which can seamlessly integrate with the HSSM class, facilitating a highly customizable analysis environment. This notebook focuses on explaining how to use different types of likelihoods with HSSM.</p>"},{"location":"tutorials/likelihoods/#colab-instructions","title":"Colab Instructions\u00b6","text":"<p>If you would like to run this tutorial on Google colab, please click this link.</p> <p>Once you are in the colab, follow the installation instructions below and then restart your runtime.</p> <p>Just uncomment the code in the next code cell and run it!</p> <p>NOTE:</p> <p>You may want to switch your runtime to have a GPU or TPU. To do so, go to Runtime &gt; Change runtime type and select the desired hardware accelerator.</p> <p>Note that if you switch your runtime you have to follow the installation instructions again.</p>"},{"location":"tutorials/likelihoods/#load-modules","title":"Load Modules\u00b6","text":""},{"location":"tutorials/likelihoods/#pre-simulate-some-data","title":"Pre-simulate some data\u00b6","text":""},{"location":"tutorials/likelihoods/#three-kinds-of-likelihoods","title":"Three Kinds of Likelihoods\u00b6","text":"<p>HSSM supports 3 kinds of likelihood functions supported via the <code>loglik_kind</code> parameter to the <code>HSSM</code> class:</p> <ul> <li><p><code>\"analytical\"</code>: These likelihoods are usually closed-form solutions to the actual likelihoods. For example, For <code>ddm</code> models, HSSM provides the analytical likelihoods in Navarro &amp; Fuss (2009). HSSM expects these functions to be written with <code>pytensor</code>, which can be compiled by <code>pytensor</code> as part of a computational graph. As such, they are differentiable as well.</p> </li> <li><p><code>\"approx_differentiable\"</code>: These likelihoods are usually approximations of the actual likelihood functions with neural networks. These networks can be trained with any popular deep learning framework such as <code>PyTorch</code> and <code>TensorFlow</code> and saved as <code>onnx</code> files. HSSM can load the <code>onnx</code> files and translate the information of the neural network with either the <code>jax</code> or the <code>pytensor</code> backends. Please see below for detailed explanations for these backends. The <code>backend</code> option can be supplied via the <code>\"backend\"</code> field in <code>model_config</code>. This field of <code>model_config</code> is not applicable to other kinds of likelihoods.</p> <ul> <li>the <code>jax</code> backend: The basic computations in the likelihood are jax operations (valid <code>JAX</code> functions), which are wrapped in a <code>pytensor</code> <code>Op</code>. When sampling using the default NUTS sampler in <code>PyMC</code>, this option might be slightly faster but more prone to compatibility issues especially during parallel sampling due how <code>JAX</code> handles paralellism.The preferred usage of this backend is together with the <code>nuts_numpyro</code> and <code>black_jax</code> (experimental) samplers. Here JAX support is native and performance is optimized.</li> <li>the <code>pytensor</code> backend: The basic computations in the likelihood are pytensor operations (valid <code>pytensor</code> functions). When sampling using the default NUTS sampler in <code>PyMC</code>, this option allows for maximum compatibility. Not recommended when using <code>JAX</code>-based samplers.</li> </ul> </li> <li><p><code>\"blackbox\"</code>: Use this option for \"black box\" likelihoods that are not differentiable. These likelihoods are typically <code>Callable</code>s in Python that cannot be directly integrated to a <code>pytensor</code> computational graph. <code>hssm</code> will wrap these <code>Callable</code>s in a <code>pytensor</code> <code>Op</code> so it can be part of the graph.</p> </li> </ul>"},{"location":"tutorials/likelihoods/#default-vs-custom-likelihoods","title":"Default vs. Custom Likelihoods\u00b6","text":"<p>HSSM provides many default likelihood functions out-of-the-box. The supported likelihoods are:</p> <ul> <li>For <code>analytical</code> kind: <code>ddm</code> and <code>ddm_sdv</code> models.</li> <li>For <code>approx_differentiable</code> kind: <code>ddm</code>, <code>ddm_sdv</code>, <code>angle</code>, <code>levy</code>, <code>ornstein</code>, <code>weibull</code>, <code>race_no_bias_angle_4</code> and <code>ddm_seq2_no_bias</code>.</li> <li>For <code>blackbox</code> kind: <code>ddm</code>, <code>ddm_sdv</code> and <code>full_ddm</code> models.</li> </ul> <p>For a model that has default likelihood functions, only the <code>model</code> argument needs to be specified.</p>"},{"location":"tutorials/likelihoods/#overriding-default-likelihoods","title":"Overriding default likelihoods\u00b6","text":"<p>Sometimes a likelihood other than the default version is preferred. In that case, you can supply a likelihood function directly to the <code>loglik</code> parameter. We will discuss acceptable likelihood function types in a moment.</p> <p>For illustration we load the basic analytical DDM likelihood, which is shipped with HSSM and supply it manually our HSSM model class.</p>"},{"location":"tutorials/likelihoods/#using-custom-likelihoods","title":"Using Custom Likelihoods\u00b6","text":"<p>If you are specifying a model with a kind of likelihood that's not included in the list above, then HSSM considers that you are using a custom model with custom likelihoods. In this case, you will need to specify your entire model. Below is the procedure to specify a custom model:</p> <ol> <li><p>Specify a <code>model</code> string. It can be any string that helps identify the model, but if it is not one of the model strings supported in the <code>ssm_simulators</code> package see full list here, you will need to supply a <code>RandomVariable</code> class to <code>model_config</code> detailed below. Otherwise, you can still perform MCMC sampling, but sampling from the posterior predictive distribution will raise a ValueError.</p> </li> <li><p>Specify a <code>model_config</code>. It typically contains the following fields:</p> <ul> <li><code>\"list_params\"</code>: Required if your <code>model</code> string is not one of <code>ddm</code>, <code>ddm_sdv</code>, <code>full_ddm</code>, <code>angle</code>, <code>levy</code>, <code>ornstein</code>, <code>weibull</code>, <code>race_no_bias_angle_4</code> and <code>ddm_seq2_no_bias</code>. A list of <code>str</code> indicating the parameters of the model. The order in which the parameters are specified in this list is important. Values for each parameter will be passed to the likelihood function in this order.</li> <li><code>\"backend\"</code>: Optional. Only used when <code>loglik_kind</code> is <code>approx_differentiable</code> and an onnx file is supplied for the likelihood approximation network (LAN). Valid values are <code>\"jax\"</code> or <code>\"pytensor\"</code>. It determines whether the LAN in ONNX should be converted to <code>\"jax\"</code> or <code>\"pytensor\"</code>. If not provided, <code>jax</code> will be used for maximum performance.</li> <li><code>\"default_priors\"</code>: Optional. A <code>dict</code> indicating the default priors for each parameter.</li> <li><code>\"bounds\"</code>: Optional. A <code>dict</code> of <code>(lower, upper)</code> tuples indicating the acceptable boundaries for each parameter. In the case of LAN, these bounds are training boundaries.</li> <li><code>\"rv\"</code>: Optional. Can be a <code>RandomVariable</code> class containing the user's own <code>rng_fn</code> function for sampling from the distribution that the user is supplying. If not supplied, HSSM will automatically generate a <code>RandomVariable</code> using the simulator identified by <code>model</code> from the <code>ssm_simulators</code> package. If <code>model</code> is not supported in <code>ssm_simulators</code>, a warning will be raised letting the user know that sampling from the <code>RandomVariable</code> will result in errors.  </li> </ul> </li> <li><p>Specify <code>loglik</code> and <code>loglik_kind</code>.</p> </li> <li><p>Specify parameter priors in <code>include</code>.</p> </li> </ol> <p>NOTE:</p> <p><code>default_priors</code> and <code>bounds</code> in <code>model_config</code> specifies  default priors and bounds for the model. Actual priors and defaults should be provided via the <code>include</code> list and will override these defaults.</p> <p>Below are a few examples:</p>"},{"location":"tutorials/likelihoods/#supported-types-of-likelihoods","title":"Supported types of likelihoods\u00b6","text":"<p>When default likelihoods are not used, custom likelihoods are supplied via <code>loglik</code> argument to <code>HSSM</code>. Depending on what <code>loglik_kind</code> is used, <code>loglik</code> supports different types of Python objects:</p> <ul> <li><p><code>Type[pm.Distribution]</code>: Supports all <code>loglik_kind</code>s.</p> <p>You can pass any subclass of <code>pm.Distribution</code> to <code>loglik</code> representing the underlying top-level distribution of the model. It has to be a class instead of an instance of the class.</p> </li> <li><p><code>Op</code>: Supports all <code>loglik_kind</code> kinds.</p> <p>You can pass a <code>pytensor</code> <code>Op</code> (an instance instead of the class itself), in which case HSSM will create a top-level <code>pm.Distribution</code>, which calls this <code>Op</code> in its <code>logp</code> function to compute the log-likelihood.</p> </li> <li><p><code>Callable</code>: Supports all <code>loglik_kind</code>s.</p> <p>You can use any Python Callable as well. When <code>loglik_kind</code> is <code>blackbox</code>, HSSM will wrap it in a <code>pytensor</code> <code>Op</code> and create a top-level <code>pm.Distribution</code> with it. Otherwise, HSSM will assume that this Python callable is created with <code>pytensor</code> and is thus differentiable.</p> </li> <li><p><code>str</code> or <code>Pathlike</code>: Only supported when <code>loglik_kind</code> is <code>approx_differentiable</code>.</p> <p>The <code>str</code> or <code>Pathlike</code> indicates the path to an <code>onnx</code> file which represents the neural network for likelihood approximation. In the case of <code>str</code>, if the path indicated by <code>str</code> is not found locally, HSSM will also look for the <code>onnx</code> file in the official HuggingFace repo. An error is thrown when the <code>onnx</code> file is not found.</p> </li> </ul> <p>Note</p> <p>When using <code>Op</code> and <code>Callable</code> types of likelihoods, they need to have the this signature:</p> <pre><code>def logp_fn(data, *):\n    ...\n</code></pre> <p>where <code>data</code> is a 2-column numpy array and <code>*</code> represents named arguments in the order of the parameters in <code>list_params</code>. For example, if a model's <code>list_params</code> is <code>[\"v\", \"a\", \"z\", \"t\"]</code>, then the <code>Op</code> or <code>Callable</code> should at least look like this:</p> <pre><code>def logp_fn(data, v, a, z, t):\n    ...\n</code></pre>"},{"location":"tutorials/likelihoods/#using-blackbox-likelihoods","title":"Using <code>blackbox</code> likelihoods\u00b6","text":"<p>HSSM also supports \"black box\" likelihood functions, which are assumed to not be differentiable. When <code>loglik_kind</code> is <code>blackbox</code>, by default, HSSM will switch to a MCMC sampler that does not use differentiation. Below is an example showing how to use a <code>blackbox</code> likelihood function. We use a log-likelihood function for <code>ddm</code> written in Cython to show that you can use any function or computation inside this function as long as the function itself has the signature defined above. See here for the function definition.</p>"},{"location":"tutorials/main_tutorial/","title":"Main Tutorial","text":"<p>This tutorial provides a comprehensive introduction to the HSSM package for Hierarchical Bayesian Estimation of Sequential Sampling Models.</p> <p>To make the most of the tutorial, let us cover the functionality of the key supporting packages that we use along the way.</p> In\u00a0[1]: Copied! <pre># If running this on Colab, please uncomment the next line\n# !pip install git+https://github.com/lnccbrown/HSSM@workshop_tutorial\n</pre> # If running this on Colab, please uncomment the next line # !pip install git+https://github.com/lnccbrown/HSSM@workshop_tutorial In\u00a0[2]: Copied! <pre>import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Basics\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nrandom_seed_sim = 134\nnp.random.seed(random_seed_sim)\n</pre> import warnings warnings.filterwarnings(\"ignore\")  # Basics import numpy as np from matplotlib import pyplot as plt  random_seed_sim = 134 np.random.seed(random_seed_sim) <p>HSSM comes with a basic simulator function supplied the <code>simulate_data()</code> function. We can use this function to create synthetic datasets.</p> <p>Below we show the most basic usecase:</p> <p>We wish to generate <code>500</code> datapoints (trials) from the standard Drift Diffusion Model with a fixed parameters, <code>v = 0.5, a = 1.5, z = 0.5, t = 0.5</code>.</p> <p>Note:</p> <p>In the course of the tutorial, we will see multiple strategies for synthetic dataset generation, this being the most straightforward one.</p> In\u00a0[3]: Copied! <pre># Single dataset\nimport arviz as az  # Visualization\nimport bambi as bmb  # Model construction\nimport hddm_wfpt\nimport jax\nimport pytensor  # Graph-based tensor library\n\nimport hssm\n\n# pytensor.config.floatX = \"float32\"\n# jax.config.update(\"jax_enable_x64\", False)\n\nv_true = 0.5\na_true = 1.5\nz_true = 0.5\nt_true = 0.5\n\n# Call the simulator function\ndataset = hssm.simulate_data(\n    model=\"ddm\", theta=dict(v=v_true, a=a_true, z=z_true, t=t_true), size=500\n)\n\ndataset\n</pre> # Single dataset import arviz as az  # Visualization import bambi as bmb  # Model construction import hddm_wfpt import jax import pytensor  # Graph-based tensor library  import hssm  # pytensor.config.floatX = \"float32\" # jax.config.update(\"jax_enable_x64\", False)  v_true = 0.5 a_true = 1.5 z_true = 0.5 t_true = 0.5  # Call the simulator function dataset = hssm.simulate_data(     model=\"ddm\", theta=dict(v=v_true, a=a_true, z=z_true, t=t_true), size=500 )  dataset Out[3]: rt response 0 6.705694 1.0 1 1.591768 1.0 2 5.085638 1.0 3 3.080099 -1.0 4 1.850662 1.0 ... ... ... 495 2.753426 1.0 496 1.122120 1.0 497 2.006791 -1.0 498 3.189377 1.0 499 1.829903 1.0 <p>500 rows \u00d7 2 columns</p> <p>If instead you wish to supply a parameter that varies by trial (a lot more on this later), you can simply supply a vector of parameters to the <code>theta</code> dictionary, when calling the simulator.</p> <p>Note:</p> <p>The <code>size</code> argument conceptually functions as number of synthetic datasets. So if you supply a parameter as a <code>(1000,)</code> vector, then the simulator assumes that one dataset consists of <code>1000</code> trials, hence if we set the <code>size = 1</code> as below, we expect in return a dataset with <code>1000</code> trials.</p> In\u00a0[4]: Copied! <pre># a changes trial wise\na_trialwise = np.random.normal(loc=2, scale=0.3, size=1000)\n\ndataset_a_trialwise = hssm.simulate_data(\n    model=\"ddm\",\n    theta=dict(\n        v=v_true,\n        a=a_trialwise,\n        z=z_true,\n        t=t_true,\n    ),\n    size=1,\n)\n\ndataset_a_trialwise\n</pre> # a changes trial wise a_trialwise = np.random.normal(loc=2, scale=0.3, size=1000)  dataset_a_trialwise = hssm.simulate_data(     model=\"ddm\",     theta=dict(         v=v_true,         a=a_trialwise,         z=z_true,         t=t_true,     ),     size=1, )  dataset_a_trialwise Out[4]: rt response 0 2.749979 1.0 1 3.617094 1.0 2 8.161730 1.0 3 2.528078 1.0 4 3.657727 1.0 ... ... ... 995 5.848658 1.0 996 7.783371 1.0 997 5.362286 1.0 998 2.358529 1.0 999 3.042042 1.0 <p>1000 rows \u00d7 2 columns</p> <p>If we wish to simulate from another model, we can do so by changing the <code>model</code> string.</p> <p>The number of models we can simulate differs from the number of models for which we have likelihoods available (both will increase over time). To get the models for which likelihood functions are supplied out of the box, we should inspect  <code>hssm.HSSM.supported_models</code>.</p> In\u00a0[5]: Copied! <pre>hssm.HSSM.supported_models\n</pre> hssm.HSSM.supported_models Out[5]: <pre>('ddm',\n 'ddm_sdv',\n 'full_ddm',\n 'angle',\n 'levy',\n 'ornstein',\n 'weibull',\n 'race_no_bias_angle_4',\n 'ddm_seq2_no_bias',\n 'lba3',\n 'lba2')</pre> <p>If we wish to check more detailed information about a given supported model, we can use the accessor <code>get_default_model_config</code> under <code>hssm.modelconfig</code>. For example, we inspect <code>ddm</code> model configuration below.</p> In\u00a0[6]: Copied! <pre>hssm.modelconfig.get_default_model_config(\"ddm\")\n</pre> hssm.modelconfig.get_default_model_config(\"ddm\") Out[6]: <pre>{'response': ['rt', 'response'],\n 'list_params': ['v', 'a', 'z', 't'],\n 'choices': [-1, 1],\n 'description': 'The Drift Diffusion Model (DDM)',\n 'likelihoods': {'analytical': {'loglik': &lt;function hssm.likelihoods.analytical.logp_ddm(data: numpy.ndarray, v: float, a: float, z: float, t: float, err: float = 1e-15, k_terms: int = 20, epsilon: float = 1e-15) -&gt; numpy.ndarray&gt;,\n   'backend': None,\n   'bounds': {'v': (-inf, inf),\n    'a': (0.0, inf),\n    'z': (0.0, 1.0),\n    't': (0.0, inf)},\n   'default_priors': {'t': {'name': 'HalfNormal', 'sigma': 2.0}},\n   'extra_fields': None},\n  'approx_differentiable': {'loglik': 'ddm.onnx',\n   'backend': 'jax',\n   'default_priors': {'t': {'name': 'HalfNormal', 'sigma': 2.0}},\n   'bounds': {'v': (-3.0, 3.0),\n    'a': (0.3, 2.5),\n    'z': (0.0, 1.0),\n    't': (0.0, 2.0)},\n   'extra_fields': None},\n  'blackbox': {'loglik': &lt;function hssm.likelihoods.blackbox.hddm_to_hssm.&lt;locals&gt;.outer(data: numpy.ndarray, *args, **kwargs)&gt;,\n   'backend': None,\n   'bounds': {'v': (-inf, inf),\n    'a': (0.0, inf),\n    'z': (0.0, 1.0),\n    't': (0.0, inf)},\n   'default_priors': {'t': {'name': 'HalfNormal', 'sigma': 2.0}},\n   'extra_fields': None}}}</pre> <p>This dictionary contains quite a bit of information. For purposes of simulating data from a given model, we will highlight two aspects:</p> <ol> <li>The key <code>list_of_params</code> provides us with the necessary information to define out <code>theta</code> dictionary</li> <li>The <code>bounds</code> key inside the <code>likelihoods</code> sub-dictionaries, provides us with an indication of reasonable parameter values.</li> </ol> <p>The <code>likelihoods</code> dictionary inhabits three sub-directories for the <code>ddm</code> model, since we have all three, an <code>analytical</code>, an <code>approx_differentiable</code> (LAN) and a <code>blackbox</code> likelihood available. For many models, we will be able to access only one or two types of likelihoods.</p> In\u00a0[7]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom ssms.basic_simulators.simulator import simulator\n\n# a changes trial wise\ntheta_mat = np.zeros((1000, 4))\ntheta_mat[:, 0] = v_true  # v\ntheta_mat[:, 1] = a_trialwise  # a\ntheta_mat[:, 2] = z_true  # z\ntheta_mat[:, 3] = t_true  # t\n\n# simulate data\nsim_out_trialwise = simulator(\n    theta=theta_mat,  # parameter_matrix\n    model=\"ddm\",  # specify model (many are included in ssms)\n    n_samples=1,  # number of samples for each set of parameters\n    # (plays the role of `size` parameter in `hssm.simulate_data`)\n)\n\n# Turn into nice dataset\ndataset_trialwise = pd.DataFrame(\n    np.column_stack(\n        [sim_out_trialwise[\"rts\"][:, 0], sim_out_trialwise[\"choices\"][:, 0]]\n    ),\n    columns=[\"rt\", \"response\"],\n)\n\ndataset_trialwise\n</pre> import numpy as np import pandas as pd from ssms.basic_simulators.simulator import simulator  # a changes trial wise theta_mat = np.zeros((1000, 4)) theta_mat[:, 0] = v_true  # v theta_mat[:, 1] = a_trialwise  # a theta_mat[:, 2] = z_true  # z theta_mat[:, 3] = t_true  # t  # simulate data sim_out_trialwise = simulator(     theta=theta_mat,  # parameter_matrix     model=\"ddm\",  # specify model (many are included in ssms)     n_samples=1,  # number of samples for each set of parameters     # (plays the role of `size` parameter in `hssm.simulate_data`) )  # Turn into nice dataset dataset_trialwise = pd.DataFrame(     np.column_stack(         [sim_out_trialwise[\"rts\"][:, 0], sim_out_trialwise[\"choices\"][:, 0]]     ),     columns=[\"rt\", \"response\"], )  dataset_trialwise Out[7]: rt response 0 3.150271 1.0 1 1.444507 1.0 2 4.544335 1.0 3 1.928551 1.0 4 5.318682 1.0 ... ... ... 995 0.925058 1.0 996 2.266826 1.0 997 4.174062 1.0 998 2.482133 1.0 999 1.970486 1.0 <p>1000 rows \u00d7 2 columns</p> <p>We will stick to <code>hssm.simulate_data()</code> in this tutorial, to keep things simple.</p> <p>Let's proceed to simulate a simple dataset for our first example.</p> In\u00a0[8]: Copied! <pre># Specify\nparam_dict_init = dict(\n    v=v_true,\n    a=a_true,\n    z=z_true,\n    t=t_true,\n)\n\n\ndataset = hssm.simulate_data(\n    model=\"ddm\",\n    theta=param_dict_init,\n    size=500,\n)\n\ndataset\n</pre> # Specify param_dict_init = dict(     v=v_true,     a=a_true,     z=z_true,     t=t_true, )   dataset = hssm.simulate_data(     model=\"ddm\",     theta=param_dict_init,     size=500, )  dataset Out[8]: rt response 0 1.908792 1.0 1 2.700515 1.0 2 5.079576 -1.0 3 0.957307 -1.0 4 3.759613 -1.0 ... ... ... 495 1.493450 1.0 496 2.354721 1.0 497 1.888187 1.0 498 1.112517 1.0 499 0.654334 1.0 <p>500 rows \u00d7 2 columns</p> In\u00a0[9]: Copied! <pre>simple_ddm_model = hssm.HSSM(data=dataset)\n</pre> simple_ddm_model = hssm.HSSM(data=dataset) <pre>Model initialized successfully.\n</pre> In\u00a0[10]: Copied! <pre>simple_ddm_model\n</pre> simple_ddm_model Out[10]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 500\n\nParameters:\n\nv:\n    Prior: Normal(mu: 0.0, sigma: 2.0)\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> <p>The <code>print()</code> function gives us some basic information about our model including the number of observations the parameters in the model and their respective prior setting. We can also create a nice little graphical representation of our model...</p> In\u00a0[11]: Copied! <pre>simple_ddm_model.graph()\n</pre> simple_ddm_model.graph() <pre>max_shape:  (500,)\nsize:  (np.int64(500),)\n</pre> Out[11]: <p>This is the simplest model we can build. The graph above follows plate notation, commonly used for probabilistic graphical models.</p> <ul> <li>We have our basic parameters (unobserved, white nodes), these are random variables in the model and we want to estimate them</li> <li>Our observed reaction times and choices (<code>SSMRandomVariable</code>, grey node), are fixed (or conditioned on).</li> <li>Rounded rectangles provide us with information about dimensionality of objects</li> <li>Rectangles with sharp edges represent deterministic, but computed quantities (not shown here, but in later models)</li> </ul> <p>This notation is helpful to get a quick overview of the structure of a given model we construct.</p> <p>The <code>graph()</code> function of course becomes a lot more interesting and useful for more complicated models!</p> In\u00a0[12]: Copied! <pre>infer_data_simple_ddm_model = simple_ddm_model.sample(\n    sampler=\"mcmc\",  # type of sampler to choose, 'nuts_numpyro',\n    # 'nuts_blackjax' of default pymc nuts sampler\n    cores=1,  # how many cores to use\n    chains=2,  # how many chains to run\n    draws=500,  # number of draws from the markov chain\n    tune=1000,  # number of burn-in samples\n    idata_kwargs=dict(log_likelihood=True),  # return log likelihood\n    mp_ctx=\"spawn\",\n)  # mp_ctx=\"forkserver\")\n</pre> infer_data_simple_ddm_model = simple_ddm_model.sample(     sampler=\"mcmc\",  # type of sampler to choose, 'nuts_numpyro',     # 'nuts_blackjax' of default pymc nuts sampler     cores=1,  # how many cores to use     chains=2,  # how many chains to run     draws=500,  # number of draws from the markov chain     tune=1000,  # number of burn-in samples     idata_kwargs=dict(log_likelihood=True),  # return log likelihood     mp_ctx=\"spawn\", )  # mp_ctx=\"forkserver\") <pre>Using default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nSequential sampling (2 chains in 1 job)\nNUTS: [t, a, z, v]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 2 chains for 1_000 tune and 500 draw iterations (2_000 + 1_000 draws total) took 12 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00&lt;00:00, 3152.89it/s]\n</pre> <p>We sampled from the model, let's look at the output...</p> In\u00a0[13]: Copied! <pre>type(infer_data_simple_ddm_model)\n</pre> type(infer_data_simple_ddm_model) Out[13]: <pre>arviz.data.inference_data.InferenceData</pre> <p>Errr... a closer look might be needed here!</p> In\u00a0[14]: Copied! <pre>infer_data_simple_ddm_model\n</pre> infer_data_simple_ddm_model Out[14]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 36kB\nDimensions:  (chain: 2, draw: 500)\nCoordinates:\n  * chain    (chain) int64 16B 0 1\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    z        (chain, draw) float64 8kB 0.4699 0.499 0.4901 ... 0.486 0.4572\n    v        (chain, draw) float64 8kB 0.5595 0.5232 0.5655 ... 0.5693 0.4741\n    t        (chain, draw) float64 8kB 0.4846 0.5535 0.4914 ... 0.4404 0.4515\n    a        (chain, draw) float64 8kB 1.448 1.453 1.493 ... 1.544 1.502 1.522\nAttributes:\n    created_at:                  2025-09-29T19:49:47.177748+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               12.301905870437622\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (4)<ul><li>z(chain, draw)float640.4699 0.499 ... 0.486 0.4572<pre>array([[0.4699042 , 0.49904497, 0.49006759, 0.4804058 , 0.50658626,\n        0.46588361, 0.48758831, 0.49569231, 0.4650026 , 0.47016651,\n        0.51296718, 0.51296718, 0.52186105, 0.51476507, 0.5296514 ,\n        0.48013469, 0.4887384 , 0.49051181, 0.50654419, 0.48185724,\n        0.49960468, 0.48471074, 0.48534833, 0.46546797, 0.50478457,\n        0.49092853, 0.49592098, 0.50999128, 0.50496112, 0.50496112,\n        0.4818446 , 0.48449353, 0.5105768 , 0.49696239, 0.48900254,\n        0.50752312, 0.49352693, 0.47841634, 0.50864008, 0.47494031,\n        0.47905729, 0.46192232, 0.50778021, 0.5190944 , 0.50620883,\n        0.47114031, 0.4777165 , 0.48813019, 0.48997113, 0.47487802,\n        0.48233148, 0.48843956, 0.4839178 , 0.46542431, 0.46692611,\n        0.51327555, 0.48471632, 0.469812  , 0.5161216 , 0.51464226,\n        0.51464226, 0.46509779, 0.44421044, 0.5105104 , 0.53641068,\n        0.52411552, 0.48502577, 0.4847865 , 0.50238062, 0.51373234,\n        0.49063949, 0.46273391, 0.46204665, 0.51295402, 0.49887088,\n        0.43930346, 0.470053  , 0.46682687, 0.53448986, 0.52013632,\n        0.4977769 , 0.52171137, 0.51464489, 0.50764646, 0.52486997,\n        0.50433478, 0.47808401, 0.48540018, 0.50723011, 0.50462831,\n        0.49018985, 0.50341477, 0.50597253, 0.50595535, 0.50683344,\n        0.48499242, 0.49603448, 0.46384928, 0.48302743, 0.49425673,\n...\n        0.4804776 , 0.49285842, 0.49285842, 0.50241804, 0.50241804,\n        0.51959454, 0.48297978, 0.50491348, 0.49991338, 0.47384407,\n        0.4746992 , 0.48833865, 0.49476196, 0.54756749, 0.52168875,\n        0.49543637, 0.49054755, 0.47253468, 0.48037588, 0.46416891,\n        0.44825084, 0.52890864, 0.43282059, 0.46658425, 0.44955703,\n        0.50966505, 0.48112862, 0.50555935, 0.50417025, 0.51086967,\n        0.50521692, 0.48235844, 0.48235844, 0.49527218, 0.49907702,\n        0.49495474, 0.52123594, 0.4989444 , 0.50094295, 0.51756145,\n        0.48769194, 0.48165421, 0.49490728, 0.50340803, 0.46659991,\n        0.47259562, 0.48321431, 0.50197676, 0.47663988, 0.53154065,\n        0.43875725, 0.47019098, 0.46312393, 0.50749881, 0.4969844 ,\n        0.47820064, 0.49349011, 0.48178029, 0.48398051, 0.49612929,\n        0.48732873, 0.49686937, 0.48669077, 0.50235347, 0.44817958,\n        0.50981431, 0.49731836, 0.4766222 , 0.49258354, 0.46082314,\n        0.5184781 , 0.50702027, 0.51565956, 0.48911703, 0.47142686,\n        0.47937115, 0.46961929, 0.50995249, 0.47532803, 0.4801773 ,\n        0.50332285, 0.53153867, 0.45467345, 0.50891975, 0.4913953 ,\n        0.49740255, 0.46636886, 0.46848073, 0.47112246, 0.51203647,\n        0.51226343, 0.49697994, 0.50481289, 0.4897052 , 0.48348977,\n        0.47560721, 0.51617524, 0.48548315, 0.48597814, 0.45718307]])</pre></li><li>v(chain, draw)float640.5595 0.5232 ... 0.5693 0.4741<pre>array([[0.55948456, 0.52323813, 0.56551741, 0.53313771, 0.46493731,\n        0.55087673, 0.50254107, 0.49712166, 0.5193884 , 0.53786283,\n        0.47628327, 0.47628327, 0.46890103, 0.42625958, 0.40481376,\n        0.5352571 , 0.51518708, 0.52785003, 0.47464317, 0.54494748,\n        0.48485207, 0.52397391, 0.47947798, 0.51051453, 0.53328515,\n        0.4828603 , 0.51039854, 0.49196315, 0.45089411, 0.45089411,\n        0.53074366, 0.59521434, 0.5280832 , 0.4959387 , 0.50225902,\n        0.45925184, 0.47507529, 0.47019087, 0.491393  , 0.54668279,\n        0.57031345, 0.51527892, 0.51723531, 0.48097444, 0.5019942 ,\n        0.50270784, 0.55318472, 0.49763915, 0.52344266, 0.52922316,\n        0.53068661, 0.60204255, 0.57409213, 0.51774487, 0.5897066 ,\n        0.42715636, 0.48483581, 0.51739792, 0.53598982, 0.52453375,\n        0.52453375, 0.54714409, 0.52415901, 0.42710464, 0.43888118,\n        0.43950797, 0.50929061, 0.52571446, 0.46817741, 0.51363496,\n        0.51353199, 0.5464202 , 0.56618318, 0.43725741, 0.51189602,\n        0.60589056, 0.50717787, 0.53483625, 0.45455116, 0.46787956,\n        0.46060881, 0.4362908 , 0.4662266 , 0.43534448, 0.48122862,\n        0.41796492, 0.57352661, 0.47166845, 0.49295676, 0.45855972,\n        0.5095185 , 0.51932247, 0.48368896, 0.49988081, 0.46324614,\n        0.53557885, 0.56492361, 0.45625511, 0.54128227, 0.47293082,\n...\n        0.54087204, 0.52589016, 0.52589016, 0.44570269, 0.44570269,\n        0.45556855, 0.52366024, 0.51892354, 0.48248938, 0.4837746 ,\n        0.51527428, 0.47644855, 0.50449364, 0.44137543, 0.45953707,\n        0.46457798, 0.49735844, 0.48976267, 0.59975141, 0.57439719,\n        0.58452694, 0.42247181, 0.58281737, 0.60702356, 0.60779257,\n        0.44381974, 0.52026999, 0.4865709 , 0.48275222, 0.4636106 ,\n        0.48842722, 0.52841603, 0.52841603, 0.47236072, 0.46536666,\n        0.4326968 , 0.49307454, 0.49261405, 0.48288692, 0.45916959,\n        0.60841893, 0.49694203, 0.46176213, 0.49802685, 0.52176412,\n        0.52726692, 0.52938726, 0.50286864, 0.5014498 , 0.42210723,\n        0.59795896, 0.5730601 , 0.5662996 , 0.48388775, 0.5168015 ,\n        0.51047207, 0.54635761, 0.52832552, 0.49942346, 0.48766876,\n        0.55853624, 0.4777943 , 0.50380103, 0.52309688, 0.55114737,\n        0.51555938, 0.50668944, 0.50043609, 0.54325946, 0.56084193,\n        0.41411473, 0.45299422, 0.48909671, 0.56575484, 0.47232775,\n        0.48047251, 0.4998052 , 0.54127344, 0.45005795, 0.53429763,\n        0.40968834, 0.44709344, 0.58236811, 0.51212206, 0.47668181,\n        0.54603539, 0.5170905 , 0.58443773, 0.6072656 , 0.46398816,\n        0.44961717, 0.50281085, 0.50883962, 0.44288355, 0.58106786,\n        0.57161634, 0.47743376, 0.50216797, 0.56928885, 0.47411196]])</pre></li><li>t(chain, draw)float640.4846 0.5535 ... 0.4404 0.4515<pre>array([[0.48458705, 0.55350789, 0.49137116, 0.46346   , 0.51377192,\n        0.4687834 , 0.48376764, 0.4708331 , 0.4843077 , 0.42168161,\n        0.52622507, 0.52622507, 0.52445349, 0.52059101, 0.53803683,\n        0.4986684 , 0.48187461, 0.50334227, 0.51988131, 0.46382329,\n        0.4923178 , 0.50235766, 0.5255414 , 0.43358638, 0.45494497,\n        0.46438112, 0.43700584, 0.4994927 , 0.54020482, 0.54020482,\n        0.4741314 , 0.55409128, 0.4790688 , 0.53979982, 0.51504932,\n        0.45641671, 0.45402082, 0.47977727, 0.51659122, 0.43958788,\n        0.43636252, 0.43659142, 0.51401167, 0.52483367, 0.51899131,\n        0.45630586, 0.48034856, 0.44180398, 0.47515016, 0.39325903,\n        0.39135063, 0.46700878, 0.46128649, 0.45441337, 0.44403958,\n        0.47079232, 0.48392   , 0.46936239, 0.52824565, 0.51922117,\n        0.51922117, 0.43233355, 0.45451571, 0.49363254, 0.53912362,\n        0.5088395 , 0.4688445 , 0.49826372, 0.46301259, 0.50861762,\n        0.48466769, 0.47678902, 0.51995394, 0.45834862, 0.48665645,\n        0.44921504, 0.51203536, 0.46500077, 0.54272496, 0.51381783,\n        0.53263419, 0.51663641, 0.51000894, 0.51968724, 0.52116064,\n        0.50737104, 0.51216371, 0.4904556 , 0.52089794, 0.4730561 ,\n        0.54588333, 0.48115785, 0.52743539, 0.47092694, 0.45740127,\n        0.50167021, 0.52541035, 0.47191001, 0.47308344, 0.49674712,\n...\n        0.47945645, 0.57683454, 0.57683454, 0.5171689 , 0.5171689 ,\n        0.55042218, 0.5114095 , 0.52805399, 0.49989074, 0.47107516,\n        0.47352807, 0.46653491, 0.45619133, 0.52606693, 0.53221888,\n        0.44174684, 0.45190157, 0.48116776, 0.46488192, 0.47422308,\n        0.44785305, 0.51853205, 0.43799676, 0.44074277, 0.45001089,\n        0.48108993, 0.52440596, 0.49258655, 0.50593188, 0.50222598,\n        0.48843664, 0.47485668, 0.47485668, 0.45490698, 0.45844727,\n        0.45649083, 0.47496442, 0.52666983, 0.50634632, 0.48091012,\n        0.48552516, 0.47466074, 0.49209767, 0.47401569, 0.47588728,\n        0.45867058, 0.43365166, 0.55757828, 0.49742082, 0.46966501,\n        0.41651593, 0.44741199, 0.46384769, 0.44257968, 0.44525443,\n        0.46067389, 0.51928545, 0.45887563, 0.51787677, 0.42699929,\n        0.5156614 , 0.484738  , 0.49953092, 0.53961087, 0.43816568,\n        0.44796802, 0.45341586, 0.47653008, 0.45698803, 0.43798287,\n        0.52134374, 0.48720175, 0.51037569, 0.46647625, 0.47720997,\n        0.45590028, 0.49506375, 0.47744816, 0.51462998, 0.54061563,\n        0.50601705, 0.50931159, 0.49620689, 0.46467432, 0.47653179,\n        0.49701786, 0.47611603, 0.48794347, 0.47071954, 0.44520535,\n        0.47764412, 0.49148085, 0.48961871, 0.44662573, 0.52227165,\n        0.46978471, 0.48955782, 0.50476567, 0.44036513, 0.45152418]])</pre></li><li>a(chain, draw)float641.448 1.453 1.493 ... 1.502 1.522<pre>array([[1.44759672, 1.45259529, 1.49343447, 1.50284014, 1.51263448,\n        1.46905961, 1.51259916, 1.5142851 , 1.49174719, 1.52606595,\n        1.4890533 , 1.4890533 , 1.49239307, 1.44428241, 1.44355249,\n        1.48295272, 1.50591479, 1.48556412, 1.46951339, 1.51798236,\n        1.48275961, 1.48975163, 1.45557554, 1.5203054 , 1.54162858,\n        1.55461358, 1.53127189, 1.45113812, 1.45857586, 1.45857586,\n        1.46028149, 1.46694656, 1.54972756, 1.41834979, 1.43647509,\n        1.49286527, 1.49946523, 1.48671874, 1.44633111, 1.58835994,\n        1.56973691, 1.52313396, 1.48754403, 1.49705979, 1.44648274,\n        1.50809462, 1.52257261, 1.53465012, 1.55215565, 1.53479831,\n        1.51981491, 1.49613553, 1.50967652, 1.54458913, 1.46549605,\n        1.47079482, 1.50489114, 1.50035196, 1.48764696, 1.48881986,\n        1.48881986, 1.49949283, 1.50380022, 1.40915048, 1.49389813,\n        1.48722457, 1.51114747, 1.52392683, 1.47222668, 1.50062151,\n        1.52343765, 1.44263854, 1.44814469, 1.56688016, 1.47678505,\n        1.51494349, 1.49287627, 1.52991733, 1.4081477 , 1.43623738,\n        1.46052317, 1.48012844, 1.47700697, 1.45474111, 1.48505027,\n        1.46358094, 1.48291172, 1.45500418, 1.47467305, 1.4819834 ,\n        1.4684883 , 1.52655626, 1.42683468, 1.52406241, 1.58424396,\n        1.48731715, 1.47675068, 1.43721115, 1.50279843, 1.46721041,\n...\n        1.47662503, 1.37104325, 1.37104325, 1.43514909, 1.43514909,\n        1.45414222, 1.45774301, 1.48131199, 1.46054528, 1.51724528,\n        1.50559308, 1.52347605, 1.49676735, 1.4916907 , 1.50429154,\n        1.49040359, 1.49407919, 1.47148584, 1.51165174, 1.51223035,\n        1.52887846, 1.42639448, 1.52031836, 1.49628017, 1.54379952,\n        1.49341916, 1.45538554, 1.48604013, 1.46762142, 1.4767554 ,\n        1.49615915, 1.51631093, 1.51631093, 1.51630459, 1.52046403,\n        1.56488977, 1.53497976, 1.41988065, 1.48759568, 1.45813692,\n        1.48076802, 1.48777629, 1.47108072, 1.49571438, 1.48113488,\n        1.47613501, 1.56743243, 1.427114  , 1.45151489, 1.56665537,\n        1.54427928, 1.54138868, 1.55339707, 1.50585043, 1.52382347,\n        1.507642  , 1.49775465, 1.46779734, 1.49038303, 1.48708287,\n        1.56038499, 1.44085695, 1.4950301 , 1.48741033, 1.49013949,\n        1.55332041, 1.5347392 , 1.48764287, 1.50277452, 1.53930332,\n        1.44601168, 1.4179183 , 1.41514028, 1.52456493, 1.49230352,\n        1.4983902 , 1.47395791, 1.4634337 , 1.45607321, 1.46780446,\n        1.46949338, 1.46783494, 1.54118123, 1.49639297, 1.50261113,\n        1.47929217, 1.50825607, 1.45877422, 1.43105101, 1.55853682,\n        1.51842697, 1.55857296, 1.51536264, 1.51218903, 1.52181187,\n        1.50340431, 1.45102542, 1.54401646, 1.50192259, 1.52215997]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-29T19:49:47.177748+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :12.301905870437622tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 4MB\nDimensions:      (chain: 2, draw: 500, __obs__: 500)\nCoordinates:\n  * chain        (chain) int64 16B 0 1\n  * draw         (draw) int64 4kB 0 1 2 3 4 5 6 ... 493 494 495 496 497 498 499\n  * __obs__      (__obs__) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\nData variables:\n    rt,response  (chain, draw, __obs__) float64 4MB -1.266 -1.822 ... -3.913\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li><li>__obs__: 500</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-1.266 -1.822 ... -1.195 -3.913<pre>array([[[-1.26560253, -1.82195553, -4.84171216, ..., -1.25191282,\n         -0.95254029, -3.86112541],\n        [-1.25363032, -1.8122603 , -4.7307417 , ..., -1.23946752,\n         -0.92578993, -5.65549131],\n        [-1.25372261, -1.79755219, -4.82268908, ..., -1.24011828,\n         -0.93418764, -3.96866334],\n        ...,\n        [-1.32889828, -1.84131958, -4.54931515, ..., -1.31598304,\n         -1.02507476, -4.21115422],\n        [-1.33369187, -1.81186336, -4.44407072, ..., -1.32178827,\n         -1.10806414, -4.83954353],\n        [-1.20755759, -1.73774137, -4.93408765, ..., -1.19494792,\n         -1.00180285, -4.33716233]],\n\n       [[-1.3352305 , -1.80341898, -4.47847829, ..., -1.32388144,\n         -1.14802066, -4.46847603],\n        [-1.34122635, -1.84903161, -4.52822131, ..., -1.32846621,\n         -1.04126746, -4.01852791],\n        [-1.31572939, -1.82237097, -4.6149579 , ..., -1.30318377,\n         -1.0404807 , -3.85772558],\n        ...,\n        [-1.28923795, -1.76955634, -4.58014876, ..., -1.27758868,\n         -1.1082608 , -4.90545328],\n        [-1.28118657, -1.81959156, -4.844929  , ..., -1.26772075,\n         -0.9368979 , -2.90302909],\n        [-1.3484081 , -1.80868791, -4.52409804, ..., -1.33762864,\n         -1.195116  , -3.91277552]]], shape=(2, 500, 500))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='__obs__', length=500))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 134kB\nDimensions:                (chain: 2, draw: 500)\nCoordinates:\n  * chain                  (chain) int64 16B 0 1\n  * draw                   (draw) int64 4kB 0 1 2 3 4 5 ... 495 496 497 498 499\nData variables: (12/18)\n    energy                 (chain, draw) float64 8kB 1.035e+03 ... 1.034e+03\n    step_size_bar          (chain, draw) float64 8kB 0.6405 0.6405 ... 0.6046\n    divergences            (chain, draw) int64 8kB 0 0 0 0 0 0 0 ... 0 0 0 0 0 0\n    perf_counter_start     (chain, draw) float64 8kB 1.667e+06 ... 1.667e+06\n    diverging              (chain, draw) bool 1kB False False ... False False\n    step_size              (chain, draw) float64 8kB 0.5453 0.5453 ... 0.5707\n    ...                     ...\n    acceptance_rate        (chain, draw) float64 8kB 0.9491 0.9308 ... 0.8723\n    lp                     (chain, draw) float64 8kB -1.03e+03 ... -1.033e+03\n    reached_max_treedepth  (chain, draw) bool 1kB False False ... False False\n    perf_counter_diff      (chain, draw) float64 8kB 0.003445 ... 0.001767\n    index_in_trajectory    (chain, draw) int64 8kB -2 2 6 -1 3 ... -2 -4 3 2 2\n    max_energy_error       (chain, draw) float64 8kB -0.8205 -0.2623 ... -0.7689\nAttributes:\n    created_at:                  2025-09-29T19:49:47.275334+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               12.301905870437622\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (18)<ul><li>energy(chain, draw)float641.035e+03 1.032e+03 ... 1.034e+03<pre>array([[1035.0020588 , 1032.26390066, 1033.27186685, 1030.11906694,\n        1030.1042046 , 1030.94182595, 1029.68168961, 1028.69647148,\n        1032.04203411, 1033.20059252, 1032.0630258 , 1033.04600413,\n        1029.61770161, 1030.85893345, 1031.33960384, 1031.72187656,\n        1028.61485009, 1028.52413384, 1029.90732892, 1028.76014548,\n        1028.87015705, 1028.21249273, 1029.72218775, 1032.57701219,\n        1032.21230391, 1031.19334624, 1030.63739564, 1032.75423998,\n        1030.68021714, 1031.03084395, 1031.86255103, 1034.57906734,\n        1033.46782705, 1030.83982286, 1030.3515545 , 1031.26830055,\n        1030.7016003 , 1031.6574841 , 1030.36134537, 1033.27703765,\n        1031.46726662, 1033.03828544, 1030.7848993 , 1031.58275205,\n        1031.29544645, 1033.08527757, 1030.71176582, 1030.73112549,\n        1030.85728806, 1033.18204277, 1034.77845929, 1035.68875549,\n        1031.67948037, 1031.04916298, 1034.19467614, 1033.1532391 ,\n        1033.95421258, 1030.24446286, 1032.01383524, 1030.72590867,\n        1031.8374021 , 1030.87291626, 1033.17106807, 1034.58933571,\n        1034.08858473, 1032.24878811, 1031.87569029, 1029.5753926 ,\n        1031.13130481, 1031.59942301, 1030.06803294, 1038.45095803,\n        1032.12550378, 1032.13426018, 1033.49331407, 1033.17730845,\n        1035.93213235, 1033.61304365, 1035.19981776, 1034.22663152,\n...\n        1032.80845205, 1033.63751318, 1034.3998207 , 1034.01481069,\n        1033.71615337, 1033.1891776 , 1031.29245934, 1029.08832293,\n        1028.54213804, 1028.78568006, 1029.60523143, 1029.57926579,\n        1029.84463648, 1029.48173129, 1029.49193762, 1033.70311749,\n        1033.18071576, 1031.77191794, 1029.95713361, 1032.06219966,\n        1032.38111983, 1031.78329253, 1029.35146797, 1030.41170838,\n        1030.0172219 , 1029.62350085, 1031.52766698, 1031.24316911,\n        1031.24845313, 1034.48493444, 1037.53161681, 1033.01347582,\n        1030.89302884, 1034.21797857, 1031.95789306, 1030.24968619,\n        1030.10685102, 1031.0602718 , 1030.21218971, 1033.86514901,\n        1034.43844636, 1034.44831617, 1029.64091215, 1032.11045377,\n        1032.70854388, 1033.86931951, 1031.02966096, 1029.88943111,\n        1031.12982867, 1031.46433883, 1032.52616366, 1034.8771135 ,\n        1034.41600849, 1033.19812808, 1031.20403101, 1030.67512747,\n        1032.8833187 , 1034.53142696, 1033.00241334, 1035.2642996 ,\n        1031.27043801, 1031.42235045, 1035.19712057, 1034.25297672,\n        1030.06312617, 1029.58520488, 1029.66238891, 1030.99088283,\n        1034.66745946, 1035.17669889, 1032.09435927, 1033.40707741,\n        1031.62911756, 1031.08339987, 1035.79494601, 1032.06556448,\n        1031.53858861, 1032.29390248, 1032.57238047, 1033.53120613]])</pre></li><li>step_size_bar(chain, draw)float640.6405 0.6405 ... 0.6046 0.6046<pre>array([[0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n        0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 , 0.6405247 ,\n...\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936,\n        0.60460936, 0.60460936, 0.60460936, 0.60460936, 0.60460936]])</pre></li><li>divergences(chain, draw)int640 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0<pre>array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n...\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</pre></li><li>perf_counter_start(chain, draw)float641.667e+06 1.667e+06 ... 1.667e+06<pre>array([[1666811.70320817, 1666811.70668213, 1666811.71048054,\n        1666811.71428058, 1666811.71833037, 1666811.72195983,\n        1666811.72550087, 1666811.72732667, 1666811.72941275,\n        1666811.73341025, 1666811.73706658, 1666811.74079008,\n        1666811.7444495 , 1666811.74647596, 1666811.7505825 ,\n        1666811.75256704, 1666811.75613958, 1666811.75962533,\n        1666811.763136  , 1666811.76717008, 1666811.77111608,\n        1666811.77485658, 1666811.77862121, 1666811.78072338,\n        1666811.78471313, 1666811.78653158, 1666811.78841008,\n        1666811.79035633, 1666811.79437579, 1666811.79639821,\n        1666811.798771  , 1666811.80282292, 1666811.8082485 ,\n        1666811.81218017, 1666811.81615575, 1666811.81992538,\n        1666811.82380304, 1666811.82761088, 1666811.82955587,\n        1666811.83311213, 1666811.83696813, 1666811.83894838,\n        1666811.84069842, 1666811.84446142, 1666811.84638192,\n        1666811.84824533, 1666811.85101533, 1666811.85469138,\n        1666811.85870425, 1666811.86238775, 1666811.86432971,\n        1666811.86524029, 1666811.86877288, 1666811.91274142,\n        1666811.92687021, 1666811.94636662, 1666811.96471929,\n        1666811.97673171, 1666811.98030075, 1666811.98602596,\n...\n        1666818.93339804, 1666818.93518613, 1666818.93868983,\n        1666818.942154  , 1666818.94559742, 1666818.94929808,\n        1666818.95293192, 1666818.95645675, 1666818.95994471,\n        1666818.96348275, 1666818.97072492, 1666818.97250996,\n        1666818.97616612, 1666818.97967296, 1666818.98330817,\n        1666818.98524167, 1666818.98890267, 1666818.992699  ,\n        1666818.99643596, 1666818.99831775, 1666819.00006737,\n        1666819.00377108, 1666819.00756437, 1666819.011265  ,\n        1666819.01495658, 1666819.01871862, 1666819.0205475 ,\n        1666819.02406704, 1666819.02761   , 1666819.02938108,\n        1666819.03320917, 1666819.03693571, 1666819.04050383,\n        1666819.044118  , 1666819.04779138, 1666819.05131483,\n        1666819.05534554, 1666819.05718575, 1666819.06079204,\n        1666819.06262292, 1666819.06631438, 1666819.06808975,\n        1666819.0717135 , 1666819.07524658, 1666819.07882071,\n        1666819.08246779, 1666819.08626571, 1666819.08810054,\n        1666819.08989362, 1666819.0935865 , 1666819.09726942,\n        1666819.10089712, 1666819.10278254, 1666819.10636088,\n        1666819.10998087, 1666819.11183437, 1666819.11544508,\n        1666819.11915446, 1666819.12283217]])</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False]])</pre></li><li>step_size(chain, draw)float640.5453 0.5453 ... 0.5707 0.5707<pre>array([[0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n        0.54525163, 0.54525163, 0.54525163, 0.54525163, 0.54525163,\n...\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ,\n        0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 , 0.5706935 ]])</pre></li><li>energy_error(chain, draw)float64-0.8205 0.2396 ... 0.003784 0.4829<pre>array([[-8.20496103e-01,  2.39583545e-01, -3.86633172e-01,\n        -1.97344130e-01,  3.34986076e-01, -1.70449677e-01,\n        -1.28016645e-01, -3.43883816e-02,  3.75938093e-01,\n        -8.98937951e-02, -5.18435611e-01,  0.00000000e+00,\n        -1.95821691e-02, -7.34098261e-02,  4.58628949e-02,\n        -1.19437814e-01, -2.90306500e-02,  2.73709764e-02,\n        -1.80711412e-02,  2.64808634e-02, -3.33218813e-02,\n         6.16642962e-02,  3.93456239e-01, -8.64629856e-02,\n         3.00219376e-01, -2.07282854e-01,  7.12117378e-03,\n         1.58228650e-01, -2.76662509e-01,  0.00000000e+00,\n         1.72617284e-01,  2.96096654e-01, -6.44959029e-01,\n        -2.95552829e-01, -7.78563395e-02,  7.09638108e-01,\n        -3.10991160e-01,  1.44336712e-01, -2.12725817e-01,\n         2.89425377e-01, -2.09078243e-01,  1.63799646e-01,\n        -2.90356865e-01,  7.67665743e-02, -4.14034036e-02,\n         1.43207943e-01, -1.51865178e-01,  6.53077042e-02,\n         4.01883053e-02,  3.91512610e-01,  5.87020796e-01,\n         2.41505962e-02, -8.61723530e-01,  1.55644035e-01,\n         7.38898840e-01, -6.02522147e-01, -4.73659910e-01,\n        -4.61863494e-02,  3.16843291e-01, -2.03787535e-01,\n...\n        -9.61429505e-01,  5.30464105e-03,  1.48741683e-01,\n        -7.16180184e-02,  3.92939887e-02, -9.44010536e-02,\n         1.23089932e-01, -1.97874742e-01,  3.27367670e-01,\n        -1.14690995e-01, -1.84121570e-01,  3.05892930e-01,\n         3.81679144e-01, -3.21408314e-01, -2.95030924e-01,\n         1.30857971e-01,  3.65362787e-01, -2.33085552e-02,\n         5.79572203e-01,  7.23204215e-02, -8.36674528e-01,\n        -3.49467843e-01,  3.37577677e-01, -5.22132455e-02,\n         1.68252418e-01, -3.79600111e-01, -5.35009957e-03,\n         3.94845588e-01, -4.13988474e-01,  2.02171394e-03,\n         9.27389261e-01,  1.38079435e-01, -8.24486206e-01,\n         3.47029820e-01, -4.09497731e-01,  6.60454807e-02,\n         1.40341240e+00, -6.83457670e-01, -3.24612542e-01,\n        -2.77376091e-01,  3.28698456e-02,  8.73313901e-01,\n        -5.01711602e-01, -6.99744635e-01,  2.53376313e-01,\n         6.19753996e-02, -1.31858902e-01,  1.34946279e+00,\n        -1.36623962e+00, -1.49193569e-01,  2.92219064e-01,\n        -3.72671020e-01,  3.33449305e-01, -1.29657250e-01,\n        -5.30465612e-01,  6.47708079e-01,  2.05470342e-01,\n         3.78364087e-03,  4.82865463e-01]])</pre></li><li>largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n...\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan]])</pre></li><li>tree_depth(chain, draw)int643 3 3 3 3 3 2 2 ... 2 3 3 2 3 3 3 2<pre>array([[3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3,\n        2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2,\n        2, 3, 3, 3, 3, 2, 1, 3, 3, 2, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 2,\n        3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 4, 2, 3, 2, 2, 2, 3, 3, 4, 3,\n        2, 2, 3, 3, 4, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3,\n        2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3,\n        3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 4, 2,\n        2, 3, 1, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3,\n        2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2,\n        3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3,\n        3, 2, 3, 2, 3, 2, 1, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2,\n        2, 3, 3, 3, 3, 4, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2,\n        2, 3, 2, 2, 1, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2,\n        3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 1, 2,\n        3, 2, 3, 3, 3, 3, 3, 2, 3, 1, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2,\n        1, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 4, 3, 2, 3, 3,\n        3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3,\n        2, 2, 3, 3, 2, 3, 2, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2,\n        3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3,\n        2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3,\n...\n        3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 1, 3, 3, 2, 2, 3,\n        2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3,\n        3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2,\n        2, 3, 3, 2, 3, 3, 4, 3, 4, 2, 2, 3, 3, 3, 2, 2, 2, 4, 3, 2, 3, 3,\n        3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 4, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3,\n        2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 4, 2, 3, 3, 2, 2, 2, 3, 3,\n        3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3,\n        3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 4, 2, 3, 3,\n        2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3,\n        3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 4, 3, 2, 3, 3, 2, 3, 3,\n        3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 4, 2, 3, 3, 2, 2, 2, 2, 3, 3,\n        2, 3, 3, 3, 2, 2, 4, 3, 4, 3, 3, 2, 3, 2, 2, 4, 3, 2, 2, 3, 3, 3,\n        3, 2, 3, 3, 3, 2, 2, 4, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2,\n        4, 4, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 1, 3, 3, 3,\n        3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 4, 3, 3, 3, 3, 2, 3,\n        3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 4, 3, 3, 2, 3, 2, 3, 2, 3,\n        3, 2, 3, 3, 4, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2,\n        3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3,\n        3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3,\n        3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2]])</pre></li><li>process_time_diff(chain, draw)float640.003502 0.003807 ... 0.001796<pre>array([[0.003502, 0.003807, 0.003778, 0.004021, 0.003633, 0.003565,\n        0.001827, 0.002061, 0.003953, 0.003688, 0.003717, 0.003674,\n        0.002004, 0.004018, 0.001977, 0.003575, 0.003498, 0.003524,\n        0.004028, 0.003941, 0.003762, 0.003756, 0.002072, 0.003963,\n        0.001807, 0.001871, 0.001924, 0.004031, 0.002   , 0.002455,\n        0.004086, 0.005489, 0.00397 , 0.004027, 0.003839, 0.003928,\n        0.003834, 0.001944, 0.003595, 0.003866, 0.001968, 0.001767,\n        0.003849, 0.001926, 0.001854, 0.002758, 0.003696, 0.004065,\n        0.003721, 0.001975, 0.000898, 0.003604, 0.007497, 0.004689,\n        0.011089, 0.00733 , 0.004538, 0.003563, 0.004005, 0.000934,\n        0.001226, 0.004992, 0.004275, 0.004028, 0.004057, 0.002029,\n        0.003926, 0.003996, 0.004019, 0.001938, 0.004146, 0.001969,\n        0.001861, 0.004032, 0.00407 , 0.003791, 0.003896, 0.00388 ,\n        0.00755 , 0.002019, 0.003671, 0.001973, 0.002075, 0.002009,\n        0.003887, 0.003936, 0.007818, 0.003659, 0.001976, 0.001867,\n        0.004319, 0.004025, 0.009353, 0.003908, 0.002174, 0.001935,\n        0.001987, 0.003534, 0.001979, 0.003796, 0.001865, 0.001942,\n        0.003794, 0.001903, 0.003847, 0.00193 , 0.001926, 0.004128,\n        0.00378 , 0.0037  , 0.00186 , 0.00183 , 0.003947, 0.001805,\n        0.003714, 0.003841, 0.003907, 0.001873, 0.003695, 0.001875,\n...\n        0.003513, 0.001754, 0.001731, 0.00172 , 0.003512, 0.007451,\n        0.003583, 0.003604, 0.003648, 0.00377 , 0.001842, 0.003627,\n        0.003549, 0.002014, 0.001953, 0.003631, 0.001765, 0.00176 ,\n        0.001761, 0.001815, 0.003637, 0.001972, 0.00361 , 0.003719,\n        0.001856, 0.007522, 0.004243, 0.003717, 0.001989, 0.003847,\n        0.001897, 0.003825, 0.001923, 0.003488, 0.005488, 0.002051,\n        0.003927, 0.003536, 0.007302, 0.001857, 0.002163, 0.003839,\n        0.003645, 0.003794, 0.003783, 0.003771, 0.00369 , 0.003716,\n        0.00198 , 0.001998, 0.00207 , 0.001889, 0.003684, 0.00352 ,\n        0.003506, 0.001833, 0.00361 , 0.001787, 0.00352 , 0.003478,\n        0.003457, 0.003688, 0.003655, 0.00354 , 0.00353 , 0.003552,\n        0.007279, 0.001789, 0.003663, 0.003522, 0.003619, 0.001932,\n        0.003671, 0.003784, 0.003729, 0.00186 , 0.001748, 0.003741,\n        0.003821, 0.003733, 0.003724, 0.003825, 0.001823, 0.003546,\n        0.003576, 0.001783, 0.003807, 0.003761, 0.003598, 0.003654,\n        0.003711, 0.003568, 0.004071, 0.001844, 0.003654, 0.00183 ,\n        0.003727, 0.001775, 0.003657, 0.003568, 0.003616, 0.003686,\n        0.003854, 0.001845, 0.001795, 0.00373 , 0.003725, 0.003651,\n        0.001863, 0.003624, 0.003678, 0.001839, 0.00365 , 0.003755,\n        0.003703, 0.001796]])</pre></li><li>n_steps(chain, draw)float647.0 7.0 7.0 7.0 ... 7.0 7.0 7.0 3.0<pre>array([[ 7.,  7.,  7.,  7.,  7.,  7.,  3.,  3.,  7.,  7.,  7.,  7.,  3.,\n         7.,  3.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,  3.,  3.,\n         3.,  7.,  3.,  3.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,\n         7.,  3.,  3.,  7.,  3.,  3.,  5.,  7.,  7.,  7.,  3.,  1.,  7.,\n         7.,  3.,  7.,  7.,  7.,  7.,  7.,  1.,  1.,  7.,  7.,  7.,  7.,\n         3.,  7.,  7.,  7.,  3.,  7.,  3.,  3.,  7.,  7.,  7.,  7.,  7.,\n        15.,  3.,  7.,  3.,  3.,  3.,  7.,  7., 15.,  7.,  3.,  3.,  7.,\n         7., 15.,  7.,  3.,  3.,  3.,  7.,  3.,  7.,  3.,  3.,  7.,  3.,\n         7.,  3.,  3.,  7.,  7.,  7.,  3.,  3.,  7.,  3.,  7.,  7.,  7.,\n         3.,  7.,  3.,  7.,  7.,  7.,  3.,  3.,  7.,  7.,  3.,  3.,  7.,\n         7.,  7.,  7.,  3.,  7.,  3.,  3.,  7.,  7.,  3.,  7.,  7.,  7.,\n         7.,  7.,  3.,  3.,  7.,  3.,  7.,  7.,  7., 11.,  3.,  3.,  7.,\n         1.,  7.,  3.,  7.,  3.,  3.,  7.,  7.,  3.,  7.,  3.,  7.,  7.,\n         7.,  7.,  3.,  3.,  3.,  3.,  7.,  3.,  7.,  7.,  3.,  7.,  7.,\n         7.,  7.,  3.,  7.,  7.,  3.,  7.,  3.,  7.,  7.,  3.,  7.,  7.,\n         3.,  3.,  3.,  7.,  3.,  7.,  3.,  3.,  3.,  7.,  7.,  7.,  3.,\n         3.,  7.,  7.,  7.,  7.,  3.,  7.,  3.,  3.,  3.,  3.,  7.,  7.,\n         3.,  7.,  3.,  7.,  3.,  1.,  5.,  7.,  3.,  7.,  7.,  3.,  3.,\n         3.,  7.,  7.,  3.,  7.,  7.,  3.,  3.,  3.,  7.,  7.,  7.,  7.,\n        15.,  7.,  3.,  3.,  3.,  3.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,\n...\n         7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,  3.,  7.,  7.,\n         7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,  7.,  3.,  7.,\n         7.,  3.,  7.,  7.,  7., 15.,  5.,  3.,  7.,  7.,  3.,  7.,  7.,\n         7.,  3.,  7.,  7.,  3.,  7.,  3.,  7.,  7.,  7.,  7.,  7., 15.,\n         3.,  7.,  7.,  3.,  3.,  3.,  3.,  7.,  7.,  3.,  7.,  7.,  7.,\n         3.,  3., 15.,  7., 11.,  7.,  7.,  3.,  7.,  3.,  3., 11.,  7.,\n         3.,  3.,  7.,  7.,  7.,  7.,  3.,  7.,  7.,  7.,  3.,  3., 15.,\n         7.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,  3.,  7.,  7.,  3.,\n         3., 15., 15.,  3.,  7.,  7.,  7.,  3.,  7.,  3.,  3.,  7.,  7.,\n         7.,  7.,  3.,  3.,  7.,  7.,  1.,  7.,  7.,  7.,  7.,  3.,  7.,\n         7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,  3.,  3.,  7., 15.,\n         7.,  7.,  7.,  7.,  3.,  7.,  7.,  3.,  3.,  7.,  3.,  3.,  3.,\n         3.,  7.,  3.,  7.,  7.,  3., 15.,  7.,  7.,  3.,  7.,  3.,  7.,\n         3.,  7.,  7.,  3.,  7.,  7., 15.,  3.,  3.,  7.,  7.,  7.,  7.,\n         7.,  7.,  7.,  3.,  3.,  3.,  3.,  7.,  7.,  7.,  3.,  7.,  3.,\n         7.,  7.,  7.,  7.,  7.,  7.,  7.,  7., 15.,  3.,  7.,  7.,  7.,\n         3.,  7.,  7.,  7.,  3.,  3.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,\n         7.,  3.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,  3.,  7.,\n         3.,  7.,  7.,  7.,  7.,  7.,  3.,  3.,  7.,  7.,  7.,  3.,  7.,\n         7.,  3.,  7.,  7.,  7.,  3.]])</pre></li><li>smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n...\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan]])</pre></li><li>acceptance_rate(chain, draw)float640.9491 0.9308 ... 0.9954 0.8723<pre>array([[0.94905645, 0.93077539, 0.98222401, 0.95751788, 0.82671455,\n        0.97052028, 1.        , 0.9823665 , 0.5821691 , 0.64212061,\n        0.70938608, 0.71652182, 1.        , 0.75956944, 0.92731908,\n        0.97263709, 0.88188691, 0.96962805, 0.62082377, 0.9767642 ,\n        0.99665668, 0.96028064, 0.7373022 , 0.86815237, 0.80942579,\n        1.        , 0.99763471, 0.8172062 , 0.78461114, 0.62658605,\n        0.76086061, 0.70424729, 1.        , 1.        , 0.82750867,\n        0.61904988, 1.        , 0.59293239, 0.9059889 , 0.85891227,\n        1.        , 0.48620716, 0.9707797 , 0.60402138, 0.65359008,\n        0.55031601, 0.8724883 , 0.93698169, 0.87537458, 0.79085958,\n        0.5559812 , 0.9352261 , 1.        , 0.92952721, 0.6216687 ,\n        0.97259832, 0.93783387, 0.97204591, 0.8404136 , 1.        ,\n        0.17450009, 1.        , 0.66279249, 0.94468412, 0.89086892,\n        0.78054166, 0.62206561, 0.80429657, 0.66971   , 0.81041613,\n        0.91757903, 0.14849645, 0.96327611, 0.99055949, 0.66617363,\n        0.91259232, 0.53367828, 0.98011931, 0.78327744, 0.91749743,\n        0.9577417 , 0.97131153, 0.98459936, 0.88038678, 0.7767771 ,\n        0.99474439, 0.44937377, 0.99356845, 1.        , 0.8204165 ,\n        0.69682351, 1.        , 0.51763588, 0.98804368, 0.8083535 ,\n        0.9133261 , 0.74713833, 0.65268644, 1.        , 0.97104987,\n...\n        1.        , 0.61074087, 0.41131609, 0.98105603, 0.3872109 ,\n        0.70026779, 0.6179015 , 0.78024726, 0.84156394, 0.68212403,\n        1.        , 0.93483567, 0.79685407, 0.86644932, 1.        ,\n        0.5997973 , 1.        , 0.9120683 , 0.81175354, 0.97599306,\n        0.93726972, 0.88738701, 0.80040813, 0.95317048, 0.92996563,\n        0.98870176, 0.90493021, 0.99570305, 0.99802233, 0.93308085,\n        0.96599106, 0.89290744, 0.56068824, 0.91293991, 0.9995863 ,\n        0.59842297, 1.        , 0.99861312, 0.98499558, 0.4135462 ,\n        0.92851315, 0.94484893, 0.99175483, 0.87341321, 0.96427117,\n        0.95002446, 0.96823986, 0.9198483 , 0.99934691, 0.77402817,\n        0.94321122, 1.        , 0.84816389, 0.59721478, 0.97818862,\n        0.94613189, 0.90259097, 0.74824518, 0.98858453, 0.69032992,\n        0.97674426, 1.        , 1.        , 0.76502902, 0.99762832,\n        0.85739127, 1.        , 0.99418613, 0.70961348, 0.91048263,\n        0.80894517, 0.50910379, 0.8671662 , 0.91815914, 0.86592396,\n        1.        , 0.88230976, 0.50277132, 0.96281453, 0.80948107,\n        1.        , 0.9892215 , 0.62281474, 1.        , 0.95920555,\n        0.7671293 , 0.97791627, 0.98344019, 0.54446643, 1.        ,\n        0.98510252, 0.5368667 , 0.70369297, 0.83133043, 0.60011556,\n        1.        , 0.69190024, 0.90255739, 0.99537272, 0.87233761]])</pre></li><li>lp(chain, draw)float64-1.03e+03 -1.031e+03 ... -1.033e+03<pre>array([[-1029.76692241, -1030.53475755, -1028.99873786, -1028.37159355,\n        -1029.25190638, -1029.36222157, -1028.12165892, -1028.26177973,\n        -1029.28985639, -1030.44333032, -1028.92272824, -1028.92272824,\n        -1029.39668176, -1029.53719076, -1030.95643073, -1028.09529588,\n        -1027.89256438, -1027.95242954, -1028.22824379, -1028.51104509,\n        -1027.89169978, -1028.02848074, -1029.19987652, -1030.36236582,\n        -1030.41945532, -1029.94182052, -1030.09183438, -1029.32221858,\n        -1029.61335503, -1029.61335503, -1029.09184078, -1033.56597699,\n        -1030.37692449, -1029.68364259, -1028.61924781, -1030.27244159,\n        -1029.30193723, -1029.2567381 , -1028.7896095 , -1031.38472339,\n        -1030.71825243, -1030.47474761, -1028.66348955, -1029.39543376,\n        -1028.77039228, -1029.25700103, -1028.68693166, -1029.4542438 ,\n        -1029.35333618, -1032.82310501, -1034.25290146, -1031.93862742,\n        -1029.67219862, -1030.48848291, -1032.61131936, -1030.92782957,\n        -1028.46250466, -1028.77130992, -1030.65082125, -1029.69309203,\n        -1029.69309203, -1030.40434195, -1032.47892408, -1032.88592395,\n        -1031.45561334, -1029.41929722, -1028.167956  , -1028.98831426,\n        -1030.02342002, -1029.15129076, -1028.29178898, -1030.37977293,\n        -1030.66212033, -1031.95835185, -1028.31611513, -1032.07571519,\n        -1030.4795727 , -1029.42754374, -1032.9119881 , -1030.19401775,\n...\n        -1031.00240628, -1031.52401727, -1033.22883861, -1031.91186592,\n        -1031.54275197, -1029.11398572, -1028.90013707, -1028.11523365,\n        -1028.02764499, -1028.35239387, -1028.15037412, -1028.17881383,\n        -1028.17881383, -1029.20706029, -1029.28419064, -1032.76872921,\n        -1030.61806794, -1029.38217978, -1027.94087774, -1030.81325394,\n        -1031.88310361, -1028.25621696, -1028.3780329 , -1028.64638895,\n        -1028.92414435, -1029.23703534, -1030.27836629, -1030.58115756,\n        -1028.716838  , -1033.37266698, -1032.88953548, -1029.69722964,\n        -1030.62158019, -1031.11980919, -1029.74671501, -1028.52832561,\n        -1029.22003636, -1029.73363176, -1029.18131569, -1032.4747194 ,\n        -1033.31575323, -1029.65666666, -1028.04528545, -1030.05546529,\n        -1031.42754317, -1031.07960692, -1029.17810996, -1028.44528517,\n        -1029.60377499, -1030.10211646, -1029.98054938, -1032.07282851,\n        -1031.82360842, -1029.41382458, -1030.10949569, -1029.33924851,\n        -1029.26705782, -1032.35172539, -1031.34302823, -1030.55552089,\n        -1030.25685843, -1030.49156343, -1033.29975346, -1030.232639  ,\n        -1028.3256051 , -1028.84525364, -1029.36350559, -1029.97643846,\n        -1034.51012972, -1031.27944164, -1029.36644264, -1030.42514223,\n        -1028.44571252, -1030.81789705, -1031.71137096, -1028.9337391 ,\n        -1030.52988191, -1031.2117316 , -1031.42257182, -1033.25723316]])</pre></li><li>reached_max_treedepth(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False]])</pre></li><li>perf_counter_diff(chain, draw)float640.003445 0.003759 ... 0.001767<pre>array([[0.00344504, 0.003759  , 0.00373146, 0.00398421, 0.00358167,\n        0.00350558, 0.00179712, 0.002042  , 0.00393242, 0.00362121,\n        0.00368108, 0.003607  , 0.00196546, 0.00403929, 0.00194046,\n        0.00353288, 0.00345642, 0.00348125, 0.00399092, 0.00388304,\n        0.00369721, 0.00370029, 0.00204308, 0.00392979, 0.00177792,\n        0.001848  , 0.00188983, 0.00394938, 0.001965  , 0.00233013,\n        0.00398446, 0.00536937, 0.00388946, 0.00393025, 0.00373454,\n        0.00383271, 0.00376225, 0.00188329, 0.0035165 , 0.00376954,\n        0.00193396, 0.00171958, 0.00373104, 0.00186371, 0.00182842,\n        0.00272354, 0.00362112, 0.00395262, 0.00362792, 0.00191046,\n        0.0008845 , 0.00346462, 0.04371446, 0.01387604, 0.01933125,\n        0.01749033, 0.01193   , 0.00351554, 0.00566204, 0.00091475,\n        0.0016415 , 0.02123212, 0.00442279, 0.0039555 , 0.00396012,\n        0.00199933, 0.00393171, 0.00391329, 0.00397971, 0.00189858,\n        0.00420237, 0.00194063, 0.00180925, 0.00395971, 0.00401179,\n        0.00366587, 0.00378775, 0.00375612, 0.00736571, 0.00198925,\n        0.00360642, 0.00193575, 0.00203404, 0.00195992, 0.00378938,\n        0.00386183, 0.00763192, 0.003553  , 0.00193817, 0.00183483,\n        0.00420629, 0.00392042, 0.00909962, 0.00385375, 0.00254779,\n        0.00197429, 0.00204062, 0.00348188, 0.001945  , 0.00372908,\n...\n        0.00174579, 0.00173521, 0.00174317, 0.001791  , 0.00359775,\n        0.00198983, 0.00356058, 0.00368271, 0.00183587, 0.00741575,\n        0.00414763, 0.00363883, 0.00195871, 0.00376325, 0.00186996,\n        0.00373788, 0.00188329, 0.00342254, 0.00531792, 0.00200092,\n        0.00381217, 0.00346629, 0.00714675, 0.00182771, 0.00209204,\n        0.00374325, 0.00356779, 0.00371146, 0.00373838, 0.00371325,\n        0.0036445 , 0.0036615 , 0.00195408, 0.00198104, 0.00203812,\n        0.00186379, 0.00363808, 0.00348125, 0.00346517, 0.00181138,\n        0.00356433, 0.00175783, 0.00347758, 0.00344075, 0.00341542,\n        0.00365554, 0.00360167, 0.00349783, 0.00345617, 0.00350554,\n        0.00718654, 0.00175867, 0.00361675, 0.00347967, 0.00356692,\n        0.00190329, 0.00362013, 0.00373921, 0.00367533, 0.00184113,\n        0.00171871, 0.00365204, 0.00375204, 0.00366283, 0.00363887,\n        0.00370658, 0.00179108, 0.003488  , 0.00351121, 0.00174654,\n        0.00373925, 0.00368017, 0.00353029, 0.00357717, 0.00363887,\n        0.00349275, 0.00398612, 0.00181204, 0.00356308, 0.0018005 ,\n        0.00365663, 0.00174458, 0.00359188, 0.00350154, 0.00354413,\n        0.00360363, 0.00375542, 0.00180725, 0.00176763, 0.00363867,\n        0.00363362, 0.00357662, 0.00183842, 0.00353379, 0.00357079,\n        0.00181238, 0.00356712, 0.00366892, 0.00363704, 0.00176738]])</pre></li><li>index_in_trajectory(chain, draw)int64-2 2 6 -1 3 -5 2 ... -5 -2 -4 3 2 2<pre>array([[ -2,   2,   6,  -1,   3,  -5,   2,   3,  -6,   2,   4,   0,   1,\n         -2,   2,   3,   4,   5,  -4,  -6,  -4,  -5,  -3,  -5,   3,  -2,\n          2,   4,  -2,   0,   2,  -2,  -6,  -5,   2,  -5,   1,   2,  -2,\n         -6,  -1,   2,  -3,   2,   2,  -3,  -1,   4,   1,   3,  -1,   4,\n          1,  -2,  -2,   5,  -1,   1,   2,  -1,   0,   4,  -1,   6,  -3,\n         -2,  -2,   1,   2,   3,   2,   2,   2,  -4,  -2,  -3,   3,   1,\n         -3,  -1,  -2,   3,   1,   2,  -1,   2,  -4,   3,   2,  -3,   5,\n          5,  -4,  -4,  -2,  -3,  -1,   2,  -2,  -5,   3,   2,   4,   3,\n         -4,   3,  -3,   4,   5,   4,  -2,  -1,   2,  -3,   6,  -6,  -3,\n         -2,  -3,  -2,  -1,   4,  -4,  -1,   3,  -3,   2,   1,   2,   4,\n         -4,  -4,  -2,   1,  -4,   2,   2,   7,  -6,  -3,   5,  -3,  -2,\n         -2,   4,  -3,  -1,  -3,  -3,  -6,   2,   1,  -2,   1,  -1,  -2,\n          1,  -2,   3,   3,   1,   2,   5,  -4,   3,   0,  -1,   1,   5,\n         -1,  -6,   1,   3,   1,   0,  -4,  -1,  -1,   1,   0,  -3,   2,\n          1,   3,   0,   3,  -2,   1,  -3,   2,   4,   2,   3,   2,  -2,\n          2,   3,   1,  -6,  -2,  -2,   2,  -1,   3,  -1,   2,   4,  -1,\n         -3,   1,   4,   5,  -2,  -2,  -3,  -3,   1,   2,  -2,  -2,   5,\n         -3,  -4,   1,   2,   2,   0,  -2,   1,   1,   4,  -4,   0,  -1,\n          2,  -2,  -1,   1,  -1,   1,   2,   1,   2,  -3,   3,  -3,   3,\n         -1,   7,   1,   0,   2,  -2,  -5,  -2,  -4,   3,  -3,   3,  -2,\n...\n          2,  -1,  -6,   1,   4,  -4,   5,   4,   5,   2,   2,   4,   6,\n         -4,  -5,   5,   3,  -2,  -3,  -6,   2,   2,  -2,  -4,  -3,   3,\n          3,   0,  -2,  -2,   7,  -3,  -2,   1,  -2,  -3,   1,   2,  -2,\n          2,   2,   1,  -1,   2,  -4,  -1,   1,  -4,  -5,   5,   6,   4,\n          2,   1,   4,   2,   2,  -1,  -3,  -1,  -3,  -2,   7,   3,   4,\n         -2,   2,   6,  -3,  -5,  -1,  -3,   3,  -4,   3,  -2,  -5,  -3,\n         -2,  -1,   4,   3,   4,   2,   1,  -3,   2,  -3,   3,  -3,  -5,\n          2,   2,  -2,   2,   2,  -3,   5,  -1,  -3,   2,  -5,  -4,  -2,\n         -1,   4, -10,  -2,  -2,   5,  -3,  -2,  -2,   1,  -1,   3,   4,\n          4,  -4,   1,  -2,   3,   2,   1,   1,  -4,  -2,   7,  -2,   5,\n         -2,  -4,  -1,   4,  -1,   2,   1,   3,  -1,   1,   2,  -2,  -5,\n          4,  -3,  -5,  -6,   1,  -4,  -4,   2,   2,   5,   3,   3,   0,\n          3,   0,   2,  -2,  -2,   1,  -8,  -1,   5,   1,   4,  -1,  -2,\n          1,   1,   2,  -1,   1,  -6,   7,   2,   2,  -3,   2,  -3,  -4,\n          3,   4,  -4,   0,   2,   1,  -1,   2,  -4,  -2,   1,   5,   3,\n         -7,   1,  -2,  -1,   3,  -6,  -3,   3,   4,  -3,   1,  -4,  -1,\n          2,   5,   3,   2,  -3,   3,   3,  -2,   1,  -4,   3,   1,  -2,\n          1,   3,   7,   1,   1,   6,  -2,  -1,   1,   2,   2,   1,  -4,\n          2,  -6,   2,  -1,   4,   3,  -3,  -1,   6,  -2,   2,  -2,   2,\n         -5,  -2,  -4,   3,   2,   2]])</pre></li><li>max_energy_error(chain, draw)float64-0.8205 -0.2623 ... -0.9015 -0.7689<pre>array([[-0.8204961 , -0.26226173, -0.38672596, -0.19734413,  0.33498608,\n        -0.28806911, -0.12801665,  0.05435113,  0.92604868,  1.33117178,\n         0.81941079,  0.62729632, -0.18978081,  0.69829255,  0.19021142,\n        -0.11943781,  0.25044593,  0.05601728,  0.96418458,  0.03763846,\n        -0.06433527,  0.07471021,  0.39345624,  0.56077356,  0.37451651,\n        -0.20728285, -0.33604124,  0.39217577,  0.52054343,  0.67412709,\n         0.46269772,  0.83489186, -1.13216678, -0.29555283,  0.45414329,\n         0.90514398, -0.61476   ,  0.90351127,  0.475898  ,  0.28942538,\n        -0.2474259 ,  1.29144583, -0.41619642,  1.12709056,  1.01496615,\n         1.19596058,  0.49522913,  0.2042445 ,  0.27061332,  0.39151261,\n         0.5870208 , -1.13412198, -1.07622576,  0.15564403,  1.01194029,\n        -1.14450196, -0.47365991, -0.13533555,  0.60267777, -0.20378753,\n         1.74583004, -0.35216715,  1.21476929, -1.21708298, -1.20482836,\n         0.53321457,  1.19879471,  0.46284017,  0.62545737,  0.84107105,\n        -0.35298632,  7.43968569, -0.14873419, -0.37113892,  1.1610376 ,\n         0.28637052,  1.46943539, -0.92972393,  0.58207681,  0.28436453,\n        -0.25634226, -0.42588123, -0.09786006,  0.17549669,  0.48095917,\n        -0.27273697,  2.00668704, -0.20843119, -0.14902852,  0.28399133,\n         0.77463943, -0.82811293,  2.0430896 , -0.07107695,  0.30970618,\n        -0.46833511,  0.38384819,  1.14626687, -1.01773214,  0.09768267,\n...\n        -0.16722192,  1.2729834 ,  1.35829814, -0.41264199,  1.70854836,\n         0.50445014,  0.84623024,  0.38148898,  0.35819612,  0.69304899,\n        -0.51160022,  0.24321484,  0.32364132,  0.43767799, -0.2499172 ,\n         0.72757901, -0.48155063,  0.36807214,  0.46074214, -0.45393569,\n         0.14623773,  0.3474669 ,  0.56505389, -0.56988829, -0.53385433,\n        -0.1862518 ,  0.2820815 , -0.14036757, -0.08016907,  0.12755974,\n         0.13255931,  0.18615363,  0.78937349,  0.11312244, -0.0876847 ,\n         0.78517403, -0.78310629, -0.34530251, -0.12283375,  1.12893126,\n        -0.82246857, -0.9614295 , -0.08622718,  0.28295858,  0.19748435,\n        -0.1571813 , -0.24278688,  0.18389023, -0.27892501,  0.47850338,\n        -0.33922561, -0.18412157,  0.30589293,  1.23001898, -0.58170955,\n        -0.29503092,  0.19328908,  0.64690706, -0.46355196,  0.5795722 ,\n        -0.73372727, -0.98211023, -0.38602299,  0.65981416, -0.38390179,\n         0.3882831 , -0.37960011, -0.13360493,  0.70466123, -0.41398847,\n         0.47269657,  1.05407751, -0.90027367, -1.02769512,  0.48411635,\n        -0.60008096,  0.28925275,  1.4034124 , -1.70092515,  0.57829378,\n        -0.63735029, -0.29631643,  1.04934956, -1.2202718 , -0.78661257,\n         0.5790597 , -0.2926384 , -0.27912436,  1.34946279, -1.530636  ,\n        -0.18212709,  1.56315048,  0.68786658,  0.34375458,  1.52578217,\n        -0.53046561,  0.6809103 , -0.79928767, -0.90151479, -0.76889497]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-29T19:49:47.275334+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :12.301905870437622tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 12kB\nDimensions:                  (__obs__: 500, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 4kB 0 1 2 3 4 ... 496 497 498 499\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 8kB 1...\nAttributes:\n    created_at:                  2025-09-29T19:49:47.278931+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 500</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float641.909 1.0 2.701 ... 1.0 0.6543 1.0<pre>array([[ 1.90879178,  1.        ],\n       [ 2.70051455,  1.        ],\n       [ 5.07957602, -1.        ],\n       [ 0.95730692, -1.        ],\n       [ 3.75961304, -1.        ],\n       [ 1.05477774,  1.        ],\n       [ 6.26821756,  1.        ],\n       [ 2.94682932, -1.        ],\n       [ 1.15484786,  1.        ],\n       [ 3.46363139,  1.        ],\n       [ 1.62390924,  1.        ],\n       [ 1.14314425, -1.        ],\n       [ 5.05555296,  1.        ],\n       [ 1.90220582, -1.        ],\n       [ 3.13259554, -1.        ],\n       [ 2.96012521,  1.        ],\n       [ 2.41100526,  1.        ],\n       [ 2.10615134,  1.        ],\n       [ 1.60785794,  1.        ],\n       [ 1.40975022,  1.        ],\n...\n       [ 2.91743708,  1.        ],\n       [ 1.57587361,  1.        ],\n       [ 3.21329093, -1.        ],\n       [ 1.49868929,  1.        ],\n       [ 1.72745574,  1.        ],\n       [ 1.17646027,  1.        ],\n       [ 1.63405263,  1.        ],\n       [ 2.31994152,  1.        ],\n       [ 1.88749003,  1.        ],\n       [ 2.36859488,  1.        ],\n       [ 2.85419393,  1.        ],\n       [ 2.84367776,  1.        ],\n       [ 1.24673474,  1.        ],\n       [ 2.03972936,  1.        ],\n       [ 2.95965099,  1.        ],\n       [ 1.49344993,  1.        ],\n       [ 2.35472059,  1.        ],\n       [ 1.88818717,  1.        ],\n       [ 1.11251736,  1.        ],\n       [ 0.65433359,  1.        ]])</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='__obs__', length=500))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (6)created_at :2025-09-29T19:49:47.278931+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> <p>We see that in our case, <code>infer_data_simple_ddm_model</code> contains four basic types of data (note: this is extensible!)</p> <ul> <li><code>posterior</code></li> <li><code>log_likelihood</code></li> <li><code>sample_stats</code></li> <li><code>observed_data</code></li> </ul> <p>The <code>posterior</code> object contains our traces for each of the parameters in the model. The <code>log_likelihood</code> field contains the trial wise log-likelihoods for each sample from the posterior. The <code>sample_stats</code> field contains information about the sampler run. This can be important for chain diagnostics, but we will not dwell on this here. Finally we retreive our <code>observed_data</code>.</p> In\u00a0[15]: Copied! <pre>infer_data_simple_ddm_model.posterior\n</pre> infer_data_simple_ddm_model.posterior Out[15]: <pre>&lt;xarray.Dataset&gt; Size: 36kB\nDimensions:  (chain: 2, draw: 500)\nCoordinates:\n  * chain    (chain) int64 16B 0 1\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    z        (chain, draw) float64 8kB 0.4699 0.499 0.4901 ... 0.486 0.4572\n    v        (chain, draw) float64 8kB 0.5595 0.5232 0.5655 ... 0.5693 0.4741\n    t        (chain, draw) float64 8kB 0.4846 0.5535 0.4914 ... 0.4404 0.4515\n    a        (chain, draw) float64 8kB 1.448 1.453 1.493 ... 1.544 1.502 1.522\nAttributes:\n    created_at:                  2025-09-29T19:49:47.177748+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               12.301905870437622\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (4)<ul><li>z(chain, draw)float640.4699 0.499 ... 0.486 0.4572<pre>array([[0.4699042 , 0.49904497, 0.49006759, 0.4804058 , 0.50658626,\n        0.46588361, 0.48758831, 0.49569231, 0.4650026 , 0.47016651,\n        0.51296718, 0.51296718, 0.52186105, 0.51476507, 0.5296514 ,\n        0.48013469, 0.4887384 , 0.49051181, 0.50654419, 0.48185724,\n        0.49960468, 0.48471074, 0.48534833, 0.46546797, 0.50478457,\n        0.49092853, 0.49592098, 0.50999128, 0.50496112, 0.50496112,\n        0.4818446 , 0.48449353, 0.5105768 , 0.49696239, 0.48900254,\n        0.50752312, 0.49352693, 0.47841634, 0.50864008, 0.47494031,\n        0.47905729, 0.46192232, 0.50778021, 0.5190944 , 0.50620883,\n        0.47114031, 0.4777165 , 0.48813019, 0.48997113, 0.47487802,\n        0.48233148, 0.48843956, 0.4839178 , 0.46542431, 0.46692611,\n        0.51327555, 0.48471632, 0.469812  , 0.5161216 , 0.51464226,\n        0.51464226, 0.46509779, 0.44421044, 0.5105104 , 0.53641068,\n        0.52411552, 0.48502577, 0.4847865 , 0.50238062, 0.51373234,\n        0.49063949, 0.46273391, 0.46204665, 0.51295402, 0.49887088,\n        0.43930346, 0.470053  , 0.46682687, 0.53448986, 0.52013632,\n        0.4977769 , 0.52171137, 0.51464489, 0.50764646, 0.52486997,\n        0.50433478, 0.47808401, 0.48540018, 0.50723011, 0.50462831,\n        0.49018985, 0.50341477, 0.50597253, 0.50595535, 0.50683344,\n        0.48499242, 0.49603448, 0.46384928, 0.48302743, 0.49425673,\n...\n        0.4804776 , 0.49285842, 0.49285842, 0.50241804, 0.50241804,\n        0.51959454, 0.48297978, 0.50491348, 0.49991338, 0.47384407,\n        0.4746992 , 0.48833865, 0.49476196, 0.54756749, 0.52168875,\n        0.49543637, 0.49054755, 0.47253468, 0.48037588, 0.46416891,\n        0.44825084, 0.52890864, 0.43282059, 0.46658425, 0.44955703,\n        0.50966505, 0.48112862, 0.50555935, 0.50417025, 0.51086967,\n        0.50521692, 0.48235844, 0.48235844, 0.49527218, 0.49907702,\n        0.49495474, 0.52123594, 0.4989444 , 0.50094295, 0.51756145,\n        0.48769194, 0.48165421, 0.49490728, 0.50340803, 0.46659991,\n        0.47259562, 0.48321431, 0.50197676, 0.47663988, 0.53154065,\n        0.43875725, 0.47019098, 0.46312393, 0.50749881, 0.4969844 ,\n        0.47820064, 0.49349011, 0.48178029, 0.48398051, 0.49612929,\n        0.48732873, 0.49686937, 0.48669077, 0.50235347, 0.44817958,\n        0.50981431, 0.49731836, 0.4766222 , 0.49258354, 0.46082314,\n        0.5184781 , 0.50702027, 0.51565956, 0.48911703, 0.47142686,\n        0.47937115, 0.46961929, 0.50995249, 0.47532803, 0.4801773 ,\n        0.50332285, 0.53153867, 0.45467345, 0.50891975, 0.4913953 ,\n        0.49740255, 0.46636886, 0.46848073, 0.47112246, 0.51203647,\n        0.51226343, 0.49697994, 0.50481289, 0.4897052 , 0.48348977,\n        0.47560721, 0.51617524, 0.48548315, 0.48597814, 0.45718307]])</pre></li><li>v(chain, draw)float640.5595 0.5232 ... 0.5693 0.4741<pre>array([[0.55948456, 0.52323813, 0.56551741, 0.53313771, 0.46493731,\n        0.55087673, 0.50254107, 0.49712166, 0.5193884 , 0.53786283,\n        0.47628327, 0.47628327, 0.46890103, 0.42625958, 0.40481376,\n        0.5352571 , 0.51518708, 0.52785003, 0.47464317, 0.54494748,\n        0.48485207, 0.52397391, 0.47947798, 0.51051453, 0.53328515,\n        0.4828603 , 0.51039854, 0.49196315, 0.45089411, 0.45089411,\n        0.53074366, 0.59521434, 0.5280832 , 0.4959387 , 0.50225902,\n        0.45925184, 0.47507529, 0.47019087, 0.491393  , 0.54668279,\n        0.57031345, 0.51527892, 0.51723531, 0.48097444, 0.5019942 ,\n        0.50270784, 0.55318472, 0.49763915, 0.52344266, 0.52922316,\n        0.53068661, 0.60204255, 0.57409213, 0.51774487, 0.5897066 ,\n        0.42715636, 0.48483581, 0.51739792, 0.53598982, 0.52453375,\n        0.52453375, 0.54714409, 0.52415901, 0.42710464, 0.43888118,\n        0.43950797, 0.50929061, 0.52571446, 0.46817741, 0.51363496,\n        0.51353199, 0.5464202 , 0.56618318, 0.43725741, 0.51189602,\n        0.60589056, 0.50717787, 0.53483625, 0.45455116, 0.46787956,\n        0.46060881, 0.4362908 , 0.4662266 , 0.43534448, 0.48122862,\n        0.41796492, 0.57352661, 0.47166845, 0.49295676, 0.45855972,\n        0.5095185 , 0.51932247, 0.48368896, 0.49988081, 0.46324614,\n        0.53557885, 0.56492361, 0.45625511, 0.54128227, 0.47293082,\n...\n        0.54087204, 0.52589016, 0.52589016, 0.44570269, 0.44570269,\n        0.45556855, 0.52366024, 0.51892354, 0.48248938, 0.4837746 ,\n        0.51527428, 0.47644855, 0.50449364, 0.44137543, 0.45953707,\n        0.46457798, 0.49735844, 0.48976267, 0.59975141, 0.57439719,\n        0.58452694, 0.42247181, 0.58281737, 0.60702356, 0.60779257,\n        0.44381974, 0.52026999, 0.4865709 , 0.48275222, 0.4636106 ,\n        0.48842722, 0.52841603, 0.52841603, 0.47236072, 0.46536666,\n        0.4326968 , 0.49307454, 0.49261405, 0.48288692, 0.45916959,\n        0.60841893, 0.49694203, 0.46176213, 0.49802685, 0.52176412,\n        0.52726692, 0.52938726, 0.50286864, 0.5014498 , 0.42210723,\n        0.59795896, 0.5730601 , 0.5662996 , 0.48388775, 0.5168015 ,\n        0.51047207, 0.54635761, 0.52832552, 0.49942346, 0.48766876,\n        0.55853624, 0.4777943 , 0.50380103, 0.52309688, 0.55114737,\n        0.51555938, 0.50668944, 0.50043609, 0.54325946, 0.56084193,\n        0.41411473, 0.45299422, 0.48909671, 0.56575484, 0.47232775,\n        0.48047251, 0.4998052 , 0.54127344, 0.45005795, 0.53429763,\n        0.40968834, 0.44709344, 0.58236811, 0.51212206, 0.47668181,\n        0.54603539, 0.5170905 , 0.58443773, 0.6072656 , 0.46398816,\n        0.44961717, 0.50281085, 0.50883962, 0.44288355, 0.58106786,\n        0.57161634, 0.47743376, 0.50216797, 0.56928885, 0.47411196]])</pre></li><li>t(chain, draw)float640.4846 0.5535 ... 0.4404 0.4515<pre>array([[0.48458705, 0.55350789, 0.49137116, 0.46346   , 0.51377192,\n        0.4687834 , 0.48376764, 0.4708331 , 0.4843077 , 0.42168161,\n        0.52622507, 0.52622507, 0.52445349, 0.52059101, 0.53803683,\n        0.4986684 , 0.48187461, 0.50334227, 0.51988131, 0.46382329,\n        0.4923178 , 0.50235766, 0.5255414 , 0.43358638, 0.45494497,\n        0.46438112, 0.43700584, 0.4994927 , 0.54020482, 0.54020482,\n        0.4741314 , 0.55409128, 0.4790688 , 0.53979982, 0.51504932,\n        0.45641671, 0.45402082, 0.47977727, 0.51659122, 0.43958788,\n        0.43636252, 0.43659142, 0.51401167, 0.52483367, 0.51899131,\n        0.45630586, 0.48034856, 0.44180398, 0.47515016, 0.39325903,\n        0.39135063, 0.46700878, 0.46128649, 0.45441337, 0.44403958,\n        0.47079232, 0.48392   , 0.46936239, 0.52824565, 0.51922117,\n        0.51922117, 0.43233355, 0.45451571, 0.49363254, 0.53912362,\n        0.5088395 , 0.4688445 , 0.49826372, 0.46301259, 0.50861762,\n        0.48466769, 0.47678902, 0.51995394, 0.45834862, 0.48665645,\n        0.44921504, 0.51203536, 0.46500077, 0.54272496, 0.51381783,\n        0.53263419, 0.51663641, 0.51000894, 0.51968724, 0.52116064,\n        0.50737104, 0.51216371, 0.4904556 , 0.52089794, 0.4730561 ,\n        0.54588333, 0.48115785, 0.52743539, 0.47092694, 0.45740127,\n        0.50167021, 0.52541035, 0.47191001, 0.47308344, 0.49674712,\n...\n        0.47945645, 0.57683454, 0.57683454, 0.5171689 , 0.5171689 ,\n        0.55042218, 0.5114095 , 0.52805399, 0.49989074, 0.47107516,\n        0.47352807, 0.46653491, 0.45619133, 0.52606693, 0.53221888,\n        0.44174684, 0.45190157, 0.48116776, 0.46488192, 0.47422308,\n        0.44785305, 0.51853205, 0.43799676, 0.44074277, 0.45001089,\n        0.48108993, 0.52440596, 0.49258655, 0.50593188, 0.50222598,\n        0.48843664, 0.47485668, 0.47485668, 0.45490698, 0.45844727,\n        0.45649083, 0.47496442, 0.52666983, 0.50634632, 0.48091012,\n        0.48552516, 0.47466074, 0.49209767, 0.47401569, 0.47588728,\n        0.45867058, 0.43365166, 0.55757828, 0.49742082, 0.46966501,\n        0.41651593, 0.44741199, 0.46384769, 0.44257968, 0.44525443,\n        0.46067389, 0.51928545, 0.45887563, 0.51787677, 0.42699929,\n        0.5156614 , 0.484738  , 0.49953092, 0.53961087, 0.43816568,\n        0.44796802, 0.45341586, 0.47653008, 0.45698803, 0.43798287,\n        0.52134374, 0.48720175, 0.51037569, 0.46647625, 0.47720997,\n        0.45590028, 0.49506375, 0.47744816, 0.51462998, 0.54061563,\n        0.50601705, 0.50931159, 0.49620689, 0.46467432, 0.47653179,\n        0.49701786, 0.47611603, 0.48794347, 0.47071954, 0.44520535,\n        0.47764412, 0.49148085, 0.48961871, 0.44662573, 0.52227165,\n        0.46978471, 0.48955782, 0.50476567, 0.44036513, 0.45152418]])</pre></li><li>a(chain, draw)float641.448 1.453 1.493 ... 1.502 1.522<pre>array([[1.44759672, 1.45259529, 1.49343447, 1.50284014, 1.51263448,\n        1.46905961, 1.51259916, 1.5142851 , 1.49174719, 1.52606595,\n        1.4890533 , 1.4890533 , 1.49239307, 1.44428241, 1.44355249,\n        1.48295272, 1.50591479, 1.48556412, 1.46951339, 1.51798236,\n        1.48275961, 1.48975163, 1.45557554, 1.5203054 , 1.54162858,\n        1.55461358, 1.53127189, 1.45113812, 1.45857586, 1.45857586,\n        1.46028149, 1.46694656, 1.54972756, 1.41834979, 1.43647509,\n        1.49286527, 1.49946523, 1.48671874, 1.44633111, 1.58835994,\n        1.56973691, 1.52313396, 1.48754403, 1.49705979, 1.44648274,\n        1.50809462, 1.52257261, 1.53465012, 1.55215565, 1.53479831,\n        1.51981491, 1.49613553, 1.50967652, 1.54458913, 1.46549605,\n        1.47079482, 1.50489114, 1.50035196, 1.48764696, 1.48881986,\n        1.48881986, 1.49949283, 1.50380022, 1.40915048, 1.49389813,\n        1.48722457, 1.51114747, 1.52392683, 1.47222668, 1.50062151,\n        1.52343765, 1.44263854, 1.44814469, 1.56688016, 1.47678505,\n        1.51494349, 1.49287627, 1.52991733, 1.4081477 , 1.43623738,\n        1.46052317, 1.48012844, 1.47700697, 1.45474111, 1.48505027,\n        1.46358094, 1.48291172, 1.45500418, 1.47467305, 1.4819834 ,\n        1.4684883 , 1.52655626, 1.42683468, 1.52406241, 1.58424396,\n        1.48731715, 1.47675068, 1.43721115, 1.50279843, 1.46721041,\n...\n        1.47662503, 1.37104325, 1.37104325, 1.43514909, 1.43514909,\n        1.45414222, 1.45774301, 1.48131199, 1.46054528, 1.51724528,\n        1.50559308, 1.52347605, 1.49676735, 1.4916907 , 1.50429154,\n        1.49040359, 1.49407919, 1.47148584, 1.51165174, 1.51223035,\n        1.52887846, 1.42639448, 1.52031836, 1.49628017, 1.54379952,\n        1.49341916, 1.45538554, 1.48604013, 1.46762142, 1.4767554 ,\n        1.49615915, 1.51631093, 1.51631093, 1.51630459, 1.52046403,\n        1.56488977, 1.53497976, 1.41988065, 1.48759568, 1.45813692,\n        1.48076802, 1.48777629, 1.47108072, 1.49571438, 1.48113488,\n        1.47613501, 1.56743243, 1.427114  , 1.45151489, 1.56665537,\n        1.54427928, 1.54138868, 1.55339707, 1.50585043, 1.52382347,\n        1.507642  , 1.49775465, 1.46779734, 1.49038303, 1.48708287,\n        1.56038499, 1.44085695, 1.4950301 , 1.48741033, 1.49013949,\n        1.55332041, 1.5347392 , 1.48764287, 1.50277452, 1.53930332,\n        1.44601168, 1.4179183 , 1.41514028, 1.52456493, 1.49230352,\n        1.4983902 , 1.47395791, 1.4634337 , 1.45607321, 1.46780446,\n        1.46949338, 1.46783494, 1.54118123, 1.49639297, 1.50261113,\n        1.47929217, 1.50825607, 1.45877422, 1.43105101, 1.55853682,\n        1.51842697, 1.55857296, 1.51536264, 1.51218903, 1.52181187,\n        1.50340431, 1.45102542, 1.54401646, 1.50192259, 1.52215997]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-29T19:49:47.177748+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :12.301905870437622tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> In\u00a0[16]: Copied! <pre>infer_data_simple_ddm_model.posterior.a.head()\n</pre> infer_data_simple_ddm_model.posterior.a.head() Out[16]: <pre>&lt;xarray.DataArray 'a' (chain: 2, draw: 5)&gt; Size: 80B\narray([[1.44759672, 1.45259529, 1.49343447, 1.50284014, 1.51263448],\n       [1.53550489, 1.48549947, 1.49767733, 1.48435983, 1.43010851]])\nCoordinates:\n  * chain    (chain) int64 16B 0 1\n  * draw     (draw) int64 40B 0 1 2 3 4</pre>xarray.DataArray'a'<ul><li>chain: 2</li><li>draw: 5</li></ul><ul><li>1.448 1.453 1.493 1.503 1.513 1.536 1.485 1.498 1.484 1.43<pre>array([[1.44759672, 1.45259529, 1.49343447, 1.50284014, 1.51263448],\n       [1.53550489, 1.48549947, 1.49767733, 1.48435983, 1.43010851]])</pre></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4<pre>array([0, 1, 2, 3, 4])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3, 4], dtype='int64', name='draw'))</pre></li></ul></li><li>Attributes: (0)</li></ul> <p>To simply access the underlying data as a <code>numpy.ndarray</code>, we can use <code>.values</code> (as e.g. when using <code>pandas.DataFrame</code> objects).</p> In\u00a0[17]: Copied! <pre>type(infer_data_simple_ddm_model.posterior.a.values)\n</pre> type(infer_data_simple_ddm_model.posterior.a.values) Out[17]: <pre>numpy.ndarray</pre> In\u00a0[18]: Copied! <pre># infer_data_simple_ddm_model.posterior.a.values\n</pre> # infer_data_simple_ddm_model.posterior.a.values In\u00a0[19]: Copied! <pre>idata_extracted = az.extract(infer_data_simple_ddm_model)\nidata_extracted\n</pre> idata_extracted = az.extract(infer_data_simple_ddm_model) idata_extracted Out[19]: <pre>&lt;xarray.Dataset&gt; Size: 56kB\nDimensions:  (sample: 1000)\nCoordinates:\n  * sample   (sample) object 8kB MultiIndex\n  * chain    (sample) int64 8kB 0 0 0 0 0 0 0 0 0 0 0 ... 1 1 1 1 1 1 1 1 1 1 1\n  * draw     (sample) int64 8kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    z        (sample) float64 8kB 0.4699 0.499 0.4901 ... 0.4855 0.486 0.4572\n    v        (sample) float64 8kB 0.5595 0.5232 0.5655 ... 0.5022 0.5693 0.4741\n    t        (sample) float64 8kB 0.4846 0.5535 0.4914 ... 0.5048 0.4404 0.4515\n    a        (sample) float64 8kB 1.448 1.453 1.493 1.503 ... 1.544 1.502 1.522\nAttributes:\n    created_at:                  2025-09-29T19:49:47.177748+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               12.301905870437622\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>sample: 1000</li></ul></li><li>Coordinates: (3)<ul><li>sample(sample)objectMultiIndex<pre>[1000 values with dtype=object]</pre></li><li>chain(sample)int640 0 0 0 0 0 0 0 ... 1 1 1 1 1 1 1 1<pre>[1000 values with dtype=int64]</pre></li><li>draw(sample)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>[1000 values with dtype=int64]</pre></li></ul></li><li>Data variables: (4)<ul><li>z(sample)float640.4699 0.499 ... 0.486 0.4572<pre>array([0.4699042 , 0.49904497, 0.49006759, 0.4804058 , 0.50658626,\n       0.46588361, 0.48758831, 0.49569231, 0.4650026 , 0.47016651,\n       0.51296718, 0.51296718, 0.52186105, 0.51476507, 0.5296514 ,\n       0.48013469, 0.4887384 , 0.49051181, 0.50654419, 0.48185724,\n       0.49960468, 0.48471074, 0.48534833, 0.46546797, 0.50478457,\n       0.49092853, 0.49592098, 0.50999128, 0.50496112, 0.50496112,\n       0.4818446 , 0.48449353, 0.5105768 , 0.49696239, 0.48900254,\n       0.50752312, 0.49352693, 0.47841634, 0.50864008, 0.47494031,\n       0.47905729, 0.46192232, 0.50778021, 0.5190944 , 0.50620883,\n       0.47114031, 0.4777165 , 0.48813019, 0.48997113, 0.47487802,\n       0.48233148, 0.48843956, 0.4839178 , 0.46542431, 0.46692611,\n       0.51327555, 0.48471632, 0.469812  , 0.5161216 , 0.51464226,\n       0.51464226, 0.46509779, 0.44421044, 0.5105104 , 0.53641068,\n       0.52411552, 0.48502577, 0.4847865 , 0.50238062, 0.51373234,\n       0.49063949, 0.46273391, 0.46204665, 0.51295402, 0.49887088,\n       0.43930346, 0.470053  , 0.46682687, 0.53448986, 0.52013632,\n       0.4977769 , 0.52171137, 0.51464489, 0.50764646, 0.52486997,\n       0.50433478, 0.47808401, 0.48540018, 0.50723011, 0.50462831,\n       0.49018985, 0.50341477, 0.50597253, 0.50595535, 0.50683344,\n       0.48499242, 0.49603448, 0.46384928, 0.48302743, 0.49425673,\n...\n       0.4804776 , 0.49285842, 0.49285842, 0.50241804, 0.50241804,\n       0.51959454, 0.48297978, 0.50491348, 0.49991338, 0.47384407,\n       0.4746992 , 0.48833865, 0.49476196, 0.54756749, 0.52168875,\n       0.49543637, 0.49054755, 0.47253468, 0.48037588, 0.46416891,\n       0.44825084, 0.52890864, 0.43282059, 0.46658425, 0.44955703,\n       0.50966505, 0.48112862, 0.50555935, 0.50417025, 0.51086967,\n       0.50521692, 0.48235844, 0.48235844, 0.49527218, 0.49907702,\n       0.49495474, 0.52123594, 0.4989444 , 0.50094295, 0.51756145,\n       0.48769194, 0.48165421, 0.49490728, 0.50340803, 0.46659991,\n       0.47259562, 0.48321431, 0.50197676, 0.47663988, 0.53154065,\n       0.43875725, 0.47019098, 0.46312393, 0.50749881, 0.4969844 ,\n       0.47820064, 0.49349011, 0.48178029, 0.48398051, 0.49612929,\n       0.48732873, 0.49686937, 0.48669077, 0.50235347, 0.44817958,\n       0.50981431, 0.49731836, 0.4766222 , 0.49258354, 0.46082314,\n       0.5184781 , 0.50702027, 0.51565956, 0.48911703, 0.47142686,\n       0.47937115, 0.46961929, 0.50995249, 0.47532803, 0.4801773 ,\n       0.50332285, 0.53153867, 0.45467345, 0.50891975, 0.4913953 ,\n       0.49740255, 0.46636886, 0.46848073, 0.47112246, 0.51203647,\n       0.51226343, 0.49697994, 0.50481289, 0.4897052 , 0.48348977,\n       0.47560721, 0.51617524, 0.48548315, 0.48597814, 0.45718307])</pre></li><li>v(sample)float640.5595 0.5232 ... 0.5693 0.4741<pre>array([0.55948456, 0.52323813, 0.56551741, 0.53313771, 0.46493731,\n       0.55087673, 0.50254107, 0.49712166, 0.5193884 , 0.53786283,\n       0.47628327, 0.47628327, 0.46890103, 0.42625958, 0.40481376,\n       0.5352571 , 0.51518708, 0.52785003, 0.47464317, 0.54494748,\n       0.48485207, 0.52397391, 0.47947798, 0.51051453, 0.53328515,\n       0.4828603 , 0.51039854, 0.49196315, 0.45089411, 0.45089411,\n       0.53074366, 0.59521434, 0.5280832 , 0.4959387 , 0.50225902,\n       0.45925184, 0.47507529, 0.47019087, 0.491393  , 0.54668279,\n       0.57031345, 0.51527892, 0.51723531, 0.48097444, 0.5019942 ,\n       0.50270784, 0.55318472, 0.49763915, 0.52344266, 0.52922316,\n       0.53068661, 0.60204255, 0.57409213, 0.51774487, 0.5897066 ,\n       0.42715636, 0.48483581, 0.51739792, 0.53598982, 0.52453375,\n       0.52453375, 0.54714409, 0.52415901, 0.42710464, 0.43888118,\n       0.43950797, 0.50929061, 0.52571446, 0.46817741, 0.51363496,\n       0.51353199, 0.5464202 , 0.56618318, 0.43725741, 0.51189602,\n       0.60589056, 0.50717787, 0.53483625, 0.45455116, 0.46787956,\n       0.46060881, 0.4362908 , 0.4662266 , 0.43534448, 0.48122862,\n       0.41796492, 0.57352661, 0.47166845, 0.49295676, 0.45855972,\n       0.5095185 , 0.51932247, 0.48368896, 0.49988081, 0.46324614,\n       0.53557885, 0.56492361, 0.45625511, 0.54128227, 0.47293082,\n...\n       0.54087204, 0.52589016, 0.52589016, 0.44570269, 0.44570269,\n       0.45556855, 0.52366024, 0.51892354, 0.48248938, 0.4837746 ,\n       0.51527428, 0.47644855, 0.50449364, 0.44137543, 0.45953707,\n       0.46457798, 0.49735844, 0.48976267, 0.59975141, 0.57439719,\n       0.58452694, 0.42247181, 0.58281737, 0.60702356, 0.60779257,\n       0.44381974, 0.52026999, 0.4865709 , 0.48275222, 0.4636106 ,\n       0.48842722, 0.52841603, 0.52841603, 0.47236072, 0.46536666,\n       0.4326968 , 0.49307454, 0.49261405, 0.48288692, 0.45916959,\n       0.60841893, 0.49694203, 0.46176213, 0.49802685, 0.52176412,\n       0.52726692, 0.52938726, 0.50286864, 0.5014498 , 0.42210723,\n       0.59795896, 0.5730601 , 0.5662996 , 0.48388775, 0.5168015 ,\n       0.51047207, 0.54635761, 0.52832552, 0.49942346, 0.48766876,\n       0.55853624, 0.4777943 , 0.50380103, 0.52309688, 0.55114737,\n       0.51555938, 0.50668944, 0.50043609, 0.54325946, 0.56084193,\n       0.41411473, 0.45299422, 0.48909671, 0.56575484, 0.47232775,\n       0.48047251, 0.4998052 , 0.54127344, 0.45005795, 0.53429763,\n       0.40968834, 0.44709344, 0.58236811, 0.51212206, 0.47668181,\n       0.54603539, 0.5170905 , 0.58443773, 0.6072656 , 0.46398816,\n       0.44961717, 0.50281085, 0.50883962, 0.44288355, 0.58106786,\n       0.57161634, 0.47743376, 0.50216797, 0.56928885, 0.47411196])</pre></li><li>t(sample)float640.4846 0.5535 ... 0.4404 0.4515<pre>array([0.48458705, 0.55350789, 0.49137116, 0.46346   , 0.51377192,\n       0.4687834 , 0.48376764, 0.4708331 , 0.4843077 , 0.42168161,\n       0.52622507, 0.52622507, 0.52445349, 0.52059101, 0.53803683,\n       0.4986684 , 0.48187461, 0.50334227, 0.51988131, 0.46382329,\n       0.4923178 , 0.50235766, 0.5255414 , 0.43358638, 0.45494497,\n       0.46438112, 0.43700584, 0.4994927 , 0.54020482, 0.54020482,\n       0.4741314 , 0.55409128, 0.4790688 , 0.53979982, 0.51504932,\n       0.45641671, 0.45402082, 0.47977727, 0.51659122, 0.43958788,\n       0.43636252, 0.43659142, 0.51401167, 0.52483367, 0.51899131,\n       0.45630586, 0.48034856, 0.44180398, 0.47515016, 0.39325903,\n       0.39135063, 0.46700878, 0.46128649, 0.45441337, 0.44403958,\n       0.47079232, 0.48392   , 0.46936239, 0.52824565, 0.51922117,\n       0.51922117, 0.43233355, 0.45451571, 0.49363254, 0.53912362,\n       0.5088395 , 0.4688445 , 0.49826372, 0.46301259, 0.50861762,\n       0.48466769, 0.47678902, 0.51995394, 0.45834862, 0.48665645,\n       0.44921504, 0.51203536, 0.46500077, 0.54272496, 0.51381783,\n       0.53263419, 0.51663641, 0.51000894, 0.51968724, 0.52116064,\n       0.50737104, 0.51216371, 0.4904556 , 0.52089794, 0.4730561 ,\n       0.54588333, 0.48115785, 0.52743539, 0.47092694, 0.45740127,\n       0.50167021, 0.52541035, 0.47191001, 0.47308344, 0.49674712,\n...\n       0.47945645, 0.57683454, 0.57683454, 0.5171689 , 0.5171689 ,\n       0.55042218, 0.5114095 , 0.52805399, 0.49989074, 0.47107516,\n       0.47352807, 0.46653491, 0.45619133, 0.52606693, 0.53221888,\n       0.44174684, 0.45190157, 0.48116776, 0.46488192, 0.47422308,\n       0.44785305, 0.51853205, 0.43799676, 0.44074277, 0.45001089,\n       0.48108993, 0.52440596, 0.49258655, 0.50593188, 0.50222598,\n       0.48843664, 0.47485668, 0.47485668, 0.45490698, 0.45844727,\n       0.45649083, 0.47496442, 0.52666983, 0.50634632, 0.48091012,\n       0.48552516, 0.47466074, 0.49209767, 0.47401569, 0.47588728,\n       0.45867058, 0.43365166, 0.55757828, 0.49742082, 0.46966501,\n       0.41651593, 0.44741199, 0.46384769, 0.44257968, 0.44525443,\n       0.46067389, 0.51928545, 0.45887563, 0.51787677, 0.42699929,\n       0.5156614 , 0.484738  , 0.49953092, 0.53961087, 0.43816568,\n       0.44796802, 0.45341586, 0.47653008, 0.45698803, 0.43798287,\n       0.52134374, 0.48720175, 0.51037569, 0.46647625, 0.47720997,\n       0.45590028, 0.49506375, 0.47744816, 0.51462998, 0.54061563,\n       0.50601705, 0.50931159, 0.49620689, 0.46467432, 0.47653179,\n       0.49701786, 0.47611603, 0.48794347, 0.47071954, 0.44520535,\n       0.47764412, 0.49148085, 0.48961871, 0.44662573, 0.52227165,\n       0.46978471, 0.48955782, 0.50476567, 0.44036513, 0.45152418])</pre></li><li>a(sample)float641.448 1.453 1.493 ... 1.502 1.522<pre>array([1.44759672, 1.45259529, 1.49343447, 1.50284014, 1.51263448,\n       1.46905961, 1.51259916, 1.5142851 , 1.49174719, 1.52606595,\n       1.4890533 , 1.4890533 , 1.49239307, 1.44428241, 1.44355249,\n       1.48295272, 1.50591479, 1.48556412, 1.46951339, 1.51798236,\n       1.48275961, 1.48975163, 1.45557554, 1.5203054 , 1.54162858,\n       1.55461358, 1.53127189, 1.45113812, 1.45857586, 1.45857586,\n       1.46028149, 1.46694656, 1.54972756, 1.41834979, 1.43647509,\n       1.49286527, 1.49946523, 1.48671874, 1.44633111, 1.58835994,\n       1.56973691, 1.52313396, 1.48754403, 1.49705979, 1.44648274,\n       1.50809462, 1.52257261, 1.53465012, 1.55215565, 1.53479831,\n       1.51981491, 1.49613553, 1.50967652, 1.54458913, 1.46549605,\n       1.47079482, 1.50489114, 1.50035196, 1.48764696, 1.48881986,\n       1.48881986, 1.49949283, 1.50380022, 1.40915048, 1.49389813,\n       1.48722457, 1.51114747, 1.52392683, 1.47222668, 1.50062151,\n       1.52343765, 1.44263854, 1.44814469, 1.56688016, 1.47678505,\n       1.51494349, 1.49287627, 1.52991733, 1.4081477 , 1.43623738,\n       1.46052317, 1.48012844, 1.47700697, 1.45474111, 1.48505027,\n       1.46358094, 1.48291172, 1.45500418, 1.47467305, 1.4819834 ,\n       1.4684883 , 1.52655626, 1.42683468, 1.52406241, 1.58424396,\n       1.48731715, 1.47675068, 1.43721115, 1.50279843, 1.46721041,\n...\n       1.47662503, 1.37104325, 1.37104325, 1.43514909, 1.43514909,\n       1.45414222, 1.45774301, 1.48131199, 1.46054528, 1.51724528,\n       1.50559308, 1.52347605, 1.49676735, 1.4916907 , 1.50429154,\n       1.49040359, 1.49407919, 1.47148584, 1.51165174, 1.51223035,\n       1.52887846, 1.42639448, 1.52031836, 1.49628017, 1.54379952,\n       1.49341916, 1.45538554, 1.48604013, 1.46762142, 1.4767554 ,\n       1.49615915, 1.51631093, 1.51631093, 1.51630459, 1.52046403,\n       1.56488977, 1.53497976, 1.41988065, 1.48759568, 1.45813692,\n       1.48076802, 1.48777629, 1.47108072, 1.49571438, 1.48113488,\n       1.47613501, 1.56743243, 1.427114  , 1.45151489, 1.56665537,\n       1.54427928, 1.54138868, 1.55339707, 1.50585043, 1.52382347,\n       1.507642  , 1.49775465, 1.46779734, 1.49038303, 1.48708287,\n       1.56038499, 1.44085695, 1.4950301 , 1.48741033, 1.49013949,\n       1.55332041, 1.5347392 , 1.48764287, 1.50277452, 1.53930332,\n       1.44601168, 1.4179183 , 1.41514028, 1.52456493, 1.49230352,\n       1.4983902 , 1.47395791, 1.4634337 , 1.45607321, 1.46780446,\n       1.46949338, 1.46783494, 1.54118123, 1.49639297, 1.50261113,\n       1.47929217, 1.50825607, 1.45877422, 1.43105101, 1.55853682,\n       1.51842697, 1.55857296, 1.51536264, 1.51218903, 1.52181187,\n       1.50340431, 1.45102542, 1.54401646, 1.50192259, 1.52215997])</pre></li></ul></li><li>Indexes: (1)<ul><li>samplechaindrawPandasMultiIndex<pre>PandasIndex(MultiIndex([(0,   0),\n            (0,   1),\n            (0,   2),\n            (0,   3),\n            (0,   4),\n            (0,   5),\n            (0,   6),\n            (0,   7),\n            (0,   8),\n            (0,   9),\n            ...\n            (1, 490),\n            (1, 491),\n            (1, 492),\n            (1, 493),\n            (1, 494),\n            (1, 495),\n            (1, 496),\n            (1, 497),\n            (1, 498),\n            (1, 499)],\n           name='sample', length=1000))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-29T19:49:47.177748+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :12.301905870437622tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> <p>Since Arviz really just calls the <code>.stack()</code> method from xarray, here the corresponding example using the lower level <code>xarray</code> interface.</p> In\u00a0[20]: Copied! <pre>infer_data_simple_ddm_model.posterior.stack(sample=(\"chain\", \"draw\"))\n</pre> infer_data_simple_ddm_model.posterior.stack(sample=(\"chain\", \"draw\")) Out[20]: <pre>&lt;xarray.Dataset&gt; Size: 56kB\nDimensions:  (sample: 1000)\nCoordinates:\n  * sample   (sample) object 8kB MultiIndex\n  * chain    (sample) int64 8kB 0 0 0 0 0 0 0 0 0 0 0 ... 1 1 1 1 1 1 1 1 1 1 1\n  * draw     (sample) int64 8kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    z        (sample) float64 8kB 0.4699 0.499 0.4901 ... 0.4855 0.486 0.4572\n    v        (sample) float64 8kB 0.5595 0.5232 0.5655 ... 0.5022 0.5693 0.4741\n    t        (sample) float64 8kB 0.4846 0.5535 0.4914 ... 0.5048 0.4404 0.4515\n    a        (sample) float64 8kB 1.448 1.453 1.493 1.503 ... 1.544 1.502 1.522\nAttributes:\n    created_at:                  2025-09-29T19:49:47.177748+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               12.301905870437622\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>sample: 1000</li></ul></li><li>Coordinates: (3)<ul><li>sample(sample)objectMultiIndex<pre>[1000 values with dtype=object]</pre></li><li>chain(sample)int640 0 0 0 0 0 0 0 ... 1 1 1 1 1 1 1 1<pre>[1000 values with dtype=int64]</pre></li><li>draw(sample)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>[1000 values with dtype=int64]</pre></li></ul></li><li>Data variables: (4)<ul><li>z(sample)float640.4699 0.499 ... 0.486 0.4572<pre>array([0.4699042 , 0.49904497, 0.49006759, 0.4804058 , 0.50658626,\n       0.46588361, 0.48758831, 0.49569231, 0.4650026 , 0.47016651,\n       0.51296718, 0.51296718, 0.52186105, 0.51476507, 0.5296514 ,\n       0.48013469, 0.4887384 , 0.49051181, 0.50654419, 0.48185724,\n       0.49960468, 0.48471074, 0.48534833, 0.46546797, 0.50478457,\n       0.49092853, 0.49592098, 0.50999128, 0.50496112, 0.50496112,\n       0.4818446 , 0.48449353, 0.5105768 , 0.49696239, 0.48900254,\n       0.50752312, 0.49352693, 0.47841634, 0.50864008, 0.47494031,\n       0.47905729, 0.46192232, 0.50778021, 0.5190944 , 0.50620883,\n       0.47114031, 0.4777165 , 0.48813019, 0.48997113, 0.47487802,\n       0.48233148, 0.48843956, 0.4839178 , 0.46542431, 0.46692611,\n       0.51327555, 0.48471632, 0.469812  , 0.5161216 , 0.51464226,\n       0.51464226, 0.46509779, 0.44421044, 0.5105104 , 0.53641068,\n       0.52411552, 0.48502577, 0.4847865 , 0.50238062, 0.51373234,\n       0.49063949, 0.46273391, 0.46204665, 0.51295402, 0.49887088,\n       0.43930346, 0.470053  , 0.46682687, 0.53448986, 0.52013632,\n       0.4977769 , 0.52171137, 0.51464489, 0.50764646, 0.52486997,\n       0.50433478, 0.47808401, 0.48540018, 0.50723011, 0.50462831,\n       0.49018985, 0.50341477, 0.50597253, 0.50595535, 0.50683344,\n       0.48499242, 0.49603448, 0.46384928, 0.48302743, 0.49425673,\n...\n       0.4804776 , 0.49285842, 0.49285842, 0.50241804, 0.50241804,\n       0.51959454, 0.48297978, 0.50491348, 0.49991338, 0.47384407,\n       0.4746992 , 0.48833865, 0.49476196, 0.54756749, 0.52168875,\n       0.49543637, 0.49054755, 0.47253468, 0.48037588, 0.46416891,\n       0.44825084, 0.52890864, 0.43282059, 0.46658425, 0.44955703,\n       0.50966505, 0.48112862, 0.50555935, 0.50417025, 0.51086967,\n       0.50521692, 0.48235844, 0.48235844, 0.49527218, 0.49907702,\n       0.49495474, 0.52123594, 0.4989444 , 0.50094295, 0.51756145,\n       0.48769194, 0.48165421, 0.49490728, 0.50340803, 0.46659991,\n       0.47259562, 0.48321431, 0.50197676, 0.47663988, 0.53154065,\n       0.43875725, 0.47019098, 0.46312393, 0.50749881, 0.4969844 ,\n       0.47820064, 0.49349011, 0.48178029, 0.48398051, 0.49612929,\n       0.48732873, 0.49686937, 0.48669077, 0.50235347, 0.44817958,\n       0.50981431, 0.49731836, 0.4766222 , 0.49258354, 0.46082314,\n       0.5184781 , 0.50702027, 0.51565956, 0.48911703, 0.47142686,\n       0.47937115, 0.46961929, 0.50995249, 0.47532803, 0.4801773 ,\n       0.50332285, 0.53153867, 0.45467345, 0.50891975, 0.4913953 ,\n       0.49740255, 0.46636886, 0.46848073, 0.47112246, 0.51203647,\n       0.51226343, 0.49697994, 0.50481289, 0.4897052 , 0.48348977,\n       0.47560721, 0.51617524, 0.48548315, 0.48597814, 0.45718307])</pre></li><li>v(sample)float640.5595 0.5232 ... 0.5693 0.4741<pre>array([0.55948456, 0.52323813, 0.56551741, 0.53313771, 0.46493731,\n       0.55087673, 0.50254107, 0.49712166, 0.5193884 , 0.53786283,\n       0.47628327, 0.47628327, 0.46890103, 0.42625958, 0.40481376,\n       0.5352571 , 0.51518708, 0.52785003, 0.47464317, 0.54494748,\n       0.48485207, 0.52397391, 0.47947798, 0.51051453, 0.53328515,\n       0.4828603 , 0.51039854, 0.49196315, 0.45089411, 0.45089411,\n       0.53074366, 0.59521434, 0.5280832 , 0.4959387 , 0.50225902,\n       0.45925184, 0.47507529, 0.47019087, 0.491393  , 0.54668279,\n       0.57031345, 0.51527892, 0.51723531, 0.48097444, 0.5019942 ,\n       0.50270784, 0.55318472, 0.49763915, 0.52344266, 0.52922316,\n       0.53068661, 0.60204255, 0.57409213, 0.51774487, 0.5897066 ,\n       0.42715636, 0.48483581, 0.51739792, 0.53598982, 0.52453375,\n       0.52453375, 0.54714409, 0.52415901, 0.42710464, 0.43888118,\n       0.43950797, 0.50929061, 0.52571446, 0.46817741, 0.51363496,\n       0.51353199, 0.5464202 , 0.56618318, 0.43725741, 0.51189602,\n       0.60589056, 0.50717787, 0.53483625, 0.45455116, 0.46787956,\n       0.46060881, 0.4362908 , 0.4662266 , 0.43534448, 0.48122862,\n       0.41796492, 0.57352661, 0.47166845, 0.49295676, 0.45855972,\n       0.5095185 , 0.51932247, 0.48368896, 0.49988081, 0.46324614,\n       0.53557885, 0.56492361, 0.45625511, 0.54128227, 0.47293082,\n...\n       0.54087204, 0.52589016, 0.52589016, 0.44570269, 0.44570269,\n       0.45556855, 0.52366024, 0.51892354, 0.48248938, 0.4837746 ,\n       0.51527428, 0.47644855, 0.50449364, 0.44137543, 0.45953707,\n       0.46457798, 0.49735844, 0.48976267, 0.59975141, 0.57439719,\n       0.58452694, 0.42247181, 0.58281737, 0.60702356, 0.60779257,\n       0.44381974, 0.52026999, 0.4865709 , 0.48275222, 0.4636106 ,\n       0.48842722, 0.52841603, 0.52841603, 0.47236072, 0.46536666,\n       0.4326968 , 0.49307454, 0.49261405, 0.48288692, 0.45916959,\n       0.60841893, 0.49694203, 0.46176213, 0.49802685, 0.52176412,\n       0.52726692, 0.52938726, 0.50286864, 0.5014498 , 0.42210723,\n       0.59795896, 0.5730601 , 0.5662996 , 0.48388775, 0.5168015 ,\n       0.51047207, 0.54635761, 0.52832552, 0.49942346, 0.48766876,\n       0.55853624, 0.4777943 , 0.50380103, 0.52309688, 0.55114737,\n       0.51555938, 0.50668944, 0.50043609, 0.54325946, 0.56084193,\n       0.41411473, 0.45299422, 0.48909671, 0.56575484, 0.47232775,\n       0.48047251, 0.4998052 , 0.54127344, 0.45005795, 0.53429763,\n       0.40968834, 0.44709344, 0.58236811, 0.51212206, 0.47668181,\n       0.54603539, 0.5170905 , 0.58443773, 0.6072656 , 0.46398816,\n       0.44961717, 0.50281085, 0.50883962, 0.44288355, 0.58106786,\n       0.57161634, 0.47743376, 0.50216797, 0.56928885, 0.47411196])</pre></li><li>t(sample)float640.4846 0.5535 ... 0.4404 0.4515<pre>array([0.48458705, 0.55350789, 0.49137116, 0.46346   , 0.51377192,\n       0.4687834 , 0.48376764, 0.4708331 , 0.4843077 , 0.42168161,\n       0.52622507, 0.52622507, 0.52445349, 0.52059101, 0.53803683,\n       0.4986684 , 0.48187461, 0.50334227, 0.51988131, 0.46382329,\n       0.4923178 , 0.50235766, 0.5255414 , 0.43358638, 0.45494497,\n       0.46438112, 0.43700584, 0.4994927 , 0.54020482, 0.54020482,\n       0.4741314 , 0.55409128, 0.4790688 , 0.53979982, 0.51504932,\n       0.45641671, 0.45402082, 0.47977727, 0.51659122, 0.43958788,\n       0.43636252, 0.43659142, 0.51401167, 0.52483367, 0.51899131,\n       0.45630586, 0.48034856, 0.44180398, 0.47515016, 0.39325903,\n       0.39135063, 0.46700878, 0.46128649, 0.45441337, 0.44403958,\n       0.47079232, 0.48392   , 0.46936239, 0.52824565, 0.51922117,\n       0.51922117, 0.43233355, 0.45451571, 0.49363254, 0.53912362,\n       0.5088395 , 0.4688445 , 0.49826372, 0.46301259, 0.50861762,\n       0.48466769, 0.47678902, 0.51995394, 0.45834862, 0.48665645,\n       0.44921504, 0.51203536, 0.46500077, 0.54272496, 0.51381783,\n       0.53263419, 0.51663641, 0.51000894, 0.51968724, 0.52116064,\n       0.50737104, 0.51216371, 0.4904556 , 0.52089794, 0.4730561 ,\n       0.54588333, 0.48115785, 0.52743539, 0.47092694, 0.45740127,\n       0.50167021, 0.52541035, 0.47191001, 0.47308344, 0.49674712,\n...\n       0.47945645, 0.57683454, 0.57683454, 0.5171689 , 0.5171689 ,\n       0.55042218, 0.5114095 , 0.52805399, 0.49989074, 0.47107516,\n       0.47352807, 0.46653491, 0.45619133, 0.52606693, 0.53221888,\n       0.44174684, 0.45190157, 0.48116776, 0.46488192, 0.47422308,\n       0.44785305, 0.51853205, 0.43799676, 0.44074277, 0.45001089,\n       0.48108993, 0.52440596, 0.49258655, 0.50593188, 0.50222598,\n       0.48843664, 0.47485668, 0.47485668, 0.45490698, 0.45844727,\n       0.45649083, 0.47496442, 0.52666983, 0.50634632, 0.48091012,\n       0.48552516, 0.47466074, 0.49209767, 0.47401569, 0.47588728,\n       0.45867058, 0.43365166, 0.55757828, 0.49742082, 0.46966501,\n       0.41651593, 0.44741199, 0.46384769, 0.44257968, 0.44525443,\n       0.46067389, 0.51928545, 0.45887563, 0.51787677, 0.42699929,\n       0.5156614 , 0.484738  , 0.49953092, 0.53961087, 0.43816568,\n       0.44796802, 0.45341586, 0.47653008, 0.45698803, 0.43798287,\n       0.52134374, 0.48720175, 0.51037569, 0.46647625, 0.47720997,\n       0.45590028, 0.49506375, 0.47744816, 0.51462998, 0.54061563,\n       0.50601705, 0.50931159, 0.49620689, 0.46467432, 0.47653179,\n       0.49701786, 0.47611603, 0.48794347, 0.47071954, 0.44520535,\n       0.47764412, 0.49148085, 0.48961871, 0.44662573, 0.52227165,\n       0.46978471, 0.48955782, 0.50476567, 0.44036513, 0.45152418])</pre></li><li>a(sample)float641.448 1.453 1.493 ... 1.502 1.522<pre>array([1.44759672, 1.45259529, 1.49343447, 1.50284014, 1.51263448,\n       1.46905961, 1.51259916, 1.5142851 , 1.49174719, 1.52606595,\n       1.4890533 , 1.4890533 , 1.49239307, 1.44428241, 1.44355249,\n       1.48295272, 1.50591479, 1.48556412, 1.46951339, 1.51798236,\n       1.48275961, 1.48975163, 1.45557554, 1.5203054 , 1.54162858,\n       1.55461358, 1.53127189, 1.45113812, 1.45857586, 1.45857586,\n       1.46028149, 1.46694656, 1.54972756, 1.41834979, 1.43647509,\n       1.49286527, 1.49946523, 1.48671874, 1.44633111, 1.58835994,\n       1.56973691, 1.52313396, 1.48754403, 1.49705979, 1.44648274,\n       1.50809462, 1.52257261, 1.53465012, 1.55215565, 1.53479831,\n       1.51981491, 1.49613553, 1.50967652, 1.54458913, 1.46549605,\n       1.47079482, 1.50489114, 1.50035196, 1.48764696, 1.48881986,\n       1.48881986, 1.49949283, 1.50380022, 1.40915048, 1.49389813,\n       1.48722457, 1.51114747, 1.52392683, 1.47222668, 1.50062151,\n       1.52343765, 1.44263854, 1.44814469, 1.56688016, 1.47678505,\n       1.51494349, 1.49287627, 1.52991733, 1.4081477 , 1.43623738,\n       1.46052317, 1.48012844, 1.47700697, 1.45474111, 1.48505027,\n       1.46358094, 1.48291172, 1.45500418, 1.47467305, 1.4819834 ,\n       1.4684883 , 1.52655626, 1.42683468, 1.52406241, 1.58424396,\n       1.48731715, 1.47675068, 1.43721115, 1.50279843, 1.46721041,\n...\n       1.47662503, 1.37104325, 1.37104325, 1.43514909, 1.43514909,\n       1.45414222, 1.45774301, 1.48131199, 1.46054528, 1.51724528,\n       1.50559308, 1.52347605, 1.49676735, 1.4916907 , 1.50429154,\n       1.49040359, 1.49407919, 1.47148584, 1.51165174, 1.51223035,\n       1.52887846, 1.42639448, 1.52031836, 1.49628017, 1.54379952,\n       1.49341916, 1.45538554, 1.48604013, 1.46762142, 1.4767554 ,\n       1.49615915, 1.51631093, 1.51631093, 1.51630459, 1.52046403,\n       1.56488977, 1.53497976, 1.41988065, 1.48759568, 1.45813692,\n       1.48076802, 1.48777629, 1.47108072, 1.49571438, 1.48113488,\n       1.47613501, 1.56743243, 1.427114  , 1.45151489, 1.56665537,\n       1.54427928, 1.54138868, 1.55339707, 1.50585043, 1.52382347,\n       1.507642  , 1.49775465, 1.46779734, 1.49038303, 1.48708287,\n       1.56038499, 1.44085695, 1.4950301 , 1.48741033, 1.49013949,\n       1.55332041, 1.5347392 , 1.48764287, 1.50277452, 1.53930332,\n       1.44601168, 1.4179183 , 1.41514028, 1.52456493, 1.49230352,\n       1.4983902 , 1.47395791, 1.4634337 , 1.45607321, 1.46780446,\n       1.46949338, 1.46783494, 1.54118123, 1.49639297, 1.50261113,\n       1.47929217, 1.50825607, 1.45877422, 1.43105101, 1.55853682,\n       1.51842697, 1.55857296, 1.51536264, 1.51218903, 1.52181187,\n       1.50340431, 1.45102542, 1.54401646, 1.50192259, 1.52215997])</pre></li></ul></li><li>Indexes: (1)<ul><li>samplechaindrawPandasMultiIndex<pre>PandasIndex(MultiIndex([(0,   0),\n            (0,   1),\n            (0,   2),\n            (0,   3),\n            (0,   4),\n            (0,   5),\n            (0,   6),\n            (0,   7),\n            (0,   8),\n            (0,   9),\n            ...\n            (1, 490),\n            (1, 491),\n            (1, 492),\n            (1, 493),\n            (1, 494),\n            (1, 495),\n            (1, 496),\n            (1, 497),\n            (1, 498),\n            (1, 499)],\n           name='sample', length=1000))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-29T19:49:47.177748+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :12.301905870437622tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> In\u00a0[21]: Copied! <pre>az.summary(\n    infer_data_simple_ddm_model,\n    var_names=[var_name.name for var_name in simple_ddm_model.pymc_model.free_RVs],\n)\n</pre> az.summary(     infer_data_simple_ddm_model,     var_names=[var_name.name for var_name in simple_ddm_model.pymc_model.free_RVs], ) Out[21]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat t 0.490 0.032 0.431 0.549 0.001 0.001 534.0 564.0 1.00 a 1.488 0.039 1.425 1.571 0.002 0.001 556.0 714.0 1.00 z 0.491 0.018 0.457 0.524 0.001 0.001 616.0 528.0 1.00 v 0.506 0.048 0.420 0.598 0.002 0.001 650.0 737.0 1.01 <p>This table returns the parameter-wise mean of our posterior and a few extra statistics.</p> <p>Of these extra statistics, the one-stop shop for flagging convergence issues is the <code>r_hat</code> value, which is reported in the right-most column.</p> <p>To navigate this statistic, here is a rule of thumb widely used in applied Bayesian statistics.</p> <p>If you find an <code>r_hat</code> value <code>&gt; 1.01</code>, it warrants investigation.</p> In\u00a0[22]: Copied! <pre>az.plot_trace(\n    infer_data_simple_ddm_model,  # we exclude the log_likelihood traces here\n    lines=[(key_, {}, param_dict_init[key_]) for key_ in param_dict_init],\n)\nplt.tight_layout()\n</pre> az.plot_trace(     infer_data_simple_ddm_model,  # we exclude the log_likelihood traces here     lines=[(key_, {}, param_dict_init[key_]) for key_ in param_dict_init], ) plt.tight_layout() <p>The <code>.sample()</code> function also sets a <code>trace</code> attribute, on our <code>hssm</code> class, so instead, we could call the plot like so:</p> In\u00a0[23]: Copied! <pre>az.plot_trace(\n    simple_ddm_model.traces,\n    lines=[(key_, {}, param_dict_init[key_]) for key_ in param_dict_init],\n);\n</pre> az.plot_trace(     simple_ddm_model.traces,     lines=[(key_, {}, param_dict_init[key_]) for key_ in param_dict_init], ); <p>In this tutorial we are most often going to use the latter way of accessing the traces, but there is no preferred option.</p> <p>Let's look at a few more plots.</p> In\u00a0[24]: Copied! <pre>az.plot_forest(simple_ddm_model.traces)\n</pre> az.plot_forest(simple_ddm_model.traces) Out[24]: <pre>array([&lt;Axes: title={'center': '94.0% HDI'}&gt;], dtype=object)</pre> In\u00a0[25]: Copied! <pre>az.plot_forest(simple_ddm_model.traces, combined=True)\n</pre> az.plot_forest(simple_ddm_model.traces, combined=True) Out[25]: <pre>array([&lt;Axes: title={'center': '94.0% HDI'}&gt;], dtype=object)</pre> In\u00a0[26]: Copied! <pre>az.plot_posterior(simple_ddm_model.traces)\n</pre> az.plot_posterior(simple_ddm_model.traces) Out[26]: <pre>array([&lt;Axes: title={'center': 'z'}&gt;, &lt;Axes: title={'center': 'v'}&gt;,\n       &lt;Axes: title={'center': 't'}&gt;, &lt;Axes: title={'center': 'a'}&gt;],\n      dtype=object)</pre> <p>Especially for parameter recovery studies, you may want to include reference values for the parameters of interest.</p> <p>You can do so using the <code>ref_val</code> argument. See the example below:</p> In\u00a0[27]: Copied! <pre>az.plot_posterior(\n    simple_ddm_model.traces,\n    ref_val=[\n        param_dict_init[var_name]\n        for var_name in simple_ddm_model.traces.posterior.data_vars\n    ],\n)\n</pre> az.plot_posterior(     simple_ddm_model.traces,     ref_val=[         param_dict_init[var_name]         for var_name in simple_ddm_model.traces.posterior.data_vars     ], ) Out[27]: <pre>array([&lt;Axes: title={'center': 'z'}&gt;, &lt;Axes: title={'center': 'v'}&gt;,\n       &lt;Axes: title={'center': 't'}&gt;, &lt;Axes: title={'center': 'a'}&gt;],\n      dtype=object)</pre> <p>Since it is sometimes useful, especially for more complex cases, below an alternative approach in which we pass <code>ref_val</code> as a dictionary.</p> In\u00a0[28]: Copied! <pre>az.plot_posterior(\n    simple_ddm_model.traces,\n    ref_val={\n        \"v\": [{\"ref_val\": param_dict_init[\"v\"]}],\n        \"a\": [{\"ref_val\": param_dict_init[\"a\"]}],\n        \"z\": [{\"ref_val\": param_dict_init[\"z\"]}],\n        \"t\": [{\"ref_val\": param_dict_init[\"t\"]}],\n    },\n)\n</pre> az.plot_posterior(     simple_ddm_model.traces,     ref_val={         \"v\": [{\"ref_val\": param_dict_init[\"v\"]}],         \"a\": [{\"ref_val\": param_dict_init[\"a\"]}],         \"z\": [{\"ref_val\": param_dict_init[\"z\"]}],         \"t\": [{\"ref_val\": param_dict_init[\"t\"]}],     }, ) Out[28]: <pre>array([&lt;Axes: title={'center': 'z'}&gt;, &lt;Axes: title={'center': 'v'}&gt;,\n       &lt;Axes: title={'center': 't'}&gt;, &lt;Axes: title={'center': 'a'}&gt;],\n      dtype=object)</pre> <p>The posterior pair plot show us bi-variate traceplots and is useful to check for simple parameter tradeoffs that may emerge. The simplest (linear) tradeoff may be a high correlation between two parameters. This can be very helpful in diagnosing sampler issues for example. If such tradeoffs exist, one often see extremely wide marginal distributions.</p> <p>In our <code>ddm</code> example, we see a little bit of a tradeoff between <code>a</code> and <code>t</code>, as well as between <code>v</code> and <code>z</code>, however nothing concerning.</p> In\u00a0[29]: Copied! <pre>az.plot_pair(\n    simple_ddm_model.traces,\n    kind=\"kde\",\n    reference_values=param_dict_init,\n    marginals=True,\n);\n</pre> az.plot_pair(     simple_ddm_model.traces,     kind=\"kde\",     reference_values=param_dict_init,     marginals=True, ); <p>The few plot we showed here are just the beginning: ArviZ has a much broader spectrum of graphs and other convenience function available. Just check the documentation.</p> In\u00a0[30]: Copied! <pre># Calculate the correlation matrix\nposterior_correlation_matrix = np.corrcoef(\n    np.stack(\n        [idata_extracted[var_].values for var_ in idata_extracted.data_vars.variables]\n    )\n)\nnum_vars = posterior_correlation_matrix.shape[0]\n\n# Make heatmap\nfig, ax = plt.subplots(1, 1)\ncax = ax.imshow(posterior_correlation_matrix, cmap=\"coolwarm\", vmin=-1, vmax=1)\nfig.colorbar(cax, ax=ax)\nax.set_title(\"Posterior Correlation Matrix\")\n\n# Add ticks\nax.set_xticks(range(posterior_correlation_matrix.shape[0]))\nax.set_xticklabels([var_ for var_ in idata_extracted.data_vars.variables])\nax.set_yticks(range(posterior_correlation_matrix.shape[0]))\nax.set_yticklabels([var_ for var_ in idata_extracted.data_vars.variables])\n\n# Annotate heatmap\nfor i in range(num_vars):\n    for j in range(num_vars):\n        ax.text(\n            j,\n            i,\n            f\"{posterior_correlation_matrix[i, j]:.2f}\",\n            ha=\"center\",\n            va=\"center\",\n            color=\"black\",\n        )\n\nplt.show()\n</pre> # Calculate the correlation matrix posterior_correlation_matrix = np.corrcoef(     np.stack(         [idata_extracted[var_].values for var_ in idata_extracted.data_vars.variables]     ) ) num_vars = posterior_correlation_matrix.shape[0]  # Make heatmap fig, ax = plt.subplots(1, 1) cax = ax.imshow(posterior_correlation_matrix, cmap=\"coolwarm\", vmin=-1, vmax=1) fig.colorbar(cax, ax=ax) ax.set_title(\"Posterior Correlation Matrix\")  # Add ticks ax.set_xticks(range(posterior_correlation_matrix.shape[0])) ax.set_xticklabels([var_ for var_ in idata_extracted.data_vars.variables]) ax.set_yticks(range(posterior_correlation_matrix.shape[0])) ax.set_yticklabels([var_ for var_ in idata_extracted.data_vars.variables])  # Annotate heatmap for i in range(num_vars):     for j in range(num_vars):         ax.text(             j,             i,             f\"{posterior_correlation_matrix[i, j]:.2f}\",             ha=\"center\",             va=\"center\",             color=\"black\",         )  plt.show() In\u00a0[31]: Copied! <pre>simple_ddm_model.loglik_kind\n</pre> simple_ddm_model.loglik_kind Out[31]: <pre>'analytical'</pre> <p>Ah... we were using an <code>analytical</code> likelihood with the DDM model in the last section. Now let's see something different!</p> In\u00a0[32]: Copied! <pre># Simulate angle data\nv_angle_true = 0.5\na_angle_true = 1.5\nz_angle_true = 0.5\nt_angle_true = 0.2\ntheta_angle_true = 0.2\n\nparam_dict_angle = dict(v=0.5, a=1.5, z=0.5, t=0.2, theta=0.2)\n\nlines_list_angle = [(key_, {}, param_dict_angle[key_]) for key_ in param_dict_angle]\n\ndataset_angle = hssm.simulate_data(model=\"angle\", theta=param_dict_angle, size=1000)\n</pre> # Simulate angle data v_angle_true = 0.5 a_angle_true = 1.5 z_angle_true = 0.5 t_angle_true = 0.2 theta_angle_true = 0.2  param_dict_angle = dict(v=0.5, a=1.5, z=0.5, t=0.2, theta=0.2)  lines_list_angle = [(key_, {}, param_dict_angle[key_]) for key_ in param_dict_angle]  dataset_angle = hssm.simulate_data(model=\"angle\", theta=param_dict_angle, size=1000) <p>We pass a single additional argument to our <code>HSSM</code> class and set <code>model='angle'</code>.</p> In\u00a0[33]: Copied! <pre>model_angle = hssm.HSSM(data=dataset_angle, model=\"angle\")\n\nmodel_angle\n</pre> model_angle = hssm.HSSM(data=dataset_angle, model=\"angle\")  model_angle <pre>Model initialized successfully.\n</pre> Out[33]: <pre>Hierarchical Sequential Sampling Model\nModel: angle\n\nResponse variable: rt,response\nLikelihood: approx_differentiable\nObservations: 1000\n\nParameters:\n\nv:\n    Prior: Uniform(lower: -3.0, upper: 3.0)\n    Explicit bounds: (-3.0, 3.0)\n\na:\n    Prior: Uniform(lower: 0.3, upper: 3.0)\n    Explicit bounds: (0.3, 3.0)\n\nz:\n    Prior: Uniform(lower: 0.1, upper: 0.9)\n    Explicit bounds: (0.1, 0.9)\n\nt:\n    Prior: Uniform(lower: 0.001, upper: 2.0)\n    Explicit bounds: (0.001, 2.0)\n\ntheta:\n    Prior: Uniform(lower: -0.1, upper: 1.3)\n    Explicit bounds: (-0.1, 1.3)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> <p>The model graph now show us an additional parameter <code>theta</code>!</p> In\u00a0[34]: Copied! <pre>model_angle.graph()\n</pre> model_angle.graph() <pre>max_shape:  (1000,)\nsize:  (np.int64(1000),)\n</pre> Out[34]: <p>Let's check the type of likelihood that is used under the hood ...</p> In\u00a0[35]: Copied! <pre>model_angle.loglik_kind\n</pre> model_angle.loglik_kind Out[35]: <pre>'approx_differentiable'</pre> <p>Ok so here we rely on a likelihood of the <code>approx_differentiable</code> kind.</p> <p>As discussed, with the initial set of pre-supplied likelihoods, this implies that we are using a LAN in the background.</p> In\u00a0[36]: Copied! <pre>jax.config.update(\"jax_enable_x64\", False)\ninfer_data_angle = model_angle.sample(\n    sampler=\"nuts_numpyro\",\n    chains=2,\n    cores=2,\n    draws=500,\n    tune=500,\n    idata_kwargs=dict(log_likelihood=False),  # no need to return likelihoods here\n    # mp_ctx=\"spawn\",\n)\n</pre> jax.config.update(\"jax_enable_x64\", False) infer_data_angle = model_angle.sample(     sampler=\"nuts_numpyro\",     chains=2,     cores=2,     draws=500,     tune=500,     idata_kwargs=dict(log_likelihood=False),  # no need to return likelihoods here     # mp_ctx=\"spawn\", ) <pre>Using default initvals. \n\n</pre> <pre>sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:10&lt;00:00, 93.30it/s, 15 steps of size 2.74e-01. acc. prob=0.94]\nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:09&lt;00:00, 104.06it/s, 23 steps of size 2.56e-01. acc. prob=0.95]\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n</pre> In\u00a0[37]: Copied! <pre>az.plot_trace(model_angle.traces, lines=lines_list_angle)\nplt.tight_layout()\n</pre> az.plot_trace(model_angle.traces, lines=lines_list_angle) plt.tight_layout() <p>HSSM allows you to specify priors quite freely. If you used HDDM previously, you may feel relieved to read that your hands are now untied!</p> <p>With HSSM we have multiple routes to priors. But let's first consider a special case:</p> <p>Assume that instead of fitting all parameters of the DDM,</p> <p>we instead want to fit only the <code>v</code> (drift) parameter, setting all other parameters to fixed scalar values.</p> <p>HSSM makes this extremely easy!</p> In\u00a0[38]: Copied! <pre>param_dict_init\n</pre> param_dict_init Out[38]: <pre>{'v': 0.5, 'a': 1.5, 'z': 0.5, 't': 0.5}</pre> In\u00a0[39]: Copied! <pre>ddm_model_only_v = hssm.HSSM(\n    data=dataset,\n    model=\"ddm\",\n    a=param_dict_init[\"a\"],\n    t=param_dict_init[\"t\"],\n    z=param_dict_init[\"z\"],\n)\n</pre> ddm_model_only_v = hssm.HSSM(     data=dataset,     model=\"ddm\",     a=param_dict_init[\"a\"],     t=param_dict_init[\"t\"],     z=param_dict_init[\"z\"], ) <pre>Model initialized successfully.\n</pre> <p>Since we fix all but one parameter, we therefore estimate only one parameter. This should be reflected in our model graph, where we expect only one free random variable <code>v</code>:</p> In\u00a0[40]: Copied! <pre>ddm_model_only_v.graph()\n</pre> ddm_model_only_v.graph() <pre>max_shape:  (500,)\nsize:  (np.int64(500),)\n</pre> Out[40]: In\u00a0[41]: Copied! <pre>ddm_model_only_v.sample(\n    sampler=\"mcmc\",\n    chains=2,\n    cores=2,\n    draws=500,\n    tune=500,\n    idata_kwargs=dict(log_likelihood=False),  # no need to return likelihoods here\n    mp_ctx=\"spawn\",\n)\n</pre> ddm_model_only_v.sample(     sampler=\"mcmc\",     chains=2,     cores=2,     draws=500,     tune=500,     idata_kwargs=dict(log_likelihood=False),  # no need to return likelihoods here     mp_ctx=\"spawn\", ) <pre>Using default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [v]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 2 chains for 500 tune and 500 draw iterations (1_000 + 1_000 draws total) took 26 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n</pre> Out[41]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 12kB\nDimensions:  (chain: 2, draw: 500)\nCoordinates:\n  * chain    (chain) int64 16B 0 1\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    v        (chain, draw) float64 8kB 0.5138 0.4859 0.4723 ... 0.5077 0.5747\nAttributes:\n    created_at:                  2025-09-29T19:50:45.261410+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               26.393324613571167\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (1)<ul><li>v(chain, draw)float640.5138 0.4859 ... 0.5077 0.5747<pre>array([[0.51380716, 0.48589455, 0.47227229, 0.40522998, 0.50007175,\n        0.46240607, 0.49910908, 0.49508718, 0.49508718, 0.49508718,\n        0.46478517, 0.53310809, 0.50442001, 0.50442001, 0.49495793,\n        0.49565038, 0.49648362, 0.50022938, 0.50022938, 0.50993313,\n        0.53345248, 0.51130382, 0.50460889, 0.50432713, 0.50204546,\n        0.50204546, 0.50204546, 0.52973583, 0.57098566, 0.58699836,\n        0.50129696, 0.48636329, 0.38749957, 0.46509329, 0.48156974,\n        0.48964833, 0.49203919, 0.55781576, 0.54031167, 0.53015157,\n        0.4748269 , 0.52939068, 0.45661543, 0.44375418, 0.54245269,\n        0.54245269, 0.53019625, 0.53673155, 0.55345139, 0.44505707,\n        0.44505707, 0.46447658, 0.53954027, 0.52073556, 0.5535405 ,\n        0.45661153, 0.45661153, 0.45984522, 0.52208224, 0.52608047,\n        0.47673279, 0.51565803, 0.51565803, 0.54779383, 0.55593892,\n        0.52399251, 0.51770993, 0.53076765, 0.50227221, 0.55753289,\n        0.55753289, 0.44531652, 0.47312453, 0.51931169, 0.51931169,\n        0.5029342 , 0.51378131, 0.5019462 , 0.50842708, 0.48138237,\n        0.48138237, 0.53788789, 0.46262904, 0.48132019, 0.46827394,\n        0.47949763, 0.48835569, 0.48835569, 0.49523793, 0.47213365,\n        0.46500187, 0.48786553, 0.48713737, 0.47952233, 0.51125921,\n        0.55416848, 0.52251005, 0.52251005, 0.54905055, 0.58577339,\n...\n        0.52948044, 0.4959339 , 0.51083329, 0.5156021 , 0.51426853,\n        0.51884579, 0.4709704 , 0.43587223, 0.49459438, 0.47111434,\n        0.46991127, 0.4825219 , 0.46117844, 0.49869979, 0.46676197,\n        0.51851931, 0.51395412, 0.51594713, 0.46442517, 0.55314517,\n        0.55314517, 0.56312421, 0.4686876 , 0.4717113 , 0.42933406,\n        0.51421063, 0.51421063, 0.54121929, 0.47022847, 0.43702131,\n        0.45772819, 0.46375974, 0.52707847, 0.51991812, 0.50488259,\n        0.5097092 , 0.52878058, 0.45697682, 0.52648121, 0.49201589,\n        0.48017061, 0.44035977, 0.48558991, 0.48280079, 0.48202285,\n        0.47817769, 0.47817769, 0.48946543, 0.48946543, 0.48927722,\n        0.46827506, 0.50080522, 0.48626615, 0.47994395, 0.48013009,\n        0.45199409, 0.48519153, 0.47610015, 0.44097352, 0.46925789,\n        0.43764336, 0.48201202, 0.48069515, 0.48765664, 0.49174821,\n        0.49700337, 0.4837505 , 0.4837505 , 0.4943414 , 0.50458098,\n        0.46917733, 0.52215183, 0.51703124, 0.51037923, 0.52051848,\n        0.53229113, 0.55106574, 0.51746955, 0.52353415, 0.45938476,\n        0.45938476, 0.49955845, 0.47638039, 0.45835979, 0.52693366,\n        0.47730732, 0.52841308, 0.52991653, 0.49092252, 0.52119929,\n        0.52281419, 0.53801605, 0.54748753, 0.58125347, 0.51999906,\n        0.57079986, 0.56885842, 0.50446238, 0.50767399, 0.57470538]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-29T19:50:45.261410+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :26.393324613571167tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 134kB\nDimensions:                (chain: 2, draw: 500)\nCoordinates:\n  * chain                  (chain) int64 16B 0 1\n  * draw                   (draw) int64 4kB 0 1 2 3 4 5 ... 495 496 497 498 499\nData variables: (12/18)\n    energy                 (chain, draw) float64 8kB 1.025e+03 ... 1.027e+03\n    step_size_bar          (chain, draw) float64 8kB 1.261 1.261 ... 1.361 1.361\n    divergences            (chain, draw) int64 8kB 0 0 0 0 0 0 0 ... 0 0 0 0 0 0\n    perf_counter_start     (chain, draw) float64 8kB 1.667e+06 ... 1.667e+06\n    diverging              (chain, draw) bool 1kB False False ... False False\n    step_size              (chain, draw) float64 8kB 1.815 1.815 ... 1.841 1.841\n    ...                     ...\n    acceptance_rate        (chain, draw) float64 8kB 1.0 0.9954 ... 0.5455\n    lp                     (chain, draw) float64 8kB -1.024e+03 ... -1.027e+03\n    reached_max_treedepth  (chain, draw) bool 1kB False False ... False False\n    perf_counter_diff      (chain, draw) float64 8kB 5.554e-05 ... 8.546e-05\n    index_in_trajectory    (chain, draw) int64 8kB -1 -2 -1 1 -1 ... 1 -1 -1 -1\n    max_energy_error       (chain, draw) float64 8kB -0.1867 -0.02249 ... 0.8843\nAttributes:\n    created_at:                  2025-09-29T19:50:45.278212+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               26.393324613571167\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (18)<ul><li>energy(chain, draw)float641.025e+03 1.024e+03 ... 1.027e+03<pre>array([[1024.50425524, 1024.27248662, 1024.4086632 , 1027.72188727,\n        1026.10780168, 1024.64462037, 1024.4159596 , 1024.13385491,\n        1024.60674147, 1025.76680213, 1024.6199487 , 1024.64547913,\n        1024.4214773 , 1024.87202067, 1024.19899034, 1024.12923897,\n        1024.17271542, 1024.1363687 , 1024.17714644, 1024.91918153,\n        1024.65956529, 1024.61397984, 1024.21098071, 1024.15306697,\n        1024.57368677, 1024.2415281 , 1024.52021277, 1027.90227279,\n        1026.40519453, 1029.89606235, 1025.95231384, 1024.45190715,\n        1029.1573783 , 1027.18014613, 1024.450132  , 1024.24400638,\n        1025.17133027, 1025.62727393, 1025.55497355, 1024.89876845,\n        1024.73662979, 1024.77429829, 1025.18249044, 1025.61074966,\n        1025.48473571, 1025.61641389, 1024.93903227, 1024.92110372,\n        1025.61781477, 1025.34687603, 1026.32772408, 1025.13111543,\n        1025.04572821, 1024.6946981 , 1025.44534963, 1025.70659329,\n        1025.91308589, 1025.0282134 , 1025.47006474, 1024.55523435,\n        1024.55714203, 1024.31121548, 1025.60496325, 1025.4402802 ,\n        1025.91566733, 1025.10674795, 1024.56537479, 1024.62949808,\n        1024.37919633, 1025.57465547, 1027.83466764, 1025.55658112,\n        1024.94043928, 1024.3717072 , 1025.72052463, 1024.23946388,\n        1024.26086278, 1024.18989624, 1024.17547385, 1024.36042161,\n...\n        1026.22643503, 1026.26381752, 1025.85418128, 1024.54290699,\n        1026.08092029, 1025.75727316, 1026.42720572, 1024.90342555,\n        1024.8549786 , 1025.69106049, 1025.48863559, 1024.87333814,\n        1024.55153285, 1024.48211868, 1024.25993198, 1024.54801305,\n        1024.52449584, 1024.84067289, 1024.74880658, 1024.4108573 ,\n        1024.38254942, 1025.48278191, 1024.97652897, 1024.43039622,\n        1024.51091772, 1024.31447686, 1025.51909131, 1024.23583382,\n        1024.19453618, 1024.16524981, 1024.48063706, 1025.06803522,\n        1024.2407531 , 1024.26870492, 1024.30154673, 1024.99646655,\n        1024.67682661, 1024.33134666, 1027.14737008, 1025.12702239,\n        1025.66850586, 1025.07309299, 1024.28045602, 1025.06236376,\n        1024.22586028, 1024.1351879 , 1024.20699879, 1024.96708996,\n        1024.17660504, 1024.15529645, 1024.50661752, 1024.42517768,\n        1024.38388874, 1024.26325887, 1024.65876431, 1024.66826455,\n        1025.40173431, 1024.91677886, 1024.44727998, 1024.81661468,\n        1027.20125965, 1024.74846786, 1024.32376177, 1025.80977402,\n        1024.69461179, 1024.44181019, 1024.56491768, 1024.6887495 ,\n        1024.47788008, 1024.57802826, 1024.46477625, 1024.84847453,\n        1025.34107781, 1027.15323928, 1025.9596443 , 1026.28348496,\n        1026.98438013, 1025.43469389, 1024.17224245, 1026.53623235]])</pre></li><li>step_size_bar(chain, draw)float641.261 1.261 1.261 ... 1.361 1.361<pre>array([[1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n        1.26093197, 1.26093197, 1.26093197, 1.26093197, 1.26093197,\n...\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859,\n        1.36132859, 1.36132859, 1.36132859, 1.36132859, 1.36132859]])</pre></li><li>divergences(chain, draw)int640 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0<pre>array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n...\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</pre></li><li>perf_counter_start(chain, draw)float641.667e+06 1.667e+06 ... 1.667e+06<pre>array([[1666862.1529635 , 1666862.15305367, 1666862.15319092,\n        1666862.15326162, 1666862.15333175, 1666862.15346012,\n        1666862.15358829, 1666862.15370342, 1666862.15382037,\n        1666862.15394696, 1666862.15408483, 1666862.15422546,\n        1666862.154365  , 1666862.15445254, 1666862.15459121,\n        1666862.15471587, 1666862.15479867, 1666862.15492446,\n        1666862.15503937, 1666862.15510896, 1666862.15523492,\n        1666862.15530329, 1666862.15543908, 1666862.15555113,\n        1666862.15561783, 1666862.15574517, 1666862.15582925,\n        1666862.1559425 , 1666862.15608446, 1666862.15622575,\n        1666862.15636675, 1666862.15650429, 1666862.15664658,\n        1666862.15673179, 1666862.15681596, 1666862.15689737,\n        1666862.157032  , 1666862.15717496, 1666862.15733892,\n        1666862.15756292, 1666862.15774079, 1666862.15788917,\n        1666862.15802638, 1666862.15815458, 1666862.15829217,\n        1666862.15843092, 1666862.15851529, 1666862.15858671,\n        1666862.15866233, 1666862.15874117, 1666862.15887354,\n        1666862.15894338, 1666862.15902554, 1666862.15915038,\n        1666862.15923167, 1666862.15930067, 1666862.15941283,\n        1666862.15947817, 1666862.15959088, 1666862.15970112,\n...\n        1666872.38007546, 1666872.38014546, 1666872.38025517,\n        1666872.38036717, 1666872.38047892, 1666872.38054733,\n        1666872.38061529, 1666872.38068021, 1666872.38074483,\n        1666872.38080929, 1666872.38088483, 1666872.38101908,\n        1666872.38113604, 1666872.38125363, 1666872.381321  ,\n        1666872.38139029, 1666872.38146092, 1666872.381529  ,\n        1666872.38164954, 1666872.38172029, 1666872.38178846,\n        1666872.38185662, 1666872.38192425, 1666872.38204692,\n        1666872.38216854, 1666872.38228096, 1666872.38239521,\n        1666872.38246387, 1666872.38253283, 1666872.38265421,\n        1666872.38276367, 1666872.38287833, 1666872.38294538,\n        1666872.38301704, 1666872.38312963, 1666872.3831975 ,\n        1666872.38331004, 1666872.38337746, 1666872.38344233,\n        1666872.38355725, 1666872.38362754, 1666872.38374154,\n        1666872.38385512, 1666872.38397158, 1666872.3840905 ,\n        1666872.38420471, 1666872.38432146, 1666872.38438733,\n        1666872.38449729, 1666872.38460783, 1666872.38467283,\n        1666872.38473792, 1666872.38485013, 1666872.38496154,\n        1666872.38507592, 1666872.38514187, 1666872.38520546,\n        1666872.38531296, 1666872.38538146]])</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False]])</pre></li><li>step_size(chain, draw)float641.815 1.815 1.815 ... 1.841 1.841<pre>array([[1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n        1.81487024, 1.81487024, 1.81487024, 1.81487024, 1.81487024,\n...\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451,\n        1.84114451, 1.84114451, 1.84114451, 1.84114451, 1.84114451]])</pre></li><li>energy_error(chain, draw)float64-0.1867 -0.02249 ... 0.8843<pre>array([[-1.86743448e-01, -2.24865630e-02,  9.25447082e-02,\n         1.48081078e+00, -1.58086624e+00,  2.25327081e-01,\n        -2.26128212e-01,  8.08634774e-04,  0.00000000e+00,\n         0.00000000e+00,  1.95564536e-01,  2.77736809e-02,\n        -2.15241616e-01,  0.00000000e+00, -7.13097741e-03,\n        -5.89719377e-04, -4.66161484e-04,  9.51523402e-04,\n         0.00000000e+00,  2.85399687e-02,  2.00369659e-01,\n        -1.95314450e-01, -2.47590991e-02, -6.88768409e-04,\n        -4.90039995e-03,  0.00000000e+00,  0.00000000e+00,\n         2.17010656e-01,  7.52416952e-01,  4.54821171e-01,\n        -1.39280270e+00,  1.92618352e-02,  2.24849519e+00,\n        -2.09164471e+00, -1.47181312e-01, -3.53409580e-02,\n        -5.08512687e-03,  6.37077840e-01, -3.11478739e-01,\n        -1.33719236e-01, -1.00431727e-01,  9.07122183e-02,\n         1.16705166e-01,  2.27742370e-01, -1.60977179e-01,\n         0.00000000e+00, -1.65854621e-01,  8.20833748e-02,\n         2.75490175e-01, -4.54760388e-02,  0.00000000e+00,\n        -3.09717080e-01,  1.18735493e-01, -2.14916338e-01,\n         4.53321935e-01, -2.70576904e-01,  0.00000000e+00,\n        -4.73150724e-02, -1.33402030e-01,  3.73465940e-02,\n...\n         4.76290884e-01, -5.00654142e-01,  1.17066449e-02,\n         3.69713249e-03,  2.13461200e-02,  0.00000000e+00,\n        -4.92339447e-02,  0.00000000e+00,  4.89909393e-04,\n         1.25377892e-01, -1.28551632e-01,  1.85338501e-02,\n         2.90266902e-02, -1.03713706e-03,  2.83526784e-01,\n        -3.07038244e-01,  4.88744734e-02,  4.27930367e-01,\n        -3.86679413e-01,  4.49542332e-01, -5.36973212e-01,\n         6.78289936e-03, -2.81755354e-02, -1.01545823e-02,\n        -5.39790570e-03,  3.02569522e-02,  0.00000000e+00,\n        -2.86408590e-02,  5.77382595e-03,  1.19459032e-01,\n        -3.44075646e-02, -3.44638855e-02, -3.29113812e-02,\n         5.64321021e-02,  1.03295302e-01,  2.49081001e-01,\n        -3.73470302e-01,  4.24813656e-02,  1.24690337e-01,\n         0.00000000e+00, -2.28122105e-01,  7.04345311e-02,\n         1.69995980e-01, -1.09140661e-01, -6.85091311e-02,\n         8.25534622e-02,  1.44217375e-02, -1.54991663e-01,\n         8.08258073e-02,  1.20417452e-02,  1.51506767e-01,\n         1.28560642e-01,  6.65624955e-01, -9.73593835e-01,\n         7.30406648e-01, -4.11515806e-02, -7.64134523e-01,\n         8.37596182e-03,  8.84274492e-01]])</pre></li><li>largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n...\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan]])</pre></li><li>tree_depth(chain, draw)int641 2 1 1 2 2 2 2 ... 2 2 2 1 1 2 1 2<pre>array([[1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2,\n        2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2,\n        2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2,\n        2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2,\n        1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2,\n        1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1,\n        2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2,\n        1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2,\n        1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1,\n        1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2,\n        1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2,\n        1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2,\n        1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1,\n        1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2,\n        1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1,\n        1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2,\n        1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2,\n        1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2,\n        2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1,\n        1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1,\n...\n        2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2,\n        1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1,\n        1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2,\n        1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2,\n        1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1,\n        1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2,\n        2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n        1, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1,\n        1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2,\n        2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2,\n        2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2,\n        1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1,\n        1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1,\n        1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1,\n        2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2,\n        2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1,\n        1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 2,\n        2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2]])</pre></li><li>process_time_diff(chain, draw)float645.5e-05 0.000104 ... 8.6e-05<pre>array([[5.50e-05, 1.04e-04, 4.60e-05, 4.60e-05, 9.80e-05, 9.60e-05,\n        8.90e-05, 9.20e-05, 9.30e-05, 1.04e-04, 1.09e-04, 1.07e-04,\n        5.50e-05, 1.07e-04, 9.40e-05, 5.10e-05, 9.90e-05, 8.90e-05,\n        4.30e-05, 1.01e-04, 4.20e-05, 1.08e-04, 8.70e-05, 4.10e-05,\n        9.60e-05, 5.30e-05, 8.90e-05, 1.10e-04, 1.09e-04, 1.08e-04,\n        1.06e-04, 1.11e-04, 5.40e-05, 5.20e-05, 5.20e-05, 1.03e-04,\n        1.11e-04, 1.01e-04, 1.34e-04, 1.33e-04, 1.14e-04, 1.04e-04,\n        9.70e-05, 1.08e-04, 1.08e-04, 5.20e-05, 4.60e-05, 4.70e-05,\n        5.00e-05, 9.90e-05, 4.60e-05, 5.00e-05, 9.50e-05, 5.10e-05,\n        4.50e-05, 8.60e-05, 4.20e-05, 8.80e-05, 8.50e-05, 8.60e-05,\n        8.50e-05, 8.70e-05, 4.10e-05, 8.70e-05, 4.20e-05, 8.50e-05,\n        8.50e-05, 4.00e-05, 4.10e-05, 8.30e-05, 4.20e-05, 8.80e-05,\n        4.20e-05, 8.70e-05, 4.60e-05, 8.90e-05, 9.20e-05, 8.70e-05,\n        8.90e-05, 9.00e-05, 4.30e-05, 9.00e-05, 8.80e-05, 4.10e-05,\n        4.60e-05, 4.20e-05, 9.10e-05, 8.40e-05, 4.30e-05, 8.60e-05,\n        4.10e-05, 4.40e-05, 9.10e-05, 7.10e-05, 1.06e-04, 4.70e-05,\n        1.60e-04, 5.20e-05, 1.18e-04, 4.60e-05, 9.30e-05, 4.60e-05,\n        4.30e-05, 4.50e-05, 1.42e-04, 9.70e-05, 4.40e-05, 9.00e-05,\n        4.60e-05, 9.10e-05, 5.50e-05, 1.00e-04, 9.10e-05, 8.70e-05,\n        9.00e-05, 4.40e-05, 4.20e-05, 8.90e-05, 4.30e-05, 8.30e-05,\n...\n        4.10e-05, 8.70e-05, 4.20e-05, 8.60e-05, 8.30e-05, 8.90e-05,\n        8.60e-05, 8.40e-05, 8.70e-05, 8.60e-05, 8.70e-05, 8.70e-05,\n        8.70e-05, 8.60e-05, 8.30e-05, 4.20e-05, 8.70e-05, 8.80e-05,\n        8.30e-05, 4.20e-05, 8.70e-05, 8.70e-05, 8.70e-05, 4.20e-05,\n        8.70e-05, 8.40e-05, 4.10e-05, 8.70e-05, 4.10e-05, 8.30e-05,\n        8.20e-05, 8.70e-05, 4.10e-05, 4.10e-05, 8.90e-05, 8.80e-05,\n        4.60e-05, 8.60e-05, 8.70e-05, 4.00e-05, 9.10e-05, 8.70e-05,\n        8.30e-05, 4.10e-05, 8.80e-05, 4.50e-05, 4.10e-05, 4.70e-05,\n        8.90e-05, 4.10e-05, 4.40e-05, 1.06e-04, 4.60e-05, 9.10e-05,\n        9.30e-05, 9.00e-05, 9.00e-05, 4.50e-05, 8.60e-05, 8.80e-05,\n        8.80e-05, 4.60e-05, 4.50e-05, 4.10e-05, 4.30e-05, 4.20e-05,\n        4.20e-05, 1.02e-04, 9.10e-05, 9.10e-05, 4.20e-05, 4.30e-05,\n        4.50e-05, 4.40e-05, 9.70e-05, 4.50e-05, 4.50e-05, 4.40e-05,\n        4.50e-05, 9.90e-05, 9.90e-05, 8.50e-05, 8.90e-05, 4.50e-05,\n        4.30e-05, 9.80e-05, 8.60e-05, 9.20e-05, 4.40e-05, 4.30e-05,\n        9.00e-05, 4.30e-05, 8.80e-05, 4.50e-05, 4.30e-05, 9.20e-05,\n        4.60e-05, 9.20e-05, 9.10e-05, 9.20e-05, 9.60e-05, 8.90e-05,\n        9.30e-05, 4.10e-05, 8.60e-05, 8.70e-05, 4.10e-05, 4.30e-05,\n        8.80e-05, 8.70e-05, 8.70e-05, 4.10e-05, 4.10e-05, 8.40e-05,\n        4.40e-05, 8.60e-05]])</pre></li><li>n_steps(chain, draw)float641.0 3.0 1.0 1.0 ... 1.0 3.0 1.0 3.0<pre>array([[1., 3., 1., 1., 3., 3., 3., 3., 3., 3., 3., 3., 1., 3., 3., 1.,\n        3., 3., 1., 3., 1., 3., 3., 1., 3., 1., 3., 3., 3., 3., 3., 3.,\n        1., 1., 1., 3., 3., 3., 1., 3., 3., 3., 3., 3., 3., 1., 1., 1.,\n        1., 3., 1., 1., 3., 1., 1., 3., 1., 3., 3., 3., 3., 3., 1., 3.,\n        1., 3., 3., 1., 1., 3., 1., 3., 1., 3., 1., 3., 3., 3., 3., 3.,\n        1., 3., 3., 1., 1., 1., 3., 3., 1., 3., 1., 1., 3., 1., 3., 1.,\n        3., 1., 3., 1., 3., 1., 1., 1., 3., 3., 1., 3., 1., 3., 1., 3.,\n        3., 3., 3., 1., 1., 3., 1., 1., 3., 1., 3., 3., 1., 3., 1., 1.,\n        1., 1., 3., 1., 3., 1., 3., 3., 1., 3., 3., 3., 1., 1., 1., 3.,\n        1., 1., 1., 3., 3., 1., 1., 3., 3., 3., 1., 3., 3., 1., 1., 3.,\n        3., 1., 3., 3., 3., 3., 3., 3., 3., 3., 3., 1., 1., 1., 1., 3.,\n        1., 3., 3., 1., 1., 3., 3., 1., 1., 1., 1., 3., 1., 3., 3., 3.,\n        3., 3., 3., 1., 3., 1., 1., 1., 3., 3., 3., 3., 3., 1., 3., 3.,\n        1., 1., 3., 3., 3., 3., 3., 1., 3., 3., 1., 3., 1., 3., 3., 1.,\n        3., 1., 1., 3., 1., 3., 1., 3., 3., 1., 3., 1., 1., 3., 3., 3.,\n        3., 3., 1., 1., 1., 3., 3., 1., 1., 1., 1., 3., 3., 1., 3., 3.,\n        3., 3., 3., 1., 3., 3., 3., 3., 1., 1., 3., 1., 1., 3., 3., 3.,\n        3., 3., 3., 3., 1., 1., 1., 1., 1., 3., 3., 1., 3., 1., 1., 3.,\n        1., 3., 1., 3., 1., 1., 1., 3., 3., 3., 3., 1., 3., 1., 1., 3.,\n        3., 1., 3., 3., 1., 1., 1., 1., 3., 3., 3., 3., 3., 3., 3., 3.,\n...\n        3., 3., 3., 3., 3., 3., 3., 3., 1., 1., 3., 3., 3., 3., 3., 3.,\n        3., 3., 3., 3., 1., 3., 3., 1., 3., 3., 3., 3., 3., 1., 1., 1.,\n        1., 1., 1., 3., 3., 3., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 3., 1., 1., 3., 3., 1., 3., 3., 3., 3., 1., 3., 3., 3., 3.,\n        1., 1., 1., 3., 3., 3., 1., 1., 1., 1., 1., 3., 3., 3., 3., 3.,\n        3., 3., 1., 3., 1., 3., 1., 3., 3., 1., 1., 1., 3., 3., 3., 1.,\n        1., 3., 3., 1., 1., 3., 3., 1., 1., 3., 1., 1., 1., 1., 1., 3.,\n        1., 3., 3., 3., 3., 3., 1., 3., 1., 3., 1., 3., 3., 3., 3., 1.,\n        3., 3., 3., 3., 3., 3., 3., 1., 1., 3., 1., 3., 3., 3., 1., 1.,\n        1., 3., 3., 3., 3., 3., 1., 3., 3., 3., 1., 1., 1., 3., 1., 1.,\n        1., 3., 3., 3., 1., 1., 1., 3., 3., 3., 3., 3., 3., 3., 1., 1.,\n        3., 3., 1., 3., 3., 1., 1., 1., 3., 1., 1., 3., 1., 1., 3., 1.,\n        1., 3., 1., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 1.,\n        3., 3., 3., 1., 3., 3., 3., 1., 3., 3., 1., 3., 1., 3., 3., 3.,\n        1., 1., 3., 3., 1., 3., 3., 1., 3., 3., 3., 1., 3., 1., 1., 1.,\n        3., 1., 1., 3., 1., 3., 3., 3., 3., 1., 3., 3., 3., 1., 1., 1.,\n        1., 1., 1., 3., 3., 3., 1., 1., 1., 1., 3., 1., 1., 1., 1., 3.,\n        3., 3., 3., 1., 1., 3., 3., 3., 1., 1., 3., 1., 3., 1., 1., 3.,\n        1., 3., 3., 3., 3., 3., 3., 1., 3., 3., 1., 1., 3., 3., 3., 1.,\n        1., 3., 1., 3.]])</pre></li><li>smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n...\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan]])</pre></li><li>acceptance_rate(chain, draw)float641.0 0.9954 0.9116 ... 0.9917 0.5455<pre>array([[1.        , 0.99536462, 0.91160845, 0.2274532 , 1.        ,\n        0.86270999, 1.        , 0.99738112, 0.78021358, 0.50906345,\n        0.86614772, 0.99086949, 1.        , 0.71354058, 0.97404624,\n        1.        , 0.97715839, 0.99593489, 0.96274329, 0.7009849 ,\n        0.81842816, 0.97722315, 0.99577214, 1.        , 0.81225177,\n        0.91944316, 0.81990434, 0.31776824, 0.67606167, 0.52602269,\n        0.98296044, 0.86615261, 0.10555795, 1.        , 1.        ,\n        0.99388833, 0.63469544, 0.6759882 , 1.        , 1.        ,\n        0.97380277, 0.90965349, 0.89777795, 0.9089795 , 1.        ,\n        0.76088818, 1.        , 0.92119515, 0.75919989, 1.        ,\n        0.61003596, 1.        , 0.95254695, 1.        , 0.63551351,\n        0.98787239, 0.50039111, 1.        , 0.75770153, 0.98778073,\n        0.99196457, 1.        , 0.36087259, 0.72833535, 0.85861488,\n        1.        , 0.97850946, 0.88452488, 1.        , 0.6752498 ,\n        0.26809324, 1.        , 1.        , 1.        , 0.35068021,\n        1.        , 0.96513567, 1.        , 0.98960535, 0.94147851,\n        0.71854689, 0.455266  , 1.        , 1.        , 0.89635398,\n        1.        , 1.        , 0.42096366, 1.        , 0.9239977 ,\n        0.92639625, 1.        , 0.41593187, 0.96098697, 0.99431444,\n        0.58923254, 1.        , 0.18926825, 0.824541  , 0.41790171,\n...\n        0.93167531, 0.8389373 , 0.98034077, 0.97737531, 0.93254312,\n        0.99124724, 0.98629758, 0.60694803, 0.87339515, 0.9234559 ,\n        0.98965876, 0.90463506, 0.83942484, 0.99959342, 0.89627086,\n        1.        , 1.        , 0.98944198, 0.92676287, 0.86234154,\n        0.75765852, 0.94531525, 0.9078847 , 1.        , 0.73298269,\n        0.93329696, 0.49241876, 0.78054843, 1.        , 0.62486014,\n        1.        , 1.        , 1.        , 1.        , 1.        ,\n        0.87231484, 0.88120623, 0.96540557, 1.        , 0.98523778,\n        0.93859525, 0.62108279, 1.        , 0.9435596 , 0.93488957,\n        0.9788801 , 0.45473367, 1.        , 0.97858842, 0.99951021,\n        0.88216348, 0.82984902, 0.97055349, 0.98912536, 1.        ,\n        0.75312295, 1.        , 0.95230066, 0.49209305, 1.        ,\n        0.63792004, 1.        , 0.99324005, 0.75524776, 0.98641998,\n        1.        , 0.97801788, 0.61710304, 1.        , 0.99520415,\n        0.91549852, 1.        , 1.        , 1.        , 0.89328192,\n        0.90186061, 0.92064578, 1.        , 0.95840832, 0.95846089,\n        0.20908473, 0.95753317, 0.94929567, 0.68866748, 1.        ,\n        1.        , 0.97009597, 0.98568176, 0.98252836, 0.8934113 ,\n        0.98803047, 0.85941207, 0.95978674, 0.79385353, 1.        ,\n        0.48171306, 1.        , 1.        , 0.99165902, 0.54550024]])</pre></li><li>lp(chain, draw)float64-1.024e+03 ... -1.027e+03<pre>array([[-1024.23220024, -1024.18272924, -1024.39005094, -1027.65923572,\n        -1024.12855602, -1024.63578732, -1024.12704123, -1024.12888439,\n        -1024.12888439, -1024.12888439, -1024.56916157, -1024.63538936,\n        -1024.14479661, -1024.14479661, -1024.1291625 , -1024.12783168,\n        -1024.12674908, -1024.12887596, -1024.12887596, -1024.18747343,\n        -1024.64529975, -1024.20190947, -1024.14585053, -1024.144289  ,\n        -1024.13402131, -1024.13402131, -1024.13402131, -1024.5433635 ,\n        -1026.2899076 , -1027.32825578, -1024.13157545, -1024.17830939,\n        -1029.15712725, -1024.56087552, -1024.2320148 , -1024.15238736,\n        -1024.13907668, -1025.58605727, -1024.8624249 , -1024.55421617,\n        -1024.33953261, -1024.53445867, -1024.81757172, -1025.32110649,\n        -1024.93788217, -1024.93788217, -1024.55539088, -1024.74441092,\n        -1025.38292209, -1025.26381727, -1025.26381727, -1024.57753874,\n        -1024.83613275, -1024.34243547, -1025.38691908, -1024.81770354,\n        -1024.81770354, -1024.71274694, -1024.36835551, -1024.4539091 ,\n        -1024.30534746, -1024.2578548 , -1024.2578548 , -1025.14201537,\n        -1025.49685852, -1024.4076275 , -1024.28953044, -1024.57055359,\n        -1024.13485224, -1025.57243582, -1025.57243582, -1025.25257844,\n        -1024.37259924, -1024.31661874, -1024.31661874, -1024.13751735,\n        -1024.23186149, -1024.13367074, -1024.17336712, -1024.23449725,\n...\n        -1025.36923436, -1025.85337265, -1024.47002314, -1024.40186581,\n        -1026.05006685, -1024.23755653, -1024.23755653, -1024.89396695,\n        -1024.43434638, -1025.63977844, -1024.78047804, -1024.59730321,\n        -1024.47726798, -1024.32741439, -1024.14742915, -1024.18525969,\n        -1024.51895182, -1024.80541204, -1024.46319254, -1024.13918382,\n        -1024.25124846, -1025.47702539, -1024.18569827, -1024.21642217,\n        -1024.22613082, -1024.28142487, -1024.28142487, -1024.15359829,\n        -1024.15359829, -1024.15487288, -1024.47990783, -1024.13021676,\n        -1024.17921055, -1024.25451588, -1024.25182955, -1024.98264028,\n        -1024.18969586, -1024.31636297, -1025.44811968, -1024.45659   ,\n        -1025.60874552, -1024.22626944, -1024.2438491 , -1024.1670484 ,\n        -1024.14044675, -1024.12636068, -1024.20524352, -1024.20524352,\n        -1024.13067689, -1024.14569295, -1024.45847129, -1024.36973471,\n        -1024.27867712, -1024.19200469, -1024.33839387, -1024.61225955,\n        -1025.27824727, -1024.28564397, -1024.39793671, -1024.72716217,\n        -1024.72716217, -1024.12765438, -1024.31144265, -1024.75988156,\n        -1024.47382903, -1024.29562929, -1024.50975514, -1024.54806366,\n        -1024.14471149, -1024.35119607, -1024.38305608, -1024.7855766 ,\n        -1025.12969675, -1026.93279872, -1024.32887775, -1026.2790321 ,\n        -1026.16700618, -1024.14503048, -1024.16700317, -1026.51331493]])</pre></li><li>reached_max_treedepth(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False]])</pre></li><li>perf_counter_diff(chain, draw)float645.554e-05 0.000104 ... 8.546e-05<pre>array([[5.55410516e-05, 1.04042003e-04, 4.58341092e-05, 4.56660055e-05,\n        9.75409057e-05, 9.70410183e-05, 8.90418887e-05, 9.05420166e-05,\n        9.33329575e-05, 1.03458064e-04, 1.08874869e-04, 1.06667168e-04,\n        5.54160215e-05, 1.07750064e-04, 9.40000173e-05, 5.07500954e-05,\n        9.99169424e-05, 8.90421215e-05, 4.20000870e-05, 1.00333011e-04,\n        4.18340787e-05, 1.08124921e-04, 8.65829643e-05, 4.14999668e-05,\n        9.63329803e-05, 5.24171628e-05, 8.92078970e-05, 1.09874876e-04,\n        1.09291868e-04, 1.08540989e-04, 1.06625026e-04, 1.10208988e-04,\n        5.38749155e-05, 5.18339220e-05, 5.20420726e-05, 1.03500206e-04,\n        1.11750094e-04, 1.01417070e-04, 1.35333976e-04, 1.32209156e-04,\n        1.14249997e-04, 1.04333973e-04, 9.79579054e-05, 1.08666951e-04,\n        1.06875086e-04, 5.24169300e-05, 4.63339966e-05, 4.65000048e-05,\n        5.00001479e-05, 9.95001756e-05, 4.50839289e-05, 5.07908408e-05,\n        9.42919869e-05, 4.99158632e-05, 4.52920794e-05, 8.66670161e-05,\n        4.11251094e-05, 8.77091661e-05, 8.53328966e-05, 8.53750389e-05,\n        8.52500089e-05, 8.73340759e-05, 4.11251094e-05, 8.63750465e-05,\n        4.14589886e-05, 8.52078665e-05, 8.59170686e-05, 4.02498990e-05,\n        4.12920490e-05, 8.30001663e-05, 4.14999668e-05, 8.70409422e-05,\n        4.19998541e-05, 8.69159121e-05, 4.65000048e-05, 8.87918286e-05,\n        9.04169865e-05, 8.68339557e-05, 8.89159273e-05, 9.04160552e-05,\n...\n        4.64590266e-05, 8.63750465e-05, 8.71249940e-05, 4.08331398e-05,\n        9.15420242e-05, 8.67918134e-05, 8.32499936e-05, 4.10841312e-05,\n        8.72497912e-05, 4.45418991e-05, 4.15840186e-05, 4.64168843e-05,\n        8.82500317e-05, 4.12911177e-05, 4.34999820e-05, 1.04959123e-04,\n        4.47500497e-05, 9.12500545e-05, 9.23338812e-05, 9.05001070e-05,\n        9.03750770e-05, 4.46669292e-05, 8.64169560e-05, 8.82090535e-05,\n        8.84591136e-05, 4.49169893e-05, 4.47079074e-05, 4.19169664e-05,\n        4.17078845e-05, 4.18750569e-05, 4.12920490e-05, 1.02041988e-04,\n        9.08328220e-05, 9.07091890e-05, 4.22908925e-05, 4.42080200e-05,\n        4.52499371e-05, 4.39588912e-05, 9.68750101e-05, 4.58341092e-05,\n        4.45418991e-05, 4.42499295e-05, 4.38329298e-05, 9.88331158e-05,\n        9.85418446e-05, 8.49170610e-05, 8.87080096e-05, 4.62918542e-05,\n        4.25409526e-05, 9.81250778e-05, 8.56670085e-05, 9.08749644e-05,\n        4.35418915e-05, 4.39579599e-05, 8.99168663e-05, 4.38329298e-05,\n        8.87911301e-05, 4.46250197e-05, 4.24170867e-05, 9.21669416e-05,\n        4.71249223e-05, 9.14169941e-05, 9.09590162e-05, 9.19580925e-05,\n        9.44999047e-05, 8.98749568e-05, 9.30419192e-05, 4.17081174e-05,\n        8.66251066e-05, 8.73750541e-05, 4.12079971e-05, 4.15830873e-05,\n        8.80840234e-05, 8.73750541e-05, 8.73338431e-05, 4.09998465e-05,\n        4.11251094e-05, 8.40830617e-05, 4.32920642e-05, 8.54579266e-05]])</pre></li><li>index_in_trajectory(chain, draw)int64-1 -2 -1 1 -1 1 ... -1 1 -1 -1 -1<pre>array([[-1, -2, -1,  1, -1,  1,  1,  2,  0,  0, -3, -2,  1,  0,  2, -1,\n         2,  2,  0, -2, -1,  3,  3, -1, -2,  0,  0,  2, -1, -3,  1, -2,\n         1, -1,  1,  3,  2, -1, -1,  1, -2, -2,  2,  1, -2,  0,  1, -1,\n         1, -2,  0, -1, -2,  1, -1,  2,  0,  1, -2, -1, -2, -2,  0, -3,\n         1,  1, -3, -1, -1,  1,  0, -2, -1,  2,  0,  1, -3,  1,  1,  2,\n         0, -2,  2,  1, -1,  1, -1,  0,  1, -1,  1, -1,  2,  1, -2,  1,\n         1,  0, -1, -1, -3,  0,  0, -1,  1,  2,  1,  2, -1,  2, -1, -1,\n         1, -1, -2,  0,  0,  0,  0,  0, -1, -1, -1,  3, -1, -2, -1, -1,\n         1, -1, -1, -1, -3,  1, -3,  1, -1, -1,  2,  2,  1,  1,  0,  2,\n        -1, -1, -1, -3, -2, -1,  1, -1,  3, -3, -1, -1,  1, -1, -1,  1,\n        -1, -1, -3,  3,  2, -1, -1, -3,  3,  3, -2, -1,  0, -1, -1, -3,\n         0,  1,  2, -1, -1,  3,  2, -1, -1,  1,  1, -2, -1, -2,  3, -2,\n         1,  3, -1,  0,  3, -1,  0,  1,  2, -1,  2,  2, -2,  1, -2,  1,\n        -1,  1,  1,  2,  3, -2,  3,  0, -2,  2,  1, -2,  0, -2,  2, -1,\n        -3,  1, -1,  1, -1, -1,  1,  2, -2,  1,  2, -1,  1, -1,  3,  2,\n         1,  0, -1,  1,  0,  1,  2,  1, -1,  1,  0,  2, -2,  0, -1, -2,\n        -1, -2,  1, -1, -2, -2,  3, -1,  1,  0, -1,  1,  1, -3,  2, -2,\n        -1, -3, -2, -2,  0,  0, -1,  1,  1,  2,  2,  1, -2, -1,  1, -2,\n        -1,  2,  1, -3, -1, -1, -1, -2, -1,  2,  3, -1, -2,  0,  0, -2,\n         2,  1, -2,  3,  1, -1,  0,  1,  1, -3,  1,  2, -3, -2,  3,  2,\n...\n         1, -2, -2,  3,  0, -3, -1,  3, -1, -1,  3, -1,  2,  1,  3,  1,\n        -1, -3, -2,  3,  1, -1,  1,  1,  2,  1,  0,  0, -1,  0, -1,  0,\n        -1, -1,  1,  1,  0, -1, -2,  1, -1,  0,  1,  1,  0, -1, -1,  1,\n         1,  2,  0,  1, -2,  1, -1,  3, -1,  1, -1,  0,  2,  2, -1,  2,\n         1, -1,  0, -3, -1, -2,  1,  1,  1, -1, -1, -2,  2, -3, -2,  0,\n        -1,  1,  1,  1, -1,  2, -1, -2, -2,  1,  1,  1,  3, -3, -3, -1,\n         1,  2,  1, -1,  1, -3,  1,  1,  1,  2,  1,  1, -1,  1, -1, -2,\n        -1, -2, -2,  1, -3, -2,  0,  3, -1,  1, -1, -1,  1,  1,  1,  1,\n        -3, -1, -2, -3,  3, -1,  1, -1, -1,  1, -1, -1, -1,  2, -1, -1,\n        -1,  1, -2,  1,  2,  1,  1, -1, -2, -3,  1, -1,  1, -1,  1, -1,\n        -1, -2, -1,  2,  0, -1, -1, -2,  1,  1, -2,  2, -3,  2,  1,  1,\n        -3,  2,  0,  3, -1, -1, -1, -1,  0,  1,  1, -1, -1,  1,  1,  1,\n        -1,  2,  1, -2,  0,  3, -3, -1,  1,  1,  2,  1, -2, -3,  0,  0,\n        -1,  2,  1,  1, -3,  1, -2,  1, -3,  1,  1, -3, -1, -1,  1, -2,\n        -1,  1,  2, -2,  0, -1, -1,  1,  1,  3,  0,  1,  2, -1, -1, -1,\n        -2, -1, -1, -2,  1,  2,  2, -3, -3, -1, -1,  3, -3, -1,  0,  1,\n         0,  1, -1, -2,  2, -1,  1, -1,  1, -1,  3, -1,  1, -1,  1, -2,\n        -3,  1,  1,  0,  1,  2, -1, -2,  1, -1, -3,  1, -1, -1, -1, -2,\n         0, -3,  1, -3, -2, -2, -2, -1, -3,  2, -1,  1,  1,  1, -1, -1,\n         1, -1, -1, -1]])</pre></li><li>max_energy_error(chain, draw)float64-0.1867 -0.02249 ... 0.8843<pre>array([[-1.86743448e-01, -2.24865630e-02,  9.25447082e-02,\n         1.48081078e+00, -1.58086624e+00,  2.27418818e-01,\n        -2.26128212e-01,  4.02643257e-03,  3.83889770e-01,\n         1.32546334e+00,  2.24376536e-01, -1.95825401e-01,\n        -2.15241616e-01,  5.64861222e-01,  4.41576482e-02,\n        -5.89719377e-04,  3.61780900e-02,  6.24076412e-03,\n         3.79684789e-02,  5.89817807e-01,  2.00369659e-01,\n        -1.95314450e-01, -2.47590991e-02, -6.88768409e-04,\n         3.38413645e-01,  8.39870532e-02,  3.01890538e-01,\n         2.67851513e+00,  7.52416952e-01,  1.05468098e+00,\n        -1.39280270e+00,  2.30097886e-01,  2.24849519e+00,\n        -2.09164471e+00, -1.47181312e-01, -3.53409580e-02,\n         8.32549168e-01,  6.89758628e-01, -3.11478739e-01,\n        -2.43287067e-01, -1.00431727e-01,  1.43339424e-01,\n         1.66084881e-01,  2.27742370e-01, -3.35656154e-01,\n         2.73268869e-01, -1.65854621e-01,  8.20833748e-02,\n         2.75490175e-01, -5.51305271e-01,  4.94237380e-01,\n        -3.09717080e-01,  1.18735493e-01, -2.14916338e-01,\n         4.53321935e-01, -2.70576904e-01,  6.92365262e-01,\n        -1.47954121e-01,  5.52750354e-01, -5.37111493e-02,\n...\n         4.76290884e-01, -5.00654142e-01,  8.95521824e-02,\n         1.03063055e-01,  2.13461200e-02,  7.88043376e-01,\n        -4.92339447e-02,  2.16441315e-02,  4.89909393e-04,\n         1.25377892e-01,  4.13536994e-01,  4.83098372e-02,\n         2.90266902e-02, -1.03713706e-03,  2.83526784e-01,\n        -3.07038244e-01,  4.88744734e-02,  1.18256081e+00,\n        -3.86679413e-01,  4.49542332e-01, -5.36973212e-01,\n         6.78289936e-03,  5.36169995e-01,  2.70592181e-02,\n        -5.39790570e-03,  3.02569522e-02,  4.82719266e-01,\n        -2.86408590e-02,  7.39161191e-03,  1.47814728e-01,\n        -1.24781369e-01, -3.44638855e-02, -3.29113812e-02,\n         1.79667671e-01,  1.03295302e-01,  2.49081001e-01,\n        -3.73470302e-01,  4.24813656e-02,  1.24690337e-01,\n         1.56501573e+00, -2.28122105e-01,  7.55789141e-02,\n         6.03309771e-01, -2.40643519e-01, -1.29696235e-01,\n         8.25534622e-02,  1.44217375e-02, -1.54991663e-01,\n         1.81195128e-01,  1.20417452e-02,  1.51506767e-01,\n        -2.18801570e-01,  6.65624955e-01, -9.91370960e-01,\n         7.30406648e-01, -4.11515806e-02, -7.64134523e-01,\n         8.37596182e-03,  8.84274492e-01]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-29T19:50:45.278212+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :26.393324613571167tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 12kB\nDimensions:                  (__obs__: 500, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 4kB 0 1 2 3 4 ... 496 497 498 499\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 8kB 1...\nAttributes:\n    created_at:                  2025-09-29T19:50:45.280985+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 500</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float641.909 1.0 2.701 ... 1.0 0.6543 1.0<pre>array([[ 1.90879178,  1.        ],\n       [ 2.70051455,  1.        ],\n       [ 5.07957602, -1.        ],\n       [ 0.95730692, -1.        ],\n       [ 3.75961304, -1.        ],\n       [ 1.05477774,  1.        ],\n       [ 6.26821756,  1.        ],\n       [ 2.94682932, -1.        ],\n       [ 1.15484786,  1.        ],\n       [ 3.46363139,  1.        ],\n       [ 1.62390924,  1.        ],\n       [ 1.14314425, -1.        ],\n       [ 5.05555296,  1.        ],\n       [ 1.90220582, -1.        ],\n       [ 3.13259554, -1.        ],\n       [ 2.96012521,  1.        ],\n       [ 2.41100526,  1.        ],\n       [ 2.10615134,  1.        ],\n       [ 1.60785794,  1.        ],\n       [ 1.40975022,  1.        ],\n...\n       [ 2.91743708,  1.        ],\n       [ 1.57587361,  1.        ],\n       [ 3.21329093, -1.        ],\n       [ 1.49868929,  1.        ],\n       [ 1.72745574,  1.        ],\n       [ 1.17646027,  1.        ],\n       [ 1.63405263,  1.        ],\n       [ 2.31994152,  1.        ],\n       [ 1.88749003,  1.        ],\n       [ 2.36859488,  1.        ],\n       [ 2.85419393,  1.        ],\n       [ 2.84367776,  1.        ],\n       [ 1.24673474,  1.        ],\n       [ 2.03972936,  1.        ],\n       [ 2.95965099,  1.        ],\n       [ 1.49344993,  1.        ],\n       [ 2.35472059,  1.        ],\n       [ 1.88818717,  1.        ],\n       [ 1.11251736,  1.        ],\n       [ 0.65433359,  1.        ]])</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='__obs__', length=500))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (6)created_at :2025-09-29T19:50:45.280985+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[42]: Copied! <pre>az.plot_trace(\n    ddm_model_only_v.traces.posterior, lines=[(\"v\", {}, param_dict_init[\"v\"])]\n);\n</pre> az.plot_trace(     ddm_model_only_v.traces.posterior, lines=[(\"v\", {}, param_dict_init[\"v\"])] ); <p>Instead of the trace on the right, a useful alternative / complement is the rank plot. As a rule of thumb, if the rank plots within chains look uniformly distributed, then our chains generally exhibit good mixing.</p> In\u00a0[43]: Copied! <pre>az.plot_trace(ddm_model_only_v.traces, kind=\"rank_bars\")\n</pre> az.plot_trace(ddm_model_only_v.traces, kind=\"rank_bars\") Out[43]: <pre>array([[&lt;Axes: title={'center': 'v'}&gt;,\n        &lt;Axes: title={'center': 'v'}, xlabel='Rank (all chains)', ylabel='Chain'&gt;]],\n      dtype=object)</pre> In\u00a0[44]: Copied! <pre>model_normal = hssm.HSSM(\n    data=dataset,\n    include=[\n        {\n            \"name\": \"v\",\n            \"prior\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 0.01},\n        }\n    ],\n)\n</pre> model_normal = hssm.HSSM(     data=dataset,     include=[         {             \"name\": \"v\",             \"prior\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 0.01},         }     ], ) <pre>Model initialized successfully.\n</pre> In\u00a0[45]: Copied! <pre>model_normal\n</pre> model_normal Out[45]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 500\n\nParameters:\n\nv:\n    Prior: Normal(mu: 0.0, sigma: 0.01)\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[46]: Copied! <pre>infer_data_normal = model_normal.sample(\n    sampler=\"mcmc\",\n    chains=2,\n    cores=2,\n    draws=500,\n    tune=500,\n    idata_kwargs=dict(log_likelihood=False),  # no need to return likelihoods here\n    mp_ctx=\"spawn\",\n)\n</pre> infer_data_normal = model_normal.sample(     sampler=\"mcmc\",     chains=2,     cores=2,     draws=500,     tune=500,     idata_kwargs=dict(log_likelihood=False),  # no need to return likelihoods here     mp_ctx=\"spawn\", ) <pre>Using default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [t, a, z, v]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 2 chains for 500 tune and 500 draw iterations (1_000 + 1_000 draws total) took 30 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n</pre> In\u00a0[47]: Copied! <pre>az.plot_trace(\n    model_normal.traces,\n    lines=[(key_, {}, param_dict_init[key_]) for key_ in param_dict_init],\n)\n</pre> az.plot_trace(     model_normal.traces,     lines=[(key_, {}, param_dict_init[key_]) for key_ in param_dict_init], ) Out[47]: <pre>array([[&lt;Axes: title={'center': 'v'}&gt;, &lt;Axes: title={'center': 'v'}&gt;],\n       [&lt;Axes: title={'center': 'a'}&gt;, &lt;Axes: title={'center': 'a'}&gt;],\n       [&lt;Axes: title={'center': 'z'}&gt;, &lt;Axes: title={'center': 'z'}&gt;],\n       [&lt;Axes: title={'center': 't'}&gt;, &lt;Axes: title={'center': 't'}&gt;]],\n      dtype=object)</pre> <p>Observe how we reused our previous dataset with underlying parameters</p> <ul> <li><code>v = 0.5</code></li> <li><code>a = 1.5</code></li> <li><code>z = 0.5</code></li> <li><code>t = 0.2</code></li> </ul> <p>In contrast to our previous sampler round, in which we used Uniform priors, here the <code>v</code> estimate is shrunk severley towared $0$ and the <code>t</code> and <code>z</code> parameter estimates are very biased to make up for this distortion. Also, overall we see a lot of divergences now, which is a sign of poor sampler performance.</p> <p>Let's first simulate some data, where the trial-by-trial parameters of the <code>v</code> parameter in our model are driven by a simple linear regression model.</p> <p>The regression model is driven by two (random) covariates <code>x</code> and <code>y</code>, respectively with coefficients of $0.8$ and $0.3$ which are also simulated below. We set the intercept to $0.3$.</p> <p>The rest of the parameters are fixed to single values as before.</p> In\u00a0[48]: Copied! <pre># Set up trial by trial parameters\nv_intercept = 0.3\nx = np.random.uniform(-1, 1, size=1000)\nv_x = 0.8\ny = np.random.uniform(-1, 1, size=1000)\nv_y = 0.3\nv_reg_v = v_intercept + (v_x * x) + (v_y * y)\n\n# rest\na_reg_v = 1.5\nz_reg_v = 0.5\nt_reg_v = 0.1\n\nparam_dict_reg_v = dict(\n    a=1.5, z=0.5, t=0.1, v=v_reg_v, v_x=v_x, v_y=v_y, v_Intercept=v_intercept, theta=0.0\n)\n\n# base dataset\ndataset_reg_v = hssm.simulate_data(model=\"ddm\", theta=param_dict_reg_v, size=1)\n\n# Adding covariates into the datsaframe\ndataset_reg_v[\"x\"] = x\ndataset_reg_v[\"y\"] = y\n</pre> # Set up trial by trial parameters v_intercept = 0.3 x = np.random.uniform(-1, 1, size=1000) v_x = 0.8 y = np.random.uniform(-1, 1, size=1000) v_y = 0.3 v_reg_v = v_intercept + (v_x * x) + (v_y * y)  # rest a_reg_v = 1.5 z_reg_v = 0.5 t_reg_v = 0.1  param_dict_reg_v = dict(     a=1.5, z=0.5, t=0.1, v=v_reg_v, v_x=v_x, v_y=v_y, v_Intercept=v_intercept, theta=0.0 )  # base dataset dataset_reg_v = hssm.simulate_data(model=\"ddm\", theta=param_dict_reg_v, size=1)  # Adding covariates into the datsaframe dataset_reg_v[\"x\"] = x dataset_reg_v[\"y\"] = y <p>We now create the <code>HSSM</code> model.</p> <p>Notice how we set the <code>include</code> argument. The include argument expects a list of dictionaries, one dictionary for each parameter to be specified via a regression model.</p> <p>Four <code>keys</code> are expected to be set:</p> <ul> <li>The <code>name</code> of the parameter,</li> <li>Potentially a <code>prior</code> for each of the regression level parameters ($\\beta$'s),</li> <li>The regression <code>formula</code></li> <li>A <code>link</code> function.</li> </ul> <p>The regression formula follows the syntax in the formulae python package (as used by the Bambi package for building Bayesian Hierarchical Regression Models.</p> <p>Bambi forms the main model-construction backend of HSSM.</p> In\u00a0[49]: Copied! <pre>model_reg_v_simple = hssm.HSSM(\n    data=dataset_reg_v, include=[{\"name\": \"v\", \"formula\": \"v ~ 1 + x + y\"}]\n)\n</pre> model_reg_v_simple = hssm.HSSM(     data=dataset_reg_v, include=[{\"name\": \"v\", \"formula\": \"v ~ 1 + x + y\"}] ) <pre>Model initialized successfully.\n</pre> In\u00a0[50]: Copied! <pre>model_reg_v_simple\n</pre> model_reg_v_simple Out[50]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 1000\n\nParameters:\n\nv:\n    Formula: v ~ 1 + x + y\n    Priors:\n        v_Intercept ~ Normal(mu: 2.0, sigma: 3.0)\n        v_x ~ Normal(mu: 0.0, sigma: 0.25)\n        v_y ~ Normal(mu: 0.0, sigma: 0.25)\n    Link: identity\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[51]: Copied! <pre>model_reg_v_simple_new = hssm.HSSM(\n    data=dataset_reg_v, include=[hssm.Param(name=\"v\", formula=\"v ~ 1 + x + y\")]\n)\n</pre> model_reg_v_simple_new = hssm.HSSM(     data=dataset_reg_v, include=[hssm.Param(name=\"v\", formula=\"v ~ 1 + x + y\")] ) <pre>Model initialized successfully.\n</pre> In\u00a0[52]: Copied! <pre>model_reg_v_simple_new\n</pre> model_reg_v_simple_new Out[52]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 1000\n\nParameters:\n\nv:\n    Formula: v ~ 1 + x + y\n    Priors:\n        v_Intercept ~ Normal(mu: 2.0, sigma: 3.0)\n        v_x ~ Normal(mu: 0.0, sigma: 0.25)\n        v_y ~ Normal(mu: 0.0, sigma: 0.25)\n    Link: identity\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[53]: Copied! <pre>model_reg_v_simple.graph()\n</pre> model_reg_v_simple.graph() <pre>max_shape:  (1000,)\nsize:  (np.int64(1000),)\n</pre> Out[53]: In\u00a0[54]: Copied! <pre>print(model_reg_v_simple)\n</pre> print(model_reg_v_simple) <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 1000\n\nParameters:\n\nv:\n    Formula: v ~ 1 + x + y\n    Priors:\n        v_Intercept ~ Normal(mu: 2.0, sigma: 3.0)\n        v_x ~ Normal(mu: 0.0, sigma: 0.25)\n        v_y ~ Normal(mu: 0.0, sigma: 0.25)\n    Link: identity\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)\n</pre> <p>These were the defaults, with a little extra labor, we can e.g. customize the choice of priors for each parameter in the model.</p> In\u00a0[55]: Copied! <pre>model_reg_v = hssm.HSSM(\n    data=dataset_reg_v,\n    include=[\n        {\n            \"name\": \"v\",\n            \"prior\": {\n                \"Intercept\": {\"name\": \"Uniform\", \"lower\": -3.0, \"upper\": 3.0},\n                \"x\": {\"name\": \"Uniform\", \"lower\": -1.0, \"upper\": 1.0},\n                \"y\": {\"name\": \"Uniform\", \"lower\": -1.0, \"upper\": 1.0},\n            },\n            \"formula\": \"v ~ 1 + x + y\",\n            \"link\": \"identity\",\n        }\n    ],\n)\n</pre> model_reg_v = hssm.HSSM(     data=dataset_reg_v,     include=[         {             \"name\": \"v\",             \"prior\": {                 \"Intercept\": {\"name\": \"Uniform\", \"lower\": -3.0, \"upper\": 3.0},                 \"x\": {\"name\": \"Uniform\", \"lower\": -1.0, \"upper\": 1.0},                 \"y\": {\"name\": \"Uniform\", \"lower\": -1.0, \"upper\": 1.0},             },             \"formula\": \"v ~ 1 + x + y\",             \"link\": \"identity\",         }     ], ) <pre>Model initialized successfully.\n</pre> In\u00a0[56]: Copied! <pre>model_reg_v\n</pre> model_reg_v Out[56]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: analytical\nObservations: 1000\n\nParameters:\n\nv:\n    Formula: v ~ 1 + x + y\n    Priors:\n        v_Intercept ~ Uniform(lower: -3.0, upper: 3.0)\n        v_x ~ Uniform(lower: -1.0, upper: 1.0)\n        v_y ~ Uniform(lower: -1.0, upper: 1.0)\n    Link: identity\n    Explicit bounds: (-inf, inf)\n\na:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\nz:\n    Prior: Uniform(lower: 0.0, upper: 1.0)\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Prior: HalfNormal(sigma: 2.0)\n    Explicit bounds: (0.0, inf)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> <p>Notice how <code>v</code> is now set as a regression.</p> In\u00a0[57]: Copied! <pre>infer_data_reg_v = model_reg_v.sample(\n    sampler=\"mcmc\",\n    chains=2,\n    cores=2,\n    draws=500,\n    tune=500,\n    mp_ctx=\"spawn\",\n)\n</pre> infer_data_reg_v = model_reg_v.sample(     sampler=\"mcmc\",     chains=2,     cores=2,     draws=500,     tune=500,     mp_ctx=\"spawn\", ) <pre>Using default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [t, a, z, v_Intercept, v_x, v_y]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 2 chains for 500 tune and 500 draw iterations (1_000 + 1_000 draws total) took 39 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00&lt;00:00, 2845.41it/s]\n</pre> In\u00a0[58]: Copied! <pre>infer_data_reg_v\n</pre> infer_data_reg_v Out[58]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 52kB\nDimensions:      (chain: 2, draw: 500)\nCoordinates:\n  * chain        (chain) int64 16B 0 1\n  * draw         (draw) int64 4kB 0 1 2 3 4 5 6 ... 493 494 495 496 497 498 499\nData variables:\n    v_x          (chain, draw) float64 8kB 0.9023 0.8891 0.7967 ... 0.913 0.875\n    v_Intercept  (chain, draw) float64 8kB 0.3388 0.2968 ... 0.3058 0.2966\n    z            (chain, draw) float64 8kB 0.5182 0.5146 0.5165 ... 0.517 0.5136\n    v_y          (chain, draw) float64 8kB 0.3807 0.4119 0.3528 ... 0.4372 0.326\n    a            (chain, draw) float64 8kB 1.469 1.488 1.447 ... 1.45 1.488\n    t            (chain, draw) float64 8kB 0.1158 0.1225 ... 0.1299 0.09166\nAttributes:\n    created_at:                  2025-09-29T19:52:06.564238+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               38.77959203720093\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (6)<ul><li>v_x(chain, draw)float640.9023 0.8891 ... 0.913 0.875<pre>array([[0.90234992, 0.88905416, 0.7966635 , 0.89654318, 0.86304162,\n        0.86324863, 0.8239293 , 0.93944321, 0.79879676, 0.87541634,\n        0.84649464, 0.8758464 , 0.85942904, 0.82083024, 0.82479311,\n        0.8848282 , 0.82475363, 0.93804625, 0.83247655, 0.87310386,\n        0.90064933, 0.86434901, 0.91202446, 0.81291843, 0.89609712,\n        0.806304  , 0.89844248, 0.92958288, 0.97122974, 0.84966004,\n        0.86951492, 0.84477635, 0.8431393 , 0.9520442 , 0.85063797,\n        0.88875382, 0.91779853, 0.82410129, 0.89435723, 0.87472798,\n        0.83693415, 0.85411334, 0.85622224, 0.88844249, 0.87816276,\n        0.83067775, 0.91290533, 0.7910528 , 0.93102992, 0.77122464,\n        0.91463285, 0.89418299, 0.79734932, 0.87231254, 0.83501698,\n        0.88952852, 0.89033491, 0.91780895, 0.8137794 , 0.86443008,\n        0.75712401, 0.87557177, 0.83836861, 0.86768051, 0.93140301,\n        0.84913306, 0.87578897, 0.81968978, 0.8182216 , 0.95168378,\n        0.89528949, 0.87774202, 0.8633396 , 0.87365902, 0.94164514,\n        0.86364403, 0.84817488, 0.85398319, 0.85981893, 0.87441309,\n        0.93609843, 0.91974052, 0.78991755, 0.78985837, 0.91101832,\n        0.94893321, 0.87931553, 0.85688583, 0.84514706, 0.78928871,\n        0.91557847, 0.79460807, 0.90627574, 0.930321  , 0.91521141,\n        0.78494212, 0.85068346, 0.86442411, 0.90328964, 0.87885304,\n...\n        0.87555586, 0.82739475, 0.83299404, 0.96349704, 0.87581238,\n        0.82829547, 0.82977407, 0.89755979, 0.9350501 , 0.93435483,\n        0.93388025, 0.81358512, 0.8816959 , 0.82696368, 0.9217439 ,\n        0.82666598, 0.91135663, 0.88057552, 0.89152117, 0.85101277,\n        0.86888993, 0.86160398, 0.82152008, 0.86286547, 0.8960538 ,\n        0.8194339 , 0.89837036, 0.8792789 , 0.8155855 , 0.90576834,\n        0.79019871, 0.87193923, 0.84129623, 0.88015231, 0.84573861,\n        0.93167534, 0.88302099, 0.88531779, 0.85568426, 0.82552498,\n        0.82267836, 0.82267836, 0.79930877, 0.88934224, 0.85445277,\n        0.91404448, 0.81181634, 0.89767465, 0.88152763, 0.78112323,\n        0.88608018, 0.9227117 , 0.94061752, 0.9620633 , 0.95215904,\n        0.93103134, 0.79769171, 0.83690742, 0.88590988, 0.84445584,\n        0.76980249, 0.85528267, 0.84559641, 0.82996276, 0.85196741,\n        0.86074314, 0.82740091, 0.88329873, 0.90790801, 0.8183808 ,\n        0.84688186, 0.7929426 , 0.88891928, 0.81855041, 0.92811469,\n        0.87084078, 0.85710064, 0.83573771, 0.81756162, 0.87687422,\n        0.89620252, 0.78471322, 0.9220724 , 0.86833578, 0.85127013,\n        0.91296099, 0.82825527, 0.8873357 , 0.91142202, 0.86162598,\n        0.86953391, 0.88430126, 0.81111377, 0.86681956, 0.78653404,\n        0.93617396, 0.93274075, 0.9085796 , 0.91302812, 0.87502283]])</pre></li><li>v_Intercept(chain, draw)float640.3388 0.2968 ... 0.3058 0.2966<pre>array([[0.33882483, 0.29679866, 0.30425955, 0.30751593, 0.30973135,\n        0.36193979, 0.36186356, 0.31863145, 0.29968568, 0.36215341,\n        0.34019639, 0.25720461, 0.34489316, 0.2764523 , 0.2888447 ,\n        0.31014385, 0.27965648, 0.27435071, 0.28569588, 0.2619622 ,\n        0.35224494, 0.2756319 , 0.33901556, 0.29500182, 0.33505121,\n        0.2582923 , 0.2280775 , 0.35084969, 0.31456509, 0.30981707,\n        0.37619337, 0.28921741, 0.3543357 , 0.28239792, 0.3382029 ,\n        0.3143442 , 0.34029509, 0.34653686, 0.34786578, 0.28743898,\n        0.30932598, 0.34869237, 0.35093004, 0.33153968, 0.32134812,\n        0.29390821, 0.32898457, 0.31083387, 0.33975135, 0.27871429,\n        0.34939947, 0.2716018 , 0.3182238 , 0.31289279, 0.33575449,\n        0.30904641, 0.33615986, 0.30327444, 0.2731647 , 0.29850085,\n        0.26887906, 0.28596267, 0.27908545, 0.3090144 , 0.31810219,\n        0.26239953, 0.36911411, 0.33838853, 0.33941567, 0.33176331,\n        0.27072448, 0.31560832, 0.29394546, 0.23590486, 0.33499089,\n        0.3330464 , 0.30468891, 0.32651508, 0.32350211, 0.30059602,\n        0.25725866, 0.26693871, 0.36582259, 0.37662374, 0.3152006 ,\n        0.2718486 , 0.29963589, 0.27675594, 0.28792081, 0.29339205,\n        0.27632457, 0.30297732, 0.28149032, 0.34371597, 0.30840443,\n        0.28425642, 0.24694373, 0.37233279, 0.33215078, 0.29407127,\n...\n        0.29476929, 0.34471507, 0.30308012, 0.3127234 , 0.35749222,\n        0.33302624, 0.30653047, 0.32751355, 0.32659348, 0.32503274,\n        0.34271795, 0.36573319, 0.33979467, 0.31214777, 0.35008668,\n        0.27480667, 0.2481786 , 0.25760166, 0.33236165, 0.32831941,\n        0.29699556, 0.30783885, 0.31389357, 0.30184475, 0.3716207 ,\n        0.3766281 , 0.34612748, 0.34034539, 0.31202412, 0.36965529,\n        0.36802713, 0.31140394, 0.26577099, 0.31909508, 0.2965158 ,\n        0.30124484, 0.31720985, 0.26128021, 0.36155971, 0.38965584,\n        0.33377534, 0.33377534, 0.33172915, 0.2668043 , 0.28531689,\n        0.28051325, 0.29097269, 0.31635065, 0.26366825, 0.29393481,\n        0.29658619, 0.32240364, 0.27363407, 0.31613546, 0.32090918,\n        0.31514571, 0.3448673 , 0.36389261, 0.37659995, 0.29701771,\n        0.3250356 , 0.31238803, 0.38091602, 0.21777985, 0.2288235 ,\n        0.37306109, 0.29102922, 0.25721676, 0.28445492, 0.27036486,\n        0.2813149 , 0.35121847, 0.29373405, 0.29664262, 0.32866167,\n        0.37065557, 0.38194896, 0.26265772, 0.26312103, 0.30105813,\n        0.28896234, 0.29036162, 0.26546171, 0.31256751, 0.26041108,\n        0.36731445, 0.25360806, 0.33327167, 0.38979313, 0.36088828,\n        0.28510003, 0.27747939, 0.28877739, 0.29768484, 0.29323601,\n        0.29460632, 0.29016395, 0.28362057, 0.30575046, 0.29656251]])</pre></li><li>z(chain, draw)float640.5182 0.5146 ... 0.517 0.5136<pre>array([[0.51815395, 0.51461917, 0.51653379, 0.50727843, 0.50950784,\n        0.48858106, 0.48829451, 0.50251557, 0.50866642, 0.48807029,\n        0.49071032, 0.53079274, 0.50270021, 0.51042316, 0.51287437,\n        0.50806465, 0.51995846, 0.51973136, 0.50407546, 0.52836865,\n        0.49719706, 0.51576954, 0.5002628 , 0.50719443, 0.50235401,\n        0.538506  , 0.53252013, 0.48980404, 0.51586236, 0.50454443,\n        0.49603087, 0.52908207, 0.49756227, 0.5150467 , 0.50061497,\n        0.50551017, 0.50430911, 0.50957285, 0.51920686, 0.51731429,\n        0.49184497, 0.49824439, 0.49696515, 0.50096105, 0.50554873,\n        0.50569   , 0.5199484 , 0.50035101, 0.50380252, 0.52851915,\n        0.50943368, 0.52234434, 0.52112512, 0.51954935, 0.50403655,\n        0.50403921, 0.49924229, 0.50584406, 0.5175674 , 0.51820658,\n        0.51051015, 0.50895574, 0.5004913 , 0.51674816, 0.50302601,\n        0.530058  , 0.49368971, 0.48445322, 0.48738682, 0.49844158,\n        0.50714709, 0.53478457, 0.52346873, 0.51972012, 0.49576248,\n        0.50209245, 0.49922153, 0.50868028, 0.50112802, 0.50053605,\n        0.51535725, 0.51916905, 0.50763502, 0.50726046, 0.480466  ,\n        0.52599768, 0.51782027, 0.50656436, 0.52192875, 0.51589962,\n        0.52230765, 0.50585278, 0.52141116, 0.51714383, 0.52373367,\n        0.50666959, 0.53482829, 0.51379552, 0.53010996, 0.5196806 ,\n...\n        0.51302308, 0.50603387, 0.50359399, 0.50725972, 0.49163886,\n        0.50878434, 0.50143918, 0.5087216 , 0.50730752, 0.50997975,\n        0.50876856, 0.47706472, 0.50279749, 0.50059399, 0.49550768,\n        0.52065433, 0.53706123, 0.52872532, 0.50694341, 0.50382349,\n        0.51471931, 0.50380848, 0.49814733, 0.51408885, 0.48055959,\n        0.48936072, 0.49010268, 0.49771458, 0.50742847, 0.48520408,\n        0.49107098, 0.51033905, 0.51647029, 0.51005736, 0.51520989,\n        0.53335148, 0.49210658, 0.51391235, 0.49563227, 0.5056668 ,\n        0.49885742, 0.49885742, 0.50138779, 0.52146553, 0.52342812,\n        0.53143866, 0.50575812, 0.51838401, 0.50429818, 0.52400152,\n        0.50894976, 0.50779383, 0.51817783, 0.50156272, 0.5056606 ,\n        0.50053517, 0.50676362, 0.49707361, 0.48876995, 0.50557209,\n        0.50730457, 0.50053686, 0.48052799, 0.5505053 , 0.53683992,\n        0.50612332, 0.50864674, 0.5109218 , 0.53725006, 0.51667872,\n        0.5270733 , 0.49944339, 0.50553443, 0.52327634, 0.50665348,\n        0.48531323, 0.49541453, 0.51755813, 0.52662711, 0.50680686,\n        0.50815156, 0.52144789, 0.53132025, 0.51141188, 0.52649613,\n        0.50170943, 0.5225374 , 0.49957653, 0.49547282, 0.49884689,\n        0.51865375, 0.5246718 , 0.51937948, 0.50401385, 0.53588083,\n        0.51219696, 0.50550784, 0.50763478, 0.5169916 , 0.51359078]])</pre></li><li>v_y(chain, draw)float640.3807 0.4119 ... 0.4372 0.326<pre>array([[0.38066974, 0.411943  , 0.35278254, 0.45418772, 0.47668296,\n        0.28471438, 0.2647706 , 0.41288333, 0.42349269, 0.32473994,\n        0.32972154, 0.41489832, 0.43553677, 0.35692003, 0.36561771,\n        0.47798811, 0.36804523, 0.30706477, 0.45495695, 0.33929816,\n        0.40695645, 0.32577414, 0.42460605, 0.31634369, 0.39830954,\n        0.33895309, 0.34335215, 0.3789906 , 0.37218778, 0.39085106,\n        0.32095162, 0.4061928 , 0.33190675, 0.45106832, 0.3230338 ,\n        0.23691337, 0.49603241, 0.31893798, 0.26686344, 0.36183237,\n        0.43087312, 0.37564576, 0.32698914, 0.50058144, 0.29086175,\n        0.3844499 , 0.3925989 , 0.34058945, 0.41643284, 0.35310713,\n        0.40456062, 0.45005916, 0.32072704, 0.39184001, 0.38983326,\n        0.38332097, 0.37590725, 0.38612983, 0.36472269, 0.37937284,\n        0.39867552, 0.42792282, 0.36619929, 0.35954992, 0.41830691,\n        0.40023187, 0.31632822, 0.34624895, 0.40155249, 0.36115779,\n        0.39428838, 0.38938296, 0.44090669, 0.33953105, 0.41395433,\n        0.3483937 , 0.3761057 , 0.33086042, 0.44297114, 0.4299297 ,\n        0.33249083, 0.34998586, 0.36766131, 0.37204763, 0.34606389,\n        0.40743908, 0.34733636, 0.30897366, 0.29792917, 0.46474704,\n        0.38040919, 0.38239586, 0.38987559, 0.35513861, 0.40656993,\n        0.40957685, 0.45117312, 0.33838286, 0.38122696, 0.40635393,\n...\n        0.35867802, 0.35794914, 0.36944251, 0.39319481, 0.3653909 ,\n        0.3708721 , 0.38393625, 0.3634015 , 0.44764463, 0.43596247,\n        0.45611047, 0.37392939, 0.38876331, 0.41196128, 0.37753279,\n        0.34981576, 0.39482809, 0.33803392, 0.39186741, 0.39730407,\n        0.3719571 , 0.36820008, 0.36771153, 0.47158294, 0.39790554,\n        0.45839518, 0.36050032, 0.33242988, 0.35860837, 0.43166288,\n        0.36838973, 0.37790021, 0.36910549, 0.4059735 , 0.35890579,\n        0.39377153, 0.39859002, 0.32941831, 0.4422683 , 0.45034187,\n        0.37661785, 0.37661785, 0.36469773, 0.39608303, 0.3625623 ,\n        0.37991276, 0.42973591, 0.34055991, 0.35075411, 0.3293542 ,\n        0.22131124, 0.36843311, 0.38536842, 0.37608767, 0.38969463,\n        0.36877267, 0.3629755 , 0.36095713, 0.36561069, 0.43540207,\n        0.36464959, 0.3939181 , 0.35206754, 0.41699504, 0.38239069,\n        0.39512123, 0.36389638, 0.39593499, 0.33141335, 0.38355177,\n        0.42277765, 0.3918054 , 0.39600084, 0.40305522, 0.37387183,\n        0.40592998, 0.37020572, 0.36487214, 0.3769793 , 0.43347669,\n        0.34408164, 0.43514671, 0.52661692, 0.43429699, 0.32271508,\n        0.39351261, 0.37611435, 0.36938779, 0.39742449, 0.37626827,\n        0.36893145, 0.45395341, 0.34570479, 0.36324049, 0.36611906,\n        0.40392698, 0.4165291 , 0.39355333, 0.43721465, 0.32603295]])</pre></li><li>a(chain, draw)float641.469 1.488 1.447 ... 1.45 1.488<pre>array([[1.46871069, 1.48797534, 1.44682325, 1.42074044, 1.42975487,\n        1.53598946, 1.5409929 , 1.52949932, 1.56653509, 1.49158401,\n        1.48331029, 1.47717667, 1.46064773, 1.45706655, 1.42953718,\n        1.49775019, 1.47096326, 1.48185488, 1.43770967, 1.49194067,\n        1.46465921, 1.50075203, 1.46563715, 1.43118031, 1.48492624,\n        1.44864006, 1.43565163, 1.50219467, 1.51380697, 1.47590587,\n        1.47533431, 1.45533081, 1.48388047, 1.49633022, 1.49433738,\n        1.43857214, 1.51271834, 1.44391007, 1.47296845, 1.43208789,\n        1.4900041 , 1.50915396, 1.52710543, 1.44082276, 1.49689591,\n        1.42625203, 1.45254211, 1.47800433, 1.51094661, 1.45932748,\n        1.4898681 , 1.48364007, 1.46294153, 1.45618163, 1.45866505,\n        1.46517372, 1.4723471 , 1.48370242, 1.44802731, 1.47513874,\n        1.43166757, 1.49817389, 1.46732854, 1.47136384, 1.45558601,\n        1.5152182 , 1.45009846, 1.41229236, 1.47340447, 1.45415307,\n        1.46916031, 1.51286209, 1.49634939, 1.45948308, 1.48122734,\n        1.46467625, 1.50238317, 1.49823788, 1.45466097, 1.45062109,\n        1.47878953, 1.4859885 , 1.449208  , 1.44465535, 1.47624541,\n        1.52584941, 1.45152897, 1.48561993, 1.53023401, 1.41256951,\n        1.51780927, 1.48378158, 1.46604668, 1.48790516, 1.49315144,\n        1.48174948, 1.47601985, 1.45254778, 1.52821364, 1.50155629,\n...\n        1.46897221, 1.5120586 , 1.52582238, 1.49255814, 1.50842491,\n        1.47618357, 1.49273568, 1.4735051 , 1.48367135, 1.47288074,\n        1.46294665, 1.48235301, 1.46901463, 1.48773507, 1.4598116 ,\n        1.44544473, 1.50127523, 1.48294058, 1.47207438, 1.47273961,\n        1.47895924, 1.49296496, 1.51293409, 1.41837193, 1.47702384,\n        1.4605252 , 1.48753586, 1.48120493, 1.4561806 , 1.48736297,\n        1.47437101, 1.46952444, 1.47162904, 1.49401925, 1.49330211,\n        1.50753918, 1.47765276, 1.48963689, 1.51576753, 1.4900152 ,\n        1.49192683, 1.49192683, 1.49905623, 1.51847142, 1.4606954 ,\n        1.48850852, 1.49949039, 1.45037219, 1.40757551, 1.46257796,\n        1.43982834, 1.4485121 , 1.51075546, 1.43626357, 1.4503781 ,\n        1.46420891, 1.5060497 , 1.50451506, 1.41384507, 1.4741924 ,\n        1.45159754, 1.46881694, 1.49326195, 1.45170761, 1.43110337,\n        1.50870319, 1.4130297 , 1.51636538, 1.48822354, 1.47333921,\n        1.46447335, 1.49672158, 1.49307699, 1.48118326, 1.48114178,\n        1.52072593, 1.49345347, 1.43317149, 1.51178571, 1.4458509 ,\n        1.50938918, 1.45794658, 1.47708441, 1.46014626, 1.46641268,\n        1.46351309, 1.46905645, 1.46221301, 1.49114597, 1.49509551,\n        1.46819065, 1.46919626, 1.43597389, 1.47298161, 1.46453695,\n        1.44981735, 1.46778583, 1.47704212, 1.450137  , 1.48784827]])</pre></li><li>t(chain, draw)float640.1158 0.1225 ... 0.1299 0.09166<pre>array([[0.11583475, 0.12247887, 0.10111406, 0.1439656 , 0.1578367 ,\n        0.05601764, 0.05623396, 0.05687448, 0.06856773, 0.12540961,\n        0.11503167, 0.10636277, 0.11881548, 0.12405844, 0.10574129,\n        0.12458664, 0.13801362, 0.08888456, 0.13522718, 0.13476092,\n        0.11701689, 0.11567353, 0.12571648, 0.12951973, 0.11617237,\n        0.15195421, 0.12679606, 0.10323972, 0.09638172, 0.11480577,\n        0.11192981, 0.12870879, 0.10680366, 0.12504287, 0.11745383,\n        0.12653752, 0.06981803, 0.12349588, 0.12477806, 0.14496295,\n        0.07900437, 0.08772401, 0.08083914, 0.13575888, 0.11581472,\n        0.11396692, 0.13760576, 0.06792242, 0.12267745, 0.12546228,\n        0.11599342, 0.11943963, 0.13064411, 0.12802323, 0.12335614,\n        0.11154952, 0.10788948, 0.11016935, 0.12307597, 0.10985094,\n        0.11809849, 0.11379432, 0.10676886, 0.10988302, 0.1284531 ,\n        0.11716873, 0.15495962, 0.10909484, 0.1018081 , 0.11221656,\n        0.08899033, 0.14165005, 0.09809975, 0.13367361, 0.09639958,\n        0.13020694, 0.10151803, 0.09511348, 0.13096647, 0.11678111,\n        0.12279293, 0.12254037, 0.13349011, 0.13529959, 0.08512828,\n        0.1177743 , 0.10812363, 0.11506583, 0.0993943 , 0.13874024,\n        0.11378321, 0.10145377, 0.14002382, 0.11474235, 0.13017455,\n        0.09328183, 0.10866988, 0.11953577, 0.11345642, 0.11178046,\n...\n        0.12193219, 0.08436964, 0.09975333, 0.11692865, 0.09367657,\n        0.08232246, 0.08071547, 0.13395692, 0.11392648, 0.11853792,\n        0.12714258, 0.09682394, 0.10150383, 0.10653299, 0.10006222,\n        0.08615269, 0.11406442, 0.11905074, 0.12407525, 0.09842775,\n        0.11105786, 0.11995715, 0.09307819, 0.13658056, 0.10333186,\n        0.11064647, 0.09873272, 0.0982283 , 0.10809968, 0.11819829,\n        0.11068861, 0.12601651, 0.12710553, 0.11622692, 0.10714932,\n        0.11013204, 0.10232644, 0.12542373, 0.12583697, 0.09553051,\n        0.08954052, 0.08954052, 0.08586966, 0.11133496, 0.10587649,\n        0.13804028, 0.08341809, 0.14330268, 0.13389886, 0.13608718,\n        0.12340966, 0.14278577, 0.08974744, 0.12873944, 0.14348987,\n        0.10476097, 0.0851296 , 0.07624134, 0.13283817, 0.12653838,\n        0.11171355, 0.10367277, 0.09503581, 0.14177988, 0.14126343,\n        0.09686078, 0.12943518, 0.10086315, 0.1196166 , 0.10520236,\n        0.12386881, 0.09385699, 0.10992012, 0.09082518, 0.10361578,\n        0.07800699, 0.10930198, 0.12986523, 0.11222185, 0.11192213,\n        0.11848498, 0.08273874, 0.12873289, 0.13202898, 0.10519654,\n        0.12320991, 0.10141369, 0.11172287, 0.13136664, 0.09111185,\n        0.128851  , 0.12318914, 0.12777571, 0.13540281, 0.10590721,\n        0.10339342, 0.12041734, 0.12516737, 0.12988343, 0.09165702]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-29T19:52:06.564238+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :38.77959203720093tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 8MB\nDimensions:      (chain: 2, draw: 500, __obs__: 1000)\nCoordinates:\n  * chain        (chain) int64 16B 0 1\n  * draw         (draw) int64 4kB 0 1 2 3 4 5 6 ... 493 494 495 496 497 498 499\n  * __obs__      (__obs__) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    rt,response  (chain, draw, __obs__) float64 8MB -5.873 -0.9084 ... -3.016\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li><li>__obs__: 1000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-5.873 -0.9084 ... -2.131 -3.016<pre>array([[[-5.87255712, -0.90841586, -1.99558743, ..., -0.71909628,\n         -2.14889438, -3.14149451],\n        [-5.83065374, -0.91949223, -1.93979645, ..., -0.69346267,\n         -2.05464728, -3.10376544],\n        [-5.86562326, -0.95662328, -2.04647446, ..., -0.75925163,\n         -2.09471502, -2.88961243],\n        ...,\n        [-5.88404657, -0.85272864, -1.98166002, ..., -0.65301358,\n         -2.11607941, -3.10612346],\n        [-5.85254751, -0.97916288, -2.06125333, ..., -0.73279768,\n         -2.12168173, -2.80477118],\n        [-5.83645402, -0.95390708, -2.01333567, ..., -0.70183644,\n         -2.1774968 , -3.04024644]],\n\n       [[-5.85072901, -0.94558807, -1.99813393, ..., -0.73504066,\n         -2.11607578, -3.04345514],\n        [-5.84617438, -1.00073021, -2.03424677, ..., -0.71190299,\n         -2.11616703, -2.92726124],\n        [-5.81236244, -0.9561274 , -1.93333469, ..., -0.7226569 ,\n         -2.03085611, -3.08694191],\n        ...,\n        [-5.82782862, -0.95143771, -1.94839785, ..., -0.68471071,\n         -2.02553353, -2.99630842],\n        [-5.86797236, -0.87910063, -1.95320989, ..., -0.709735  ,\n         -2.02934636, -3.05581577],\n        [-5.82665235, -1.01376355, -2.00700262, ..., -0.73617243,\n         -2.13121051, -3.01646555]]], shape=(2, 500, 1000))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 134kB\nDimensions:                (chain: 2, draw: 500)\nCoordinates:\n  * chain                  (chain) int64 16B 0 1\n  * draw                   (draw) int64 4kB 0 1 2 3 4 5 ... 495 496 497 498 499\nData variables: (12/18)\n    energy                 (chain, draw) float64 8kB 1.949e+03 ... 1.951e+03\n    step_size_bar          (chain, draw) float64 8kB 0.5283 0.5283 ... 0.6212\n    divergences            (chain, draw) int64 8kB 0 0 0 0 0 0 0 ... 0 0 0 0 0 0\n    perf_counter_start     (chain, draw) float64 8kB 1.667e+06 ... 1.667e+06\n    diverging              (chain, draw) bool 1kB False False ... False False\n    step_size              (chain, draw) float64 8kB 0.4589 0.4589 ... 0.5872\n    ...                     ...\n    acceptance_rate        (chain, draw) float64 8kB 1.0 1.0 ... 0.8678 0.5956\n    lp                     (chain, draw) float64 8kB -1.946e+03 ... -1.946e+03\n    reached_max_treedepth  (chain, draw) bool 1kB False False ... False False\n    perf_counter_diff      (chain, draw) float64 8kB 0.006973 ... 0.006783\n    index_in_trajectory    (chain, draw) int64 8kB -6 2 -6 3 -1 ... -1 1 -1 -2\n    max_energy_error       (chain, draw) float64 8kB -1.105 -0.4064 ... 1.978\nAttributes:\n    created_at:                  2025-09-29T19:52:06.577971+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               38.77959203720093\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (18)<ul><li>energy(chain, draw)float641.949e+03 1.947e+03 ... 1.951e+03<pre>array([[1948.95617998, 1946.71246142, 1948.99467916, 1950.52315727,\n        1950.55494074, 1955.8944341 , 1954.54724758, 1957.6079737 ,\n        1955.35311167, 1956.50326929, 1949.5552014 , 1947.53611943,\n        1948.52505203, 1948.27965213, 1948.14901287, 1950.99553302,\n        1949.87434679, 1954.293161  , 1951.73519553, 1952.37337582,\n        1947.93398546, 1949.02466946, 1948.09241978, 1949.14983249,\n        1947.14193223, 1950.98550519, 1952.10110671, 1956.06152066,\n        1952.01481123, 1949.41369345, 1949.44758989, 1950.47965616,\n        1947.11462786, 1951.05605098, 1951.91816884, 1953.95916507,\n        1954.29446099, 1953.84759258, 1957.77651743, 1952.2473142 ,\n        1953.75495251, 1948.84283973, 1949.88481149, 1951.56198167,\n        1951.06702002, 1952.57779485, 1949.09373287, 1950.74263046,\n        1950.38109989, 1949.37916942, 1947.63767726, 1949.00632554,\n        1948.68833182, 1948.23938438, 1946.15122393, 1945.35396533,\n        1945.70715352, 1946.15251851, 1945.81829015, 1946.41553688,\n        1949.09700612, 1951.27383419, 1950.20968095, 1948.95396228,\n        1947.13422052, 1949.15102844, 1953.06435397, 1952.94528742,\n        1951.82884046, 1951.88026581, 1950.45159469, 1953.10783649,\n        1952.58841993, 1951.55158928, 1948.21889444, 1949.29237764,\n        1948.61369761, 1948.54099147, 1946.78865985, 1947.36304629,\n...\n        1946.54200678, 1947.51629814, 1948.36984136, 1952.48687631,\n        1955.44727283, 1950.53645277, 1949.77710005, 1947.44341283,\n        1948.13659536, 1950.18098617, 1954.60539174, 1950.13236286,\n        1946.61164283, 1945.79912088, 1946.79475102, 1950.03483912,\n        1949.61033563, 1952.60234022, 1952.53045483, 1954.99500112,\n        1952.71594696, 1948.68722692, 1946.90494695, 1949.74540585,\n        1947.78698324, 1948.58273299, 1950.38358304, 1948.47317461,\n        1950.48001761, 1951.04966329, 1953.5786145 , 1953.96759548,\n        1952.26932622, 1954.16855019, 1949.83539214, 1953.32510784,\n        1950.33414229, 1950.4779819 , 1954.41669789, 1951.47206512,\n        1951.47831338, 1948.71990666, 1949.74895122, 1951.83423665,\n        1951.44358256, 1951.72785564, 1951.7836408 , 1954.62517076,\n        1953.62986383, 1949.89664168, 1946.65095582, 1948.845145  ,\n        1954.13687666, 1949.12483056, 1948.45718685, 1950.05868717,\n        1950.66384168, 1948.66972873, 1950.15667829, 1949.27260371,\n        1949.36659948, 1952.53663294, 1960.07372841, 1952.96559849,\n        1948.26724677, 1948.39311974, 1948.99092887, 1946.77252952,\n        1952.05418527, 1950.5962885 , 1948.0140422 , 1946.71369864,\n        1947.38456472, 1948.22827744, 1950.52812483, 1951.40820324,\n        1948.74874866, 1947.98927732, 1949.12812169, 1951.06193744]])</pre></li><li>step_size_bar(chain, draw)float640.5283 0.5283 ... 0.6212 0.6212<pre>array([[0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n        0.52831538, 0.52831538, 0.52831538, 0.52831538, 0.52831538,\n...\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ,\n        0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 , 0.6212335 ]])</pre></li><li>divergences(chain, draw)int640 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0<pre>array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n...\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</pre></li><li>perf_counter_start(chain, draw)float641.667e+06 1.667e+06 ... 1.667e+06<pre>array([[1666941.25263404, 1666941.2596825 , 1666941.26654721,\n        1666941.27348233, 1666941.27693412, 1666941.28392162,\n        1666941.29085421, 1666941.29437408, 1666941.30150692,\n        1666941.30828533, 1666941.31538667, 1666941.31891942,\n        1666941.32601896, 1666941.33316371, 1666941.34027037,\n        1666941.34375004, 1666941.35135142, 1666941.35874721,\n        1666941.365624  , 1666941.37278954, 1666941.38674429,\n        1666941.39370725, 1666941.40805454, 1666941.41513354,\n        1666941.42196613, 1666941.42903033, 1666941.4361615 ,\n        1666941.44296946, 1666941.44993467, 1666941.45760363,\n        1666941.46471892, 1666941.47168521, 1666941.47861792,\n        1666941.49251792, 1666941.49960613, 1666941.50646608,\n        1666941.51378687, 1666941.52096412, 1666941.52794063,\n        1666941.53149096, 1666941.53864438, 1666941.545673  ,\n        1666941.54934296, 1666941.5528015 , 1666941.5604655 ,\n        1666941.56830742, 1666941.57542996, 1666941.57919321,\n        1666941.586146  , 1666941.593026  , 1666941.60708475,\n        1666941.61427213, 1666941.62129054, 1666941.62833558,\n        1666941.63554621, 1666941.64948242, 1666941.65627042,\n        1666941.663435  , 1666941.70127412, 1666941.72914521,\n...\n        1666953.74589871, 1666953.74951862, 1666953.75299387,\n        1666953.76364029, 1666953.77087088, 1666953.77436092,\n        1666953.78138146, 1666953.78835592, 1666953.79508821,\n        1666953.80188142, 1666953.80857279, 1666953.81554142,\n        1666953.82276308, 1666953.83041479, 1666953.83404725,\n        1666953.8378135 , 1666953.84491533, 1666953.8520815 ,\n        1666953.85904242, 1666953.86620221, 1666953.87282487,\n        1666953.87995742, 1666953.887071  , 1666953.893876  ,\n        1666953.90096508, 1666953.90794625, 1666953.91497175,\n        1666953.92227817, 1666953.92947558, 1666953.9366165 ,\n        1666953.94382592, 1666953.97973137, 1666954.03202   ,\n        1666954.03564483, 1666954.04785254, 1666954.058279  ,\n        1666954.06524508, 1666954.0723835 , 1666954.07944133,\n        1666954.08657833, 1666954.09336025, 1666954.09681537,\n        1666954.10376675, 1666954.11060208, 1666954.11755608,\n        1666954.12433808, 1666954.13103912, 1666954.13773137,\n        1666954.14478979, 1666954.15212162, 1666954.15929563,\n        1666954.16279396, 1666954.17001175, 1666954.17344383,\n        1666954.18055496, 1666954.18784329, 1666954.19133437,\n        1666954.19664904, 1666954.20368437]])</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False]])</pre></li><li>step_size(chain, draw)float640.4589 0.4589 ... 0.5872 0.5872<pre>array([[0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n        0.45893149, 0.45893149, 0.45893149, 0.45893149, 0.45893149,\n...\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461,\n        0.58722461, 0.58722461, 0.58722461, 0.58722461, 0.58722461]])</pre></li><li>energy_error(chain, draw)float64-0.7411 -0.3265 ... -0.1452 0.2915<pre>array([[-7.41133368e-01, -3.26499128e-01,  6.73305678e-01,\n        -1.14858784e+00,  3.57835861e-01, -1.26960513e-01,\n         2.14095364e-01, -2.40236851e-02,  6.18279396e-02,\n        -9.74291443e-01, -5.59090418e-01, -1.12248459e-02,\n        -2.19134750e-01,  1.99687269e-02,  7.51760773e-01,\n        -2.95900247e-01, -1.04494083e-01,  1.66233089e+00,\n        -8.42825018e-01,  1.67220372e-01, -4.09043166e-01,\n         4.11943790e-02, -4.50902550e-01, -1.35723242e-01,\n        -2.82279101e-02,  3.75648096e-01,  6.81995331e-01,\n        -7.30222914e-01,  1.54217055e-02, -5.48378792e-01,\n         2.26885836e-01, -1.28363874e-01, -4.22084464e-02,\n         3.21322640e-01,  6.77345649e-02,  4.93535895e-02,\n         2.15072545e-01, -8.33025465e-01,  1.49243652e-01,\n        -6.35065649e-01,  2.69747645e-01, -2.69444434e-01,\n         1.26328015e-01,  7.63984908e-02,  3.77052171e-02,\n         5.78274246e-02, -3.10814290e-01,  9.55332475e-01,\n         7.55672854e-02, -6.71020127e-01, -1.54899425e-01,\n        -5.69144086e-02,  2.25256068e-01, -2.98169209e-01,\n        -5.55023580e-02,  4.64062091e-02, -2.84804231e-02,\n         5.43277794e-02, -1.42058676e-01,  1.09326004e-01,\n...\n         0.00000000e+00,  1.61379520e-01, -5.44143153e-02,\n        -1.05728849e-02, -3.72783734e-02, -2.39006404e-02,\n        -5.06134611e-02,  4.75084455e-01, -3.80021575e-01,\n         4.64368692e-01, -9.14642278e-01,  1.23687878e-01,\n         7.60829838e-01,  6.07738916e-03,  5.04276150e-01,\n         5.26858763e-02, -1.92601538e-01,  6.45243481e-02,\n        -1.24792539e-01, -3.27554992e-02, -9.39344535e-02,\n         1.64477842e-01,  7.18604316e-02, -1.03109818e-01,\n         2.91770331e-01, -1.85257867e-01,  7.31210430e-01,\n        -4.12210364e-01, -3.16925825e-01, -5.49305443e-02,\n         1.94104978e-01,  4.10529119e-01,  7.08789977e-02,\n        -3.40621468e-01,  3.63866318e-03,  6.10626679e-02,\n        -1.73557555e-01,  5.35421527e-01, -1.35243461e-02,\n         2.80448724e-01,  4.59851419e-01, -1.58911896e+00,\n        -3.92223819e-01,  4.08369255e-01, -4.68918751e-02,\n        -6.99770950e-02, -2.61498837e-01,  1.17497463e-01,\n        -4.38308439e-01, -2.41191406e-01,  1.13567824e-01,\n        -7.55699290e-02,  6.29184014e-01,  3.92830756e-01,\n        -1.45717250e-01, -6.46307674e-01,  1.04582201e-01,\n        -1.45203328e-01,  2.91457757e-01]])</pre></li><li>largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n...\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan]])</pre></li><li>tree_depth(chain, draw)int643 3 3 2 3 3 2 3 ... 3 2 3 3 2 3 3 3<pre>array([[3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 4, 3, 4,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3,\n        3, 3, 2, 3, 3, 4, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3,\n        3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 1, 4, 3, 3, 2,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3,\n        2, 2, 4, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3,\n        3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 1, 2, 3, 3, 2, 3, 3, 3, 4, 3, 3,\n        4, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 4, 3, 3, 3,\n        3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 4, 3, 3, 3, 3, 3, 3, 2, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 4, 3, 3, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2,\n        3, 3, 3, 2, 3, 3, 3, 3, 3, 4, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2,\n        3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 4, 3,\n        3, 3, 3, 4, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2,\n        4, 2, 3, 2, 3, 4, 3, 3, 2, 3, 3, 3, 3, 4, 2, 3, 2, 2, 3, 2, 3, 3,\n...\n        4, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 4,\n        2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2,\n        4, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2,\n        3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 4, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 2, 3, 1, 2, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 3, 2, 3, 4, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 2, 4, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3,\n        3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 4, 3, 3,\n        3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3,\n        3, 3, 3, 3, 3, 3, 2, 3, 4, 3, 2, 4, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2,\n        3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2,\n        3, 2, 2, 4, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3]])</pre></li><li>process_time_diff(chain, draw)float640.006973 0.006803 ... 0.006782<pre>array([[0.006973, 0.006803, 0.006891, 0.003411, 0.006907, 0.006882,\n        0.003478, 0.007058, 0.006732, 0.007014, 0.003487, 0.007022,\n        0.007088, 0.007021, 0.003434, 0.007358, 0.007258, 0.006832,\n        0.007073, 0.013864, 0.006888, 0.014245, 0.007011, 0.00679 ,\n        0.006977, 0.007066, 0.006765, 0.006908, 0.007473, 0.007034,\n        0.006873, 0.006874, 0.013806, 0.007006, 0.006817, 0.007233,\n        0.007077, 0.006927, 0.003471, 0.007108, 0.006961, 0.003607,\n        0.00342 , 0.007311, 0.007643, 0.007052, 0.003667, 0.006878,\n        0.006834, 0.01399 , 0.007092, 0.006928, 0.006988, 0.00713 ,\n        0.013866, 0.006749, 0.007082, 0.01616 , 0.018628, 0.014128,\n        0.003444, 0.007936, 0.007447, 0.003499, 0.007623, 0.007016,\n        0.006984, 0.007011, 0.006881, 0.003422, 0.006825, 0.003356,\n        0.003578, 0.0068  , 0.006844, 0.006915, 0.006859, 0.006842,\n        0.007191, 0.007008, 0.007175, 0.003538, 0.007235, 0.001863,\n        0.014398, 0.007043, 0.006702, 0.003523, 0.006997, 0.006868,\n        0.00699 , 0.006552, 0.006959, 0.006905, 0.006663, 0.006988,\n        0.007154, 0.006981, 0.007019, 0.003632, 0.003535, 0.006824,\n        0.007241, 0.003487, 0.003444, 0.006584, 0.007096, 0.007086,\n        0.007128, 0.00724 , 0.003626, 0.003544, 0.014085, 0.007519,\n        0.003599, 0.006983, 0.007102, 0.006724, 0.007087, 0.007109,\n...\n        0.006968, 0.006963, 0.006791, 0.007116, 0.006961, 0.003613,\n        0.007127, 0.007145, 0.006926, 0.003695, 0.003342, 0.006826,\n        0.00707 , 0.007013, 0.00689 , 0.006895, 0.006787, 0.006977,\n        0.00337 , 0.00683 , 0.014046, 0.006677, 0.003612, 0.014373,\n        0.007029, 0.003508, 0.0036  , 0.006907, 0.006753, 0.003417,\n        0.006795, 0.006779, 0.006795, 0.003359, 0.006779, 0.006767,\n        0.006918, 0.003497, 0.00717 , 0.007005, 0.007146, 0.006803,\n        0.007339, 0.007003, 0.007124, 0.007064, 0.006974, 0.007186,\n        0.006819, 0.006836, 0.006781, 0.006708, 0.007216, 0.007346,\n        0.006784, 0.003723, 0.007017, 0.003555, 0.003432, 0.010529,\n        0.007139, 0.003445, 0.006955, 0.006926, 0.006689, 0.006742,\n        0.006654, 0.006898, 0.00713 , 0.007349, 0.003553, 0.003678,\n        0.007055, 0.007116, 0.006923, 0.00708 , 0.006582, 0.007043,\n        0.007039, 0.00676 , 0.007015, 0.006936, 0.006957, 0.00726 ,\n        0.007157, 0.007052, 0.00713 , 0.014832, 0.012372, 0.003557,\n        0.010193, 0.008669, 0.006869, 0.007074, 0.006962, 0.007026,\n        0.006737, 0.003417, 0.00688 , 0.006787, 0.00688 , 0.006737,\n        0.006661, 0.00665 , 0.006965, 0.007239, 0.007077, 0.003446,\n        0.007146, 0.003392, 0.007033, 0.007189, 0.003437, 0.005275,\n        0.006966, 0.006782]])</pre></li><li>n_steps(chain, draw)float647.0 7.0 7.0 3.0 ... 3.0 5.0 7.0 7.0<pre>array([[ 7.,  7.,  7.,  3.,  7.,  7.,  3.,  7.,  7.,  7.,  3.,  7.,  7.,\n         7.,  3.,  7.,  7.,  7.,  7., 15.,  7., 15.,  7.,  7.,  7.,  7.,\n         7.,  7.,  7.,  7.,  7.,  7., 15.,  7.,  7.,  7.,  7.,  7.,  3.,\n         7.,  7.,  3.,  3.,  7.,  7.,  7.,  3.,  7.,  7., 15.,  7.,  7.,\n         7.,  7., 15.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,  7.,  3.,  7.,\n         7.,  7.,  7.,  7.,  3.,  7.,  3.,  3.,  7.,  7.,  7.,  7.,  7.,\n         7.,  7.,  7.,  3.,  7.,  1., 15.,  7.,  7.,  3.,  7.,  7.,  7.,\n         7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,  3.,  7.,  7.,  3.,\n         3.,  7.,  7.,  7.,  7.,  7.,  3.,  3., 15.,  7.,  3.,  7.,  7.,\n         7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n         7.,  7.,  7.,  7.,  7.,  3.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n         7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,  7.,  7.,  7.,\n         7.,  7.,  7.,  7.,  3.,  7.,  7.,  7.,  7.,  1.,  3.,  7.,  7.,\n         3.,  7.,  7.,  7., 15.,  7.,  7., 15.,  3.,  7.,  3.,  7.,  7.,\n         7.,  7.,  7.,  3.,  7.,  7.,  7.,  7.,  3.,  7.,  7.,  7., 15.,\n         7.,  7.,  7.,  7.,  3.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,  3.,\n         7.,  7.,  3., 15.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,  7.,\n         7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,  7.,  7.,  7.,\n         3.,  7.,  3.,  7.,  3.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n         7.,  7.,  7.,  3.,  3.,  7.,  3.,  7.,  7.,  7.,  7.,  7.,  7.,\n...\n         7.,  3., 15.,  3.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n         7.,  7.,  7.,  7.,  7., 15.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n         7.,  7.,  3.,  7.,  3.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n         7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,\n         7.,  7.,  7.,  7.,  3.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n         3.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,  7.,  7.,  7.,  7.,\n         7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n         3.,  3.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n         7.,  7.,  7.,  7.,  3.,  7.,  7.,  3.,  7.,  7.,  3.,  7.,  7.,\n         7.,  3.,  7.,  3.,  7.,  3.,  7., 15.,  7.,  7.,  7.,  7.,  3.,\n         7.,  7.,  7.,  7.,  3.,  3.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,\n         7.,  7.,  7.,  3.,  3.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,\n         7., 15.,  7.,  3., 15.,  7.,  3.,  3.,  7.,  7.,  3.,  7.,  7.,\n         7.,  3.,  7.,  7.,  7.,  3.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n         7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,  3.,\n         3., 11.,  7.,  3.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,\n         3.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n         7.,  7.,  7., 11.,  7.,  3.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n         3.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,  7.,  3.,\n         7.,  7.,  3.,  5.,  7.,  7.]])</pre></li><li>smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n...\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan]])</pre></li><li>acceptance_rate(chain, draw)float641.0 1.0 0.5844 ... 0.8678 0.5956<pre>array([[1.        , 1.        , 0.58439642, 0.84501956, 0.89288138,\n        0.98761858, 0.92216764, 0.812281  , 0.89949614, 0.83161425,\n        1.        , 0.99673638, 0.99774229, 0.9138023 , 0.42072685,\n        0.92711509, 0.9970954 , 0.24779813, 0.9313653 , 0.53232242,\n        1.        , 0.84160854, 0.90356526, 0.95074073, 0.98811558,\n        0.62969138, 0.56737085, 0.85587354, 0.85281289, 0.94265068,\n        0.65705268, 0.79269937, 0.96879741, 0.63221624, 0.49538383,\n        0.52569683, 0.90678586, 0.90730417, 0.37787437, 0.99789447,\n        0.73118088, 1.        , 0.86225877, 0.93451112, 0.91033017,\n        0.58628071, 0.8502042 , 0.46028174, 0.98876526, 0.98962283,\n        1.        , 0.93186492, 0.89021882, 0.9755112 , 0.97397314,\n        0.96916759, 0.96424519, 0.95612718, 0.99135773, 0.83063825,\n        0.72788031, 0.71444167, 0.67199312, 0.83430596, 0.91058279,\n        0.89535225, 0.82678605, 1.        , 0.96950936, 0.26076728,\n        0.96192136, 0.87525994, 1.        , 0.71075167, 1.        ,\n        0.99782826, 0.66256854, 0.90225086, 0.97139106, 0.90720622,\n        0.85547681, 1.        , 0.66423061, 0.84899463, 0.89376521,\n        0.96533754, 0.9771596 , 1.        , 0.35973621, 0.99981683,\n        0.80867469, 0.99897897, 0.79217699, 0.79627473, 0.95250049,\n        0.97694462, 0.8758808 , 0.64992238, 0.96662374, 0.98564705,\n...\n        0.5005108 , 0.54669496, 0.69032134, 0.80651033, 0.92817157,\n        0.75261958, 0.98484456, 0.97259669, 0.89761955, 0.94271893,\n        0.92574964, 0.98049669, 0.97593273, 0.95160316, 0.68676632,\n        0.61647818, 0.96288471, 0.94259238, 0.91373269, 0.8575343 ,\n        0.86137297, 0.44944262, 0.62144381, 0.79361886, 0.71748664,\n        0.94209455, 1.        , 0.98709079, 0.750216  , 0.78831153,\n        0.4088036 , 0.77345348, 0.75602525, 1.        , 0.94831088,\n        0.6567654 , 0.99559453, 0.85870513, 0.73782586, 0.60767431,\n        0.89745939, 0.7067846 , 0.95032302, 0.85922476, 0.9989838 ,\n        0.93489755, 0.99289574, 1.        , 0.89081419, 0.95606818,\n        0.72680362, 0.89795384, 0.82349671, 0.6523292 , 0.95365113,\n        0.41384059, 0.97002695, 1.        , 0.85823838, 0.98644042,\n        0.77453242, 0.88626568, 0.74842216, 0.95293067, 0.95268331,\n        0.78185305, 0.91419989, 0.58883309, 0.58627056, 0.90715242,\n        0.94297381, 0.79947811, 0.45161554, 0.86506602, 0.95941138,\n        0.95568786, 0.93938193, 0.98113389, 0.43034803, 0.93939226,\n        0.83261202, 0.80569766, 0.78921601, 0.9520005 , 0.59123238,\n        0.97791781, 0.95552484, 0.98183664, 0.50321324, 1.        ,\n        0.81942742, 0.8812699 , 0.98147293, 0.59202048, 0.9064563 ,\n        1.        , 0.92353111, 0.93262819, 0.86777182, 0.59560969]])</pre></li><li>lp(chain, draw)float64-1.946e+03 ... -1.946e+03<pre>array([[-1946.40823927, -1945.0417671 , -1947.04633381, -1948.16650966,\n        -1949.37847571, -1952.60057413, -1954.24265386, -1951.51350271,\n        -1952.78766199, -1949.10037656, -1946.74078852, -1946.85901539,\n        -1945.68755983, -1945.1019685 , -1947.24623186, -1947.79919505,\n        -1945.81942404, -1950.29262627, -1947.59515127, -1947.40539816,\n        -1945.81546563, -1946.35306323, -1946.03652812, -1946.12905077,\n        -1945.06577776, -1948.73966327, -1949.14687437, -1947.43083847,\n        -1949.02575858, -1944.39284441, -1947.32593198, -1945.66045947,\n        -1945.97890219, -1948.59417703, -1946.46781039, -1950.01987361,\n        -1952.09683635, -1946.79614765, -1950.29090255, -1945.81322132,\n        -1948.47219008, -1946.23060406, -1948.11615984, -1949.23456563,\n        -1947.16416613, -1946.55689176, -1946.70170663, -1949.55875163,\n        -1947.87666226, -1946.74565025, -1946.14144929, -1946.34415815,\n        -1946.90811154, -1944.91237124, -1944.83683235, -1944.7405743 ,\n        -1945.00137717, -1945.36006652, -1945.020745  , -1944.47761062,\n        -1948.02625312, -1946.0081073 , -1946.06215974, -1944.66937938,\n        -1946.48897741, -1947.4500758 , -1951.1066808 , -1950.20488722,\n        -1946.75923961, -1947.92323933, -1948.08344589, -1951.47544599,\n        -1946.63297725, -1947.56551633, -1947.56337394, -1945.33012654,\n        -1945.867796  , -1945.69204162, -1946.01735061, -1946.03875082,\n...\n        -1944.26388859, -1945.48280934, -1946.73797197, -1948.40374029,\n        -1947.91061216, -1948.92421034, -1946.32209573, -1945.78844856,\n        -1944.93763829, -1948.46439533, -1947.57332672, -1944.34277308,\n        -1945.27467713, -1944.87657064, -1944.68791945, -1948.60151297,\n        -1946.09582892, -1947.36540203, -1950.46754024, -1950.83791618,\n        -1945.83529883, -1945.83529883, -1946.66435484, -1946.99047091,\n        -1945.74315626, -1947.51834083, -1947.55758601, -1946.45407438,\n        -1949.45202087, -1947.07171391, -1951.0052516 , -1946.6386774 ,\n        -1948.26272836, -1948.92102426, -1948.02753075, -1946.65583728,\n        -1947.6398169 , -1947.97767372, -1949.80406065, -1945.93133913,\n        -1946.34859712, -1944.8970111 , -1948.38531492, -1950.58196824,\n        -1948.12546861, -1947.681941  , -1946.6355543 , -1948.01390315,\n        -1948.3951518 , -1945.29969906, -1945.44761947, -1947.09915383,\n        -1945.31565577, -1947.20832062, -1945.93303567, -1948.64816807,\n        -1947.22691481, -1945.71113297, -1947.19935719, -1946.21578182,\n        -1947.28177576, -1951.27317071, -1951.50693779, -1945.2300191 ,\n        -1946.71706729, -1946.76596191, -1946.34769051, -1945.01626322,\n        -1949.57442054, -1946.30208058, -1944.57832158, -1946.25192711,\n        -1945.45593541, -1946.23597675, -1949.33211623, -1948.43645513,\n        -1946.68466617, -1945.96677216, -1946.25784354, -1946.13157179]])</pre></li><li>reached_max_treedepth(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False]])</pre></li><li>perf_counter_diff(chain, draw)float640.006973 0.006814 ... 0.006783<pre>array([[0.00697317, 0.00681379, 0.00689054, 0.00341092, 0.0069205 ,\n        0.00688146, 0.00347858, 0.00706367, 0.00673129, 0.00702217,\n        0.00348658, 0.007029  , 0.00709579, 0.00702787, 0.00343392,\n        0.00752317, 0.00732133, 0.00683121, 0.00707987, 0.01387475,\n        0.006891  , 0.01425804, 0.00702067, 0.00678958, 0.00698404,\n        0.0070725 , 0.00676529, 0.00691408, 0.007607  , 0.00704992,\n        0.00688017, 0.00687392, 0.01381613, 0.00701825, 0.00681758,\n        0.00723979, 0.00708625, 0.00692613, 0.00349492, 0.007109  ,\n        0.00696125, 0.00361192, 0.00341996, 0.00759029, 0.00774462,\n        0.00706662, 0.00367479, 0.00688629, 0.00683404, 0.01399442,\n        0.007099  , 0.00693946, 0.00698842, 0.0071385 , 0.01387238,\n        0.00674917, 0.00708929, 0.03760804, 0.027354  , 0.020415  ,\n        0.00344471, 0.00811637, 0.00848196, 0.00349892, 0.00783762,\n        0.00701588, 0.00699187, 0.00702042, 0.00688138, 0.0034205 ,\n        0.00683617, 0.00335633, 0.00358492, 0.00680408, 0.00684467,\n        0.00692371, 0.00685879, 0.006842  , 0.0071975 , 0.007015  ,\n        0.0072025 , 0.00358113, 0.00725442, 0.00186275, 0.01440242,\n        0.0070615 , 0.00670912, 0.00352254, 0.00701583, 0.00686721,\n        0.00700271, 0.00655242, 0.00696517, 0.00691692, 0.00666354,\n        0.00700729, 0.00722217, 0.00699467, 0.00702571, 0.00364417,\n...\n        0.00678846, 0.00699112, 0.00337037, 0.0068305 , 0.01407521,\n        0.00667667, 0.0036195 , 0.01437529, 0.0070295 , 0.00350813,\n        0.00360275, 0.00690721, 0.00675321, 0.00341825, 0.00679433,\n        0.00677958, 0.00679537, 0.00335838, 0.00677971, 0.00677829,\n        0.00692608, 0.00350717, 0.00717754, 0.00700454, 0.00715275,\n        0.00680925, 0.00734625, 0.00700229, 0.007125  , 0.00707075,\n        0.00697325, 0.00719   , 0.006819  , 0.00683804, 0.0067815 ,\n        0.00670858, 0.00723787, 0.00817529, 0.00678338, 0.00372979,\n        0.00701788, 0.00355912, 0.00343325, 0.01053671, 0.00715504,\n        0.00344579, 0.00695704, 0.00692696, 0.00668858, 0.00674292,\n        0.00665396, 0.00689992, 0.00713046, 0.00757812, 0.00356967,\n        0.00368508, 0.00705571, 0.00711625, 0.00692333, 0.00709092,\n        0.00658192, 0.00705129, 0.00704325, 0.00676004, 0.00701658,\n        0.006936  , 0.00696429, 0.00726063, 0.00715683, 0.00707279,\n        0.00713796, 0.03522179, 0.05217004, 0.00355646, 0.01195571,\n        0.01032429, 0.006879  , 0.00707442, 0.00696996, 0.00705212,\n        0.00673717, 0.00341696, 0.00688308, 0.00678658, 0.00688287,\n        0.00673821, 0.00666146, 0.00664983, 0.00697679, 0.00724942,\n        0.00708492, 0.00344517, 0.00715458, 0.00339163, 0.00703963,\n        0.00720042, 0.00343783, 0.0052745 , 0.00697071, 0.00678308]])</pre></li><li>index_in_trajectory(chain, draw)int64-6 2 -6 3 -1 6 ... 2 -5 -1 1 -1 -2<pre>array([[-6,  2, -6,  3, -1,  6,  1,  3, -2,  6, -1,  7, -4, -2, -1, -3,\n         5,  4, -6,  4,  6,  7,  7,  4, -4,  5, -2,  7,  4,  3,  3,  5,\n        -7,  6,  5,  2,  5,  5, -2, -2, -6, -2, -1, -4, -4,  4,  3,  4,\n        -4, -6,  5,  3,  5,  2, -2,  3, -2,  2,  5, -5, -2, -2, -2, -1,\n        -3, -4, -5, -2,  2,  2, -6,  2, -2, -4,  7,  2,  5,  1, -4,  1,\n         6, -1, -3, -1,  3, -5,  3,  2,  2,  6, -4, -4,  3,  2, -7, -3,\n         2,  5, -7,  2,  2, -1, -7, -1, -2, -5,  3, -6, -3, -6,  0,  2,\n        -5, -1, -2, -2, -4,  1, -3, -1,  3,  5,  5,  2, -3,  5,  1,  7,\n         5,  4, -4, -5, -4, -3,  5,  2,  6, -4,  3, -7, -6,  2, -1,  3,\n        -3, -5,  7, -6,  5, -4,  4, -1,  1,  2,  2,  6,  3, -1,  5,  4,\n        -1,  2, -2,  1, -4,  0, -1,  2, -5,  1,  5, -1, -5, -5, -6,  3,\n        -7,  3,  3, -3,  5,  2,  3, -2, -1, -2, -2, -6, -4,  3, -2, -3,\n         3,  5, -2,  5, -1, -2, -4, -3,  2, -4, -2, -6,  2, -1,  3,  3,\n         5,  2,  0, -3,  4, -6,  1, -5,  5, -5, -2, -1,  5, -5,  2, -5,\n        -3, -1,  2, -2,  3, -2,  3, -3, -6, -4,  2,  1, -2, -5, -1,  2,\n         2,  2, -1, -1, -2, -2,  5, -6, -6, -1,  3, -2, -4,  3,  5,  1,\n         0, -6,  2,  4,  7, -3,  6, -2,  2,  1, -6, -1,  4,  7, -1,  1,\n         7, -2, -7, -4,  3,  2,  2,  3, -7, -6, -5, -3,  7,  3, -7, -4,\n        -5,  4, -3,  3, -2, -6, -2,  1,  1, -4, -2,  2,  1, -1,  4,  2,\n        -2,  3,  3,  1, -1,  4,  3, -3, -4, -2, -3,  2,  4, -6, -2,  3,\n...\n         7, -2, -2,  3, -5, -5, -3,  1,  4,  3, -6, -6, -2, -4, -1, -3,\n        -4,  7,  2, -1,  5,  6, -2,  5,  3, -2, -5,  0, -5,  4,  1,  0,\n        -3,  2,  1, -3, -5, -2, -2, -7, -4,  5,  2, -5,  3,  2, -2, -4,\n        -3,  5, -3,  4, -2, -5,  5, -5,  2, -4, -2, -3, -6, -5, -1,  5,\n        -3, -2, -3, -2, -2,  4, -6,  5, -2, -2,  3, -5, -3, -6, -2, -1,\n         3,  1, -2,  1, -2,  3,  4, -2, -2,  4, -3,  4,  5, -3, -5, -4,\n         1,  2, -6, -6, -4,  3, -7,  3,  4,  1, -5, -2, -2,  6, -1,  2,\n        -3, -2,  4, -4, -1,  4,  1,  4, -2,  3, -4,  4,  5, -1, -4,  3,\n        -4, -4, -4,  7, -7,  2,  2,  5, -4,  7,  3,  4, -4, -6,  5, -3,\n         1,  6, -1, -1, -1, -3,  4, -4, -5, -4,  2,  5, -1,  5, -1, -2,\n        -5, -5,  4, -1, -5, -1, -3,  2, -6,  1,  4, -4, -6, -3, -6, -2,\n         3,  2, -2, -2,  3, -7, -6,  6, -3, -2, -2, -4,  5, -1, -3, -4,\n         2, -1, -1,  3, -3,  0,  5,  6,  3, -2,  3,  5, -5, -2, -2,  3,\n        -2,  4,  1,  4, -4,  4,  1,  4, -3,  2, -1, -3, -3,  3,  2,  6,\n        -4, -3, -6,  4, -5,  2, -4, -3, -3, -2,  3, -1,  3, -4,  2, -2,\n         5,  4, -5,  2,  3, -6,  5,  1, -2,  0, -1,  4,  3, -2, -4, -5,\n         2, -2, -2,  2,  6, -7,  1, -1,  4,  1, -4,  3,  1,  1,  3,  6,\n         2, -5,  5, -5,  3, -3, -2, -7,  1,  3, -4,  4,  2,  4, -3,  3,\n        -6,  3, -3,  2, -4,  6,  5,  6,  2,  2, -5,  2,  3,  3,  2, -5,\n        -1,  1, -1, -2]])</pre></li><li>max_energy_error(chain, draw)float64-1.105 -0.4064 ... 0.3808 1.978<pre>array([[-1.10531339, -0.40639154,  1.18214434, -1.14858784,  0.35783586,\n        -0.38726369,  0.21409536,  0.45956015, -1.1589164 , -0.97429144,\n        -0.67811329, -0.32953321, -0.21913475,  0.19453939,  1.15252309,\n        -0.55542819, -0.53162366,  2.54294575, -0.98058772,  0.91295703,\n        -0.51356306,  0.55052499,  0.50385967,  0.26420806,  0.07624281,\n         0.78959324,  1.14212775, -1.10484791,  0.46531863, -0.95146941,\n         0.6805998 ,  0.4821373 , -0.12344309,  0.66075038,  2.24115628,\n         1.56181328, -0.40819313, -0.83302547,  4.31756757, -0.63506565,\n         0.63220423, -0.26944443,  0.17619595,  0.14913545,  0.24281866,\n         1.60436276,  0.5967238 ,  1.27853273, -0.32514703, -0.78769336,\n        -0.37733437,  0.21237073,  0.22769871, -0.32011541,  0.08516305,\n         0.10105026,  0.12105945,  0.20619205, -0.14205868,  0.36526071,\n         0.5632268 ,  1.36304397,  1.12311922,  0.62226893,  0.25185768,\n         0.28484554,  0.76534847, -1.20261239, -0.75780602,  3.40844634,\n         0.23562385, -0.72116412, -1.04260742,  0.83885424, -0.41947927,\n        -0.37096461,  0.7038976 ,  0.34154344,  0.07795583,  0.26762531,\n         0.35393033, -0.42600674,  0.85958265,  0.16370242,  0.96960985,\n        -1.03078102, -0.80458647, -0.29504153,  4.52335069, -0.22756536,\n         0.58563764, -0.27356832,  0.41771638,  0.65370964,  0.38301317,\n        -0.18659411,  0.62758245,  0.91404888, -0.56515552, -0.74032071,\n...\n         1.97383222,  0.98209879,  0.61220612,  0.67693651, -0.54339246,\n         0.46698816, -0.40988083, -0.23386563,  0.40849336,  0.13965013,\n         0.12245641,  0.11638696, -0.24943156,  0.15687668,  0.80487332,\n         1.02648504, -0.85267801,  0.10226633,  0.21460175,  0.24898849,\n         0.3663674 ,  1.25600234,  1.09741137,  0.73300826,  0.72016976,\n        -0.20543322, -0.40259931,  0.05195105,  0.59611852,  0.38776301,\n         2.34228852,  0.72013197,  0.64718853, -0.22655155,  0.14138315,\n         0.5720119 , -0.35293506,  0.38768344,  0.8891894 ,  0.92966124,\n        -1.00203981,  0.53137245,  0.16137952,  0.47041842, -0.39724291,\n         0.16081079, -0.29931226, -0.20968069,  0.47508445, -0.66243751,\n         0.50828622, -0.91464228,  0.46420388,  2.32440356,  0.14270205,\n         1.29041181,  0.14756929, -0.40012798,  0.40127716, -0.29406558,\n         0.50785663,  0.22588785,  0.55346105,  0.20871039,  0.20184811,\n         0.42007077,  0.29626313,  1.45121416,  1.69626414, -0.31692583,\n         0.19057487,  0.5420776 ,  1.59523072,  0.28617129, -0.4207276 ,\n        -0.33968278, -0.13444363, -0.17355756,  1.81293207, -0.40842645,\n         0.49179045,  0.45985142,  2.06409993, -0.41367562,  1.04097472,\n         0.1541195 , -0.16944699, -0.32573743,  1.52967275, -0.61478928,\n         0.40395558,  0.19648697, -0.07556993,  0.92077098, -0.64237689,\n        -0.90927029, -0.64630767, -0.35478492,  0.38082671,  1.97763148]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-29T19:52:06.577971+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :38.77959203720093tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 24kB\nDimensions:                  (__obs__: 1000, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 8kB 0 1 2 3 4 ... 996 997 998 999\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 16kB ...\nAttributes:\n    created_at:                  2025-09-29T19:52:06.580600+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 1000</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float649.994 -1.0 1.055 ... 0.5972 -1.0<pre>array([[ 9.99363422, -1.        ],\n       [ 1.05531836,  1.        ],\n       [ 2.44326901, -1.        ],\n       ...,\n       [ 1.18518233,  1.        ],\n       [ 1.41290545, -1.        ],\n       [ 0.59721935, -1.        ]], shape=(1000, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (6)created_at :2025-09-29T19:52:06.580600+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[59]: Copied! <pre># az.plot_forest(model_reg_v.traces)\n</pre> # az.plot_forest(model_reg_v.traces) In\u00a0[60]: Copied! <pre>az.plot_trace(\n    model_reg_v.traces,\n    var_names=[\"~v\"],\n    lines=[(key_, {}, param_dict_reg_v[key_]) for key_ in param_dict_reg_v],\n)\n</pre> az.plot_trace(     model_reg_v.traces,     var_names=[\"~v\"],     lines=[(key_, {}, param_dict_reg_v[key_]) for key_ in param_dict_reg_v], ) Out[60]: <pre>array([[&lt;Axes: title={'center': 'v_x'}&gt;, &lt;Axes: title={'center': 'v_x'}&gt;],\n       [&lt;Axes: title={'center': 'v_Intercept'}&gt;,\n        &lt;Axes: title={'center': 'v_Intercept'}&gt;],\n       [&lt;Axes: title={'center': 'z'}&gt;, &lt;Axes: title={'center': 'z'}&gt;],\n       [&lt;Axes: title={'center': 'v_y'}&gt;, &lt;Axes: title={'center': 'v_y'}&gt;],\n       [&lt;Axes: title={'center': 'a'}&gt;, &lt;Axes: title={'center': 'a'}&gt;],\n       [&lt;Axes: title={'center': 't'}&gt;, &lt;Axes: title={'center': 't'}&gt;]],\n      dtype=object)</pre> In\u00a0[61]: Copied! <pre>az.plot_trace(\n    model_reg_v.traces,\n    var_names=[\"~v\"],\n    lines=[(key_, {}, param_dict_reg_v[key_]) for key_ in param_dict_reg_v],\n)\nplt.tight_layout()\n</pre> az.plot_trace(     model_reg_v.traces,     var_names=[\"~v\"],     lines=[(key_, {}, param_dict_reg_v[key_]) for key_ in param_dict_reg_v], ) plt.tight_layout() In\u00a0[62]: Copied! <pre># Looks like parameter recovery was successful\naz.summary(model_reg_v.traces, var_names=[\"~v\"])\n</pre> # Looks like parameter recovery was successful az.summary(model_reg_v.traces, var_names=[\"~v\"]) Out[62]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat v_x 0.869 0.045 0.785 0.952 0.002 0.002 602.0 302.0 1.0 v_Intercept 0.308 0.035 0.249 0.377 0.001 0.001 769.0 664.0 1.0 z 0.510 0.013 0.487 0.534 0.000 0.000 679.0 496.0 1.0 v_y 0.379 0.046 0.301 0.472 0.001 0.002 1203.0 726.0 1.0 a 1.475 0.026 1.426 1.520 0.001 0.001 713.0 719.0 1.0 t 0.114 0.017 0.081 0.146 0.001 0.001 841.0 739.0 1.0 In\u00a0[63]: Copied! <pre>model_reg_v_angle = hssm.HSSM(\n    data=dataset_reg_v,\n    model=\"angle\",\n    include=[\n        {\n            \"name\": \"v\",\n            \"prior\": {\n                \"Intercept\": {\n                    \"name\": \"Uniform\",\n                    \"lower\": -3.0,\n                    \"upper\": 3.0,\n                },\n                \"x\": {\n                    \"name\": \"Uniform\",\n                    \"lower\": -1.0,\n                    \"upper\": 1.0,\n                },\n                \"y\": {\"name\": \"Uniform\", \"lower\": -1.0, \"upper\": 1.0},\n            },\n            \"formula\": \"v ~ 1 + x + y\",\n            \"link\": \"identity\",\n        }\n    ],\n)\n</pre> model_reg_v_angle = hssm.HSSM(     data=dataset_reg_v,     model=\"angle\",     include=[         {             \"name\": \"v\",             \"prior\": {                 \"Intercept\": {                     \"name\": \"Uniform\",                     \"lower\": -3.0,                     \"upper\": 3.0,                 },                 \"x\": {                     \"name\": \"Uniform\",                     \"lower\": -1.0,                     \"upper\": 1.0,                 },                 \"y\": {\"name\": \"Uniform\", \"lower\": -1.0, \"upper\": 1.0},             },             \"formula\": \"v ~ 1 + x + y\",             \"link\": \"identity\",         }     ], ) <pre>Model initialized successfully.\n</pre> In\u00a0[64]: Copied! <pre>model_reg_v_angle.graph()\n</pre> model_reg_v_angle.graph() <pre>max_shape:  (1000,)\nsize:  (np.int64(1000),)\n</pre> Out[64]: In\u00a0[65]: Copied! <pre>trace_reg_v_angle = model_reg_v_angle.sample(\n    sampler=\"mcmc\",\n    chains=1,\n    cores=1,\n    draws=1000,\n    tune=500,\n    mp_ctx=\"spawn\",\n)\n</pre> trace_reg_v_angle = model_reg_v_angle.sample(     sampler=\"mcmc\",     chains=1,     cores=1,     draws=1000,     tune=500,     mp_ctx=\"spawn\", ) <pre>Using default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nSequential sampling (1 chains in 1 job)\nNUTS: [a, z, theta, t, v_Intercept, v_x, v_y]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 1 chain for 500 tune and 1_000 draw iterations (500 + 1_000 draws total) took 38 seconds.\nOnly one chain was sampled, this makes it impossible to run some convergence checks\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00&lt;00:00, 1957.82it/s]\n</pre> In\u00a0[66]: Copied! <pre>az.plot_trace(\n    model_reg_v_angle.traces,\n    var_names=[\"~v\"],\n    lines=[(key_, {}, param_dict_reg_v[key_]) for key_ in param_dict_reg_v],\n)\nplt.tight_layout()\n</pre> az.plot_trace(     model_reg_v_angle.traces,     var_names=[\"~v\"],     lines=[(key_, {}, param_dict_reg_v[key_]) for key_ in param_dict_reg_v], ) plt.tight_layout() <p>Great! <code>theta</code> is recovered correctly, on top of that, we have reasonable recovery for all other parameters!</p> <p>Let's get a bit more ambitious. We may, for example, want to try a regression on a few of our basic model parameters at once. Below we show an example where we model both the <code>a</code> and the <code>v</code> parameters with a regression.</p> <p>NOTE:</p> <p>In our dataset of this section, only <code>v</code> is actually driven by a trial-by-trial regression, so we expect the regression coefficients for <code>a</code> to hover around $0$ in our posterior.</p> In\u00a0[67]: Copied! <pre># Instantiate our hssm model\nfrom copy import deepcopy\n\nparam_dict_reg_v_a = deepcopy(param_dict_reg_v)\nparam_dict_reg_v_a[\"a_Intercept\"] = param_dict_reg_v_a[\"a\"]\nparam_dict_reg_v_a[\"a_x\"] = 0\nparam_dict_reg_v_a[\"a_y\"] = 0\n\nhssm_reg_v_a_angle = hssm.HSSM(\n    data=dataset_reg_v,\n    model=\"angle\",\n    include=[\n        {\n            \"name\": \"v\",\n            \"prior\": {\n                \"Intercept\": {\"name\": \"Uniform\", \"lower\": -3.0, \"upper\": 3.0},\n                \"x\": {\"name\": \"Uniform\", \"lower\": -1.0, \"upper\": 1.0},\n                \"y\": {\"name\": \"Uniform\", \"lower\": -1.0, \"upper\": 1.0},\n            },\n            \"formula\": \"v ~ 1 + x + y\",\n        },\n        {\n            \"name\": \"a\",\n            \"prior\": {\n                \"Intercept\": {\"name\": \"Uniform\", \"lower\": 0.5, \"upper\": 3.0},\n                \"x\": {\"name\": \"Uniform\", \"lower\": -1.0, \"upper\": 1.0},\n                \"y\": {\"name\": \"Uniform\", \"lower\": -1.0, \"upper\": 1.0},\n            },\n            \"formula\": \"a ~ 1 + x + y\",\n        },\n    ],\n)\n</pre> # Instantiate our hssm model from copy import deepcopy  param_dict_reg_v_a = deepcopy(param_dict_reg_v) param_dict_reg_v_a[\"a_Intercept\"] = param_dict_reg_v_a[\"a\"] param_dict_reg_v_a[\"a_x\"] = 0 param_dict_reg_v_a[\"a_y\"] = 0  hssm_reg_v_a_angle = hssm.HSSM(     data=dataset_reg_v,     model=\"angle\",     include=[         {             \"name\": \"v\",             \"prior\": {                 \"Intercept\": {\"name\": \"Uniform\", \"lower\": -3.0, \"upper\": 3.0},                 \"x\": {\"name\": \"Uniform\", \"lower\": -1.0, \"upper\": 1.0},                 \"y\": {\"name\": \"Uniform\", \"lower\": -1.0, \"upper\": 1.0},             },             \"formula\": \"v ~ 1 + x + y\",         },         {             \"name\": \"a\",             \"prior\": {                 \"Intercept\": {\"name\": \"Uniform\", \"lower\": 0.5, \"upper\": 3.0},                 \"x\": {\"name\": \"Uniform\", \"lower\": -1.0, \"upper\": 1.0},                 \"y\": {\"name\": \"Uniform\", \"lower\": -1.0, \"upper\": 1.0},             },             \"formula\": \"a ~ 1 + x + y\",         },     ], ) <pre>Model initialized successfully.\n</pre> In\u00a0[68]: Copied! <pre>hssm_reg_v_a_angle\n</pre> hssm_reg_v_a_angle Out[68]: <pre>Hierarchical Sequential Sampling Model\nModel: angle\n\nResponse variable: rt,response\nLikelihood: approx_differentiable\nObservations: 1000\n\nParameters:\n\nv:\n    Formula: v ~ 1 + x + y\n    Priors:\n        v_Intercept ~ Uniform(lower: -3.0, upper: 3.0)\n        v_x ~ Uniform(lower: -1.0, upper: 1.0)\n        v_y ~ Uniform(lower: -1.0, upper: 1.0)\n    Link: identity\n    Explicit bounds: (-3.0, 3.0)\n\na:\n    Formula: a ~ 1 + x + y\n    Priors:\n        a_Intercept ~ Uniform(lower: 0.5, upper: 3.0)\n        a_x ~ Uniform(lower: -1.0, upper: 1.0)\n        a_y ~ Uniform(lower: -1.0, upper: 1.0)\n    Link: identity\n    Explicit bounds: (0.3, 3.0)\n\nz:\n    Prior: Uniform(lower: 0.1, upper: 0.9)\n    Explicit bounds: (0.1, 0.9)\n\nt:\n    Prior: Uniform(lower: 0.001, upper: 2.0)\n    Explicit bounds: (0.001, 2.0)\n\ntheta:\n    Prior: Uniform(lower: -0.1, upper: 1.3)\n    Explicit bounds: (-0.1, 1.3)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[69]: Copied! <pre>hssm_reg_v_a_angle.graph()\n</pre> hssm_reg_v_a_angle.graph() <pre>max_shape:  (1000,)\nsize:  (np.int64(1000),)\n</pre> Out[69]: In\u00a0[70]: Copied! <pre>infer_data_reg_v_a = hssm_reg_v_a_angle.sample(\n    sampler=\"mcmc\",\n    chains=2,\n    cores=1,\n    draws=1000,\n    tune=1000,\n    mp_ctx=\"spawn\",\n)\n</pre> infer_data_reg_v_a = hssm_reg_v_a_angle.sample(     sampler=\"mcmc\",     chains=2,     cores=1,     draws=1000,     tune=1000,     mp_ctx=\"spawn\", ) <pre>Using default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nSequential sampling (2 chains in 1 job)\nNUTS: [z, theta, t, v_Intercept, v_x, v_y, a_Intercept, a_x, a_y]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 116 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:00&lt;00:00, 2143.22it/s]\n</pre> In\u00a0[71]: Copied! <pre>az.summary(\n    infer_data_reg_v_a, var_names=[\"~a\", \"~v\"]\n)  # , var_names=[\"~rt,response_a\"])\n</pre> az.summary(     infer_data_reg_v_a, var_names=[\"~a\", \"~v\"] )  # , var_names=[\"~rt,response_a\"]) Out[71]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat v_x 0.865 0.048 0.775 0.953 0.001 0.001 1484.0 556.0 1.00 a_y 0.100 0.039 0.030 0.175 0.001 0.001 1949.0 1487.0 1.00 v_Intercept 0.291 0.034 0.225 0.349 0.001 0.001 1733.0 1351.0 1.00 z 0.523 0.013 0.500 0.547 0.000 0.000 1692.0 1590.0 1.00 a_x 0.039 0.043 -0.040 0.119 0.001 0.001 1828.0 1298.0 1.00 v_y 0.394 0.048 0.303 0.484 0.001 0.001 1767.0 1204.0 1.00 a_Intercept 1.591 0.057 1.483 1.699 0.003 0.002 559.0 394.0 1.01 theta 0.073 0.025 0.027 0.120 0.001 0.001 692.0 612.0 1.01 t 0.075 0.027 0.018 0.120 0.001 0.001 545.0 257.0 1.00 In\u00a0[72]: Copied! <pre>az.plot_trace(\n    hssm_reg_v_a_angle.traces,\n    var_names=[\"~v\", \"~a\"],\n    lines=[(key_, {}, param_dict_reg_v_a[key_]) for key_ in param_dict_reg_v_a],\n)\nplt.tight_layout()\n</pre> az.plot_trace(     hssm_reg_v_a_angle.traces,     var_names=[\"~v\", \"~a\"],     lines=[(key_, {}, param_dict_reg_v_a[key_]) for key_ in param_dict_reg_v_a], ) plt.tight_layout() <p>We successfully recover our regression betas for <code>a</code>! Moreover, no warning signs concerning our chains.</p> In\u00a0[73]: Copied! <pre># Set up trial by trial parameters\nx = np.random.choice(4, size=1000).astype(int)\nx_offset = np.array([0, 1, -0.5, 0.75])\n\ny = np.random.uniform(-1, 1, size=1000)\nv_y = 0.3\nv_reg_v = 0 + (v_y * y) + x_offset[x]\n\n# rest\na_reg_v = 1.5\nz_reg_v = 0.5\nt_reg_v = 0.1\n\n# base dataset\ndataset_reg_v_cat = hssm.simulate_data(\n    model=\"ddm\", theta=dict(v=v_reg_v, a=a_reg_v, z=z_reg_v, t=t_reg_v), size=1\n)\n\n# Adding covariates into the datsaframe\ndataset_reg_v_cat[\"x\"] = x\ndataset_reg_v_cat[\"y\"] = y\n</pre> # Set up trial by trial parameters x = np.random.choice(4, size=1000).astype(int) x_offset = np.array([0, 1, -0.5, 0.75])  y = np.random.uniform(-1, 1, size=1000) v_y = 0.3 v_reg_v = 0 + (v_y * y) + x_offset[x]  # rest a_reg_v = 1.5 z_reg_v = 0.5 t_reg_v = 0.1  # base dataset dataset_reg_v_cat = hssm.simulate_data(     model=\"ddm\", theta=dict(v=v_reg_v, a=a_reg_v, z=z_reg_v, t=t_reg_v), size=1 )  # Adding covariates into the datsaframe dataset_reg_v_cat[\"x\"] = x dataset_reg_v_cat[\"y\"] = y In\u00a0[74]: Copied! <pre>model_reg_v_cat = hssm.HSSM(\n    data=dataset_reg_v_cat,\n    model=\"angle\",\n    include=[\n        {\n            \"name\": \"v\",\n            \"formula\": \"v ~ 0 + C(x) + y\",\n            \"link\": \"identity\",\n        }\n    ],\n)\n</pre> model_reg_v_cat = hssm.HSSM(     data=dataset_reg_v_cat,     model=\"angle\",     include=[         {             \"name\": \"v\",             \"formula\": \"v ~ 0 + C(x) + y\",             \"link\": \"identity\",         }     ], ) <pre>Model initialized successfully.\n</pre> In\u00a0[75]: Copied! <pre>model_reg_v_cat.graph()\n</pre> model_reg_v_cat.graph() <pre>max_shape:  (1000,)\nsize:  (np.int64(1000),)\n</pre> Out[75]: In\u00a0[76]: Copied! <pre>infer_data_reg_v_cat = model_reg_v_cat.sample(\n    sampler=\"mcmc\",\n    chains=2,\n    cores=1,\n    draws=1000,\n    tune=500,\n    mp_ctx=\"spawn\",\n)\n</pre> infer_data_reg_v_cat = model_reg_v_cat.sample(     sampler=\"mcmc\",     chains=2,     cores=1,     draws=1000,     tune=500,     mp_ctx=\"spawn\", ) <pre>Using default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nSequential sampling (2 chains in 1 job)\nNUTS: [a, z, theta, t, v_C(x), v_y]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 2 chains for 500 tune and 1_000 draw iterations (1_000 + 2_000 draws total) took 61 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:00&lt;00:00, 2094.22it/s]\n</pre> In\u00a0[77]: Copied! <pre>az.plot_forest(infer_data_reg_v_cat, var_names=[\"~v\"])\n</pre> az.plot_forest(infer_data_reg_v_cat, var_names=[\"~v\"]) Out[77]: <pre>array([&lt;Axes: title={'center': '94.0% HDI'}&gt;], dtype=object)</pre> <p>Let's try to fit a hierarchical model now. We will simulate a dataset with $15$ participants, with $200$ observations / trials for each participant.</p> <p>We define a group mean <code>mean_v</code> and a group standard deviation <code>sd_v</code> for the intercept parameter of the regression on <code>v</code>, which we sample from a corresponding normal distribution for each participant.</p> In\u00a0[78]: Copied! <pre># Make some hierarchical data\nn_participants = 15  # number of participants\nn_trials = 200  # number of trials per participant\n\nsd_v = 0.5  # sd for v-intercept\nmean_v = 0.5  # mean for v-intercept\n\ndata_list = []\nfor i in range(n_participants):\n    # Make parameters for participant i\n    v_intercept_hier = np.random.normal(mean_v, sd_v, size=1)\n    x = np.random.uniform(-1, 1, size=n_trials)\n    v_x_hier = 0.8\n    y = np.random.uniform(-1, 1, size=n_trials)\n    v_y_hier = 0.3\n    v_hier = v_intercept_hier + (v_x_hier * x) + (v_y_hier * y)\n\n    a_hier = 1.5\n    t_hier = 0.5\n    z_hier = 0.5\n\n    # true_values = np.column_stack(\n    #     [v, np.repeat([[1.5, 0.5, 0.5, 0.0]], axis=0, repeats=n_trials)]\n    # )\n\n    data_tmp = hssm.simulate_data(\n        model=\"ddm\", theta=dict(v=v_hier, a=a_hier, z=z_hier, t=t_hier), size=1\n    )\n    data_tmp[\"participant_id\"] = i\n    data_tmp[\"x\"] = x\n    data_tmp[\"y\"] = y\n\n    data_list.append(data_tmp)\n\n# Make single dataframe out of participant-wise datasets\ndataset_reg_v_hier = pd.concat(data_list)\ndataset_reg_v_hier\n</pre> # Make some hierarchical data n_participants = 15  # number of participants n_trials = 200  # number of trials per participant  sd_v = 0.5  # sd for v-intercept mean_v = 0.5  # mean for v-intercept  data_list = [] for i in range(n_participants):     # Make parameters for participant i     v_intercept_hier = np.random.normal(mean_v, sd_v, size=1)     x = np.random.uniform(-1, 1, size=n_trials)     v_x_hier = 0.8     y = np.random.uniform(-1, 1, size=n_trials)     v_y_hier = 0.3     v_hier = v_intercept_hier + (v_x_hier * x) + (v_y_hier * y)      a_hier = 1.5     t_hier = 0.5     z_hier = 0.5      # true_values = np.column_stack(     #     [v, np.repeat([[1.5, 0.5, 0.5, 0.0]], axis=0, repeats=n_trials)]     # )      data_tmp = hssm.simulate_data(         model=\"ddm\", theta=dict(v=v_hier, a=a_hier, z=z_hier, t=t_hier), size=1     )     data_tmp[\"participant_id\"] = i     data_tmp[\"x\"] = x     data_tmp[\"y\"] = y      data_list.append(data_tmp)  # Make single dataframe out of participant-wise datasets dataset_reg_v_hier = pd.concat(data_list) dataset_reg_v_hier Out[78]: rt response participant_id x y 0 1.117741 1.0 0 -0.535751 0.573336 1 2.871194 1.0 0 -0.158811 0.024659 2 4.013333 1.0 0 -0.556658 0.021410 3 1.424530 -1.0 0 -0.610495 -0.315694 4 0.753883 1.0 0 0.499722 0.272538 ... ... ... ... ... ... 195 1.164056 1.0 14 -0.918860 0.234293 196 2.720412 1.0 14 -0.899047 -0.707518 197 1.787280 1.0 14 0.282337 0.789865 198 0.847754 1.0 14 0.152510 -0.697267 199 0.920196 1.0 14 -0.340144 0.562373 <p>3000 rows \u00d7 5 columns</p> <p>We can now define our <code>HSSM</code> model.</p> <p>We specify the regression as <code>v ~ 1 + (1|participant_id) + x + y</code>.</p> <p><code>(1|participant_id)</code> tells the model to create a participant-wise offset for the intercept parameter. The rest of the regression $\\beta$'s is fit globally.</p> <p>As an R user you may recognize this syntax from the lmer package.</p> <p>Our Bambi backend is essentially a Bayesian version of lmer, quite like the BRMS package in R, which operates on top of STAN.</p> <p>As a previous HDDM user, you may recognize that now proper mixed-effect models are viable!</p> <p>You should be able to handle between and within participant effects naturally now!</p> In\u00a0[79]: Copied! <pre>model_reg_v_angle_hier = hssm.HSSM(\n    data=dataset_reg_v_hier,\n    model=\"angle\",\n    noncentered=True,\n    include=[\n        {\n            \"name\": \"v\",\n            \"prior\": {\n                \"Intercept\": {\n                    \"name\": \"Normal\",\n                    \"mu\": 0.0,\n                    \"sigma\": 0.5,\n                },\n                \"x\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 0.5},\n                \"y\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 0.5},\n            },\n            \"formula\": \"v ~ 1 + (1|participant_id) + x + y\",\n            \"link\": \"identity\",\n        }\n    ],\n)\n</pre> model_reg_v_angle_hier = hssm.HSSM(     data=dataset_reg_v_hier,     model=\"angle\",     noncentered=True,     include=[         {             \"name\": \"v\",             \"prior\": {                 \"Intercept\": {                     \"name\": \"Normal\",                     \"mu\": 0.0,                     \"sigma\": 0.5,                 },                 \"x\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 0.5},                 \"y\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 0.5},             },             \"formula\": \"v ~ 1 + (1|participant_id) + x + y\",             \"link\": \"identity\",         }     ], ) <pre>Model initialized successfully.\n</pre> In\u00a0[80]: Copied! <pre>model_reg_v_angle_hier.graph()\n</pre> model_reg_v_angle_hier.graph() <pre>max_shape:  (3000,)\nsize:  (np.int64(3000),)\n</pre> Out[80]: In\u00a0[81]: Copied! <pre>jax.config.update(\"jax_enable_x64\", False)\nmodel_reg_v_angle_hier.sample(\n    sampler=\"mcmc\",\n    chains=2,\n    cores=2,\n    draws=500,\n    tune=500,\n    mp_ctx=\"spawn\",\n)\n</pre> jax.config.update(\"jax_enable_x64\", False) model_reg_v_angle_hier.sample(     sampler=\"mcmc\",     chains=2,     cores=2,     draws=500,     tune=500,     mp_ctx=\"spawn\", ) <pre>Using default initvals. \n\nParallel sampling might not work with `jax` backend and the PyMC NUTS sampler on some platforms. Please consider using `nuts_numpyro` or `nuts_blackjax` sampler if that is a problem.\n</pre> <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [a, z, theta, t, v_Intercept, v_x, v_y, v_1|participant_id_sigma, v_1|participant_id_offset]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 2 chains for 500 tune and 500 draw iterations (1_000 + 1_000 draws total) took 580 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:01&lt;00:00, 931.69it/s]\n</pre> Out[81]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 308kB\nDimensions:                         (chain: 2, draw: 500,\n                                     v_1|participant_id__factor_dim: 15)\nCoordinates:\n  * chain                           (chain) int64 16B 0 1\n  * draw                            (draw) int64 4kB 0 1 2 3 ... 496 497 498 499\n  * v_1|participant_id__factor_dim  (v_1|participant_id__factor_dim) &lt;U2 120B ...\nData variables:\n    v_x                             (chain, draw) float64 8kB 0.8682 ... 0.8462\n    v_Intercept                     (chain, draw) float64 8kB 0.6753 ... 0.4235\n    z                               (chain, draw) float64 8kB 0.4833 ... 0.4874\n    theta                           (chain, draw) float64 8kB 0.0119 ... 0.05599\n    v_1|participant_id_sigma        (chain, draw) float64 8kB 0.4595 ... 0.3857\n    v_y                             (chain, draw) float64 8kB 0.2697 ... 0.2698\n    v_1|participant_id              (chain, draw, v_1|participant_id__factor_dim) float64 120kB ...\n    a                               (chain, draw) float64 8kB 1.471 ... 1.547\n    t                               (chain, draw) float64 8kB 0.5005 ... 0.4801\n    v_1|participant_id_offset       (chain, draw, v_1|participant_id__factor_dim) float64 120kB ...\nAttributes:\n    created_at:                  2025-09-29T20:05:48.538765+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               579.8671550750732\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li><li>v_1|participant_id__factor_dim: 15</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>v_1|participant_id__factor_dim(v_1|participant_id__factor_dim)&lt;U2'0' '1' '2' '3' ... '12' '13' '14'<pre>array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n       '13', '14'], dtype='&lt;U2')</pre></li></ul></li><li>Data variables: (10)<ul><li>v_x(chain, draw)float640.8682 0.8659 ... 0.8335 0.8462<pre>array([[0.8681577 , 0.86587605, 0.89747135, 0.80884319, 0.85626091,\n        0.84639013, 0.8522481 , 0.81609929, 0.829586  , 0.83519892,\n        0.84719628, 0.83335308, 0.81855478, 0.85997525, 0.79863297,\n        0.88603058, 0.85017091, 0.82552381, 0.82855649, 0.89956363,\n        0.84050666, 0.84089281, 0.80326954, 0.84544165, 0.83011405,\n        0.85607445, 0.83441451, 0.83491815, 0.85173761, 0.82366334,\n        0.82679838, 0.89378943, 0.79989404, 0.89041705, 0.81551266,\n        0.8110027 , 0.7996234 , 0.90120227, 0.85991563, 0.85418221,\n        0.84991197, 0.81317949, 0.80755632, 0.79501085, 0.84627257,\n        0.82908239, 0.81662898, 0.8428843 , 0.81782414, 0.811925  ,\n        0.84828234, 0.84678934, 0.82796416, 0.83915089, 0.86368769,\n        0.83853606, 0.89446292, 0.8560597 , 0.88704368, 0.80399494,\n        0.85260556, 0.8629601 , 0.82111304, 0.82851137, 0.85472885,\n        0.86613803, 0.84476067, 0.88922885, 0.88051203, 0.83521466,\n        0.83380965, 0.82614947, 0.91121492, 0.93184225, 0.86948929,\n        0.85478559, 0.87448644, 0.87934239, 0.84820249, 0.84149337,\n        0.84412842, 0.85173188, 0.83869488, 0.82974781, 0.83402664,\n        0.79687716, 0.85853387, 0.87396046, 0.80896509, 0.8385001 ,\n        0.7907948 , 0.77734666, 0.86350007, 0.86836938, 0.8054984 ,\n        0.80688872, 0.82414503, 0.86627414, 0.86435021, 0.80034267,\n...\n        0.80804845, 0.82429524, 0.86913168, 0.80898783, 0.78396249,\n        0.8981729 , 0.83582714, 0.84275204, 0.83840117, 0.79205918,\n        0.81604562, 0.85651631, 0.8420739 , 0.77602756, 0.90094064,\n        0.77555779, 0.84970286, 0.84767645, 0.839432  , 0.83044461,\n        0.81477922, 0.82289037, 0.86412029, 0.82834099, 0.83998359,\n        0.86989056, 0.80815108, 0.79621757, 0.80720237, 0.85438495,\n        0.84588681, 0.84072082, 0.83004126, 0.86848834, 0.78629748,\n        0.79958632, 0.8481777 , 0.83425032, 0.84140111, 0.88512632,\n        0.85506549, 0.85424742, 0.87808111, 0.85519956, 0.82950775,\n        0.82584384, 0.85613071, 0.85451896, 0.89642496, 0.81510368,\n        0.85980898, 0.87954566, 0.84607325, 0.86193689, 0.82762037,\n        0.83172064, 0.82627709, 0.84745269, 0.87338496, 0.86724289,\n        0.81793155, 0.86456161, 0.86773786, 0.85937161, 0.82126926,\n        0.89149449, 0.87365251, 0.88084067, 0.86985763, 0.90059625,\n        0.8933864 , 0.80783039, 0.85472509, 0.864017  , 0.83313416,\n        0.84935997, 0.89480397, 0.822988  , 0.82547658, 0.83172931,\n        0.82906536, 0.83720349, 0.84349823, 0.78517725, 0.76414271,\n        0.89265383, 0.84142504, 0.86540028, 0.86664042, 0.7993811 ,\n        0.8694195 , 0.81183657, 0.83005118, 0.8347962 , 0.81993274,\n        0.8673043 , 0.84652314, 0.89791926, 0.83346203, 0.84615848]])</pre></li><li>v_Intercept(chain, draw)float640.6753 0.703 ... 0.2525 0.4235<pre>array([[0.67526631, 0.70303645, 0.66030016, 0.83081636, 0.5457735 ,\n        0.53072059, 0.47546387, 0.50733059, 0.48203523, 0.77782856,\n        0.93710678, 0.78365847, 0.86318786, 0.81637257, 0.57276031,\n        0.82232096, 0.92481495, 0.85798179, 0.82460425, 0.6929572 ,\n        0.70701935, 0.59456829, 0.56575385, 0.52769672, 0.55530414,\n        0.52763808, 0.67052741, 0.65417719, 0.64119711, 0.51947   ,\n        0.51756717, 0.5135865 , 0.47564873, 0.45193428, 0.49593233,\n        0.53470906, 0.53419484, 0.69363733, 0.6569252 , 0.65992333,\n        0.65452853, 0.60100032, 0.61735342, 0.59734827, 0.5913682 ,\n        0.5598582 , 0.55005247, 0.49139648, 0.58842153, 0.61582201,\n        0.59126895, 0.5451528 , 0.55231848, 0.576429  , 0.62477058,\n        0.53304357, 0.67986725, 0.63035962, 0.66113887, 0.46960278,\n        0.35322414, 0.4264909 , 0.603022  , 0.51986968, 0.59159382,\n        0.48513188, 0.52608762, 0.48750648, 0.54818325, 0.46418004,\n        0.46580428, 0.51156478, 0.51576536, 0.5385293 , 0.51323548,\n        0.50266435, 0.44836775, 0.48386869, 0.48288089, 0.36097466,\n        0.37858281, 0.40028895, 0.53044338, 0.51553977, 0.59028339,\n        0.59414695, 0.55025454, 0.54865473, 0.50098734, 0.55898121,\n        0.562651  , 0.56646932, 0.6242037 , 0.66981149, 0.61806805,\n        0.64632174, 0.64152912, 0.60602037, 0.61777706, 0.53968606,\n...\n        0.37457157, 0.33615932, 0.66906382, 0.55670868, 0.61510687,\n        0.71826761, 0.786165  , 0.65913716, 0.67818573, 0.45884191,\n        0.43568143, 0.39371235, 0.4432289 , 0.39229817, 0.4929734 ,\n        0.53217478, 0.55985456, 0.47328762, 0.39038212, 0.47926844,\n        0.50577489, 0.59297847, 0.69323741, 0.71167312, 0.6633171 ,\n        0.66686985, 0.57375381, 0.56184801, 0.54076892, 0.56723305,\n        0.56288184, 0.29059452, 0.36824443, 0.44869445, 0.3725373 ,\n        0.50560756, 0.55171725, 0.47589709, 0.45086728, 0.50083738,\n        0.47565597, 0.41827834, 0.52688727, 0.34567856, 0.45341323,\n        0.49762571, 0.65934743, 0.68987562, 0.70836166, 0.67998619,\n        0.62175317, 0.59993445, 0.5953304 , 0.62131638, 0.62084193,\n        0.53556777, 0.53917069, 0.62434975, 0.6440255 , 0.62096402,\n        0.61224323, 0.61815355, 0.57949911, 0.59871245, 0.6041097 ,\n        0.59131114, 0.55240944, 0.57147173, 0.55407435, 0.57284508,\n        0.62242149, 0.64822322, 0.85128672, 0.86435571, 0.88161631,\n        0.81173969, 0.79949177, 0.67468506, 0.510697  , 0.50264928,\n        0.54741673, 0.53271493, 0.57217923, 0.46104703, 0.55453144,\n        0.42788786, 0.50972517, 0.56133234, 0.5107506 , 0.52338963,\n        0.58345339, 0.64121162, 0.61587159, 0.63262892, 0.63755447,\n        0.72347311, 0.73774297, 0.67876491, 0.25245264, 0.42345454]])</pre></li><li>z(chain, draw)float640.4833 0.4831 ... 0.5065 0.4874<pre>array([[0.48329029, 0.4831174 , 0.48833775, 0.49891189, 0.50080606,\n        0.4947405 , 0.49290819, 0.49372767, 0.50710834, 0.49773436,\n        0.48974593, 0.50247153, 0.4900864 , 0.47704869, 0.51311906,\n        0.48263165, 0.50392601, 0.48735418, 0.48881296, 0.49878777,\n        0.49110798, 0.50360852, 0.49064423, 0.49801757, 0.4889951 ,\n        0.50096067, 0.50429387, 0.50436229, 0.4917108 , 0.49357853,\n        0.49942674, 0.50495091, 0.48745581, 0.50772303, 0.50133111,\n        0.48311157, 0.48998118, 0.5014993 , 0.4996103 , 0.50115952,\n        0.49949554, 0.50644281, 0.508396  , 0.5141419 , 0.498851  ,\n        0.49170637, 0.47587308, 0.50691296, 0.49485257, 0.49447771,\n        0.50234977, 0.49037811, 0.49099285, 0.48357747, 0.50625885,\n        0.48664286, 0.50257972, 0.50072825, 0.49661246, 0.49773622,\n        0.5087111 , 0.49683373, 0.49641363, 0.48426468, 0.4878892 ,\n        0.49712677, 0.49667594, 0.50329929, 0.50074576, 0.49513543,\n        0.48866381, 0.48753484, 0.49788127, 0.49864595, 0.50176052,\n        0.50019059, 0.49725798, 0.49181738, 0.49849338, 0.49764152,\n        0.49754808, 0.49992984, 0.48798643, 0.49367373, 0.47978518,\n        0.47940224, 0.49975255, 0.49571692, 0.50421648, 0.49696396,\n        0.48870961, 0.48698458, 0.49716001, 0.49510543, 0.47892316,\n        0.47903698, 0.47804672, 0.49940038, 0.48983799, 0.50047048,\n...\n        0.49181398, 0.50201736, 0.48624752, 0.49580166, 0.49820929,\n        0.49630906, 0.48776508, 0.50967846, 0.49915941, 0.49926541,\n        0.49985029, 0.49658   , 0.48263669, 0.49718427, 0.4933544 ,\n        0.50243571, 0.47904844, 0.50987518, 0.49228229, 0.49869542,\n        0.49746344, 0.50964903, 0.5004247 , 0.48625252, 0.48636852,\n        0.502433  , 0.48870447, 0.49793734, 0.49300902, 0.49486405,\n        0.49978989, 0.49494165, 0.48637728, 0.4994161 , 0.497387  ,\n        0.50782205, 0.49706651, 0.49674426, 0.49215256, 0.47836425,\n        0.49285937, 0.49409901, 0.50886769, 0.50381913, 0.47868674,\n        0.48140115, 0.5029464 , 0.49841495, 0.50450156, 0.49320853,\n        0.48904912, 0.50337825, 0.49812679, 0.50613766, 0.48396815,\n        0.50926257, 0.51026999, 0.48676725, 0.49814599, 0.4862228 ,\n        0.50981725, 0.48996272, 0.49290334, 0.49573276, 0.49038045,\n        0.50063599, 0.49884825, 0.4972365 , 0.49709394, 0.49452215,\n        0.48629293, 0.50836877, 0.486406  , 0.48876006, 0.47590511,\n        0.51077027, 0.515038  , 0.48760521, 0.5134996 , 0.5078248 ,\n        0.49727606, 0.49483291, 0.48836252, 0.50888309, 0.49794675,\n        0.49733062, 0.49364772, 0.49611937, 0.49218849, 0.49928974,\n        0.49886555, 0.49071775, 0.49470127, 0.48478826, 0.49329982,\n        0.492088  , 0.48948355, 0.50264239, 0.50654878, 0.48737942]])</pre></li><li>theta(chain, draw)float640.0119 0.0176 ... 0.04522 0.05599<pre>array([[ 0.01189686,  0.01759936,  0.03117155,  0.05081091,  0.02904571,\n         0.0336919 ,  0.02994044,  0.02345752,  0.05587892,  0.03196248,\n         0.03408715,  0.04239614,  0.03473252,  0.02935361,  0.04557938,\n         0.03031583,  0.0377734 ,  0.01754612,  0.0199668 ,  0.05330342,\n         0.02610615,  0.03252415,  0.03673307,  0.04071809,  0.05545578,\n         0.03210694,  0.03680502,  0.0349615 ,  0.01717475,  0.05190449,\n         0.04314974,  0.053393  ,  0.04643827,  0.07065032,  0.03609986,\n         0.03446239,  0.03229345,  0.0366585 ,  0.05176029,  0.04665875,\n         0.04623884,  0.06399656,  0.05629559,  0.0416704 ,  0.02840181,\n         0.02095247,  0.01738531,  0.01593306,  0.03711861,  0.03774307,\n         0.01408142,  0.01667192,  0.03600855,  0.03016511,  0.02850858,\n         0.03243518,  0.03059665,  0.02758074,  0.02687944,  0.04744838,\n         0.03547878,  0.06325565,  0.01881543,  0.04170818,  0.05368431,\n         0.0620034 ,  0.06599038,  0.05572781,  0.04831493,  0.03969986,\n         0.03708643,  0.03467361,  0.03734692,  0.04042786,  0.03655836,\n         0.05232026,  0.02809176,  0.0327971 ,  0.02732037,  0.02573392,\n         0.03146331,  0.04753317,  0.022958  ,  0.02141239,  0.03838233,\n         0.03427055,  0.01978231,  0.0176772 ,  0.02851789,  0.02432065,\n         0.05400299,  0.05632296,  0.04243647,  0.04145857,  0.05389265,\n         0.05078842,  0.05766565,  0.03405816,  0.04476057,  0.05298955,\n...\n         0.05007445,  0.05286399,  0.012287  ,  0.04770281,  0.0356072 ,\n         0.04649822,  0.05312874,  0.00577505,  0.0419288 ,  0.03161189,\n         0.03171232,  0.00987708,  0.05743775,  0.04464741,  0.03920071,\n         0.0305919 ,  0.034897  ,  0.04816817,  0.01101274,  0.05690149,\n         0.0486455 ,  0.04766214,  0.0322687 ,  0.0409452 ,  0.03874797,\n         0.03852321,  0.04322551,  0.03816009,  0.04395   ,  0.02676559,\n         0.05020687,  0.03441054,  0.01753718,  0.0560774 ,  0.01207357,\n         0.05764992,  0.02313287,  0.04617382,  0.06438373,  0.05975629,\n         0.05792476,  0.03932891,  0.03140163,  0.05341713,  0.03123632,\n         0.01947364,  0.0486715 ,  0.04329478,  0.02278388,  0.05403618,\n         0.05713677,  0.04781462,  0.03778348,  0.04503791,  0.02742907,\n         0.01190486,  0.01397266,  0.06032426,  0.07322008,  0.04577358,\n         0.02753419,  0.02660081,  0.02971266,  0.03087122,  0.05673635,\n         0.02090101,  0.02796185,  0.03799798,  0.04525212,  0.03215825,\n         0.02970219,  0.04292632,  0.03244648,  0.03856185,  0.03505368,\n         0.03724702,  0.03260011,  0.04301469,  0.07238276,  0.07094218,\n         0.06413572,  0.06784454,  0.04333594,  0.04597741,  0.02974131,\n         0.052797  ,  0.04195736,  0.03665667,  0.03680004,  0.05962404,\n         0.00886187,  0.04609575,  0.03178263,  0.04263325,  0.04759415,\n         0.02325481,  0.02611078,  0.03289902,  0.04522379,  0.05599476]])</pre></li><li>v_1|participant_id_sigma(chain, draw)float640.4595 0.4802 ... 0.693 0.3857<pre>array([[0.45954937, 0.48018455, 0.42895611, 0.45764808, 0.38102764,\n        0.40453911, 0.34039248, 0.30669653, 0.38857526, 0.58689915,\n        0.4336039 , 0.57687099, 0.57375252, 0.5321182 , 0.40412543,\n        0.42494202, 0.48607389, 0.60613225, 0.57168748, 0.37918421,\n        0.37390556, 0.47255131, 0.44092476, 0.36241413, 0.37894925,\n        0.40619594, 0.5384768 , 0.54413313, 0.40723474, 0.42541487,\n        0.39994378, 0.41364729, 0.33406629, 0.42846572, 0.49039688,\n        0.40565705, 0.42346667, 0.41868628, 0.39530301, 0.39628868,\n        0.38675651, 0.36320172, 0.39020488, 0.40856993, 0.45245857,\n        0.42834652, 0.47654293, 0.40086962, 0.57611326, 0.58983673,\n        0.88283733, 0.71063521, 0.47360498, 0.46362895, 0.42450756,\n        0.38442387, 0.46016358, 0.45179374, 0.45720014, 0.40969386,\n        0.37956386, 0.37876533, 0.42604318, 0.38953001, 0.37667636,\n        0.3627432 , 0.35667456, 0.34620624, 0.3493636 , 0.32573173,\n        0.34775666, 0.33781349, 0.38431239, 0.38811615, 0.36335363,\n        0.43727762, 0.45584899, 0.44448047, 0.45589278, 0.49654275,\n        0.39609182, 0.44438972, 0.5351684 , 0.41394751, 0.39396592,\n        0.3817656 , 0.38219729, 0.35981319, 0.43335803, 0.44615953,\n        0.47162671, 0.47062489, 0.3751006 , 0.4092612 , 0.39043451,\n        0.39191894, 0.38591018, 0.39821733, 0.41712685, 0.37782838,\n...\n        0.43785025, 0.41921307, 0.38816183, 0.48352901, 0.36730995,\n        0.43177053, 0.49893498, 0.38533708, 0.37418769, 0.48272275,\n        0.47222393, 0.59718256, 0.36080516, 0.32527673, 0.41038713,\n        0.38382014, 0.38814938, 0.40164738, 0.43156579, 0.49224287,\n        0.42743665, 0.54588663, 0.47445727, 0.44896473, 0.40651517,\n        0.511576  , 0.43182929, 0.47291771, 0.44820122, 0.44620291,\n        0.5434147 , 0.59973112, 0.60283047, 0.42588064, 0.49750627,\n        0.34884874, 0.35571161, 0.52699486, 0.54060608, 0.51211727,\n        0.43108596, 0.60797353, 0.41877795, 0.46345592, 0.60656804,\n        0.6086656 , 0.36556807, 0.39289185, 0.41485708, 0.37655293,\n        0.4650172 , 0.47245606, 0.46292458, 0.42940056, 0.44774556,\n        0.46825637, 0.48538454, 0.41385779, 0.48118467, 0.52724291,\n        0.62054989, 0.43834713, 0.42086943, 0.43223209, 0.52009132,\n        0.42746018, 0.46113662, 0.38901813, 0.38198391, 0.47101836,\n        0.40179125, 0.51408558, 0.44973676, 0.44852426, 0.45253804,\n        0.50517961, 0.55290078, 0.48998595, 0.44260727, 0.4069255 ,\n        0.44856566, 0.42915767, 0.36192698, 0.38808108, 0.36420845,\n        0.37310659, 0.3502232 , 0.43218102, 0.40924674, 0.41652117,\n        0.42709398, 0.60960192, 0.64717586, 0.54601812, 0.59399698,\n        0.43169651, 0.38457138, 0.43673572, 0.69296353, 0.38566624]])</pre></li><li>v_y(chain, draw)float640.2697 0.2703 ... 0.2655 0.2698<pre>array([[0.2697238 , 0.27029938, 0.25821737, 0.26301052, 0.26533882,\n        0.2726418 , 0.2458575 , 0.23520979, 0.22376262, 0.2856251 ,\n        0.25553925, 0.29751203, 0.29940533, 0.31783402, 0.24905024,\n        0.29169678, 0.26557007, 0.28638605, 0.28547773, 0.26543778,\n        0.30459762, 0.22703218, 0.25954717, 0.27797139, 0.2658489 ,\n        0.25522084, 0.26937444, 0.27325112, 0.27134978, 0.24685926,\n        0.25094288, 0.29269229, 0.25878881, 0.27886839, 0.2606472 ,\n        0.25588025, 0.24643012, 0.27951915, 0.25985317, 0.28074542,\n        0.27810554, 0.30448514, 0.3035386 , 0.29698112, 0.27890018,\n        0.259635  , 0.3076105 , 0.24302718, 0.24934374, 0.24420609,\n        0.27936621, 0.28031393, 0.27103639, 0.2513006 , 0.30970658,\n        0.24883361, 0.23947485, 0.24695108, 0.25778617, 0.29106521,\n        0.27959367, 0.32792714, 0.26969233, 0.28452341, 0.2879993 ,\n        0.2458979 , 0.2368308 , 0.27281605, 0.24156396, 0.27094298,\n        0.2868561 , 0.26691143, 0.26407441, 0.26067337, 0.29747444,\n        0.27130587, 0.27275337, 0.27581051, 0.24794441, 0.22631501,\n        0.31421986, 0.24223042, 0.31103067, 0.24812295, 0.29138619,\n        0.28875219, 0.24126403, 0.23626242, 0.23724537, 0.25056931,\n        0.29333252, 0.27716722, 0.26366516, 0.27659465, 0.26316149,\n        0.26383857, 0.29382575, 0.22668082, 0.24699806, 0.22710316,\n...\n        0.28536685, 0.29529732, 0.2541548 , 0.22831062, 0.2240169 ,\n        0.3285527 , 0.22299135, 0.305112  , 0.29879699, 0.29255651,\n        0.31438118, 0.2316243 , 0.24197862, 0.28683345, 0.25387964,\n        0.27274183, 0.2674344 , 0.28506339, 0.24842399, 0.25746332,\n        0.2524141 , 0.32696247, 0.26453023, 0.30042682, 0.28384358,\n        0.22990863, 0.29663524, 0.28825474, 0.28615701, 0.26392397,\n        0.28482455, 0.26320395, 0.27809595, 0.27189387, 0.25580558,\n        0.29468094, 0.23055695, 0.30050934, 0.26391304, 0.31268223,\n        0.28978998, 0.28929674, 0.28058319, 0.27893504, 0.26582927,\n        0.25155658, 0.28822217, 0.30429134, 0.26680602, 0.23508   ,\n        0.3074442 , 0.26090873, 0.27214498, 0.26374075, 0.24258399,\n        0.25537614, 0.25736238, 0.27520953, 0.26538575, 0.28300193,\n        0.26817554, 0.28079267, 0.27543241, 0.26939795, 0.26460071,\n        0.26138418, 0.27188821, 0.27692121, 0.26192421, 0.25614171,\n        0.27057285, 0.26488659, 0.26566995, 0.31445638, 0.33688315,\n        0.20353597, 0.24735114, 0.29719307, 0.29564585, 0.27033452,\n        0.30106312, 0.30851988, 0.25683601, 0.32344459, 0.28400278,\n        0.24450376, 0.28893082, 0.27166261, 0.28127941, 0.25520155,\n        0.25461302, 0.2892836 , 0.27315623, 0.25662541, 0.24490895,\n        0.28737803, 0.31100474, 0.24854457, 0.26546947, 0.26980021]])</pre></li><li>v_1|participant_id(chain, draw, v_1|participant_id__factor_dim)float640.1171 -0.1595 ... -0.2887 1.053<pre>array([[[ 0.11707724, -0.15947801, -0.52927717, ..., -0.48739397,\n         -0.64829716,  0.79977626],\n        [ 0.12198391, -0.20766431, -0.62188408, ..., -0.55949426,\n         -0.63814655,  0.9676149 ],\n        [ 0.10824284, -0.12268805, -0.54449293, ..., -0.54844746,\n         -0.59610928,  0.75899236],\n        ...,\n        [ 0.20952398, -0.03814617, -0.41599552, ..., -0.4296364 ,\n         -0.454718  ,  1.00143704],\n        [ 0.20744551,  0.00851127, -0.44231836, ..., -0.34410688,\n         -0.45049901,  0.85774914],\n        [ 0.23680683, -0.0089414 , -0.42479555, ..., -0.45374988,\n         -0.4483718 ,  0.97744008]],\n\n       [[ 0.16905671,  0.02033648, -0.2928605 , ..., -0.34324143,\n         -0.43557713,  1.00208359],\n        [ 0.22270875,  0.10788367, -0.35024825, ..., -0.366408  ,\n         -0.56475074,  0.98911789],\n        [ 0.20475028,  0.04027181, -0.54776363, ..., -0.44965101,\n         -0.38778106,  1.04501546],\n        ...,\n        [ 0.16855196, -0.11700139, -0.63279316, ..., -0.5475427 ,\n         -0.64055435,  0.88859609],\n        [ 0.53740581,  0.23671146, -0.18479231, ..., -0.257893  ,\n         -0.24573776,  1.2595053 ],\n        [ 0.31771279,  0.09030689, -0.32059108, ..., -0.19695615,\n         -0.28874747,  1.05255723]]], shape=(2, 500, 15))</pre></li><li>a(chain, draw)float641.471 1.476 1.493 ... 1.508 1.547<pre>array([[1.47055908, 1.47633186, 1.49342085, 1.5300901 , 1.52014695,\n        1.48858776, 1.49794605, 1.51948548, 1.53073763, 1.49782433,\n        1.53091312, 1.5288276 , 1.52147331, 1.51215174, 1.520539  ,\n        1.52462511, 1.52918631, 1.50546984, 1.48860903, 1.54924417,\n        1.48630939, 1.49998265, 1.50399174, 1.5328456 , 1.54856846,\n        1.51492129, 1.52524642, 1.52443108, 1.47194452, 1.54371577,\n        1.53382708, 1.54137575, 1.55390387, 1.56551289, 1.53344137,\n        1.50322727, 1.53091394, 1.53388189, 1.52889977, 1.56064038,\n        1.5638415 , 1.52623661, 1.53733942, 1.51820047, 1.47908665,\n        1.44932257, 1.47319788, 1.47937924, 1.46946294, 1.47224522,\n        1.50309555, 1.47197309, 1.50665739, 1.52579805, 1.49064643,\n        1.52085979, 1.51729362, 1.46961389, 1.49223822, 1.50835935,\n        1.51412719, 1.55310664, 1.47598287, 1.53815715, 1.55014393,\n        1.57700125, 1.56873805, 1.53660885, 1.56313503, 1.51052788,\n        1.51477873, 1.54382144, 1.51840637, 1.52553249, 1.51516093,\n        1.52156607, 1.51604719, 1.52335392, 1.47318153, 1.49990159,\n        1.51729428, 1.52740292, 1.48160173, 1.51057145, 1.4868857 ,\n        1.49837808, 1.48057466, 1.47420197, 1.47813997, 1.48648268,\n        1.54390119, 1.53588574, 1.5490497 , 1.55750566, 1.56864021,\n        1.56317057, 1.54574977, 1.54537739, 1.55677997, 1.54976564,\n...\n        1.53074398, 1.54953973, 1.47655357, 1.54341919, 1.513621  ,\n        1.54363836, 1.54441697, 1.46629255, 1.48205449, 1.53888918,\n        1.5303336 , 1.48071399, 1.55499157, 1.53342274, 1.53359166,\n        1.5001254 , 1.51006387, 1.5299753 , 1.50154205, 1.5281082 ,\n        1.535379  , 1.53776393, 1.49982992, 1.53557601, 1.5323429 ,\n        1.54522527, 1.52185532, 1.52832121, 1.5162001 , 1.48303229,\n        1.54693026, 1.52661466, 1.46244019, 1.55688628, 1.44402997,\n        1.52544824, 1.47845101, 1.53772187, 1.58016174, 1.58766546,\n        1.59930599, 1.483909  , 1.52768492, 1.52117483, 1.47544615,\n        1.48028959, 1.50410206, 1.50171178, 1.5041682 , 1.50899499,\n        1.56740784, 1.54103248, 1.49015205, 1.53656744, 1.4804043 ,\n        1.46173813, 1.45385602, 1.57972641, 1.61058086, 1.54024789,\n        1.48372607, 1.5105796 , 1.50901743, 1.50445851, 1.54891304,\n        1.50140705, 1.49554003, 1.50914015, 1.511233  , 1.53912881,\n        1.509729  , 1.52076825, 1.53227192, 1.50881128, 1.52726391,\n        1.47692606, 1.50179847, 1.55818114, 1.5613697 , 1.57193643,\n        1.56330811, 1.56485836, 1.52603039, 1.50392866, 1.50057932,\n        1.54588567, 1.55239121, 1.5161158 , 1.51395795, 1.56637966,\n        1.48278273, 1.51732653, 1.50037168, 1.51960362, 1.52214261,\n        1.46839872, 1.4822428 , 1.52536222, 1.50801059, 1.54709204]])</pre></li><li>t(chain, draw)float640.5005 0.5031 ... 0.5166 0.4801<pre>array([[0.50051971, 0.50313345, 0.51004253, 0.5075409 , 0.50930283,\n        0.51077441, 0.49559954, 0.50369559, 0.5126473 , 0.51055412,\n        0.49897485, 0.50773243, 0.50031861, 0.49176329, 0.51908859,\n        0.49295892, 0.51986879, 0.50875956, 0.50977079, 0.50455869,\n        0.50422439, 0.52146233, 0.49983667, 0.50332554, 0.4871685 ,\n        0.50211862, 0.50073428, 0.50235613, 0.52029619, 0.49287714,\n        0.49528968, 0.51013737, 0.46072021, 0.5075836 , 0.49387904,\n        0.49365688, 0.48225281, 0.50921515, 0.5022006 , 0.503701  ,\n        0.50458577, 0.50615592, 0.50312331, 0.50367195, 0.53295704,\n        0.52060769, 0.51640955, 0.50978223, 0.52526185, 0.52506256,\n        0.5093309 , 0.51348558, 0.49138052, 0.49461923, 0.52611245,\n        0.49932084, 0.51587433, 0.5156214 , 0.51497451, 0.51851291,\n        0.50871796, 0.50047266, 0.51307145, 0.4778798 , 0.48452535,\n        0.47235262, 0.4740478 , 0.50013427, 0.49967713, 0.49616382,\n        0.50916781, 0.48935257, 0.51484016, 0.51058849, 0.50656738,\n        0.51269096, 0.50248336, 0.49729766, 0.52803923, 0.50994587,\n        0.50727726, 0.5073115 , 0.52149373, 0.49588529, 0.50615644,\n        0.51301525, 0.50042041, 0.50452308, 0.52910148, 0.51271712,\n        0.48064073, 0.48474579, 0.49362807, 0.48989789, 0.46999702,\n        0.47166896, 0.48429905, 0.48431987, 0.47897497, 0.50380586,\n...\n        0.49643838, 0.48350668, 0.50980915, 0.49195418, 0.50106771,\n        0.49108046, 0.4890481 , 0.52025676, 0.53751452, 0.49508207,\n        0.49478315, 0.50774072, 0.48695778, 0.48670033, 0.49444295,\n        0.5157813 , 0.49371276, 0.50301894, 0.50441217, 0.51418557,\n        0.50390375, 0.50424163, 0.52630062, 0.50371823, 0.50771335,\n        0.50330637, 0.50169623, 0.48614298, 0.50051269, 0.51284815,\n        0.48489736, 0.49984759, 0.51697512, 0.50958965, 0.52431155,\n        0.50087423, 0.50636965, 0.49198281, 0.47702783, 0.45401618,\n        0.45967311, 0.52432847, 0.50031229, 0.5343498 , 0.50528888,\n        0.51644949, 0.52590938, 0.532846  , 0.5006916 , 0.50457275,\n        0.49542139, 0.48490833, 0.52248727, 0.51218226, 0.52084075,\n        0.52876011, 0.52845877, 0.47049355, 0.47465191, 0.48165898,\n        0.52058142, 0.50389086, 0.50319787, 0.5064641 , 0.49211578,\n        0.51154258, 0.5147287 , 0.51202627, 0.50972751, 0.49432812,\n        0.48839572, 0.51187904, 0.49890882, 0.49057159, 0.50073642,\n        0.52815695, 0.53324053, 0.4788684 , 0.50676236, 0.50248594,\n        0.48465323, 0.48854044, 0.48646544, 0.51139674, 0.50988702,\n        0.5083953 , 0.48649398, 0.51342922, 0.49797282, 0.48788182,\n        0.51647347, 0.50857991, 0.50472209, 0.49859218, 0.50797872,\n        0.51995247, 0.51697721, 0.50434725, 0.51656089, 0.48007735]])</pre></li><li>v_1|participant_id_offset(chain, draw, v_1|participant_id__factor_dim)float640.2548 -0.347 ... -0.7487 2.729<pre>array([[[ 0.25476531, -0.3470313 , -1.15173083, ..., -1.06059111,\n         -1.41072364,  1.74034896],\n        [ 0.25403547, -0.43246771, -1.29509391, ..., -1.16516506,\n         -1.32896101,  2.01508962],\n        [ 0.25234013, -0.2860154 , -1.26934415, ..., -1.27856311,\n         -1.38967429,  1.76939399],\n        ...,\n        [ 0.36772262, -0.06694799, -0.73008809, ..., -0.75402836,\n         -0.79804753,  1.75756041],\n        [ 0.34699514,  0.01423684, -0.73986813, ..., -0.57558931,\n         -0.75355194,  1.43476128],\n        [ 0.3681783 , -0.01390175, -0.66045604, ..., -0.70547314,\n         -0.6971115 ,  1.51968683]],\n\n       [[ 0.35888888,  0.04317212, -0.62171079, ..., -0.72866399,\n         -0.92468257,  2.12731378],\n        [ 0.66640652,  0.32281795, -1.04804018, ..., -1.09639465,\n         -1.68989132,  2.95971587],\n        [ 0.54206248,  0.10661688, -1.45016702, ..., -1.19042053,\n         -1.02662404,  2.76660748],\n        ...,\n        [ 0.38593582, -0.26789976, -1.44891551, ..., -1.25371631,\n         -1.46668642,  2.0346311 ],\n        [ 0.77551817,  0.34159295, -0.2666696 , ..., -0.37215955,\n         -0.3546186 ,  1.81756362],\n        [ 0.82380242,  0.23415814, -0.83126559, ..., -0.51069067,\n         -0.7486978 ,  2.72919201]]], shape=(2, 500, 15))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>v_1|participant_id__factor_dimPandasIndex<pre>PandasIndex(Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n       '13', '14'],\n      dtype='object', name='v_1|participant_id__factor_dim'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-29T20:05:48.538765+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :579.8671550750732tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 24MB\nDimensions:      (chain: 2, draw: 500, __obs__: 3000)\nCoordinates:\n  * chain        (chain) int64 16B 0 1\n  * draw         (draw) int64 4kB 0 1 2 3 4 5 6 ... 493 494 495 496 497 498 499\n  * __obs__      (__obs__) int64 24kB 0 1 2 3 4 5 ... 2995 2996 2997 2998 2999\nData variables:\n    rt,response  (chain, draw, __obs__) float64 24MB -1.022 -1.886 ... -0.3678\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li><li>__obs__: 3000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 ... 2996 2997 2998 2999<pre>array([   0,    1,    2, ..., 2997, 2998, 2999], shape=(3000,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-1.022 -1.886 ... -0.5133 -0.3678<pre>array([[[-1.02216361, -1.88570436, -2.69758009, ..., -1.24989392,\n         -0.49324215, -0.35516379],\n        [-0.98367359, -1.88207399, -2.68437114, ..., -1.44049093,\n         -0.31807694, -0.18158243],\n        [-1.07081358, -1.87045107, -2.70420604, ..., -1.18559054,\n         -0.58246751, -0.44391669],\n        ...,\n        [-1.10098695, -1.85172341, -2.66416773, ..., -1.27127449,\n         -0.42922196, -0.32009035],\n        [-1.02168399, -1.86836601, -2.68575933, ..., -1.19328236,\n         -0.62239516, -0.40826869],\n        [-0.97077367, -1.85968207, -2.64412763, ..., -1.3053334 ,\n         -0.47660731, -0.28350937]],\n\n       [[-1.12445533, -1.87714756, -2.73136148, ..., -1.26182956,\n         -0.53853421, -0.32768532],\n        [-1.05841516, -1.85985086, -2.67878662, ..., -1.28252102,\n         -0.48778833, -0.32540735],\n        [-1.0484433 , -1.89119047, -2.7246004 , ..., -1.36357064,\n         -0.37185662, -0.22528607],\n        ...,\n        [-0.96214202, -1.87244753, -2.64631008, ..., -1.34684503,\n         -0.37040546, -0.27270361],\n        [-0.93242394, -1.88760909, -2.69511599, ..., -1.31152253,\n         -0.4223708 , -0.26298656],\n        [-1.09208773, -1.84897265, -2.67393025, ..., -1.24381809,\n         -0.51329422, -0.36777233]]], shape=(2, 500, 3000))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999],\n      dtype='int64', name='__obs__', length=3000))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 134kB\nDimensions:                (chain: 2, draw: 500)\nCoordinates:\n  * chain                  (chain) int64 16B 0 1\n  * draw                   (draw) int64 4kB 0 1 2 3 4 5 ... 495 496 497 498 499\nData variables: (12/18)\n    energy                 (chain, draw) float64 8kB 5.188e+03 ... 5.176e+03\n    step_size_bar          (chain, draw) float64 8kB 0.1623 0.1623 ... 0.1717\n    divergences            (chain, draw) int64 8kB 0 0 0 0 0 0 0 ... 0 0 0 0 0 0\n    perf_counter_start     (chain, draw) float64 8kB 1.668e+06 ... 1.668e+06\n    diverging              (chain, draw) bool 1kB False False ... False False\n    step_size              (chain, draw) float64 8kB 0.2192 0.2192 ... 0.1931\n    ...                     ...\n    acceptance_rate        (chain, draw) float64 8kB 0.9118 0.7445 ... 0.7873\n    lp                     (chain, draw) float64 8kB -5.173e+03 ... -5.163e+03\n    reached_max_treedepth  (chain, draw) bool 1kB False False ... False False\n    perf_counter_diff      (chain, draw) float64 8kB 0.5618 0.3479 ... 0.2888\n    index_in_trajectory    (chain, draw) int64 8kB 13 -2 5 -24 28 ... 4 10 31 16\n    max_energy_error       (chain, draw) float64 8kB -0.5562 0.5962 ... 0.8798\nAttributes:\n    created_at:                  2025-09-29T20:05:48.566663+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               579.8671550750732\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (18)<ul><li>energy(chain, draw)float645.188e+03 5.187e+03 ... 5.176e+03<pre>array([[5187.65150182, 5186.98106376, 5186.86733534, 5178.46704048,\n        5174.92322047, 5171.32990573, 5187.87626685, 5182.52665358,\n        5181.59366481, 5172.70808436, 5175.10627338, 5173.48064302,\n        5173.29634893, 5169.91390576, 5173.83345217, 5178.51138861,\n        5177.40890214, 5175.84687959, 5187.27883641, 5184.56283576,\n        5182.39505596, 5181.81388808, 5174.69961381, 5168.67553399,\n        5174.49654019, 5168.99800088, 5168.18254379, 5165.31015312,\n        5166.59198152, 5168.85181149, 5166.65109261, 5169.71271797,\n        5180.14985652, 5184.69559183, 5176.44644368, 5169.52430024,\n        5170.66806639, 5175.15539214, 5169.60834844, 5175.09768174,\n        5174.97475278, 5186.70325015, 5183.30624319, 5177.87640333,\n        5174.29730398, 5173.84050901, 5183.4196754 , 5182.8441492 ,\n        5176.45274656, 5168.80123343, 5169.47871032, 5168.58357745,\n        5172.74832239, 5168.30179004, 5179.85080409, 5176.02693559,\n        5177.20945107, 5175.72921934, 5173.99657965, 5179.59849225,\n        5176.86446573, 5177.69759034, 5182.2201463 , 5172.36536346,\n        5176.23891181, 5180.13641789, 5184.45535728, 5182.74515067,\n        5183.97847042, 5180.44881232, 5184.05908637, 5180.06690606,\n        5179.55917488, 5174.09584613, 5178.93457384, 5181.208937  ,\n        5176.02077338, 5178.47216619, 5169.15354159, 5171.98926501,\n...\n        5179.67888223, 5177.44577884, 5170.51678947, 5175.089866  ,\n        5181.15238918, 5177.01535578, 5178.98139401, 5179.92991288,\n        5169.45197613, 5169.41218943, 5170.11282692, 5166.69623699,\n        5168.21749475, 5169.17041713, 5175.69327221, 5184.005847  ,\n        5177.26804766, 5170.37055877, 5172.51058179, 5177.00479711,\n        5180.63989082, 5173.25993649, 5171.46722008, 5173.65847984,\n        5177.0712715 , 5177.36332904, 5173.62817481, 5179.31158543,\n        5176.23290808, 5175.75750469, 5176.92131218, 5168.75657014,\n        5169.28758372, 5173.01408632, 5171.53289475, 5177.75569027,\n        5182.1984776 , 5181.62396974, 5177.10649223, 5170.41641727,\n        5169.63246367, 5166.08977263, 5166.20818382, 5171.15623581,\n        5165.86259521, 5169.68801947, 5169.06403924, 5169.1120586 ,\n        5171.36610342, 5172.18103859, 5179.42384129, 5183.82968662,\n        5178.79653878, 5174.04007576, 5176.17333608, 5180.49961207,\n        5181.29031794, 5174.14812833, 5185.64548671, 5183.69089879,\n        5175.8279482 , 5172.06861046, 5179.71422795, 5178.36309671,\n        5180.88288654, 5177.89175059, 5175.01743654, 5174.20236344,\n        5174.07663451, 5176.16379062, 5170.95535036, 5167.98202231,\n        5165.86279527, 5170.37045769, 5177.18333054, 5179.84144414,\n        5174.12467498, 5173.34436987, 5171.83457559, 5175.97423892]])</pre></li><li>step_size_bar(chain, draw)float640.1623 0.1623 ... 0.1717 0.1717<pre>array([[0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n        0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 , 0.1623173 ,\n...\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451,\n        0.17173451, 0.17173451, 0.17173451, 0.17173451, 0.17173451]])</pre></li><li>divergences(chain, draw)int640 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0<pre>array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n...\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</pre></li><li>perf_counter_start(chain, draw)float641.668e+06 1.668e+06 ... 1.668e+06<pre>array([[1667532.04068617, 1667532.60266067, 1667532.9507215 ,\n        1667533.16229408, 1667533.62015688, 1667534.69751342,\n        1667535.03329554, 1667535.53306067, 1667535.96401425,\n        1667536.284369  , 1667537.15215267, 1667537.67707508,\n        1667538.08118346, 1667538.30364271, 1667538.65141187,\n        1667539.16387846, 1667539.97877321, 1667540.49795833,\n        1667541.57842054, 1667541.69048908, 1667542.24206854,\n        1667542.70023525, 1667543.373052  , 1667543.59415087,\n        1667543.97723113, 1667544.54143454, 1667545.03335017,\n        1667545.58309392, 1667545.80628225, 1667546.78260996,\n        1667547.27528425, 1667547.89881883, 1667548.41276479,\n        1667548.93091142, 1667549.46487225, 1667549.94121217,\n        1667550.54625842, 1667551.28797375, 1667552.45962188,\n        1667553.98973963, 1667554.16815425, 1667554.61093496,\n        1667554.96521492, 1667555.10010183, 1667555.36429133,\n        1667556.16442254, 1667556.51031167, 1667556.76705683,\n        1667557.456832  , 1667557.95635587, 1667558.5016895 ,\n        1667558.70919546, 1667558.93640229, 1667559.36544221,\n        1667559.81987767, 1667560.34041958, 1667561.20532138,\n        1667562.08690862, 1667562.92448783, 1667563.58645283,\n...\n        1667744.35432154, 1667744.67168854, 1667745.00976271,\n        1667745.64101938, 1667746.26477071, 1667746.53111633,\n        1667746.90984354, 1667747.28433471, 1667747.47267225,\n        1667747.99073221, 1667748.73455125, 1667748.91851525,\n        1667749.39201958, 1667749.66290425, 1667750.17206925,\n        1667750.82617179, 1667750.90566771, 1667751.30084583,\n        1667751.47441179, 1667751.88535229, 1667752.20279637,\n        1667752.58849804, 1667753.47020942, 1667753.85513237,\n        1667754.23707062, 1667754.6686985 , 1667755.06389325,\n        1667755.41077896, 1667756.13940854, 1667756.502649  ,\n        1667757.18383292, 1667757.51893733, 1667758.29204021,\n        1667758.46408612, 1667758.85141546, 1667759.34330067,\n        1667759.62964587, 1667762.23894308, 1667762.55419958,\n        1667763.10517546, 1667763.52885004, 1667763.62338883,\n        1667763.99203267, 1667764.23529592, 1667764.42446563,\n        1667765.27463817, 1667765.45802388, 1667765.90786063,\n        1667766.37704196, 1667771.66739104, 1667771.90262533,\n        1667772.27613646, 1667772.33746263, 1667772.467668  ,\n        1667772.58476746, 1667772.83014262, 1667773.09261363,\n        1667773.42488004, 1667774.19907867]])</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False]])</pre></li><li>step_size(chain, draw)float640.2192 0.2192 ... 0.1931 0.1931<pre>array([[0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n        0.21917358, 0.21917358, 0.21917358, 0.21917358, 0.21917358,\n...\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121,\n        0.19312121, 0.19312121, 0.19312121, 0.19312121, 0.19312121]])</pre></li><li>energy_error(chain, draw)float64-0.0389 0.2796 ... -0.04904 0.01193<pre>array([[-3.88996216e-02,  2.79619977e-01, -8.46623100e-01,\n         3.09333702e-01, -5.97832607e-02,  1.81781319e-01,\n         1.74457867e-01,  7.96607563e-02, -2.79972890e-01,\n        -2.32657668e-01, -1.18002104e-01,  5.40158895e-01,\n        -3.33010134e-01,  1.04279111e-01, -1.67292836e-02,\n         5.56122128e-01, -3.50591363e-01, -1.68313110e-01,\n         8.16533787e-01, -6.30957270e-02, -3.90364105e-01,\n        -4.29733101e-02, -1.19287116e-01,  3.47845177e-02,\n        -1.77500434e-01,  4.83031204e-02, -2.15945919e-01,\n         1.22759296e-01,  2.41364621e-01,  6.19470127e-02,\n        -1.39215068e-01, -1.23647176e-01,  2.98274310e-01,\n        -3.60046807e-01,  3.48116313e-01, -2.92698456e-01,\n        -2.47122304e-02, -1.89304214e-01,  1.82024936e-02,\n         4.76063013e-01, -8.58246416e-02,  2.51290049e-01,\n        -5.69653402e-01,  1.59949116e-01, -3.71555889e-01,\n         2.97168854e-01,  3.28915546e-01,  1.12134143e-01,\n        -2.40930516e-01, -4.33029628e-01, -4.52326115e-01,\n         5.96216210e-02,  9.22473817e-01,  1.44518001e-01,\n         1.32860998e-01, -6.61621082e-02, -5.03153015e-01,\n         1.26093055e-01,  8.49474802e-02,  1.70963919e+00,\n...\n         3.15434139e-01, -6.39559320e-02, -2.00433320e-01,\n        -2.69697395e-02, -1.38697111e-01,  2.20449403e-01,\n         9.23532303e-02, -9.43571901e-02,  3.46000873e-01,\n        -2.54587160e-01, -1.29786778e-01, -1.41258054e-01,\n         5.35661079e-01,  2.72071278e-01,  2.47320729e-01,\n        -1.40072225e-02,  1.08434518e-01, -1.16199093e+00,\n        -4.55235796e-02, -4.55909337e-02,  5.24488354e-02,\n         8.91030481e-02, -6.74520913e-02,  1.11764488e-01,\n         2.26988426e-01, -1.54506350e-01, -7.46178675e-02,\n         7.12385634e-01, -3.11255415e-01,  5.51564205e-02,\n        -2.88536370e-01, -4.68375899e-01, -9.13031099e-02,\n        -1.44853446e-01,  5.46058213e-01,  5.01419151e-01,\n        -9.01260928e-02, -5.89253513e-01,  3.48605000e-01,\n        -3.63143202e-02, -2.18188185e-01,  3.32261614e-01,\n        -2.99367733e-01,  1.30114657e-01,  1.94720891e-01,\n         8.53173049e-02,  2.60202070e-01, -9.34590933e-02,\n        -2.06875449e-01, -1.01864551e-01, -5.40274450e-01,\n         1.74941728e-01,  1.91840601e-01,  1.61809713e-01,\n        -6.15070321e-02, -1.29971155e-01,  5.54408824e-02,\n        -4.90432598e-02,  1.19316336e-02]])</pre></li><li>largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n...\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan]])</pre></li><li>tree_depth(chain, draw)int645 4 4 5 6 4 5 5 ... 3 4 4 5 5 5 6 5<pre>array([[5, 4, 4, 5, 6, 4, 5, 5, 4, 6, 5, 5, 4, 4, 5, 5, 5, 6, 3, 5, 5, 5,\n        4, 5, 5, 5, 6, 4, 6, 5, 5, 5, 5, 5, 4, 5, 5, 6, 6, 3, 4, 4, 3, 4,\n        5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 5, 6, 5, 5, 5, 6, 4, 4, 6, 5, 5, 4,\n        4, 5, 4, 6, 4, 4, 5, 4, 4, 5, 4, 5, 5, 6, 5, 5, 6, 5, 4, 4, 4, 4,\n        5, 5, 5, 5, 6, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 5, 6, 6, 4, 5, 5, 5,\n        5, 3, 4, 4, 4, 5, 6, 5, 4, 6, 5, 5, 4, 5, 6, 6, 5, 5, 4, 6, 5, 6,\n        4, 5, 4, 4, 4, 4, 5, 3, 4, 5, 4, 5, 4, 4, 6, 6, 5, 3, 3, 4, 6, 4,\n        5, 5, 4, 5, 6, 4, 3, 4, 5, 5, 6, 5, 5, 6, 5, 6, 5, 4, 6, 4, 5, 4,\n        4, 5, 5, 5, 4, 5, 5, 5, 6, 5, 6, 5, 4, 6, 3, 5, 6, 5, 6, 5, 4, 4,\n        4, 6, 4, 4, 4, 5, 4, 4, 5, 6, 6, 4, 5, 6, 5, 6, 5, 5, 6, 3, 6, 6,\n        6, 3, 5, 5, 6, 6, 5, 6, 5, 6, 5, 3, 6, 5, 5, 4, 5, 5, 4, 5, 4, 4,\n        4, 5, 5, 4, 3, 5, 5, 5, 5, 5, 5, 6, 5, 5, 4, 5, 4, 4, 4, 5, 5, 4,\n        4, 4, 6, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 6, 6, 4, 4, 6, 6, 3,\n        4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 6, 5, 6, 5, 6, 6,\n        4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 4, 3, 4, 4, 4, 4, 5, 4,\n        5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4,\n        6, 6, 6, 4, 5, 4, 5, 6, 5, 6, 6, 4, 5, 6, 5, 4, 4, 5, 5, 5, 6, 4,\n        4, 5, 5, 6, 4, 5, 5, 4, 6, 5, 6, 5, 5, 5, 5, 3, 5, 4, 5, 5, 5, 6,\n        5, 4, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 6, 5, 5, 5, 5, 6, 5,\n        6, 5, 4, 6, 5, 4, 4, 6, 5, 5, 4, 5, 5, 6, 5, 6, 4, 5, 6, 5, 5, 4,\n...\n        4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 3, 5, 5, 5, 4, 4, 4, 6, 5, 5, 5, 6,\n        5, 5, 5, 5, 5, 3, 3, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 5, 5, 5,\n        6, 4, 5, 6, 5, 5, 5, 6, 5, 4, 5, 5, 6, 4, 5, 4, 4, 5, 5, 5, 4, 4,\n        5, 5, 4, 4, 6, 5, 6, 5, 5, 5, 4, 5, 5, 4, 5, 4, 5, 3, 3, 3, 6, 4,\n        5, 5, 4, 6, 5, 4, 5, 6, 4, 6, 4, 5, 5, 4, 5, 6, 5, 5, 6, 5, 5, 5,\n        4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 6, 5, 4, 5, 3, 4, 4, 5, 5, 6, 4,\n        4, 6, 4, 6, 5, 6, 5, 5, 6, 6, 4, 6, 6, 4, 5, 4, 5, 5, 5, 5, 4, 6,\n        5, 6, 4, 4, 5, 4, 6, 6, 4, 6, 4, 5, 4, 5, 6, 5, 3, 4, 6, 4, 6, 4,\n        6, 6, 5, 4, 4, 4, 5, 6, 5, 4, 4, 4, 4, 6, 6, 4, 5, 6, 4, 4, 4, 5,\n        5, 5, 5, 5, 6, 5, 6, 5, 6, 5, 5, 6, 4, 5, 5, 6, 4, 5, 5, 4, 5, 6,\n        5, 5, 5, 4, 4, 4, 5, 5, 6, 4, 5, 3, 4, 5, 6, 5, 6, 4, 4, 5, 5, 6,\n        5, 5, 3, 6, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 5, 4, 6, 4, 6, 5, 5,\n        5, 4, 5, 5, 5, 5, 3, 4, 5, 5, 4, 6, 4, 4, 4, 6, 5, 5, 6, 4, 4, 4,\n        4, 5, 4, 5, 5, 6, 4, 6, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 3, 5, 5,\n        6, 6, 6, 6, 5, 5, 6, 6, 5, 6, 5, 5, 6, 3, 6, 5, 5, 4, 4, 5, 4, 6,\n        6, 5, 5, 5, 4, 4, 6, 5, 5, 5, 5, 5, 4, 6, 5, 5, 6, 4, 5, 5, 5, 4,\n        5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 6, 5, 5, 5, 6, 4, 6, 5, 4,\n        4, 5, 5, 6, 6, 4, 5, 5, 4, 5, 6, 4, 5, 4, 5, 5, 3, 5, 4, 5, 5, 5,\n        6, 5, 5, 5, 5, 5, 5, 5, 6, 5, 6, 4, 5, 5, 4, 6, 5, 5, 5, 3, 4, 4,\n        4, 6, 4, 5, 5, 5, 5, 5, 3, 4, 4, 5, 5, 5, 6, 5]])</pre></li><li>process_time_diff(chain, draw)float641.223 0.6052 0.5878 ... 2.389 1.144<pre>array([[1.223458, 0.60522 , 0.587752, 1.236755, 2.420818, 0.60114 ,\n        1.195148, 1.180844, 0.613438, 2.386791, 1.233442, 1.214141,\n        0.604964, 0.63753 , 1.25904 , 1.309589, 0.980349, 2.46077 ,\n        0.30031 , 1.277738, 1.29735 , 1.279294, 0.64064 , 1.196874,\n        1.254406, 1.230073, 1.573098, 0.631612, 1.847915, 1.156345,\n        1.226587, 1.253339, 1.211114, 1.176396, 0.63202 , 1.247211,\n        1.249151, 1.949616, 2.397283, 0.31154 , 0.662382, 0.595195,\n        0.282507, 0.580085, 1.20931 , 0.91558 , 0.621936, 1.314721,\n        1.301845, 1.230664, 0.588406, 0.594078, 1.202557, 0.589888,\n        1.18829 , 1.865852, 1.234724, 1.274083, 1.264476, 2.512176,\n        0.614551, 0.606214, 1.789742, 1.209701, 1.214834, 0.608576,\n        0.620108, 1.250182, 0.583397, 1.919193, 0.613611, 0.630842,\n        1.271955, 0.657298, 0.61959 , 1.246903, 0.635374, 1.281537,\n        1.246811, 2.176146, 1.207644, 1.278449, 2.468162, 1.230742,\n        0.606763, 0.621885, 0.640889, 0.634682, 1.204524, 1.2809  ,\n        1.211436, 1.181081, 2.392661, 0.683004, 0.608841, 0.616619,\n        0.604105, 0.699425, 0.632843, 1.176847, 0.595002, 1.165869,\n        0.612053, 1.19022 , 2.603412, 2.475484, 0.632611, 1.242209,\n        1.284705, 1.204932, 0.914405, 0.320871, 0.638735, 0.601312,\n        0.594799, 1.231804, 2.42796 , 1.254708, 0.598048, 1.958035,\n...\n        1.263796, 1.255836, 2.330363, 0.311201, 2.487038, 1.24767 ,\n        1.281868, 0.614422, 0.612188, 1.273517, 0.626301, 2.481691,\n        1.566538, 1.233962, 1.190295, 1.184896, 0.597794, 0.624944,\n        1.852072, 1.254079, 1.265073, 1.203539, 1.239429, 1.233589,\n        0.630245, 2.467386, 1.212488, 1.224505, 2.617226, 0.654812,\n        1.248998, 1.190449, 1.287765, 0.572713, 1.201122, 1.238096,\n        0.635189, 0.622525, 1.206963, 1.230874, 0.610879, 0.607849,\n        1.210264, 1.18794 , 1.287628, 1.18251 , 1.225018, 2.398446,\n        1.296254, 1.257862, 1.157187, 2.385822, 0.6025  , 1.829617,\n        1.24338 , 0.61894 , 0.625963, 1.21309 , 1.21095 , 2.423825,\n        1.852008, 0.599415, 1.173181, 1.20403 , 0.596731, 1.210448,\n        2.461296, 0.617804, 1.22661 , 0.701084, 1.264192, 1.24142 ,\n        0.277925, 1.202597, 0.600771, 1.260144, 1.227122, 1.199546,\n        2.190787, 1.171161, 0.917621, 1.235759, 1.219148, 1.238211,\n        1.227959, 1.208481, 1.87972 , 1.270793, 2.524581, 0.601069,\n        1.219392, 1.244285, 0.611184, 2.516324, 1.192452, 1.175221,\n        1.20749 , 0.297589, 0.580059, 0.637783, 0.612502, 2.418764,\n        0.584879, 1.234007, 1.346942, 0.897657, 1.152248, 1.164216,\n        0.290948, 0.571986, 0.581029, 1.194979, 1.145769, 1.172012,\n        2.389012, 1.143896]])</pre></li><li>n_steps(chain, draw)float6431.0 15.0 15.0 ... 31.0 63.0 31.0<pre>array([[31., 15., 15., 31., 63., 15., 31., 31., 15., 63., 31., 31., 15.,\n        15., 31., 31., 23., 63.,  7., 31., 31., 31., 15., 31., 31., 31.,\n        39., 15., 47., 31., 31., 31., 31., 31., 15., 31., 31., 47., 63.,\n         7., 15., 15.,  7., 15., 31., 23., 15., 31., 31., 31., 15., 15.,\n        31., 15., 31., 47., 31., 31., 31., 63., 15., 15., 47., 31., 31.,\n        15., 15., 31., 15., 47., 15., 15., 31., 15., 15., 31., 15., 31.,\n        31., 55., 31., 31., 63., 31., 15., 15., 15., 15., 31., 31., 31.,\n        31., 63., 15., 15., 15., 15., 15., 15., 31., 15., 31., 15., 31.,\n        63., 63., 15., 31., 31., 31., 23.,  7., 15., 15., 15., 31., 63.,\n        31., 15., 47., 31., 31., 15., 31., 63., 47., 31., 31., 15., 39.,\n        31., 39., 15., 31., 15., 15., 15., 15., 31.,  7., 15., 31., 15.,\n        31., 15., 15., 39., 47., 31.,  7.,  7., 15., 63., 15., 31., 31.,\n        15., 31., 63., 15.,  7., 15., 31., 31., 63., 31., 31., 63., 31.,\n        47., 23., 15., 47., 15., 31., 15., 15., 31., 23., 31., 15., 31.,\n        31., 31., 63., 31., 47., 31., 15., 63.,  7., 31., 47., 31., 47.,\n        31., 15., 15., 15., 63., 15., 15., 15., 31., 15., 15., 31., 47.,\n        63., 15., 31., 63., 31., 63., 31., 31., 63.,  7., 63., 63., 47.,\n         7., 31., 31., 47., 63., 31., 63., 31., 47., 31.,  7., 63., 31.,\n        31., 15., 23., 31., 15., 31., 15., 15., 15., 31., 31., 15.,  7.,\n        31., 31., 31., 31., 31., 31., 63., 31., 31., 15., 23., 15., 15.,\n...\n        15., 31., 47., 31., 15., 15., 15., 15., 63., 63., 15., 31., 63.,\n        15., 15., 15., 31., 31., 31., 31., 31., 63., 31., 63., 31., 47.,\n        31., 31., 47., 15., 31., 31., 47., 15., 31., 31., 15., 31., 63.,\n        31., 31., 31., 15., 15., 15., 31., 31., 63., 15., 31.,  7., 15.,\n        31., 47., 31., 47., 15., 15., 31., 31., 47., 31., 31.,  7., 63.,\n        31., 15., 15., 15., 23., 15., 15., 15., 31., 15., 15., 31., 15.,\n        63., 15., 63., 23., 23., 31., 15., 31., 31., 31., 31.,  7., 15.,\n        31., 31., 15., 63., 15., 15., 15., 63., 31., 31., 47., 15., 15.,\n        15., 15., 31., 15., 31., 31., 63., 15., 63., 15., 31., 31., 15.,\n        31., 31., 31., 31., 15., 31., 31.,  7., 31., 31., 63., 63., 63.,\n        63., 31., 31., 63., 47., 31., 47., 31., 31., 63.,  7., 63., 31.,\n        31., 15., 15., 31., 15., 63., 39., 31., 31., 31., 15., 15., 47.,\n        31., 31., 31., 31., 31., 15., 63., 31., 31., 63., 15., 31., 31.,\n        31., 15., 31., 31., 15., 15., 31., 31., 15., 15., 31., 31., 31.,\n        31., 31., 63., 31., 31., 31., 63., 15., 47., 31., 15., 15., 31.,\n        31., 63., 47., 15., 31., 31., 15., 31., 63., 15., 31., 15., 31.,\n        31.,  7., 31., 15., 31., 31., 31., 55., 31., 23., 31., 31., 31.,\n        31., 31., 47., 31., 63., 15., 31., 31., 15., 63., 31., 31., 31.,\n         7., 15., 15., 15., 63., 15., 31., 31., 23., 31., 31.,  7., 15.,\n        15., 31., 31., 31., 63., 31.]])</pre></li><li>smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n...\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan]])</pre></li><li>acceptance_rate(chain, draw)float640.9118 0.7445 ... 0.9929 0.7873<pre>array([[0.9118112 , 0.74450388, 0.52696492, 0.79442815, 0.94561239,\n        0.88535439, 0.30887205, 0.88015602, 0.99580477, 0.98603086,\n        0.79497905, 0.73737534, 0.992339  , 0.98537834, 0.96476576,\n        0.69798711, 0.95857257, 0.63552724, 0.30761773, 0.99660514,\n        1.        , 0.84909511, 0.54687729, 0.99338994, 0.80878268,\n        0.87270223, 0.97329611, 0.89917009, 0.89694695, 0.8388284 ,\n        0.80942978, 0.88569241, 0.60696805, 0.97446981, 0.75453222,\n        1.        , 0.79108674, 0.81133678, 0.71389339, 0.65200184,\n        1.        , 0.88211872, 0.37802592, 0.85518322, 0.95591536,\n        0.82104471, 0.78602179, 0.94711153, 0.99333205, 0.99701749,\n        0.99460611, 0.79212278, 0.4480728 , 0.96863968, 0.94475039,\n        0.96974847, 0.95467968, 0.85366593, 0.97162339, 0.5446708 ,\n        0.95177048, 0.88889836, 0.74166447, 0.88414676, 0.85663665,\n        0.78413286, 0.97956435, 0.80505287, 0.98940659, 0.99865069,\n        0.94146924, 0.97742371, 0.8201876 , 0.98572515, 0.84768743,\n        0.86737591, 0.99847205, 0.96914524, 0.85879125, 0.88299538,\n        0.99708701, 0.8282068 , 0.79722072, 0.99062146, 0.92787151,\n        0.95935935, 0.88925584, 0.93352683, 0.89919842, 1.        ,\n        0.94805136, 0.99604329, 0.75353772, 0.78677394, 0.74019889,\n        0.9625998 , 0.91503739, 0.71803703, 0.96697154, 0.84093625,\n...\n        1.        , 0.91530846, 0.99892423, 0.61578094, 0.98458484,\n        0.77790264, 0.86447361, 0.19194135, 0.76609486, 0.96364252,\n        0.33155314, 0.97498625, 0.80801075, 0.74505759, 0.98968904,\n        0.95872577, 0.81805333, 0.89869706, 0.73273334, 0.99519056,\n        0.9376513 , 0.99036749, 1.        , 0.81881886, 0.82457083,\n        0.993149  , 0.6389458 , 0.40451927, 0.94168191, 0.95796718,\n        0.72213146, 0.94291721, 0.92288047, 0.93119187, 0.9198084 ,\n        0.87570932, 0.85843968, 0.87754392, 0.74405976, 0.79904336,\n        0.99827919, 0.58150772, 0.96206223, 0.9947343 , 0.84834641,\n        0.57186996, 0.79030439, 0.86147458, 0.99442137, 0.72907153,\n        0.6201961 , 0.99668957, 0.88881093, 0.62925273, 0.84043479,\n        0.46531353, 0.86697169, 0.98753551, 0.95994042, 0.98271827,\n        0.86397252, 0.9448899 , 0.95116588, 0.92937729, 0.96432719,\n        0.8618489 , 0.95049433, 0.96867105, 0.37661511, 0.99509048,\n        0.8251397 , 0.7827579 , 0.7974067 , 0.93032461, 0.99208033,\n        0.43991368, 0.79159694, 0.93371584, 0.86127337, 0.70777337,\n        0.9779843 , 0.87956548, 0.79847034, 0.91662823, 0.63647277,\n        0.96971366, 0.97805782, 0.85640461, 1.        , 0.98894831,\n        0.99046814, 1.        , 0.72228492, 0.6906144 , 0.58412342,\n        0.69154347, 0.91639441, 0.95365462, 0.99294029, 0.7872645 ]])</pre></li><li>lp(chain, draw)float64-5.173e+03 ... -5.163e+03<pre>array([[-5172.82552817, -5171.86798281, -5162.94580283, -5163.63585968,\n        -5165.14655219, -5165.45480545, -5170.2942033 , -5169.86018924,\n        -5165.43622637, -5159.75965071, -5164.7844513 , -5162.18940477,\n        -5160.78929736, -5164.25571014, -5161.828643  , -5168.03544433,\n        -5165.0418301 , -5161.72896391, -5173.16903875, -5172.41288535,\n        -5169.60811302, -5163.40135395, -5161.62011743, -5163.17136681,\n        -5160.91383313, -5161.26226441, -5157.97602646, -5156.72136612,\n        -5159.74230054, -5159.92954201, -5160.25171109, -5160.86064445,\n        -5170.23793977, -5163.50224937, -5161.55224934, -5162.57575476,\n        -5163.98098958, -5162.30095911, -5159.10886644, -5169.6769486 ,\n        -5169.68190195, -5173.95279331, -5165.41195846, -5166.31976899,\n        -5158.75582327, -5165.8748005 , -5168.74170046, -5166.49023899,\n        -5162.53289831, -5161.78675409, -5159.95389119, -5157.55919972,\n        -5161.43695671, -5162.45546696, -5167.5178147 , -5165.4133532 ,\n        -5164.32436366, -5161.9147644 , -5163.21992563, -5168.98519218,\n        -5164.21858209, -5169.05954878, -5159.0330461 , -5163.48164746,\n        -5167.57485131, -5170.63023087, -5169.22394048, -5170.93711982,\n        -5171.08139724, -5169.18072967, -5168.00393553, -5168.15049072,\n        -5163.37964133, -5168.63642701, -5164.67786834, -5166.91631179,\n        -5167.07707069, -5163.73013692, -5159.2118212 , -5164.16790786,\n...\n        -5164.77020421, -5163.2310316 , -5160.10037545, -5164.00632919,\n        -5166.06405296, -5164.86395908, -5166.08877352, -5161.83625466,\n        -5159.81175297, -5157.61267168, -5158.11102924, -5157.71019648,\n        -5159.44888705, -5161.85146396, -5163.93485031, -5168.01262032,\n        -5164.1659488 , -5160.48658724, -5159.63134357, -5171.5206486 ,\n        -5163.92658392, -5158.48792575, -5162.09241049, -5165.65689682,\n        -5161.44949737, -5161.51851691, -5164.60525019, -5167.74969789,\n        -5164.4680057 , -5164.95310418, -5162.49085348, -5161.81564374,\n        -5159.68290434, -5161.30032608, -5164.13046095, -5167.09316922,\n        -5169.83216611, -5168.06977091, -5163.54594307, -5159.42398601,\n        -5158.35281982, -5158.07698761, -5159.54260618, -5158.62030345,\n        -5160.03227255, -5160.75688655, -5160.17544324, -5159.4236945 ,\n        -5165.27397798, -5164.33321269, -5168.83665911, -5164.21035542,\n        -5163.90940746, -5166.86700046, -5168.17372111, -5168.11444744,\n        -5165.30982208, -5164.17867358, -5166.70096057, -5168.20447095,\n        -5162.11510533, -5164.30434718, -5164.27629412, -5168.81720522,\n        -5165.74147194, -5168.29075716, -5165.15501384, -5165.17615735,\n        -5164.64808597, -5161.56488094, -5164.14593562, -5159.49811032,\n        -5159.96893763, -5161.00401026, -5164.99082679, -5165.41652542,\n        -5163.48381725, -5163.37202903, -5160.53761037, -5163.13131434]])</pre></li><li>reached_max_treedepth(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False]])</pre></li><li>perf_counter_diff(chain, draw)float640.5618 0.3479 ... 0.7741 0.2888<pre>array([[0.56178504, 0.34788608, 0.21138754, 0.45768633, 1.0767425 ,\n        0.33561242, 0.49953779, 0.43079071, 0.3201605 , 0.86760908,\n        0.52474492, 0.40395454, 0.22185179, 0.34760396, 0.51227033,\n        0.81461742, 0.51895212, 1.08027321, 0.11191213, 0.55140504,\n        0.45780004, 0.67264917, 0.22093242, 0.38290625, 0.56365662,\n        0.49153671, 0.54956713, 0.22301962, 0.97540675, 0.49247954,\n        0.62335   , 0.51376333, 0.51796796, 0.5336725 , 0.47611625,\n        0.60485479, 0.740693  , 1.17127292, 1.52954446, 0.17817233,\n        0.44242754, 0.35408996, 0.13452   , 0.26400837, 0.79964575,\n        0.34571604, 0.25656979, 0.68904963, 0.49932242, 0.54515517,\n        0.20731933, 0.22674246, 0.42887704, 0.45426921, 0.52036446,\n        0.86473712, 0.8814    , 0.83741783, 0.66165467, 1.24247883,\n        0.21935521, 0.2926235 , 0.86778067, 0.72273013, 0.60650492,\n        0.23423171, 0.20735096, 0.61131546, 0.22649142, 1.02121342,\n        0.2968685 , 0.46614271, 0.579712  , 0.36533421, 0.20016254,\n        0.48069171, 0.30831267, 0.64558075, 0.52437675, 0.95200487,\n        0.47083808, 0.601385  , 1.04406175, 0.41164879, 0.22671958,\n        0.35944746, 0.33560538, 0.24425213, 0.96695483, 1.53759917,\n        0.92646808, 0.83993125, 1.16466037, 0.26823458, 0.45183787,\n        0.20309279, 0.24707317, 0.36196854, 0.26938258, 0.43531892,\n...\n        0.3576255 , 0.20836354, 0.55573896, 0.4886195 , 0.38520379,\n        0.32909212, 0.45418908, 0.45980188, 0.22835571, 0.87300408,\n        0.38498404, 0.51213229, 0.88300954, 0.33775842, 0.34883079,\n        0.36621767, 1.46720971, 0.23113296, 0.5061765 , 0.51445475,\n        0.219861  , 0.24354321, 0.38935346, 0.54625342, 0.15559687,\n        0.19760133, 0.30226229, 0.33361767, 0.48570992, 0.43697925,\n        0.55766408, 0.64387538, 0.40926213, 0.45912408, 0.36579146,\n        0.67237946, 0.16889437, 0.52118408, 0.32808442, 0.19713688,\n        0.24980221, 0.31722363, 0.33792487, 0.63101396, 0.62360271,\n        0.26617058, 0.37857221, 0.37426012, 0.18818246, 0.51764113,\n        0.74363313, 0.18377529, 0.47335658, 0.27071979, 0.50898846,\n        0.65393621, 0.0793345 , 0.39499879, 0.17337683, 0.41077067,\n        0.31728413, 0.38554771, 0.87824467, 0.38474983, 0.38175179,\n        0.4314495 , 0.394999  , 0.34670058, 0.72824408, 0.36307321,\n        0.68102971, 0.33494446, 0.77294433, 0.17189667, 0.3871665 ,\n        0.49093604, 0.28618375, 2.60756958, 0.31507913, 0.55081204,\n        0.42353071, 0.09384454, 0.36847083, 0.24308287, 0.18898933,\n        0.85002846, 0.18323704, 0.44967396, 0.46902908, 0.22137346,\n        0.23508208, 0.37330417, 0.06117767, 0.1300445 , 0.11693025,\n        0.24520096, 0.26204617, 0.33208337, 0.77406171, 0.28878083]])</pre></li><li>index_in_trajectory(chain, draw)int6413 -2 5 -24 28 9 ... -20 4 10 31 16<pre>array([[ 13,  -2,   5, -24,  28,   9,  14,  -5, -13,  28, -24,  18,   6,\n         11, -24,  21, -12,  17,   4,  16,  10,  20,   7,  -9,  -7,  14,\n         25,  -1,  20, -25,  -3,  -7,  19,  11,  10,  -8,  -7,  25,  -7,\n          4,  -1,  -8,   2,  -6,  11,  -4,  -9,   9, -14,   1, -12,  -6,\n         13,   5,  18,  14,  27,   4,   3,  29,   8,  10,  28,  25,  12,\n         15,   3,  12,  -7,   8,  -8, -13, -10,   3,  -9,  -6, -13,  -2,\n         14,  11,  19, -11, -19,  10,   9,   6,  -9,   2,  10,  -8,  -9,\n         -5,  13,   4, -13,   1,   4,  10,   7,  10,  -4, -11,  -9,   7,\n         13, -16,  13,  25,  14, -12,   9,   3,   5,   6, -11,   5, -23,\n         -7,  -7, -13,  -2,   7,   4,  -5, -28, -26,  11, -24,   2, -15,\n         10,  21,  -7,  23,  -8,  -3, -14,  -7, -16,  -4,   6, -12,  11,\n        -14,   6,   9, -22,  16, -18,   2,  -3,  12,  33,  -9,  14, -13,\n          2,  17, -16,  -8,   4,   6,   4,  -5, -16,   7, -13,   1,   6,\n        -14,  -6, -14,  14,   8,  21,   9,   8,  21,  -7, -14,   3, -11,\n         -5,  18,   4,  -7,  -9,  14,  -8, -17,  -6,  -7, -10, -10, -15,\n        -12, -12,   1,  -5, -30,   3,   8,  -7,   4,  -8,  -7,  -4, -27,\n         14, -10, -11,  27,  -3, -29,  -7,  17, -22,   6,  11,  21,  20,\n          3,   2, -11,  28,  28, -10, -16,  -7,  10,   3,   4, -46,  21,\n        -11, -10,   5,   4,  -8,  -3, -11,   3,   8,  24,   6,  -4,   3,\n          4,  17,  24, -24, -14,   3,  22,  12, -10,   9,   8,   9,   6,\n...\n        -13,  16,  17, -24,   7,   5,   5,  -5, -33,  17,  -3,  12,  15,\n          2,  -7,   5, -16, -14,  13,  -6,   3, -30,  -5,  -9,   4,  -7,\n         18,  13,  11,  -2,  -8,  23,  30, -14,  23, -10,   4,  -5,  12,\n        -28,  13,  -8,   4,   7,  -9,  14,   4,  19,  -9,   7,   6,  -9,\n        -10,  -8,  10, -25,  -8,  10,  26, -20,  18, -15,  -7,  -4, -22,\n         25, -14, -10, -11,  -5, -10,   6,  -7,  10,  -3,  -3, -19,  -2,\n        -25, -12,  -5,  11,  -8, -11,  15,  18,  -4,  -4,   5,  -5,  -4,\n        -15,  18,   5,  43,   7,   3,   6, -16,  -9,   9,  16,  -5,  13,\n         -8,   7,  -7,  -4, -15, -12, -17,   9,  10, -10, -11,  19,  -7,\n         12, -10,  17,  -9,   4,   9,  -6,  -3, -13,   3,  29, -33,  24,\n        -12,   2,  10,   8,  10,  -7, -20, -24,  12, -18,  -2, -15,  27,\n        -10,   9,   5,  -6,  -3, -62,  22,   4,  -8, -28,  -9,   9,  23,\n         24,  -8,  17,  10, -16,  -8, -28,  -2, -20,  33,   9,  19,  17,\n        -24, -15,  20, -11,   5, -10, -29,  16,   2,  11, -10,   4,   4,\n         21, -17, -17,  -9,  22,  23, -11, -14,  22, -13,  -9,   8, -10,\n        -10,  29,  19,   4, -18,   3,  -9, -12,  19,  -7, -10,  -5, -15,\n         -7,  -1,  17,  11,  -8, -17,  21,  -2,   1,   9, -13,  -5,   4,\n         -4,  -7,  -5,  17,  16,  -9,  -6,  18,  -8,  24,   9,   3,   5,\n          2,   5,  11,   9,  13,  11,  13,   6,  -7,  27,  15,   6,   6,\n          4, -20,   4,  10,  31,  16]])</pre></li><li>max_energy_error(chain, draw)float64-0.5562 0.5962 ... -0.668 0.8798<pre>array([[-0.55624999,  0.59620649,  1.91905557,  0.632831  , -0.39988093,\n         0.33857797,  2.76179797,  0.41634585, -0.27997289, -0.39253031,\n         0.74786968,  0.92127337, -0.57328097, -0.27758041, -0.40127708,\n         0.88203454, -0.67086427,  1.10335327,  2.20316947, -0.6463305 ,\n        -0.42807092,  0.47894639,  1.77294061, -0.41630354,  0.56700371,\n         0.31778876, -0.2412881 ,  0.31053526,  0.54002315,  0.6021178 ,\n         0.78790542,  0.36899408,  1.12566765, -0.36004681,  0.55303726,\n        -0.57419006,  0.69070752,  0.58121936,  1.0521363 ,  0.68052866,\n        -0.53449906, -0.25625916,  2.7093335 ,  0.4576624 , -0.37155589,\n         0.41222904,  0.60499057, -0.45892187, -0.88877952, -0.81818047,\n        -0.82613141,  1.00114524,  1.38143854, -0.47948547, -0.61979006,\n        -0.94670396, -0.57601438,  0.45193136, -0.28354267,  1.85472504,\n        -2.05831586,  0.40577484,  0.62352436,  0.25063839,  0.32757084,\n         0.58595445, -0.52686705,  0.57025329, -0.14923718, -0.28138166,\n         0.2572797 , -0.15907974,  0.54395611, -0.10718116,  0.38568109,\n         0.28954468, -0.29367757, -0.20429014,  0.4885047 ,  0.32327984,\n        -0.30532002,  0.44820465,  0.56691421, -0.62606199,  0.25803826,\n        -0.26903905,  0.36964504, -0.32439787,  0.3813873 , -0.57562586,\n         0.31180138, -0.33518831,  0.71388276,  0.58865851,  1.00350552,\n        -0.7794705 ,  0.23546239,  0.72789978,  0.20747398,  0.56093576,\n...\n        -0.26519556,  0.38885796, -0.49599738,  1.1701905 , -1.17554933,\n         0.81503877,  0.46197753,  4.57337541,  0.86178152, -0.22994439,\n         4.37544359, -0.4916727 ,  0.49100918,  0.4925612 , -0.42070643,\n        -0.18897355,  0.48382598,  0.2930296 ,  1.02630633, -0.76448043,\n        -0.37581359, -0.53203403, -0.50757515,  0.54221337,  0.54116682,\n        -0.47395582,  0.87940493,  3.00743231, -0.23657754, -0.26183083,\n         0.73719331,  0.30920354,  0.31510164, -0.27574451,  0.25923076,\n         0.28885745,  0.45003631,  0.79942838,  0.80335425,  0.75927028,\n        -0.67266342,  2.19071943, -1.26047529, -0.71097789, -0.74635157,\n         1.50617182,  0.34915105,  0.31396762, -0.16692245,  0.68141395,\n         1.53767656, -0.30645912,  0.38282358,  0.85325018,  0.78322678,\n         2.67069716, -0.72104767, -0.82507611, -1.16199093, -0.17372637,\n         0.42351249,  0.18628487,  0.19514137,  0.25923093, -0.2107092 ,\n         0.43437682,  0.29436481, -0.49753939,  2.48866661, -1.46006199,\n         0.67702002,  1.06723192,  0.83577518, -0.35309594, -0.38383395,\n         1.93062801,  0.95357523, -1.23041558,  0.60476035,  0.82775059,\n        -0.33786315,  0.29731773,  0.5751035 ,  0.39535478,  1.15318466,\n        -0.34155514, -0.29036886, -0.60122751, -0.84667881, -0.77142783,\n        -0.58219904, -0.66281583,  0.69715127,  1.1056013 ,  1.33642312,\n         1.28806763,  0.34490911,  0.16530096, -0.66798259,  0.87976044]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-29T20:05:48.566663+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :579.8671550750732tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 72kB\nDimensions:                  (__obs__: 3000, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 24kB 0 1 2 3 ... 2997 2998 2999\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 48kB ...\nAttributes:\n    created_at:                  2025-09-29T20:05:48.570621+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 3000</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 ... 2996 2997 2998 2999<pre>array([   0,    1,    2, ..., 2997, 2998, 2999], shape=(3000,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float641.118 1.0 2.871 ... 1.0 0.9202 1.0<pre>array([[1.11774075, 1.        ],\n       [2.87119389, 1.        ],\n       [4.01333284, 1.        ],\n       ...,\n       [1.78728008, 1.        ],\n       [0.84775424, 1.        ],\n       [0.92019612, 1.        ]], shape=(3000, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999],\n      dtype='int64', name='__obs__', length=3000))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (6)created_at :2025-09-29T20:05:48.570621+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> <p>Let's look at the posteriors!</p> In\u00a0[82]: Copied! <pre>az.plot_forest(model_reg_v_angle_hier.traces, var_names=[\"~v\", \"~a\"], combined=False)\n</pre> az.plot_forest(model_reg_v_angle_hier.traces, var_names=[\"~v\", \"~a\"], combined=False) Out[82]: <pre>array([&lt;Axes: title={'center': '94.0% HDI'}&gt;], dtype=object)</pre> <p>Fitting single models is all well and good. We are however, often interested in comparing how well a few different models account for the same data.</p> <p>Through ArviZ, we have out of the box access to modern Bayesian Model Comparison. We will keep it simple here, just to illustrate the basic idea.</p> In\u00a0[83]: Copied! <pre># Parameters\nparam_dict_mod_comp = dict(v=0.5, a=1.5, z=0.5, t=0.2)\n\n# Simulation\ndataset_model_comp = hssm.simulate_data(\n    model=\"ddm\", theta=param_dict_mod_comp, size=500\n)\n\nprint(dataset_model_comp)\n</pre> # Parameters param_dict_mod_comp = dict(v=0.5, a=1.5, z=0.5, t=0.2)  # Simulation dataset_model_comp = hssm.simulate_data(     model=\"ddm\", theta=param_dict_mod_comp, size=500 )  print(dataset_model_comp) <pre>           rt  response\n0    0.947397       1.0\n1    0.859091       1.0\n2    1.819618       1.0\n3    2.348086      -1.0\n4    1.360105       1.0\n..        ...       ...\n495  1.980245       1.0\n496  2.088078       1.0\n497  0.558926       1.0\n498  0.595095       1.0\n499  2.158128       1.0\n\n[500 rows x 2 columns]\n</pre> In\u00a0[84]: Copied! <pre># 'wrong' model\nmodel_model_comp_1 = hssm.HSSM(\n    data=dataset_model_comp,\n    model=\"angle\",\n    a=1.0,\n)\n</pre> # 'wrong' model model_model_comp_1 = hssm.HSSM(     data=dataset_model_comp,     model=\"angle\",     a=1.0, ) <pre>Model initialized successfully.\n</pre> In\u00a0[85]: Copied! <pre># 'correct' model\nmodel_model_comp_2 = hssm.HSSM(\n    data=dataset_model_comp,\n    model=\"angle\",\n    a=1.5,\n)\n</pre> # 'correct' model model_model_comp_2 = hssm.HSSM(     data=dataset_model_comp,     model=\"angle\",     a=1.5, ) <pre>Model initialized successfully.\n</pre> In\u00a0[86]: Copied! <pre># 'wrong' model ddm\nmodel_model_comp_3 = hssm.HSSM(\n    data=dataset_model_comp,\n    model=\"ddm\",\n    a=1.0,\n)\n</pre> # 'wrong' model ddm model_model_comp_3 = hssm.HSSM(     data=dataset_model_comp,     model=\"ddm\",     a=1.0, ) <pre>Model initialized successfully.\n</pre> In\u00a0[87]: Copied! <pre>infer_data_model_comp_1 = model_model_comp_1.sample(\n    sampler=\"mcmc\",\n    cores=1,\n    chains=2,\n    draws=1000,\n    tune=1000,\n    idata_kwargs=dict(\n        log_likelihood=True\n    ),  # model comparison metrics usually need this!\n    mp_ctx=\"spawn\",\n)\n</pre> infer_data_model_comp_1 = model_model_comp_1.sample(     sampler=\"mcmc\",     cores=1,     chains=2,     draws=1000,     tune=1000,     idata_kwargs=dict(         log_likelihood=True     ),  # model comparison metrics usually need this!     mp_ctx=\"spawn\", ) <pre>Using default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nSequential sampling (2 chains in 1 job)\nNUTS: [z, theta, t, v]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 86 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:01&lt;00:00, 1367.80it/s]\n</pre> In\u00a0[88]: Copied! <pre>infer_data_model_comp_2 = model_model_comp_2.sample(\n    sampler=\"mcmc\",\n    cores=1,\n    chains=2,\n    draws=1000,\n    tune=1000,\n    idata_kwargs=dict(\n        log_likelihood=True\n    ),  # model comparison metrics usually need this!\n    mp_ctx=\"spawn\",\n)\n</pre> infer_data_model_comp_2 = model_model_comp_2.sample(     sampler=\"mcmc\",     cores=1,     chains=2,     draws=1000,     tune=1000,     idata_kwargs=dict(         log_likelihood=True     ),  # model comparison metrics usually need this!     mp_ctx=\"spawn\", ) <pre>Using default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nSequential sampling (2 chains in 1 job)\nNUTS: [z, theta, t, v]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 66 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:01&lt;00:00, 1432.74it/s]\n</pre> In\u00a0[89]: Copied! <pre>infer_data_model_comp_3 = model_model_comp_3.sample(\n    sampler=\"mcmc\",\n    cores=1,\n    chains=2,\n    draws=1000,\n    tune=1000,\n    idata_kwargs=dict(\n        log_likelihood=True\n    ),  # model comparison metrics usually need this!\n    mp_ctx=\"spawn\",\n)\n</pre> infer_data_model_comp_3 = model_model_comp_3.sample(     sampler=\"mcmc\",     cores=1,     chains=2,     draws=1000,     tune=1000,     idata_kwargs=dict(         log_likelihood=True     ),  # model comparison metrics usually need this!     mp_ctx=\"spawn\", ) <pre>Using default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nSequential sampling (2 chains in 1 job)\nNUTS: [t, z, v]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 9 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:00&lt;00:00, 5398.52it/s]\n</pre> In\u00a0[90]: Copied! <pre>compare_data = az.compare(\n    {\n        \"a_fixed_1(wrong)\": model_model_comp_1.traces,\n        \"a_fixed_1.5(correct)\": model_model_comp_2.traces,\n        \"a_fixed_1_ddm(wrong)\": model_model_comp_3.traces,\n    }\n)\n\ncompare_data\n</pre> compare_data = az.compare(     {         \"a_fixed_1(wrong)\": model_model_comp_1.traces,         \"a_fixed_1.5(correct)\": model_model_comp_2.traces,         \"a_fixed_1_ddm(wrong)\": model_model_comp_3.traces,     } )  compare_data Out[90]: rank elpd_loo p_loo elpd_diff weight se dse warning scale a_fixed_1.5(correct) 0 -991.190525 3.971025 0.000000 1.000000e+00 24.882089 0.000000 False log a_fixed_1(wrong) 1 -1058.022613 5.342018 66.832088 0.000000e+00 30.377535 11.145312 True log a_fixed_1_ddm(wrong) 2 -1135.806276 4.812125 144.615751 5.314671e-10 35.463456 17.204704 True log <p>Notice how the posterior weight on the <code>correct</code> model is close to (or equal to ) $1$ here. In other words model comparison points us to the correct model with a very high degree of certainty here!</p> <p>We can also use the <code>.plot_compare()</code> function to illustrate the model comparison visually.</p> In\u00a0[91]: Copied! <pre>az.plot_compare(compare_data)\n</pre> az.plot_compare(compare_data) Out[91]: <pre>&lt;Axes: title={'center': 'Model comparison\\nhigher is better'}, xlabel='elpd_loo (log)', ylabel='ranked models'&gt;</pre> <p>Using the forest plot we can take a look at what goes wrong for the \"wrong\" model.</p> <p>To make up for the mistplaced setting of the <code>a</code> parameter, the posterior seems to compensate by mis-estimating the other parameters.</p> In\u00a0[92]: Copied! <pre>az.plot_forest(\n    [model_model_comp_1.traces, model_model_comp_2.traces, model_model_comp_3.traces],\n    model_names=[\"a_fixed_1(wrong)\", \"a_fixed_1.5(correct)\", \"a_fixed_1(wrong)_ddm\"],\n)\n</pre> az.plot_forest(     [model_model_comp_1.traces, model_model_comp_2.traces, model_model_comp_3.traces],     model_names=[\"a_fixed_1(wrong)\", \"a_fixed_1.5(correct)\", \"a_fixed_1(wrong)_ddm\"], ) Out[92]: <pre>array([&lt;Axes: title={'center': '94.0% HDI'}&gt;], dtype=object)</pre> <p>We have seen a few examples of HSSM models at this point. Add a model via a string, maybe toy a bit with with the priors and set regression functions for a given parameter. Turn it hierarchical...  Here we begin to peak a bit under the hood.</p> <p>After all, we want to encourage you to contribute models to the package yourself.</p> <p>Let's remind ourself of the <code>model_config</code> dictionaries that define model properties for us. Again let's start with the DDM.</p> In\u00a0[93]: Copied! <pre>hssm.config.default_model_config[\"ddm\"].keys()\n</pre> hssm.config.default_model_config[\"ddm\"].keys() Out[93]: <pre>dict_keys(['response', 'list_params', 'choices', 'description', 'likelihoods'])</pre> <p>The dictionary has a few high level keys.</p> <ol> <li><p><code>response</code></p> </li> <li><p><code>list_params</code></p> </li> <li><p><code>description</code></p> </li> <li><p><code>likelihoods</code></p> </li> </ol> <p>Let us take a look at the available <code>likelihoods</code>:</p> In\u00a0[94]: Copied! <pre>hssm.config.default_model_config[\"ddm\"][\"likelihoods\"]\n</pre> hssm.config.default_model_config[\"ddm\"][\"likelihoods\"] Out[94]: <pre>{'analytical': {'loglik': &lt;function hssm.likelihoods.analytical.logp_ddm(data: numpy.ndarray, v: float, a: float, z: float, t: float, err: float = 1e-15, k_terms: int = 20, epsilon: float = 1e-15) -&gt; numpy.ndarray&gt;,\n  'backend': None,\n  'bounds': {'v': (-inf, inf),\n   'a': (0.0, inf),\n   'z': (0.0, 1.0),\n   't': (0.0, inf)},\n  'default_priors': {'t': {'name': 'HalfNormal', 'sigma': 2.0}},\n  'extra_fields': None},\n 'approx_differentiable': {'loglik': 'ddm.onnx',\n  'backend': 'jax',\n  'default_priors': {'t': {'name': 'HalfNormal', 'sigma': 2.0}},\n  'bounds': {'v': (-3.0, 3.0),\n   'a': (0.3, 2.5),\n   'z': (0.0, 1.0),\n   't': (0.0, 2.0)},\n  'extra_fields': None},\n 'blackbox': {'loglik': &lt;function hssm.likelihoods.blackbox.hddm_to_hssm.&lt;locals&gt;.outer(data: numpy.ndarray, *args, **kwargs)&gt;,\n  'backend': None,\n  'bounds': {'v': (-inf, inf),\n   'a': (0.0, inf),\n   'z': (0.0, 1.0),\n   't': (0.0, inf)},\n  'default_priors': {'t': {'name': 'HalfNormal', 'sigma': 2.0}},\n  'extra_fields': None}}</pre> <p>For the DDM we have available all three types of likelihoods that HSSM deals with:</p> <ol> <li><code>analytical</code></li> <li><code>approx_differentiable</code></li> <li><code>blackbox</code></li> </ol> <p>Let's expand the dictionary contents more:</p> In\u00a0[95]: Copied! <pre>hssm.config.default_model_config[\"ddm\"][\"likelihoods\"][\"analytical\"]\n</pre> hssm.config.default_model_config[\"ddm\"][\"likelihoods\"][\"analytical\"] Out[95]: <pre>{'loglik': &lt;function hssm.likelihoods.analytical.logp_ddm(data: numpy.ndarray, v: float, a: float, z: float, t: float, err: float = 1e-15, k_terms: int = 20, epsilon: float = 1e-15) -&gt; numpy.ndarray&gt;,\n 'backend': None,\n 'bounds': {'v': (-inf, inf),\n  'a': (0.0, inf),\n  'z': (0.0, 1.0),\n  't': (0.0, inf)},\n 'default_priors': {'t': {'name': 'HalfNormal', 'sigma': 2.0}},\n 'extra_fields': None}</pre> <p>We see three properties (key) in this dictionary, of which two are essential:</p> <ul> <li>The <code>loglik</code> field, which points to the likelihood function</li> <li>The <code>backend</code> field, which can be either <code>None</code> (defaulting to pytensor for <code>analytical</code> likelihoods), <code>jax</code> or <code>pytensor</code></li> <li>The <code>bounds</code> field, which specifies bounds on a subset of the model parameters</li> <li>The <code>default_priors</code> field, which specifies parameter wise priors</li> </ul> <p>If you provide <code>bounds</code> for a parameter, but no <code>default_priors</code>, a Uniform prior that respects the specified bounds will be applied.</p> <p>Next, let's look at the <code>approx_differentiable</code> part. The likelihood in this part is based on a LAN which was available in HDDM through the LAN extension.</p> In\u00a0[96]: Copied! <pre>hssm.config.default_model_config[\"ddm\"][\"likelihoods\"][\"approx_differentiable\"]\n</pre> hssm.config.default_model_config[\"ddm\"][\"likelihoods\"][\"approx_differentiable\"] Out[96]: <pre>{'loglik': 'ddm.onnx',\n 'backend': 'jax',\n 'default_priors': {'t': {'name': 'HalfNormal', 'sigma': 2.0}},\n 'bounds': {'v': (-3.0, 3.0),\n  'a': (0.3, 2.5),\n  'z': (0.0, 1.0),\n  't': (0.0, 2.0)},\n 'extra_fields': None}</pre> <p>We see that the <code>loglik</code> field is now a string that points to a <code>.onnx</code> file. Onnx is a meta framework for Neural Network specification, that allows translation between deep learning Frameworks. This is the preferred format for the neural networks we store in our model reservoir on HuggingFace.</p> <p>Moreover notice that we now have a <code>backend</code> field. We allow for two primary backends in the <code>approx_differentiable</code> field.</p> <ol> <li><code>pytensor</code></li> <li><code>jax</code></li> </ol> <p>The <code>jax</code> backend assumes that your likelihood is described as a jax function, the <code>pytensor</code> backend assumes that your likelihood is described as a <code>pytensor</code> function. Ok not that surprising...</p> <p>We won't dwell on this here, however the key idea is to provide users with a large degree of flexibility in describing their likelihood functions and moreover to allow targeted optimization towards MCMC sampler types that PyMC allows us to access.</p> <p>You can find a dedicated tutorial in the documentation, which describes the different likelihoods in much more detail.</p> <p>Instead, let's take a quick look at how these newfound insights can be used for custom model definition.</p> In\u00a0[97]: Copied! <pre>hssm_alternative_model = hssm.HSSM(\n    data=dataset,\n    model=\"ddm\",\n    loglik_kind=\"approx_differentiable\",\n)\n</pre> hssm_alternative_model = hssm.HSSM(     data=dataset,     model=\"ddm\",     loglik_kind=\"approx_differentiable\", ) <pre>Model initialized successfully.\n</pre> In\u00a0[98]: Copied! <pre>hssm_alternative_model.loglik_kind\n</pre> hssm_alternative_model.loglik_kind Out[98]: <pre>'approx_differentiable'</pre> <p>In this case we actually built the model class with an <code>approx_differentiable</code> LAN likelihood, instead of the default <code>analytical</code> likelihood we used in the beginning of the tutorial. The assumed generative model remains the <code>ddm</code> however!</p> In\u00a0[99]: Copied! <pre>hssm_alternative_model.sample(\n    sampler=\"mcmc\",\n    cores=1,\n    chains=2,\n    draws=1000,\n    tune=1000,\n    idata_kwargs=dict(\n        log_likelihood=False\n    ),  # model comparison metrics usually need this!\n    mp_ctx=\"spawn\",\n)\n</pre> hssm_alternative_model.sample(     sampler=\"mcmc\",     cores=1,     chains=2,     draws=1000,     tune=1000,     idata_kwargs=dict(         log_likelihood=False     ),  # model comparison metrics usually need this!     mp_ctx=\"spawn\", ) <pre>Using default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nSequential sampling (2 chains in 1 job)\nNUTS: [t, a, z, v]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 63 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n</pre> Out[99]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 72kB\nDimensions:  (chain: 2, draw: 1000)\nCoordinates:\n  * chain    (chain) int64 16B 0 1\n  * draw     (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\nData variables:\n    v        (chain, draw) float64 16kB 0.5368 0.5377 0.5226 ... 0.5525 0.5804\n    a        (chain, draw) float64 16kB 1.481 1.437 1.512 ... 1.432 1.441 1.491\n    z        (chain, draw) float64 16kB 0.4624 0.4922 0.4705 ... 0.508 0.4946\n    t        (chain, draw) float64 16kB 0.4729 0.5499 0.4761 ... 0.5121 0.5558\nAttributes:\n    created_at:                  2025-09-29T20:10:00.510764+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               63.46027493476868\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 1000</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li></ul></li><li>Data variables: (4)<ul><li>v(chain, draw)float640.5368 0.5377 ... 0.5525 0.5804<pre>array([[0.5367852 , 0.53770214, 0.52256472, ..., 0.58830804, 0.53755784,\n        0.54063826],\n       [0.51420987, 0.48289991, 0.45934233, ..., 0.46968172, 0.55245757,\n        0.58040368]], shape=(2, 1000))</pre></li><li>a(chain, draw)float641.481 1.437 1.512 ... 1.441 1.491<pre>array([[1.48067775, 1.43748834, 1.51182284, ..., 1.51024226, 1.5043503 ,\n        1.48309005],\n       [1.40205208, 1.41546807, 1.43105894, ..., 1.43151721, 1.44102976,\n        1.49081092]], shape=(2, 1000))</pre></li><li>z(chain, draw)float640.4624 0.4922 ... 0.508 0.4946<pre>array([[0.46237949, 0.492217  , 0.47047993, ..., 0.47102136, 0.47337422,\n        0.48334764],\n       [0.49472919, 0.48320483, 0.49254194, ..., 0.48238667, 0.50799577,\n        0.49459403]], shape=(2, 1000))</pre></li><li>t(chain, draw)float640.4729 0.5499 ... 0.5121 0.5558<pre>array([[0.47293771, 0.54989712, 0.47611603, ..., 0.47590237, 0.44832839,\n        0.46896161],\n       [0.53995213, 0.5253624 , 0.49427825, ..., 0.52110777, 0.5121066 ,\n        0.55578313]], shape=(2, 1000))</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-29T20:10:00.510764+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :63.46027493476868tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 268kB\nDimensions:                (chain: 2, draw: 1000)\nCoordinates:\n  * chain                  (chain) int64 16B 0 1\n  * draw                   (draw) int64 8kB 0 1 2 3 4 5 ... 995 996 997 998 999\nData variables: (12/18)\n    energy                 (chain, draw) float64 16kB 1.03e+03 ... 1.034e+03\n    step_size_bar          (chain, draw) float64 16kB 0.6152 0.6152 ... 0.661\n    divergences            (chain, draw) int64 16kB 0 0 0 0 0 0 ... 0 0 0 0 0 0\n    perf_counter_start     (chain, draw) float64 16kB 1.668e+06 ... 1.668e+06\n    diverging              (chain, draw) bool 2kB False False ... False False\n    step_size              (chain, draw) float64 16kB 0.5823 0.5823 ... 0.8641\n    ...                     ...\n    acceptance_rate        (chain, draw) float64 16kB 1.0 0.8801 ... 0.6555\n    lp                     (chain, draw) float64 16kB -1.027e+03 ... -1.032e+03\n    reached_max_treedepth  (chain, draw) bool 2kB False False ... False False\n    perf_counter_diff      (chain, draw) float64 16kB 0.006734 ... 0.007361\n    index_in_trajectory    (chain, draw) int64 16kB 1 -3 -4 2 -2 ... -2 3 3 -1\n    max_energy_error       (chain, draw) float64 16kB -0.4834 0.2833 ... 0.5488\nAttributes:\n    created_at:                  2025-09-29T20:10:00.527562+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               63.46027493476868\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 1000</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li></ul></li><li>Data variables: (18)<ul><li>energy(chain, draw)float641.03e+03 1.029e+03 ... 1.034e+03<pre>array([[1029.61127388, 1029.14845513, 1031.18594023, ..., 1029.06965825,\n        1028.77079922, 1028.22514354],\n       [1031.83950911, 1028.22929168, 1028.02579231, ..., 1029.20322569,\n        1029.07353841, 1034.07802398]], shape=(2, 1000))</pre></li><li>step_size_bar(chain, draw)float640.6152 0.6152 ... 0.661 0.661<pre>array([[0.61522826, 0.61522826, 0.61522826, ..., 0.61522826, 0.61522826,\n        0.61522826],\n       [0.66099071, 0.66099071, 0.66099071, ..., 0.66099071, 0.66099071,\n        0.66099071]], shape=(2, 1000))</pre></li><li>divergences(chain, draw)int640 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0<pre>array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], shape=(2, 1000))</pre></li><li>perf_counter_start(chain, draw)float641.668e+06 1.668e+06 ... 1.668e+06<pre>array([[1667988.14751983, 1667988.15431867, 1667988.16820404, ...,\n        1668002.19757975, 1668002.22751471, 1668002.23632138],\n       [1668017.14086625, 1668017.15454642, 1668017.16138079, ...,\n        1668032.45761938, 1668032.46575429, 1668032.48066812]],\n      shape=(2, 1000))</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]], shape=(2, 1000))</pre></li><li>step_size(chain, draw)float640.5823 0.5823 ... 0.8641 0.8641<pre>array([[0.58232783, 0.58232783, 0.58232783, ..., 0.58232783, 0.58232783,\n        0.58232783],\n       [0.86405844, 0.86405844, 0.86405844, ..., 0.86405844, 0.86405844,\n        0.86405844]], shape=(2, 1000))</pre></li><li>energy_error(chain, draw)float64-0.4834 0.2417 ... 0.5506 0.5488<pre>array([[-0.48343918,  0.24173168,  0.36574827, ...,  0.01943607,\n        -0.08258222,  0.07327642],\n       [ 0.        , -0.02010898,  0.10920972, ..., -0.1694339 ,\n         0.55055022,  0.54880098]], shape=(2, 1000))</pre></li><li>largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], shape=(2, 1000))</pre></li><li>tree_depth(chain, draw)int642 3 4 2 2 3 2 2 ... 3 2 3 2 2 2 3 2<pre>array([[2, 3, 4, ..., 3, 2, 2],\n       [3, 2, 3, ..., 2, 3, 2]], shape=(2, 1000))</pre></li><li>process_time_diff(chain, draw)float640.01055 0.02035 ... 0.02257 0.01116<pre>array([[0.010549, 0.020354, 0.05066 , ..., 0.021956, 0.013735, 0.011326],\n       [0.021493, 0.010828, 0.024254, ..., 0.011478, 0.022569, 0.011156]],\n      shape=(2, 1000))</pre></li><li>n_steps(chain, draw)float643.0 7.0 15.0 3.0 ... 3.0 7.0 3.0<pre>array([[ 3.,  7., 15., ...,  7.,  3.,  3.],\n       [ 7.,  3.,  7., ...,  3.,  7.,  3.]], shape=(2, 1000))</pre></li><li>smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], shape=(2, 1000))</pre></li><li>acceptance_rate(chain, draw)float641.0 0.8801 0.4528 ... 0.8117 0.6555<pre>array([[1.        , 0.8801289 , 0.45280002, ..., 0.95731727, 0.96907875,\n        0.92180452],\n       [0.50474676, 0.9175048 , 0.88120086, ..., 0.89730899, 0.81168951,\n        0.65553916]], shape=(2, 1000))</pre></li><li>lp(chain, draw)float64-1.027e+03 ... -1.032e+03<pre>array([[-1027.09702259, -1027.61681789, -1027.54985651, ...,\n        -1027.81596349, -1027.30467895, -1026.64487962],\n       [-1027.03308513, -1026.70733863, -1026.88994815, ...,\n        -1026.91647751, -1028.71924633, -1032.02610939]], shape=(2, 1000))</pre></li><li>reached_max_treedepth(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]], shape=(2, 1000))</pre></li><li>perf_counter_diff(chain, draw)float640.006734 0.01288 ... 0.007361<pre>array([[0.00673362, 0.01288233, 0.03564733, ..., 0.02983779, 0.00872192,\n        0.00739346],\n       [0.01359483, 0.00677196, 0.01545817, ..., 0.0080395 , 0.01480883,\n        0.00736079]], shape=(2, 1000))</pre></li><li>index_in_trajectory(chain, draw)int641 -3 -4 2 -2 5 ... 1 -2 -2 3 3 -1<pre>array([[ 1, -3, -4, ..., -4,  3, -1],\n       [ 0, -1, -2, ...,  3,  3, -1]], shape=(2, 1000))</pre></li><li>max_energy_error(chain, draw)float64-0.4834 0.2833 ... 0.5506 0.5488<pre>array([[-0.48343918,  0.28333921,  1.73196281, ..., -0.24185607,\n         0.09735239,  0.12711977],\n       [ 1.47032823,  0.1636309 ,  0.25673897, ...,  0.36827488,\n         0.55055022,  0.54880098]], shape=(2, 1000))</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-29T20:10:00.527562+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :63.46027493476868tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 12kB\nDimensions:                  (__obs__: 500, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 4kB 0 1 2 3 4 ... 496 497 498 499\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 8kB 1...\nAttributes:\n    created_at:                  2025-09-29T20:10:00.531110+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 500</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float641.909 1.0 2.701 ... 1.0 0.6543 1.0<pre>array([[ 1.90879178,  1.        ],\n       [ 2.70051455,  1.        ],\n       [ 5.07957602, -1.        ],\n       [ 0.95730692, -1.        ],\n       [ 3.75961304, -1.        ],\n       [ 1.05477774,  1.        ],\n       [ 6.26821756,  1.        ],\n       [ 2.94682932, -1.        ],\n       [ 1.15484786,  1.        ],\n       [ 3.46363139,  1.        ],\n       [ 1.62390924,  1.        ],\n       [ 1.14314425, -1.        ],\n       [ 5.05555296,  1.        ],\n       [ 1.90220582, -1.        ],\n       [ 3.13259554, -1.        ],\n       [ 2.96012521,  1.        ],\n       [ 2.41100526,  1.        ],\n       [ 2.10615134,  1.        ],\n       [ 1.60785794,  1.        ],\n       [ 1.40975022,  1.        ],\n...\n       [ 2.91743708,  1.        ],\n       [ 1.57587361,  1.        ],\n       [ 3.21329093, -1.        ],\n       [ 1.49868929,  1.        ],\n       [ 1.72745574,  1.        ],\n       [ 1.17646027,  1.        ],\n       [ 1.63405263,  1.        ],\n       [ 2.31994152,  1.        ],\n       [ 1.88749003,  1.        ],\n       [ 2.36859488,  1.        ],\n       [ 2.85419393,  1.        ],\n       [ 2.84367776,  1.        ],\n       [ 1.24673474,  1.        ],\n       [ 2.03972936,  1.        ],\n       [ 2.95965099,  1.        ],\n       [ 1.49344993,  1.        ],\n       [ 2.35472059,  1.        ],\n       [ 1.88818717,  1.        ],\n       [ 1.11251736,  1.        ],\n       [ 0.65433359,  1.        ]])</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='__obs__', length=500))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (6)created_at :2025-09-29T20:10:00.531110+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[100]: Copied! <pre>az.plot_forest(hssm_alternative_model.traces)\n</pre> az.plot_forest(hssm_alternative_model.traces) Out[100]: <pre>array([&lt;Axes: title={'center': '94.0% HDI'}&gt;], dtype=object)</pre> <p>We can take this further and specify a completely custom likelihood. See the dedicated tutorial for more examples!</p> <p>We will see one specific example below to illustrate another type of likelihood function we have available for model building in HSSM, the Blackbox likelihood.</p> <p>We will do something simpler to keep it short and sweet, but really... the possibilities are endless!</p> <p> </p> <p>As always, let's begin by generating some simple dataset.</p> In\u00a0[101]: Copied! <pre># Set parameters\nparam_dict_blackbox = dict(v=0.5, a=1.5, z=0.5, t=0.5)\n\n# Simulate\ndataset_blackbox = hssm.simulate_data(model=\"ddm\", theta=param_dict_blackbox, size=1000)\n</pre> # Set parameters param_dict_blackbox = dict(v=0.5, a=1.5, z=0.5, t=0.5)  # Simulate dataset_blackbox = hssm.simulate_data(model=\"ddm\", theta=param_dict_blackbox, size=1000) <p>Now the fun part... we simply define a Python function <code>my_blackbox_loglik</code> which takes in our <code>data</code> as well as a bunch of model parameters (in our case the familiar <code>v</code>,<code>a</code>, <code>z</code>, <code>t</code> from the DDM).</p> <p>The function then does some arbitrary computation inside (in our case e.g. we pass the data and parameters to the DDM log-likelihood from our predecessor package HDDM).</p> <p>The important part is that inside <code>my_blackbox_loglik</code> anything can happen. We happen to call a little custom function that defines the likelihood of a DDM.</p> <p>Fun fact: It is de-facto the likelihood which is called by HDDM.</p> In\u00a0[102]: Copied! <pre>def my_blackbox_loglik(data, v, a, z, t, err=1e-8):\n    \"\"\"Create a custom blackbox likelihood function.\"\"\"\n    data = data[:, 0] * data[:, 1]\n    data_nrows = data.shape[0]\n    # Our function expects inputs as float64, but they are not guaranteed to\n    # come in as such --&gt; we type convert\n    return hddm_wfpt.wfpt.wiener_logp_array(\n        np.float64(data),\n        (np.ones(data_nrows) * v).astype(np.float64),\n        np.ones(data_nrows) * 0,\n        (np.ones(data_nrows) * 2 * a).astype(np.float64),\n        (np.ones(data_nrows) * z).astype(np.float64),\n        np.ones(data_nrows) * 0,\n        (np.ones(data_nrows) * t).astype(np.float64),\n        np.ones(data_nrows) * 0,\n        err,\n        1,\n    )\n</pre> def my_blackbox_loglik(data, v, a, z, t, err=1e-8):     \"\"\"Create a custom blackbox likelihood function.\"\"\"     data = data[:, 0] * data[:, 1]     data_nrows = data.shape[0]     # Our function expects inputs as float64, but they are not guaranteed to     # come in as such --&gt; we type convert     return hddm_wfpt.wfpt.wiener_logp_array(         np.float64(data),         (np.ones(data_nrows) * v).astype(np.float64),         np.ones(data_nrows) * 0,         (np.ones(data_nrows) * 2 * a).astype(np.float64),         (np.ones(data_nrows) * z).astype(np.float64),         np.ones(data_nrows) * 0,         (np.ones(data_nrows) * t).astype(np.float64),         np.ones(data_nrows) * 0,         err,         1,     ) <p>We can now define our HSSM model class as usual, however passing our <code>my_blackbox_loglik()</code> function to the <code>loglik</code> argument, and passing as <code>loglik_kind = blackbox</code>.</p> <p>The rest of the model config is as usual. Here we can reuse our <code>ddm</code> model config, and simply specify bounds on the parameters (e.g. your Blackbox Likelihood might be trustworthy only on a restricted parameters space).</p> In\u00a0[103]: Copied! <pre>blackbox_model = hssm.HSSM(\n    data=dataset_blackbox,\n    model=\"ddm\",\n    loglik=my_blackbox_loglik,\n    loglik_kind=\"blackbox\",\n    model_config={\n        \"bounds\": {\n            \"v\": (-10.0, 10.0),\n            \"a\": (0.5, 5.0),\n            \"z\": (0.0, 1.0),\n        }\n    },\n    t=bmb.Prior(\"Uniform\", lower=0.0, upper=2.0),\n)\n</pre> blackbox_model = hssm.HSSM(     data=dataset_blackbox,     model=\"ddm\",     loglik=my_blackbox_loglik,     loglik_kind=\"blackbox\",     model_config={         \"bounds\": {             \"v\": (-10.0, 10.0),             \"a\": (0.5, 5.0),             \"z\": (0.0, 1.0),         }     },     t=bmb.Prior(\"Uniform\", lower=0.0, upper=2.0), ) <pre>Model initialized successfully.\n</pre> In\u00a0[104]: Copied! <pre>blackbox_model.graph()\n</pre> blackbox_model.graph() <pre>max_shape:  (1000,)\nsize:  (np.int64(1000),)\n</pre> Out[104]: In\u00a0[105]: Copied! <pre>sample = blackbox_model.sample()\n</pre> sample = blackbox_model.sample() <pre>Using default initvals. \n\n</pre> <pre>Multiprocess sampling (4 chains in 4 jobs)\nCompoundStep\n&gt;Slice: [t]\n&gt;Slice: [a]\n&gt;Slice: [z]\n&gt;Slice: [v]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 26 seconds.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4000/4000 [00:01&lt;00:00, 2078.36it/s]\n</pre> <p>NOTE:</p> <p>Since Blackbox likelihood functions are assumed to not be differentiable, our default sampler for such likelihood functions is a <code>Slice</code> sampler. HSSM allows you to choose any other suitable sampler from the PyMC package instead. A bunch of options are available for gradient-free samplers.</p> In\u00a0[106]: Copied! <pre>az.summary(sample)\n</pre> az.summary(sample) Out[106]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat z 0.473 0.013 0.446 0.496 0.000 0.000 1204.0 1738.0 1.0 v 0.596 0.034 0.531 0.658 0.001 0.001 1369.0 2017.0 1.0 t 0.472 0.024 0.428 0.516 0.001 0.000 1280.0 1797.0 1.0 a 1.536 0.029 1.480 1.590 0.001 0.000 1499.0 1827.0 1.0 In\u00a0[107]: Copied! <pre>az.plot_trace(\n    sample,\n    lines=[(key_, {}, param_dict_blackbox[key_]) for key_ in param_dict_blackbox],\n)\nplt.tight_layout()\n</pre> az.plot_trace(     sample,     lines=[(key_, {}, param_dict_blackbox[key_]) for key_ in param_dict_blackbox], ) plt.tight_layout() In\u00a0[108]: Copied! <pre># DDM models (the Wiener First-Passage Time distribution)\nfrom hssm.distribution_utils import make_distribution\nfrom hssm.likelihoods import DDM\n</pre> # DDM models (the Wiener First-Passage Time distribution) from hssm.distribution_utils import make_distribution from hssm.likelihoods import DDM In\u00a0[109]: Copied! <pre># Simulate\nparam_dict_pymc = dict(v=0.5, a=1.5, z=0.5, t=0.5)\n\ndataset_pymc = hssm.simulate_data(model=\"ddm\", theta=param_dict_pymc, size=1000)\n</pre> # Simulate param_dict_pymc = dict(v=0.5, a=1.5, z=0.5, t=0.5)  dataset_pymc = hssm.simulate_data(model=\"ddm\", theta=param_dict_pymc, size=1000) <p>We can now use our custom random variable <code>DDM</code> directly in a PyMC model.</p> In\u00a0[110]: Copied! <pre>import pymc as pm\n\nwith pm.Model() as ddm_pymc:\n    v = pm.Uniform(\"v\", lower=-10.0, upper=10.0)\n    a = pm.HalfNormal(\"a\", sigma=2.0)\n    z = pm.Uniform(\"z\", lower=0.01, upper=0.99)\n    t = pm.Uniform(\"t\", lower=0.0, upper=0.6)\n\n    ddm = DDM(\n        \"DDM\", observed=dataset_pymc[[\"rt\", \"response\"]].values, v=v, a=a, z=z, t=t, \n    )\n</pre> import pymc as pm  with pm.Model() as ddm_pymc:     v = pm.Uniform(\"v\", lower=-10.0, upper=10.0)     a = pm.HalfNormal(\"a\", sigma=2.0)     z = pm.Uniform(\"z\", lower=0.01, upper=0.99)     t = pm.Uniform(\"t\", lower=0.0, upper=0.6)      ddm = DDM(         \"DDM\", observed=dataset_pymc[[\"rt\", \"response\"]].values, v=v, a=a, z=z, t=t,      ) <p>Let's check the model graph:</p> In\u00a0[111]: Copied! <pre>pm.model_to_graphviz(model=ddm_pymc)\n</pre> pm.model_to_graphviz(model=ddm_pymc) <pre>size all scalar:  (np.int64(1000),)\n</pre> Out[111]: <p>Looks remarkably close to our HSSM version!</p> <p>We can use PyMC directly to sample and finally return to ArviZ for some plotting!</p> In\u00a0[112]: Copied! <pre>with ddm_pymc:\n    ddm_pymc_trace = pm.sample()\n</pre> with ddm_pymc:     ddm_pymc_trace = pm.sample() <pre>Initializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [v, a, z, t]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 17 seconds.\n</pre> In\u00a0[113]: Copied! <pre>az.plot_trace(\n    ddm_pymc_trace,\n    lines=[(key_, {}, param_dict_pymc[key_]) for key_ in param_dict_pymc],\n)\n\nplt.tight_layout()\n</pre> az.plot_trace(     ddm_pymc_trace,     lines=[(key_, {}, param_dict_pymc[key_]) for key_ in param_dict_pymc], )  plt.tight_layout() In\u00a0[114]: Copied! <pre>az.plot_forest(ddm_pymc_trace)\n</pre> az.plot_forest(ddm_pymc_trace) Out[114]: <pre>array([&lt;Axes: title={'center': '94.0% HDI'}&gt;], dtype=object)</pre> In\u00a0[115]: Copied! <pre>from hssm.distribution_utils import make_likelihood_callable\n\nangle_loglik = make_likelihood_callable(\n    loglik=\"angle.onnx\",\n    loglik_kind=\"approx_differentiable\",\n    backend=\"jax\",\n    params_is_reg=[0, 0, 0, 0, 0],\n)\n\nANGLE = make_distribution(\n    \"angle\",\n    loglik=angle_loglik,\n    list_params=hssm.defaults.default_model_config[\"angle\"][\"list_params\"],\n)\n</pre> from hssm.distribution_utils import make_likelihood_callable  angle_loglik = make_likelihood_callable(     loglik=\"angle.onnx\",     loglik_kind=\"approx_differentiable\",     backend=\"jax\",     params_is_reg=[0, 0, 0, 0, 0], )  ANGLE = make_distribution(     \"angle\",     loglik=angle_loglik,     list_params=hssm.defaults.default_model_config[\"angle\"][\"list_params\"], ) <p>Note that we need to supply the <code>params_is_reg</code> argument (\"reg\" for \"regression\"). This is a boolean vector, which specifies for each input to the likelihood function, whether or not it is defined to be \"trial-wise\", as is expected if the parameter is the output e.g. of a regression function.</p> In\u00a0[116]: Copied! <pre>import pymc as pm\n\n# Angle pymc\nwith pm.Model() as angle_pymc:\n    # Define parameters\n    v = pm.Uniform(\"v\", lower=-10.0, upper=10.0)\n    a = pm.Uniform(\"a\", lower=0.5, upper=2.5)\n    z = pm.Uniform(\"z\", lower=0.01, upper=0.99)\n    t = pm.Uniform(\"t\", lower=0.0, upper=0.6)\n    theta = pm.Uniform(\"theta\", lower=-0.1, upper=1.0)\n\n    # Our RV\n    angle = ANGLE(\n        \"ANGLE\",\n        v=v,\n        a=a,\n        z=z,\n        t=t,\n        theta=theta,\n        observed=dataset_pymc[[\"rt\", \"response\"]].values,\n    )\n</pre> import pymc as pm  # Angle pymc with pm.Model() as angle_pymc:     # Define parameters     v = pm.Uniform(\"v\", lower=-10.0, upper=10.0)     a = pm.Uniform(\"a\", lower=0.5, upper=2.5)     z = pm.Uniform(\"z\", lower=0.01, upper=0.99)     t = pm.Uniform(\"t\", lower=0.0, upper=0.6)     theta = pm.Uniform(\"theta\", lower=-0.1, upper=1.0)      # Our RV     angle = ANGLE(         \"ANGLE\",         v=v,         a=a,         z=z,         t=t,         theta=theta,         observed=dataset_pymc[[\"rt\", \"response\"]].values,     ) In\u00a0[117]: Copied! <pre>with angle_pymc:\n    idata_object = pm.sample(nuts_sampler=\"numpyro\")\n</pre> with angle_pymc:     idata_object = pm.sample(nuts_sampler=\"numpyro\") <pre>sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:15&lt;00:00, 131.79it/s, 7 steps of size 3.93e-01. acc. prob=0.89] \nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:15&lt;00:00, 125.09it/s, 7 steps of size 3.35e-01. acc. prob=0.92] \nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:16&lt;00:00, 123.72it/s, 7 steps of size 3.25e-01. acc. prob=0.93] \nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:17&lt;00:00, 116.59it/s, 7 steps of size 2.88e-01. acc. prob=0.93] \n</pre> In\u00a0[118]: Copied! <pre>az.plot_trace(\n    idata_object, lines=[(key_, {}, param_dict_pymc[key_]) for key_ in param_dict_pymc]\n)\n\nplt.tight_layout()\n</pre> az.plot_trace(     idata_object, lines=[(key_, {}, param_dict_pymc[key_]) for key_ in param_dict_pymc] )  plt.tight_layout() In\u00a0[119]: Copied! <pre>from typing import Optional\n\n\ndef make_params_is_reg_vec(\n    reg_parameters: Optional[list] = None, parameter_names: Optional[list] = None\n):\n    \"\"\"Make a list of Trues and Falses to indicate which parameters are vectors.\"\"\"\n    if (not isinstance(reg_parameters, list)) or (\n        not isinstance(parameter_names, list)\n    ):\n        raise ValueError(\"Both reg_parameters and parameter_names should be lists\")\n\n    bool_list = [0] * len(parameter_names)\n    for param in reg_parameters:\n        bool_list[parameter_names.index(param)] = 1\n    return bool_list\n</pre> from typing import Optional   def make_params_is_reg_vec(     reg_parameters: Optional[list] = None, parameter_names: Optional[list] = None ):     \"\"\"Make a list of Trues and Falses to indicate which parameters are vectors.\"\"\"     if (not isinstance(reg_parameters, list)) or (         not isinstance(parameter_names, list)     ):         raise ValueError(\"Both reg_parameters and parameter_names should be lists\")      bool_list = [0] * len(parameter_names)     for param in reg_parameters:         bool_list[parameter_names.index(param)] = 1     return bool_list In\u00a0[120]: Copied! <pre># Set up trial by trial parameters\nv_intercept_pymc_reg = 0.3\nx_pymc_reg = np.random.uniform(-1, 1, size=1000)\nv_x_pymc_reg = 0.8\ny_pymc_reg = np.random.uniform(-1, 1, size=1000)\nv_y_pymc_reg = 0.3\nv_pymc_reg = v_intercept + (v_x * x) + (v_y * y)\n\nparam_dict_pymc_reg = dict(\n    v_Intercept=v_intercept_pymc_reg,\n    v_x=v_x_pymc_reg,\n    v_y=v_y_pymc_reg,\n    v=v_pymc_reg,\n    a=1.5,\n    z=0.5,\n    t=0.1,\n    theta=0.0,\n)\n\n# base dataset\npymc_reg_data = hssm.simulate_data(model=\"ddm\", theta=param_dict_pymc_reg, size=1)\n\n# Adding covariates into the datsaframe\npymc_reg_data[\"x\"] = x\npymc_reg_data[\"y\"] = y\n\n# Make the boolean vector for params_is_reg argument\nbool_param_reg = make_params_is_reg_vec(\n    reg_parameters=[\"v\"],\n    parameter_names=hssm.defaults.default_model_config[\"angle\"][\"list_params\"],\n)\n\nangle_loglik = make_likelihood_callable(\n    loglik=\"angle.onnx\",\n    loglik_kind=\"approx_differentiable\",\n    backend=\"jax\",\n    params_is_reg=bool_param_reg,\n)\n\nANGLE = make_distribution(\n    \"angle\",\n    loglik=angle_loglik,\n    list_params=hssm.defaults.default_model_config[\"angle\"][\"list_params\"],\n)\n</pre> # Set up trial by trial parameters v_intercept_pymc_reg = 0.3 x_pymc_reg = np.random.uniform(-1, 1, size=1000) v_x_pymc_reg = 0.8 y_pymc_reg = np.random.uniform(-1, 1, size=1000) v_y_pymc_reg = 0.3 v_pymc_reg = v_intercept + (v_x * x) + (v_y * y)  param_dict_pymc_reg = dict(     v_Intercept=v_intercept_pymc_reg,     v_x=v_x_pymc_reg,     v_y=v_y_pymc_reg,     v=v_pymc_reg,     a=1.5,     z=0.5,     t=0.1,     theta=0.0, )  # base dataset pymc_reg_data = hssm.simulate_data(model=\"ddm\", theta=param_dict_pymc_reg, size=1)  # Adding covariates into the datsaframe pymc_reg_data[\"x\"] = x pymc_reg_data[\"y\"] = y  # Make the boolean vector for params_is_reg argument bool_param_reg = make_params_is_reg_vec(     reg_parameters=[\"v\"],     parameter_names=hssm.defaults.default_model_config[\"angle\"][\"list_params\"], )  angle_loglik = make_likelihood_callable(     loglik=\"angle.onnx\",     loglik_kind=\"approx_differentiable\",     backend=\"jax\",     params_is_reg=bool_param_reg, )  ANGLE = make_distribution(     \"angle\",     loglik=angle_loglik,     list_params=hssm.defaults.default_model_config[\"angle\"][\"list_params\"], ) In\u00a0[121]: Copied! <pre>import pytensor.tensor as pt\n\nwith pm.Model(\n    coords={\n        \"idx\": pymc_reg_data.index,\n        \"resp\": [\"rt\", \"response\"],\n        \"features\": [\"x\", \"y\"],\n    }\n) as pymc_model_reg:\n    # Features\n    x_ = pm.Data(\"x\", pymc_reg_data[\"x\"].values, dims=\"idx\")\n    y_ = pm.Data(\"y\", pymc_reg_data[\"y\"].values, dims=\"idx\")\n    # Target\n    obs = pm.Data(\"obs\", pymc_reg_data[[\"rt\", \"response\"]].values, dims=(\"idx\", \"resp\"))\n\n    # Priors\n    a = pm.Uniform(\"a\", lower=0.5, upper=2.5)\n    z = pm.Uniform(\"z\", lower=0.01, upper=0.99)\n    t = pm.Uniform(\"t\", lower=0.0, upper=0.6)\n    theta = pm.Uniform(\"theta\", lower=-0.1, upper=1.0)\n    v_Intercept = pm.Uniform(\"v_Intercept\", lower=-3, upper=3)\n    v_betas = pm.Normal(\"v_beta\", mu=[0, 0], sigma=0.5, dims=(\"features\"))\n\n    # Regression equation\n    v = pm.Deterministic(\n        \"v\", v_Intercept + pt.stack([x_, y_], axis=1) @ v_betas, dims=\"idx\"\n    )\n\n    # Our RV\n    angle = ANGLE(\n        \"angle\",\n        v=v.squeeze(),\n        a=a,\n        z=z,\n        t=t,\n        theta=theta,\n        observed=obs,\n        dims=(\"idx\", \"resp\"),\n    )\n</pre> import pytensor.tensor as pt  with pm.Model(     coords={         \"idx\": pymc_reg_data.index,         \"resp\": [\"rt\", \"response\"],         \"features\": [\"x\", \"y\"],     } ) as pymc_model_reg:     # Features     x_ = pm.Data(\"x\", pymc_reg_data[\"x\"].values, dims=\"idx\")     y_ = pm.Data(\"y\", pymc_reg_data[\"y\"].values, dims=\"idx\")     # Target     obs = pm.Data(\"obs\", pymc_reg_data[[\"rt\", \"response\"]].values, dims=(\"idx\", \"resp\"))      # Priors     a = pm.Uniform(\"a\", lower=0.5, upper=2.5)     z = pm.Uniform(\"z\", lower=0.01, upper=0.99)     t = pm.Uniform(\"t\", lower=0.0, upper=0.6)     theta = pm.Uniform(\"theta\", lower=-0.1, upper=1.0)     v_Intercept = pm.Uniform(\"v_Intercept\", lower=-3, upper=3)     v_betas = pm.Normal(\"v_beta\", mu=[0, 0], sigma=0.5, dims=(\"features\"))      # Regression equation     v = pm.Deterministic(         \"v\", v_Intercept + pt.stack([x_, y_], axis=1) @ v_betas, dims=\"idx\"     )      # Our RV     angle = ANGLE(         \"angle\",         v=v.squeeze(),         a=a,         z=z,         t=t,         theta=theta,         observed=obs,         dims=(\"idx\", \"resp\"),     ) In\u00a0[122]: Copied! <pre>with pymc_model_reg:\n    idata_pymc_reg = pm.sample(\n        nuts_sampler=\"numpyro\", idata_kwargs={\"log_likelihood\": True}\n    )\n</pre> with pymc_model_reg:     idata_pymc_reg = pm.sample(         nuts_sampler=\"numpyro\", idata_kwargs={\"log_likelihood\": True}     ) <pre>sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:13&lt;00:00, 150.54it/s, 31 steps of size 1.84e-01. acc. prob=0.95]\nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:09&lt;00:00, 204.08it/s, 7 steps of size 3.12e-01. acc. prob=0.87] \nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:10&lt;00:00, 199.93it/s, 15 steps of size 2.98e-01. acc. prob=0.88]\nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:11&lt;00:00, 167.67it/s, 7 steps of size 2.35e-01. acc. prob=0.91] \n</pre> In\u00a0[123]: Copied! <pre>az.plot_forest(idata_pymc_reg, var_names=[\"~v\"])\n</pre> az.plot_forest(idata_pymc_reg, var_names=[\"~v\"]) Out[123]: <pre>array([&lt;Axes: title={'center': '94.0% HDI'}&gt;], dtype=object)</pre> <p>All layers peeled back, the only limit in your modeling endeavors becomes the limit of the PyMC universe!</p> <p> </p>  Enjoy the exploration!"},{"location":"tutorials/main_tutorial/#colab-instructions","title":"Colab Instructions\u00b6","text":"<p>If you would like to run this tutorial on Google colab, please click this link.</p> <p>Once you are in the colab, follow the installation instructions below and then restart your runtime.</p> <p>Just uncomment the code in the next code cell and run it!</p> <p>NOTE:</p> <p>You may want to switch your runtime to have a GPU or TPU. To do so, go to Runtime &gt; Change runtime type and select the desired hardware accelerator.</p> <p>Note that if you switch your runtime you have to follow the installation instructions again.</p>"},{"location":"tutorials/main_tutorial/#basic-imports","title":"Basic Imports\u00b6","text":""},{"location":"tutorials/main_tutorial/#data-simulation","title":"Data Simulation\u00b6","text":"<p>We will rely on the ssms package for data simulation repeatedly. Let's look at a basic isolated use case below.</p> <p>As an example, let's use ssms to simulate from the basic Drift Diffusion Model (a running example in this tutorial).</p> <p>If you are not familiar with the DDM. For now just consider that it has four parameters.</p> <ul> <li><code>v</code> the drift rate</li> <li><code>a</code> the boundary separation</li> <li><code>t</code> the non-decision time</li> <li><code>z</code> the a priori decision bias (starting point)</li> </ul>"},{"location":"tutorials/main_tutorial/#using-simulate_data","title":"Using <code>simulate_data()</code>\u00b6","text":""},{"location":"tutorials/main_tutorial/#using-ssm-simulators","title":"Using <code>ssm-simulators</code>\u00b6","text":"<p>Internally, HSSM natively makes use of the ssm-simulators package for forward simulation of models. <code>hssm.simulate_data()</code> functions essentially as a convenience-wrapper.</p> <p>Below we illustrate how to simulate data using the <code>ssm-simulators</code> package directly, to generate an equivalent dataset as created above. We will use the third way of passing parameters to the simulator, which is as a parameter-matrix.</p> <p>Notes:</p> <ol> <li><p>If you pass parameters as a parameter matrix, make sure the column ordering is correct. You can follow the parameter ordering under <code>hssm.defaults.default_model_config['ddm']['list_params']</code>.</p> </li> <li><p>This is a minimal example, for more information about the package, check the associated github-page.</p> </li> </ol>"},{"location":"tutorials/main_tutorial/#arviz-for-plotting","title":"ArviZ for Plotting\u00b6","text":"<p>We use the ArviZ package for most of our plotting needs. ArviZ is a useful aid for plotting when doing anything Bayesian.</p> <p>It works with HSSM out of the box, by virtue of HSSMs reliance on PyMC for model construction and sampling.</p> <p>Checking out the ArviZ Documentation is a good idea to give you communication superpowers for not only your HSSM results, but also other libraries in the Bayesian Toolkit such as NumPyro or STAN.</p> <p>We will see ArviZ plots throughout the notebook.</p>"},{"location":"tutorials/main_tutorial/#main-tutorial","title":"Main Tutorial\u00b6","text":""},{"location":"tutorials/main_tutorial/#initial-dataset","title":"Initial Dataset\u00b6","text":""},{"location":"tutorials/main_tutorial/#first-hssm-model","title":"First HSSM Model\u00b6","text":"<p>In this example we will use the analytical likelihood function computed as suggested in this paper.</p>"},{"location":"tutorials/main_tutorial/#instantiate-the-model","title":"Instantiate the model\u00b6","text":"<p>To instantiate our <code>HSSM</code> class, in the simplest version, we only need to provide an appropriate dataset. The dataset is expected to be a <code>pandas.DataFrame</code> with at least two columns, respectively called <code>rt</code> (for reaction time) and <code>response</code>. Our data simulated above is already in the correct format, so let us try to construct the class.</p> <p>NOTE:</p> <p>If you are a user of the HDDM python package, this workflow should seem very familiar.</p>"},{"location":"tutorials/main_tutorial/#model-graph","title":"Model Graph\u00b6","text":"<p>Since <code>HSSM</code> creates a <code>PyMC Model</code>, we can can use the <code>.graph()</code> function, to get a graphical representation of the the model we created.</p>"},{"location":"tutorials/main_tutorial/#sample-from-the-model","title":"Sample from the Model\u00b6","text":"<p>We can now call the <code>.sample()</code> function, to get posterior samples. The main arguments you may want to change are listed in the function call below.</p> <p>Importantly, multiple backends are possible. We choose the <code>nuts_numpyro</code> backend below, which in turn compiles the model to a <code>JAX</code> function.</p>"},{"location":"tutorials/main_tutorial/#inference-data-what-gets-returned-from-the-sampler","title":"Inference Data / What gets returned from the sampler?\u00b6","text":"<p>The sampler returns an ArviZ <code>InferenceData</code> object.</p> <p>To understand all the logic behind these objects and how they mesh with the Bayesian Workflow, we refer you to the ArviZ Documentation.</p> <p><code>InferenceData</code> is build on top of xarrays. The xarray documentation will help you understand in more detail how to manipulate these objects.</p> <p>But let's take a quick high-level look to understand roughly what we are dealing with here!</p>"},{"location":"tutorials/main_tutorial/#basic-manipulation","title":"Basic Manipulation\u00b6","text":""},{"location":"tutorials/main_tutorial/#accessing-groups-and-variables","title":"Accessing groups and variables\u00b6","text":""},{"location":"tutorials/main_tutorial/#combine-chain-and-draw-dimension","title":"Combine <code>chain</code> and <code>draw</code> dimension\u00b6","text":"<p>When operating directly on the <code>xarray</code>, you will often find it useful to collapse the <code>chain</code> and <code>draw</code> coordinates into a single coordinate. Arviz makes this easy via the <code>extract</code> method.</p>"},{"location":"tutorials/main_tutorial/#making-use-of-arviz","title":"Making use of ArviZ\u00b6","text":"<p>Working with the <code>InferenceData</code> directly, is very helpful if you want to include custom computations into your workflow. For a basic Bayesian Workflow however, you will often find that standard functionality available through ArviZ suffices.</p> <p>Below we provide a few examples of useful Arviz outputs, which come handy for analyzing your traces (MCMC samples).</p>"},{"location":"tutorials/main_tutorial/#summary-table","title":"Summary table\u00b6","text":"<p>Let's take a look at a summary table for our posterior.</p>"},{"location":"tutorials/main_tutorial/#trace-plot","title":"Trace plot\u00b6","text":""},{"location":"tutorials/main_tutorial/#forest-plot","title":"Forest Plot\u00b6","text":"<p>The forest plot is commonly used for a quick visual check of the marginal posteriors. It is very effective for intuitive communication of results.</p>"},{"location":"tutorials/main_tutorial/#combining-chains","title":"Combining Chains\u00b6","text":"<p>By default, chains are separated out into separate caterpillars, however sometimes, especially if you are looking at a forest plot which includes many posterior parameters at once, you want to declutter and collapse the chains into single caterpillars. In this case you can <code>combine</code> chains instead.</p>"},{"location":"tutorials/main_tutorial/#basic-marginal-posterior-plot","title":"Basic Marginal Posterior Plot\u00b6","text":"<p>Another way to view the marginal posteriors is provided by the <code>plot_posterior()</code> function. It shows the mean and by default the $94\\%$ HDIs.</p>"},{"location":"tutorials/main_tutorial/#posterior-pair-plot","title":"Posterior Pair Plot\u00b6","text":""},{"location":"tutorials/main_tutorial/#compute-quantities-from-idata","title":"Compute Quantities from idata\u00b6","text":""},{"location":"tutorials/main_tutorial/#example-mean-and-covariance-of-posterior-parameters","title":"Example: Mean and Covariance of Posterior Parameters\u00b6","text":"<p>As a simple example, let us calculate the covariance matrix for our posterior samples.</p>"},{"location":"tutorials/main_tutorial/#hssm-model-based-on-lan-likelihood","title":"HSSM Model based on LAN likelihood\u00b6","text":"<p>With HSSM you can switch between pre-supplied models with a simple change of argument. The type of likelihood that will be accessed might change in the background for you.</p> <p>Here we see an example in which the underlying likelihood is now a LAN.</p> <p>We will talk more about different types of likelihood functions and backends later in the tutorial. For now just keep the following in mind:</p> <p>There are three types of likelihoods</p> <ol> <li><code>analytic</code></li> <li><code>approx_differentiable</code></li> <li><code>blackbox</code></li> </ol> <p>To check which type is used in your HSSM model simple type:</p>"},{"location":"tutorials/main_tutorial/#simulating-angle-data","title":"Simulating Angle Data\u00b6","text":"<p>Again, let us simulate a simple dataset. This time we will use the <code>angle</code> model (passed via the <code>model</code> argument to the <code>simulator()</code> function).</p> <p>This model is distinguished from the basic <code>ddm</code> model by an additional <code>theta</code> parameter which specifies the angle with which the decision boundaries collapse over time.</p> <p>DDMs with collapsing bounds have been of significant interest in the theoretical literature, but applications were rare due to a lack of analytical likelihoods. HSSM facilitates inference with such models via the our <code>approx_differentiable</code> likelihoods. HSSM ships with a few predefined models based on LANs, but really we don't want to overemphasize those. They reflect the research interest of our and adjacent labs to a great extend.</p> <p>Instead, we encourage the community to contribute to this model reservoir (more on this later).</p>"},{"location":"tutorials/main_tutorial/#choosing-priors","title":"Choosing Priors\u00b6","text":""},{"location":"tutorials/main_tutorial/#fixing-a-parameter-to-a-given-value","title":"Fixing a parameter to a given value\u00b6","text":""},{"location":"tutorials/main_tutorial/#named-priors","title":"Named priors\u00b6","text":"<p>We can choose any PyMC <code>Distribution</code> to specify a prior for a given parameter.</p> <p>Even better, if natural parameter bounds are provided, HSSM automatically truncates the prior distribution so that it respect these bounds.</p> <p>Below is an example in which we specify a Normal prior on the <code>v</code> parameter of the DDM.</p> <p>We choose a ridiculously low $\\sigma$ value, to illustrate it's regularizing effect on the parameter (just so we see a difference and you are convinced that something changed).</p>"},{"location":"tutorials/main_tutorial/#hssm-model-with-regression","title":"HSSM Model with Regression\u00b6","text":"<p>Crucial to the scope of HSSM is the ability to link parameters with trial-by-trial covariates via (hierarchical, but more on this later) general linear models.</p> <p>In this section we explore how HSSM deals with these models. No big surprise here... it's simple!</p>"},{"location":"tutorials/main_tutorial/#case-1-one-parameter-is-a-regression-target","title":"Case 1: One parameter is a Regression Target\u00b6","text":""},{"location":"tutorials/main_tutorial/#simulating-data","title":"Simulating Data\u00b6","text":""},{"location":"tutorials/main_tutorial/#basic-model","title":"Basic Model\u00b6","text":""},{"location":"tutorials/main_tutorial/#param-class","title":"<code>Param</code> class\u00b6","text":"<p>As illustrated below, there is an alternative way of specifying the parameter specific data via the <code>Param</code> class.</p>"},{"location":"tutorials/main_tutorial/#custom-model","title":"Custom Model\u00b6","text":""},{"location":"tutorials/main_tutorial/#case-2-one-parameter-is-a-regression-lan","title":"Case 2: One parameter is a Regression (LAN)\u00b6","text":"<p>We can do the same thing with the <code>angle</code> model.</p> <p>Note:</p> <p>Our dataset was generated from the basic DDM here, so since the DDM assumes stable bounds, we expect the <code>theta</code> (angle of linear collapse) parameter to be recovered as close to $0$.</p>"},{"location":"tutorials/main_tutorial/#case-3-multiple-parameters-are-regression-targets-lan","title":"Case 3: Multiple Parameters are Regression Targets (LAN)\u00b6","text":""},{"location":"tutorials/main_tutorial/#case-4-categorical-covariates","title":"Case 4: Categorical covariates\u00b6","text":""},{"location":"tutorials/main_tutorial/#hierarchical-inference","title":"Hierarchical Inference\u00b6","text":""},{"location":"tutorials/main_tutorial/#simulate-data","title":"Simulate Data\u00b6","text":""},{"location":"tutorials/main_tutorial/#basic-hierarchical-model","title":"Basic Hierarchical Model\u00b6","text":""},{"location":"tutorials/main_tutorial/#model-comparison","title":"Model Comparison\u00b6","text":""},{"location":"tutorials/main_tutorial/#scenario","title":"Scenario\u00b6","text":"<p>The following scenario is explored.</p> <p>First we generate data from a <code>ddm</code> model with fixed parameters, specifically we set the <code>a</code> parameter to $1.5$.</p> <p>We then define two <code>HSSM</code> models:</p> <ol> <li>A model which allows fitting all but the <code>a</code> parameter, which is fixed to $1.0$ (wrong)</li> <li>A model which allows fitting all but the <code>a</code> parameter, which is fixed to $1.5$ (correct)</li> </ol> <p>We then use the ArviZ's <code>compare()</code> function, to perform model comparison via <code>elpd_loo</code>.</p>"},{"location":"tutorials/main_tutorial/#data-simulation","title":"Data Simulation\u00b6","text":""},{"location":"tutorials/main_tutorial/#defining-the-models","title":"Defining the Models\u00b6","text":""},{"location":"tutorials/main_tutorial/#compare","title":"Compare\u00b6","text":""},{"location":"tutorials/main_tutorial/#closer-look","title":"Closer look!\u00b6","text":""},{"location":"tutorials/main_tutorial/#blackbox-likelihoods","title":"'Blackbox' Likelihoods\u00b6","text":""},{"location":"tutorials/main_tutorial/#what-is-a-blackbox-likelihood-function","title":"What is a Blackbox Likelihood Function?\u00b6","text":"<p>A Blackbox Likelihood Function is essentially any Python <code>callable</code> (function) that provides trial by trial likelihoods for your model of interest. What kind of computations are performed in this Python function is completely arbitrary.</p> <p>E.g. you could built a function that performs forward simulation from you model, constructs are kernel-density estimate for the resulting likelihood functions and evaluates your datapoints on this ad-hoc generated approximate likelihood.</p> <p>What I just described is a once state-of-the-art method of performing simulation based inference on Sequential Sampling models, a precursor to LANs if you will.</p>"},{"location":"tutorials/main_tutorial/#simulating-simple-dataset-from-the-ddm","title":"Simulating simple dataset from the DDM\u00b6","text":""},{"location":"tutorials/main_tutorial/#define-the-likelihood","title":"Define the likelihood\u00b6","text":""},{"location":"tutorials/main_tutorial/#define-hssm-class-with-our-blackbox-likelihood","title":"Define HSSM class with our Blackbox Likelihood\u00b6","text":""},{"location":"tutorials/main_tutorial/#results","title":"Results\u00b6","text":""},{"location":"tutorials/main_tutorial/#hssm-random-variables-in-pymc","title":"HSSM Random Variables in PyMC\u00b6","text":"<p>We covered a lot of ground in this tutorial so far. You are now a sophisticated HSSM user.</p> <p>It is therefore time to reveal a secret. We can actuallly peel back one more layer...  </p> <p>Instead of letting HSSM help you build the entire model, we can instead use HSSM to construct valid PyMC distributions and then proceed to build a custom PyMC model by ourselves...  </p> <p>We will illustrate the simplest example below. It sets a pattern that can be exploited for much more complicated modeling exercises, which importantly go far beyond what our basic HSSM class may facilitate for you!</p> <p>See the dedicated tutorial in the documentation if you are interested.</p> <p>Let's start by importing a few convenience functions:</p>"},{"location":"tutorials/main_tutorial/#simulate-some-data","title":"Simulate some data\u00b6","text":""},{"location":"tutorials/main_tutorial/#build-a-custom-pymc-model","title":"Build a custom PyMC Model\u00b6","text":""},{"location":"tutorials/main_tutorial/#alternative-models-with-pymc","title":"Alternative Models with PyMC\u00b6","text":"<p>With very little extra work, we can in fact load any of the models accessible via HSSM. Here is an example, where we load the <code>angle</code> model instead.</p> <p>We first construction the likelihood function, using <code>make_likelihood_callable()</code>.</p> <p>Then we produce a valid <code>pymc.distribution</code> using the <code>make_distribution()</code> utility function.</p> <p>Just like the <code>DDM</code> class above, we can then use this distribution inside a PyMC model.</p>"},{"location":"tutorials/main_tutorial/#regression-via-pymc","title":"Regression via PyMC\u00b6","text":"<p>Finally to illustrate the usage of PyMC a little more elaborately, let us build a PyMC model with regression components.</p>"},{"location":"tutorials/main_tutorial/#end","title":"End\u00b6","text":""},{"location":"tutorials/plotting/","title":"Plotting in HSSM","text":"In\u00a0[1]: Copied! <pre># If running this on Colab, please uncomment the next line and\n# !pip install hssm\n</pre> # If running this on Colab, please uncomment the next line and # !pip install hssm In\u00a0[2]: Copied! <pre>from pathlib import Path\n\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nimport hssm\nimport hssm.plotting\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</pre> from pathlib import Path  import arviz as az import matplotlib.pyplot as plt import pandas as pd import numpy as np import seaborn as sns  import hssm import hssm.plotting  %matplotlib inline %config InlineBackend.figure_format='retina' In\u00a0[3]: Copied! <pre>fixtures_dir = Path(\"../../tests/fixtures\")\ncav_data_test = pd.read_csv(fixtures_dir / \"cavanagh_theta_test.csv\")\ncav_data_traces = az.from_netcdf(fixtures_dir / \"cavanagh_idata.nc\")\n</pre> fixtures_dir = Path(\"../../tests/fixtures\") cav_data_test = pd.read_csv(fixtures_dir / \"cavanagh_theta_test.csv\") cav_data_traces = az.from_netcdf(fixtures_dir / \"cavanagh_idata.nc\") In\u00a0[4]: Copied! <pre># For demonstration purposes,\n# `cav_data_test` is a subset of the `cavanagh_theta_nn` dataset\n# with 5 subjects and 100 observation each\n\ncav_data_test\n</pre> # For demonstration purposes, # `cav_data_test` is a subset of the `cavanagh_theta_nn` dataset # with 5 subjects and 100 observation each  cav_data_test Out[4]: participant_id stim rt response theta dbs conf 0 0 WL 0.928 -1.0 -0.521933 0 LC 1 0 WL 0.661 1.0 -0.219645 1 LC 2 0 WW 2.350 -1.0 -0.168728 1 HC 3 0 LL 1.250 -1.0 -0.104636 1 HC 4 0 LL 1.170 -1.0 1.122720 1 HC ... ... ... ... ... ... ... ... 495 4 WL 0.606 -1.0 -0.635942 0 LC 496 4 WL 0.745 -1.0 -0.166833 0 LC 497 4 WW 1.320 1.0 -0.283396 1 HC 498 4 LL 1.640 1.0 0.462584 1 HC 499 4 WL 0.822 1.0 -0.019645 0 LC <p>500 rows \u00d7 7 columns</p> In\u00a0[5]: Copied! <pre># Model parameter specification\n\ncav_model = hssm.HSSM(\n    model=\"ddm\",\n    data=cav_data_test,\n    include=[\n        {\n            \"name\": \"v\",\n            \"prior\": {\n                \"Intercept\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 1.0},\n                \"theta\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 1.0},\n            },\n            \"formula\": \"v ~  theta + (1|participant_id)\",\n            \"link\": \"identity\",\n        },\n    ],\n)\n\n# Perform sampling\n# cav_model.sample()\n</pre> # Model parameter specification  cav_model = hssm.HSSM(     model=\"ddm\",     data=cav_data_test,     include=[         {             \"name\": \"v\",             \"prior\": {                 \"Intercept\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 1.0},                 \"theta\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 1.0},             },             \"formula\": \"v ~  theta + (1|participant_id)\",             \"link\": \"identity\",         },     ], )  # Perform sampling # cav_model.sample() <pre>Model initialized successfully.\n</pre> <p>For demonstration purposes, we inject into the model an existing trace with posterior predictive sampling already performed. A <code>posterior_predictive</code> attribute is added to the <code>traces</code> object. A <code>rt,response_mean</code> variable is also added to the <code>posterior</code> attribute during predictive sampling.</p> In\u00a0[6]: Copied! <pre># In practice, you would obtain this object by sampling from the model.\ncav_model._inference_obj = cav_data_traces\n# Sample prior predictive\n# cav_model.sample_prior_predictive(draws = 1000)\n\n# Print\ncav_model.traces\n</pre> # In practice, you would obtain this object by sampling from the model. cav_model._inference_obj = cav_data_traces # Sample prior predictive # cav_model.sample_prior_predictive(draws = 1000)  # Print cav_model.traces Out[6]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 4MB\nDimensions:                         (chain: 2, draw: 500,\n                                     v_1|participant_id__factor_dim: 5,\n                                     __obs__: 500)\nCoordinates:\n  * chain                           (chain) int64 16B 0 1\n  * draw                            (draw) int64 4kB 0 1 2 3 ... 496 497 498 499\n  * v_1|participant_id__factor_dim  (v_1|participant_id__factor_dim) &lt;U1 20B ...\n  * __obs__                         (__obs__) int64 4kB 0 1 2 3 ... 497 498 499\nData variables:\n    v_Intercept                     (chain, draw) float64 8kB ...\n    v_theta                         (chain, draw) float32 4kB ...\n    a                               (chain, draw) float32 4kB ...\n    z                               (chain, draw) float32 4kB ...\n    t                               (chain, draw) float32 4kB ...\n    v_1|participant_id_sigma        (chain, draw) float32 4kB ...\n    v_1|participant_id              (chain, draw, v_1|participant_id__factor_dim) float32 20kB ...\n    v                               (chain, draw, __obs__) float64 4MB ...\nAttributes:\n    created_at:                  2023-11-14T18:35:04.027433\n    arviz_version:               0.14.0\n    inference_library:           pymc\n    inference_library_version:   5.9.1\n    sampling_time:               22.505457878112793\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.12.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li><li>v_1|participant_id__factor_dim: 5</li><li>__obs__: 500</li></ul></li><li>Coordinates: (4)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>v_1|participant_id__factor_dim(v_1|participant_id__factor_dim)&lt;U1'0' '1' '2' '3' '4'<pre>array(['0', '1', '2', '3', '4'], dtype='&lt;U1')</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (8)<ul><li>v_Intercept(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>v_theta(chain, draw)float32...<pre>[1000 values with dtype=float32]</pre></li><li>a(chain, draw)float32...<pre>[1000 values with dtype=float32]</pre></li><li>z(chain, draw)float32...<pre>[1000 values with dtype=float32]</pre></li><li>t(chain, draw)float32...<pre>[1000 values with dtype=float32]</pre></li><li>v_1|participant_id_sigma(chain, draw)float32...<pre>[1000 values with dtype=float32]</pre></li><li>v_1|participant_id(chain, draw, v_1|participant_id__factor_dim)float32...<pre>[5000 values with dtype=float32]</pre></li><li>v(chain, draw, __obs__)float64...<pre>[500000 values with dtype=float64]</pre></li></ul></li><li>Indexes: (4)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>v_1|participant_id__factor_dimPandasIndex<pre>PandasIndex(Index(['0', '1', '2', '3', '4'], dtype='object', name='v_1|participant_id__factor_dim'))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='__obs__', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2023-11-14T18:35:04.027433arviz_version :0.14.0inference_library :pymcinference_library_version :5.9.1sampling_time :22.505457878112793tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.12.0</li></ul> </ul> </li> <li> posterior_predictive <ul> <pre>&lt;xarray.Dataset&gt; Size: 4MB\nDimensions:          (chain: 2, draw: 500, __obs__: 500, rt,response_dim: 2)\nCoordinates:\n  * chain            (chain) int64 16B 0 1\n  * draw             (draw) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\n  * __obs__          (__obs__) int64 4kB 0 1 2 3 4 5 ... 494 495 496 497 498 499\n  * rt,response_dim  (rt,response_dim) int64 16B 0 1\nData variables:\n    rt,response      (chain, draw, __obs__, rt,response_dim) float32 4MB ...\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.12.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li><li>__obs__: 500</li><li>rt,response_dim: 2</li></ul></li><li>Coordinates: (4)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>rt,response_dim(rt,response_dim)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__, rt,response_dim)float32...<pre>[1000000 values with dtype=float32]</pre></li></ul></li><li>Indexes: (4)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='__obs__', length=500))</pre></li><li>rt,response_dimPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_dim'))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.12.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 126kB\nDimensions:                (chain: 2, draw: 500)\nCoordinates:\n  * chain                  (chain) int64 16B 0 1\n  * draw                   (draw) int64 4kB 0 1 2 3 4 5 ... 495 496 497 498 499\nData variables: (12/17)\n    energy_error           (chain, draw) float64 8kB ...\n    perf_counter_diff      (chain, draw) float64 8kB ...\n    energy                 (chain, draw) float64 8kB ...\n    max_energy_error       (chain, draw) float64 8kB ...\n    smallest_eigval        (chain, draw) float64 8kB ...\n    step_size_bar          (chain, draw) float64 8kB ...\n    ...                     ...\n    perf_counter_start     (chain, draw) float64 8kB ...\n    process_time_diff      (chain, draw) float64 8kB ...\n    index_in_trajectory    (chain, draw) int64 8kB ...\n    reached_max_treedepth  (chain, draw) bool 1kB ...\n    lp                     (chain, draw) float64 8kB ...\n    largest_eigval         (chain, draw) float64 8kB ...\nAttributes:\n    arviz_version:               0.14.0\n    created_at:                  2023-11-14T18:35:04.032437\n    inference_library:           pymc\n    inference_library_version:   5.9.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.12.0\n    sampling_time:               22.505457878112793\n    tuning_steps:                1000</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (17)<ul><li>energy_error(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>perf_counter_diff(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>energy(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>max_energy_error(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>smallest_eigval(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>step_size_bar(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>step_size(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>acceptance_rate(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>tree_depth(chain, draw)int64...<pre>[1000 values with dtype=int64]</pre></li><li>n_steps(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>diverging(chain, draw)bool...<pre>[1000 values with dtype=bool]</pre></li><li>perf_counter_start(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>process_time_diff(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>index_in_trajectory(chain, draw)int64...<pre>[1000 values with dtype=int64]</pre></li><li>reached_max_treedepth(chain, draw)bool...<pre>[1000 values with dtype=bool]</pre></li><li>lp(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>largest_eigval(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)arviz_version :0.14.0created_at :2023-11-14T18:35:04.032437inference_library :pymcinference_library_version :5.9.1modeling_interface :bambimodeling_interface_version :0.12.0sampling_time :22.505457878112793tuning_steps :1000</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 8kB\nDimensions:                  (__obs__: 500, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 4kB 0 1 2 3 4 ... 496 497 498 499\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float32 4kB ...\nAttributes:\n    arviz_version:               0.14.0\n    created_at:                  2023-11-14T18:35:04.034261\n    inference_library:           pymc\n    inference_library_version:   5.9.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.12.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 500</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float32...<pre>[1000 values with dtype=float32]</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='__obs__', length=500))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (6)arviz_version :0.14.0created_at :2023-11-14T18:35:04.034261inference_library :pymcinference_library_version :5.9.1modeling_interface :bambimodeling_interface_version :0.12.0</li></ul> </ul> </li> </ul> In\u00a0[7]: Copied! <pre>set(list(cav_model.traces.posterior.data_vars.keys()))\n</pre> set(list(cav_model.traces.posterior.data_vars.keys())) Out[7]: <pre>{'a',\n 't',\n 'v',\n 'v_1|participant_id',\n 'v_1|participant_id_sigma',\n 'v_Intercept',\n 'v_theta',\n 'z'}</pre> In\u00a0[8]: Copied! <pre>cav_model.summary()\n</pre> cav_model.summary() Out[8]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat v_theta 0.062 0.048 -0.027 0.150 0.002 0.002 620.0 471.0 1.01 z 0.504 0.020 0.467 0.538 0.001 0.001 766.0 643.0 1.00 v_Intercept 0.425 0.268 -0.086 0.936 0.017 0.012 262.0 359.0 1.00 t 0.337 0.013 0.312 0.363 0.001 0.000 646.0 639.0 1.01 a 1.044 0.024 0.995 1.085 0.001 0.001 831.0 737.0 1.00 v_1|participant_id_sigma 0.621 0.281 0.237 1.200 0.022 0.019 183.0 221.0 1.01 <p>Compare this with the output of <code>ArviZ</code> <code>az.summary()</code>:</p> In\u00a0[9]: Copied! <pre># Because of posterior predictive sampling, an `rt,response_mean` field was added to\n# the traces object by default. ArviZ include these values by default.\n\n# This is equivalent to calling\n# cav_model.summary(include_computed_values=True)\n\naz.summary(cav_model.traces)\n</pre> # Because of posterior predictive sampling, an `rt,response_mean` field was added to # the traces object by default. ArviZ include these values by default.  # This is equivalent to calling # cav_model.summary(include_computed_values=True)  az.summary(cav_model.traces) Out[9]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat v_Intercept 0.425 0.268 -0.086 0.936 0.017 0.012 262.0 359.0 1.00 v_theta 0.062 0.048 -0.027 0.150 0.002 0.002 620.0 471.0 1.01 a 1.044 0.024 0.995 1.085 0.001 0.001 831.0 737.0 1.00 z 0.504 0.020 0.467 0.538 0.001 0.001 766.0 643.0 1.00 t 0.337 0.013 0.312 0.363 0.001 0.000 646.0 639.0 1.01 ... ... ... ... ... ... ... ... ... ... v[495] 1.054 0.172 0.754 1.394 0.006 0.004 747.0 742.0 1.00 v[496] 1.083 0.169 0.740 1.369 0.006 0.004 790.0 735.0 1.00 v[497] 1.076 0.170 0.727 1.363 0.006 0.004 780.0 760.0 1.00 v[498] 1.122 0.170 0.805 1.432 0.006 0.004 861.0 714.0 1.00 v[499] 1.092 0.169 0.752 1.382 0.006 0.004 805.0 760.0 1.00 <p>511 rows \u00d7 9 columns</p> In\u00a0[10]: Copied! <pre>cav_model.plot_trace()\n</pre> cav_model.plot_trace() In\u00a0[11]: Copied! <pre>hssm.plotting.plot_predictive(cav_model,\n                              predictive_group=\"posterior_predictive\");\n</pre> hssm.plotting.plot_predictive(cav_model,                               predictive_group=\"posterior_predictive\"); <p>The <code>predictive_group</code> argument lets you choose between the <code>posterior_predictive</code> and the <code>prior_predictive</code> groups.</p> In\u00a0[12]: Copied! <pre>hssm.plotting.plot_predictive(cav_model,\n                              predictive_group=\"prior_predictive\",\n                              x_range = (-10, 10));\n</pre> hssm.plotting.plot_predictive(cav_model,                               predictive_group=\"prior_predictive\",                               x_range = (-10, 10)); <pre>No prior_predictive samples found. Generating prior_predictive samples using the provided InferenceData object and the original data. This will modify the provided InferenceData object, or if not provided, the traces object stored inside the model.\n</pre> <pre>Sampling: [a, rt,response, t, v_1|participant_id_offset, v_1|participant_id_sigma, v_Intercept, v_theta, z]\n/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/bambi/models.py:851: FutureWarning: 'mean' has been replaced by 'response_params' and is not going to work in the future\n  warnings.warn(\n</pre> <p>This API is designed to be a light wrapper around Seaborn's <code>sns.histplot()</code> API. It accepts most arguments that <code>sns.histplot()</code> accepts. It also returns an <code>ax</code> object in matplotlib, so you can manipulate it further.</p> In\u00a0[13]: Copied! <pre>ax = hssm.plotting.plot_predictive(cav_model)\nsns.despine()\nax.set_ylabel(\"\")\nplt.title(\"Posterior Predictive Plot\")\n</pre> ax = hssm.plotting.plot_predictive(cav_model) sns.despine() ax.set_ylabel(\"\") plt.title(\"Posterior Predictive Plot\") Out[13]: <pre>Text(0.5, 1.0, 'Posterior Predictive Plot')</pre> <p>You can also plot subsets of data in subplots:</p> In\u00a0[14]: Copied! <pre>hssm.plotting.plot_predictive(\n    cav_model,\n    col=\"participant_id\",\n    col_wrap=3,  # limits to 3 columns per row\n)\n</pre> hssm.plotting.plot_predictive(     cav_model,     col=\"participant_id\",     col_wrap=3,  # limits to 3 columns per row ) Out[14]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x349ce7e10&gt;</pre> In\u00a0[15]: Copied! <pre>hssm.plotting.plot_predictive(\n    cav_model,\n    col=\"participant_id\",\n    col_wrap=3,  # limits to 3 columns per row\n    predictive_group=\"prior_predictive\",\n)\n</pre> hssm.plotting.plot_predictive(     cav_model,     col=\"participant_id\",     col_wrap=3,  # limits to 3 columns per row     predictive_group=\"prior_predictive\", ) Out[15]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x349e78c10&gt;</pre> <p>2-dimensional grids are also possible:</p> In\u00a0[16]: Copied! <pre>hssm.plotting.plot_predictive(\n    cav_model,\n    col=\"participant_id\",\n    row=\"conf\",\n)\n</pre> hssm.plotting.plot_predictive(     cav_model,     col=\"participant_id\",     row=\"conf\", ) Out[16]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x349bcd250&gt;</pre> <p>When grids are used, this function returns a <code>sns.FacetGrid</code> object. You can also further customize your plot with this object as well. For example, you can set the titles each individual subplot according to a template or save the figure to disk (<code>g.savefig()</code>).</p> In\u00a0[17]: Copied! <pre>g = hssm.plotting.plot_predictive(\n    cav_model,\n    col=\"participant_id\",\n    row=\"conf\",\n)\n\ng.set_titles(template=\"{row_name} | Participant {col_name}\")\n</pre> g = hssm.plotting.plot_predictive(     cav_model,     col=\"participant_id\",     row=\"conf\", )  g.set_titles(template=\"{row_name} | Participant {col_name}\") Out[17]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x34e45b990&gt;</pre> <p><code>hssm.plotting.plot_quantile_probability()</code> works similarly to <code>hssm.plotting.plot_predictive()</code> in that when only producing one plot (no grid), it returns an axis object, and when it returns multiple plots, it produces a <code>FacetGrid</code> object</p> In\u00a0[18]: Copied! <pre># Single plot, returns an axis object, which can be worked on further\n\nax = hssm.plotting.plot_quantile_probability(cav_model, cond=\"stim\")\nax.set_ylim(0, 3);\n</pre> # Single plot, returns an axis object, which can be worked on further  ax = hssm.plotting.plot_quantile_probability(cav_model, cond=\"stim\") ax.set_ylim(0, 3); In\u00a0[19]: Copied! <pre># Multiple plots, returns a FacetGrid\n\ng = hssm.plotting.plot_quantile_probability(\n    cav_model,\n    cond=\"stim\",\n    col=\"participant_id\",\n    col_wrap=3,\n    grid_kwargs=dict(\n        ylim=(0, 3)\n    ),  # additional kwargs to the grid can be passed through `grid_kwargs`\n)\n</pre> # Multiple plots, returns a FacetGrid  g = hssm.plotting.plot_quantile_probability(     cav_model,     cond=\"stim\",     col=\"participant_id\",     col_wrap=3,     grid_kwargs=dict(         ylim=(0, 3)     ),  # additional kwargs to the grid can be passed through `grid_kwargs` ) <p>The <code>predictive_group</code> argument works here too:</p> In\u00a0[20]: Copied! <pre># Multiple plots, returns a FacetGrid\n\ng = hssm.plotting.plot_quantile_probability(\n    cav_model,\n    cond=\"stim\",\n    col=\"participant_id\",\n    predictive_group=\"prior_predictive\",\n    col_wrap=3,\n    grid_kwargs=dict(\n        ylim=(0, 3)\n    ),  # additional kwargs to the grid can be passed through `grid_kwargs`\n)\n</pre> # Multiple plots, returns a FacetGrid  g = hssm.plotting.plot_quantile_probability(     cav_model,     cond=\"stim\",     col=\"participant_id\",     predictive_group=\"prior_predictive\",     col_wrap=3,     grid_kwargs=dict(         ylim=(0, 3)     ),  # additional kwargs to the grid can be passed through `grid_kwargs` ) <p>This part showcases the <code>plot_model_cartoon()</code> function, which is part of the HSSM <code>plotting</code> submodule. The idea with these plots is to provide a pictorial representation of the underlying process that we recover with our model fits. You can explore various options to include posterior uncertainty graphically in these plots.</p> In\u00a0[21]: Copied! <pre>ax_1 = hssm.plotting.plot_model_cartoon(\n    cav_model,\n    n_samples=10,\n    bins=20,\n    col=\"stim\",\n    row=\"participant_id\",\n    groups=[\"dbs\"],\n    plot_predictive_mean=True,\n    plot_predictive_samples=True,\n    n_trajectories=2,  # extra arguments for the underlying plot_model_cartoon() function\n);\n</pre> ax_1 = hssm.plotting.plot_model_cartoon(     cav_model,     n_samples=10,     bins=20,     col=\"stim\",     row=\"participant_id\",     groups=[\"dbs\"],     plot_predictive_mean=True,     plot_predictive_samples=True,     n_trajectories=2,  # extra arguments for the underlying plot_model_cartoon() function ); <pre>No posterior_predictive samples found. Generating posterior_predictive samples using the provided InferenceData object and the original data. This will modify the provided InferenceData object, or if not provided, the traces object stored inside the model.\n</pre> In\u00a0[22]: Copied! <pre>ax_2 = hssm.plotting.plot_model_cartoon(\n    cav_model,\n    n_samples=10,\n    bins=20,\n    col=\"stim\",\n    row=\"participant_id\",\n    groups=[\"dbs\"],\n    plot_predictive_mean=True,\n    plot_predictive_samples=True,\n    predictive_group=\"prior_predictive\",\n    n_trajectories=2,  # extra arguments for the underlying plot_model_cartoon() function\n);\n</pre> ax_2 = hssm.plotting.plot_model_cartoon(     cav_model,     n_samples=10,     bins=20,     col=\"stim\",     row=\"participant_id\",     groups=[\"dbs\"],     plot_predictive_mean=True,     plot_predictive_samples=True,     predictive_group=\"prior_predictive\",     n_trajectories=2,  # extra arguments for the underlying plot_model_cartoon() function ); In\u00a0[23]: Copied! <pre>ax = hssm.plotting.plot_model_cartoon(\n    cav_model,\n    n_samples=100,\n    bins=10,\n    col=\"stim\",\n    row=\"participant_id\",\n    plot_predictive_mean=True,\n    plot_predictive_samples=True,\n    alpha_mean=1.0,\n    alpha_predictive=0.01,\n    alpha_trajectories=0.5,\n    n_trajectories=2,  # extra arguments for the underlying plot_model_cartoon() function\n);\n</pre> ax = hssm.plotting.plot_model_cartoon(     cav_model,     n_samples=100,     bins=10,     col=\"stim\",     row=\"participant_id\",     plot_predictive_mean=True,     plot_predictive_samples=True,     alpha_mean=1.0,     alpha_predictive=0.01,     alpha_trajectories=0.5,     n_trajectories=2,  # extra arguments for the underlying plot_model_cartoon() function ); <pre>No posterior_predictive samples found. Generating posterior_predictive samples using the provided InferenceData object and the original data. This will modify the provided InferenceData object, or if not provided, the traces object stored inside the model.\n</pre> In\u00a0[24]: Copied! <pre>ax = hssm.plotting.plot_model_cartoon(\n    cav_model,\n    n_samples=10,\n    bins=20,\n    col=\"stim\",\n    row=\"participant_id\",\n    groups=[\"dbs\"],\n    plot_predictive_mean=True,\n    plot_predictive_samples=True,\n    predictive_group=\"prior_predictive\",\n    alpha_mean=1.0,\n    alpha_predictive=0.01,\n    alpha_trajectories=0.5,\n    n_trajectories=2,  # extra arguments for the underlying plot_model_cartoon() function\n);\n</pre> ax = hssm.plotting.plot_model_cartoon(     cav_model,     n_samples=10,     bins=20,     col=\"stim\",     row=\"participant_id\",     groups=[\"dbs\"],     plot_predictive_mean=True,     plot_predictive_samples=True,     predictive_group=\"prior_predictive\",     alpha_mean=1.0,     alpha_predictive=0.01,     alpha_trajectories=0.5,     n_trajectories=2,  # extra arguments for the underlying plot_model_cartoon() function ); In\u00a0[25]: Copied! <pre>stim_v = [0.0, 0.75, 1.0]\nstim_names = [\"low\", \"medium\", \"high\"]\ndatasets = []\n\na_vec = np.random.normal(loc=1.25, scale=0.3, size=5)\n\nfor v_tmp in stim_v:\n    for participant_id in range(5):\n        a_tmp = a_vec[participant_id]\n        data_tmp = hssm.simulate_data(\n            model=\"race_no_bias_angle_4\",\n            theta=dict(\n                a=a_tmp,\n                v0=v_tmp,\n                v1=v_tmp + 0.25,\n                v2=v_tmp + 0.5,\n                v3=v_tmp + 0.75,\n                z=0.5,\n                t=0.2,\n                theta=0.1,\n            ),\n            size=200,\n        )\n        data_tmp[\"stim\"] = stim_names[stim_v.index(v_tmp)]\n        data_tmp[\"participant_id\"] = str(participant_id)\n        datasets.append(data_tmp)\n\ndataset = pd.concat(datasets).reset_index(drop=True)\nparam_dict = {\n    \"v\": {\n        \"low\": np.repeat(stim_v[0], 5),\n        \"medium\": np.repeat(stim_v[1], 5),\n        \"high\": np.repeat(stim_v[2], 5),\n    },\n    \"a\": {\"participant_id\"},\n}\n</pre> stim_v = [0.0, 0.75, 1.0] stim_names = [\"low\", \"medium\", \"high\"] datasets = []  a_vec = np.random.normal(loc=1.25, scale=0.3, size=5)  for v_tmp in stim_v:     for participant_id in range(5):         a_tmp = a_vec[participant_id]         data_tmp = hssm.simulate_data(             model=\"race_no_bias_angle_4\",             theta=dict(                 a=a_tmp,                 v0=v_tmp,                 v1=v_tmp + 0.25,                 v2=v_tmp + 0.5,                 v3=v_tmp + 0.75,                 z=0.5,                 t=0.2,                 theta=0.1,             ),             size=200,         )         data_tmp[\"stim\"] = stim_names[stim_v.index(v_tmp)]         data_tmp[\"participant_id\"] = str(participant_id)         datasets.append(data_tmp)  dataset = pd.concat(datasets).reset_index(drop=True) param_dict = {     \"v\": {         \"low\": np.repeat(stim_v[0], 5),         \"medium\": np.repeat(stim_v[1], 5),         \"high\": np.repeat(stim_v[2], 5),     },     \"a\": {\"participant_id\"}, } In\u00a0[26]: Copied! <pre>race_model = hssm.HSSM(\n    model=\"race_no_bias_angle_4\",\n    data=dataset,\n    include=[\n        {\n            \"name\": \"v0\",\n            \"prior\": {\n                \"Intercept\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 1.5},\n            },\n            \"formula\": \"v0 ~ 1 + stim\",\n            \"link\": \"identity\",\n        },\n        {\n            \"name\": \"a\",\n            \"prior\": {\n                \"Intercept\": {\"name\": \"Normal\", \"mu\": 1.5, \"sigma\": 0.5},\n            },\n            \"formula\": \"a ~ 1 + (1|participant_id)\",\n            \"link\": \"identity\",\n        },\n    ],\n    p_outlier=0.00,\n)\n</pre> race_model = hssm.HSSM(     model=\"race_no_bias_angle_4\",     data=dataset,     include=[         {             \"name\": \"v0\",             \"prior\": {                 \"Intercept\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 1.5},             },             \"formula\": \"v0 ~ 1 + stim\",             \"link\": \"identity\",         },         {             \"name\": \"a\",             \"prior\": {                 \"Intercept\": {\"name\": \"Normal\", \"mu\": 1.5, \"sigma\": 0.5},             },             \"formula\": \"a ~ 1 + (1|participant_id)\",             \"link\": \"identity\",         },     ],     p_outlier=0.00, ) <pre>You have specified the `lapse` argument to include a lapse distribution, but `p_outlier` is set to either 0 or None. Your lapse distribution will be ignored.\nModel initialized successfully.\n</pre> In\u00a0[27]: Copied! <pre>idata_race = race_model.sample(\n    sampler=\"nuts_numpyro\",\n    chains=2,\n    cores=2,\n    chain_method=\"vectorized\",\n    draws=500,\n    tune=500,\n    idata_kwargs=dict(log_likelihood=False),  # no need to return likelihoods here\n)\n</pre> idata_race = race_model.sample(     sampler=\"nuts_numpyro\",     chains=2,     cores=2,     chain_method=\"vectorized\",     draws=500,     tune=500,     idata_kwargs=dict(log_likelihood=False),  # no need to return likelihoods here ) <pre>Using default initvals. \n\n</pre> <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/pymc/sampling/jax.py:475: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  pmap_numpyro = MCMC(\nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:14&lt;00:00, 13.34it/s, 18 steps of size 3.26e-02. acc. prob=0.84]\nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [02:47&lt;00:00,  5.95it/s, 57 steps of size 6.43e-03. acc. prob=0.91]\nThere were 1000 divergences after tuning. Increase `target_accept` or reparameterize.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n</pre> In\u00a0[28]: Copied! <pre>ax = hssm.plotting.plot_model_cartoon(\n    race_model,\n    n_samples=10,\n    col=\"stim\",\n    row=\"participant_id\",\n    plot_pp_mean=True,\n    plot_pp_samples=False,\n    n_trajectories=1,\n    ylims=(0, 5),\n    alpha_pp=0.2,\n    xlims=(0.0, 2),\n);\n</pre> ax = hssm.plotting.plot_model_cartoon(     race_model,     n_samples=10,     col=\"stim\",     row=\"participant_id\",     plot_pp_mean=True,     plot_pp_samples=False,     n_trajectories=1,     ylims=(0, 5),     alpha_pp=0.2,     xlims=(0.0, 2), ); <pre>No posterior_predictive samples found. Generating posterior_predictive samples using the provided InferenceData object and the original data. This will modify the provided InferenceData object, or if not provided, the traces object stored inside the model.\n</pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre>"},{"location":"tutorials/plotting/#plotting-in-hssm","title":"Plotting in HSSM\u00b6","text":"<p>This tutorial demonstrates the plotting functionalities in HSSM.</p> <p>While the <code>ArviZ</code> package provides many plotting utilities, HSSM aims complement the <code>ArviZ</code> package with additional types of plots specific for hierarchical sequential sampling models. In addition, HSSM also provides some plotting API directly from the <code>HSSM</code> model object with some additional tweaks for convenience.</p> <p>Most of the plotting and summary functionalities can be found in <code>hssm.plotting</code> module, and additional convenience functions are exposed through the top-level <code>HSSM</code> model as well.</p>"},{"location":"tutorials/plotting/#model-setup","title":"Model setup\u00b6","text":""},{"location":"tutorials/plotting/#convenience-functions-from-the-top-level-hssmhssm-model-object","title":"Convenience functions from the top-level <code>hssm.HSSM</code> model object\u00b6","text":"<p>The <code>ArviZ</code> package provides <code>az.summary()</code> and <code>az.plot_trace()</code> functions that are very frequently used. We have added these functions through the top-level <code>hssm.HSSM</code> model object. The goal is to provide convenience to HSSM users. In addition, these functions provide some nice defaults. For example, in some cases, when some parameters are the targets of regressions, <code>ArviZ</code> will also plot the computed values for each regressor on each observation, which is highly inefficient. When convenience functions are used, these computed values will be excluded by default.</p>"},{"location":"tutorials/plotting/#1-modelsummary","title":"1. <code>model.summary()</code>\u00b6","text":"<p>The [<code>model.summary()</code>][hssm.HSSM.summary] convenience funciton automatically filters out undesirable outputs.</p>"},{"location":"tutorials/plotting/#2-modelplot_trace","title":"2. <code>model.plot_trace()</code>\u00b6","text":"<p>Likewise, [<code>model.plot_trace()</code>][hssm.HSSM.plot_trace] is also equivalent to calling <code>az.plot_trace()</code> on the model with computed values removed and also calling <code>plt.tight_layout()</code>:</p>"},{"location":"tutorials/plotting/#hssm-specific-plots","title":"HSSM-specific plots\u00b6","text":"<p>HSSM also offers various types of plots specific to hierarchical sequential sampling models.</p>"},{"location":"tutorials/plotting/#1-posterior-predictive-plots","title":"1. Posterior predictive plots\u00b6","text":"<p>Posterior predictive plots <code>hssm.plot_predictive()</code> plots the distribution of posterior predictive samples against the observed data.</p>"},{"location":"tutorials/plotting/#2-quantile-probability-plots","title":"2. Quantile probability plots\u00b6","text":"<p>Quantile probability plots [<code>hssm.plot_quantile_probability()</code>][hssm.plotting.plot_quantile_probability] are a type of plots useful for SSMs. For more information on quantile probability plots, please see here.</p>"},{"location":"tutorials/plotting/#model-cartoon-plots","title":"Model Cartoon Plots\u00b6","text":""},{"location":"tutorials/plotting/#2-choices","title":"2 choices\u00b6","text":""},{"location":"tutorials/plotting/#fully-decomposed","title":"Fully decomposed\u00b6","text":""},{"location":"tutorials/plotting/#posterior-predictive","title":"Posterior Predictive\u00b6","text":""},{"location":"tutorials/plotting/#prior-predictive","title":"Prior Predictive\u00b6","text":"<p>Note, the model cartoon plots look particularly unwieldy when using the prior predictive. If you want, take a look at the prior specifications and you can reason why and how it translates into such an unruly plot.</p>"},{"location":"tutorials/plotting/#no-split-by-group-inclue-posterior-uncertainty","title":"No split by group | Inclue posterior uncertainty\u00b6","text":""},{"location":"tutorials/plotting/#n-choices","title":"N Choices\u00b6","text":""},{"location":"tutorials/plotting/#data-simulation","title":"Data simulation\u00b6","text":""},{"location":"tutorials/plotting/#hssm-model","title":"HSSM Model\u00b6","text":""},{"location":"tutorials/plotting/#plot","title":"Plot\u00b6","text":""},{"location":"tutorials/pymc/","title":"Using the low-level API from HSSM directly with PyMC","text":"In\u00a0[1]: Copied! <pre># If running this on Colab, please uncomment the next line\n# !pip install hssm\n</pre> # If running this on Colab, please uncomment the next line # !pip install hssm In\u00a0[2]: Copied! <pre>import arviz as az\nimport pymc as pm\nfrom matplotlib import pyplot as plt\nimport hssm\n# hssm.set_floatX(\"float32\")\n</pre> import arviz as az import pymc as pm from matplotlib import pyplot as plt import hssm # hssm.set_floatX(\"float32\") In\u00a0[3]: Copied! <pre># Simulate some data\nv_true, a_true, z_true, t_true, sv_true = [0.5, 1.5, 0.5, 0.5, 0.1]\ndataset = hssm.simulate_data(\n    model=\"ddm_sdv\",\n    theta=[v_true, a_true, z_true, t_true, sv_true],\n    size=1000,\n)\n\ndataset\n</pre> # Simulate some data v_true, a_true, z_true, t_true, sv_true = [0.5, 1.5, 0.5, 0.5, 0.1] dataset = hssm.simulate_data(     model=\"ddm_sdv\",     theta=[v_true, a_true, z_true, t_true, sv_true],     size=1000, )  dataset Out[3]: rt response 0 4.017200 1.0 1 1.679124 1.0 2 2.461139 1.0 3 0.753674 1.0 4 1.308612 1.0 ... ... ... 995 1.703980 1.0 996 4.416162 1.0 997 2.761944 1.0 998 1.405405 1.0 999 1.762197 1.0 <p>1000 rows \u00d7 2 columns</p> In\u00a0[4]: Copied! <pre># This is a pm.Distribution available in HSSM\n# There is also a DDM_SDV class for DDMs with the sv parameter\nfrom hssm.likelihoods import DDM\n\nwith pm.Model() as ddm_pymc:\n    v = pm.Uniform(\"v\", lower=-10.0, upper=10.0)\n    a = pm.HalfNormal(\"a\", sigma=2.0)\n    z = pm.Uniform(\"z\", lower=0.01, upper=0.99)\n    t = pm.Uniform(\"t\", lower=0.0, upper=0.6, initval=0.1)\n\n    ddm = DDM(\"ddm\", v=v, a=a, z=z, t=t, observed=dataset.values)\n\n    ddm_pymc_trace = pm.sample(mp_ctx=\"spawn\", tune=200, draws=200)\n\naz.plot_trace(ddm_pymc_trace)\nplt.tight_layout()\n</pre> # This is a pm.Distribution available in HSSM # There is also a DDM_SDV class for DDMs with the sv parameter from hssm.likelihoods import DDM  with pm.Model() as ddm_pymc:     v = pm.Uniform(\"v\", lower=-10.0, upper=10.0)     a = pm.HalfNormal(\"a\", sigma=2.0)     z = pm.Uniform(\"z\", lower=0.01, upper=0.99)     t = pm.Uniform(\"t\", lower=0.0, upper=0.6, initval=0.1)      ddm = DDM(\"ddm\", v=v, a=a, z=z, t=t, observed=dataset.values)      ddm_pymc_trace = pm.sample(mp_ctx=\"spawn\", tune=200, draws=200)  az.plot_trace(ddm_pymc_trace) plt.tight_layout() <pre>Initializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [v, a, z, t]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 200 tune and 200 draw iterations (800 + 800 draws total) took 48 seconds.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n</pre> In\u00a0[5]: Copied! <pre># This is a likelihood function for the DDM with SDV\n# Different from DDM which we imported in the previous example,\n# which is a pm.Distribution\nfrom hssm.distribution_utils import make_distribution\nfrom hssm.likelihoods import logp_ddm_sdv\n\n# We use `make_distribution` to wrap the likelihood function into a pm.Distribution\nDDM_SDV = make_distribution(\n    rv=\"ddm_sdv\",\n    loglik=logp_ddm_sdv,\n    list_params=[\"v\", \"a\", \"z\", \"t\", \"sv\"],\n    bounds={\"t\": (0, 1)},\n)\n\nwith pm.Model() as ddm_sdv_model:\n    v = pm.Uniform(\"v\", lower=-10.0, upper=10.0)\n    a = pm.HalfNormal(\"a\", sigma=2.0)\n    z = pm.Uniform(\"z\", lower=0.01, upper=0.99)\n    t = pm.Uniform(\"t\", lower=0.0, upper=0.6, initval=0.1)\n    sv = pm.HalfNormal(\"sv\", sigma=2.0)\n\n    ddm = DDM_SDV(\"ddm\", v=v, a=a, z=z, t=t, sv=sv, observed=dataset.values)\n\n    ddm_sdv_trace = pm.sample(mp_ctx=\"spawn\", tune=200, draws=200)\n\naz.plot_trace(ddm_sdv_trace)\nplt.tight_layout()\n</pre> # This is a likelihood function for the DDM with SDV # Different from DDM which we imported in the previous example, # which is a pm.Distribution from hssm.distribution_utils import make_distribution from hssm.likelihoods import logp_ddm_sdv  # We use `make_distribution` to wrap the likelihood function into a pm.Distribution DDM_SDV = make_distribution(     rv=\"ddm_sdv\",     loglik=logp_ddm_sdv,     list_params=[\"v\", \"a\", \"z\", \"t\", \"sv\"],     bounds={\"t\": (0, 1)}, )  with pm.Model() as ddm_sdv_model:     v = pm.Uniform(\"v\", lower=-10.0, upper=10.0)     a = pm.HalfNormal(\"a\", sigma=2.0)     z = pm.Uniform(\"z\", lower=0.01, upper=0.99)     t = pm.Uniform(\"t\", lower=0.0, upper=0.6, initval=0.1)     sv = pm.HalfNormal(\"sv\", sigma=2.0)      ddm = DDM_SDV(\"ddm\", v=v, a=a, z=z, t=t, sv=sv, observed=dataset.values)      ddm_sdv_trace = pm.sample(mp_ctx=\"spawn\", tune=200, draws=200)  az.plot_trace(ddm_sdv_trace) plt.tight_layout() <pre>Initializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [v, a, z, t, sv]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 200 tune and 200 draw iterations (800 + 800 draws total) took 55 seconds.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n</pre> <p>The above example shows how to use the <code>make_distribution</code> factory function to create a <code>pm.Distribution</code> class that can be used for modeling in <code>PyMC</code> in the code above. All you need is a likelihood function that can be directly used with <code>pytensor</code>. This mostly is a Python function written in <code>pytensor</code>, as is the case with <code>logp_ddm_sdv</code>. If you have a likelihood function written with JAX or Cython, or if you have an approximate differentiable likelihood saved in an <code>onnx</code> file, you need to wrap this likelihood in a pytensor <code>Op</code> before using the <code>make_distribution</code> function. The next section describes how to do so. Please refer to the next section to create a <code>pytensor</code>-compatible likelihood function before coming back to this section to create a <code>PyMC</code> <code>Distribution</code> class.</p> <p>Detailed usage of the <code>make_distribution</code> factory function is as follows:</p> <ul> <li><p><code>rv</code>: a <code>str</code> or a <code>RandomVariable</code>. If a <code>str</code> is provided, a <code>RandomVariable</code> class will be created automatically. This <code>RandomVariable</code> will use the <code>str</code> to identify a simulator provided in the <code>ssm_simulators</code> package as its <code>rng_fn</code> (sampling) function. If this <code>str</code> is not one of the entries to the <code>model_config</code> <code>dict</code> specified here, then the <code>Distribution</code> will still be created but with a warning that any attempt to sample from the <code>RandomVariable</code> will result in an Error. That includes sampling from the posterior distribution. The user could create his/her own <code>RandomVariable</code> class and define its <code>rng_fn</code> class method for sampling.</p> </li> <li><p><code>loglik</code>: an instance of a <code>pytensor</code> <code>Op</code> object or a Python <code>Callable</code>. This is where the likelihood function is passed in. <code>make_distribution</code> assumes that the likelihood function is directly usable as part of a <code>pytensor</code> graph, such as a function written in <code>pytensor</code> or an instance of a <code>pytensor</code> <code>Op</code> object. Otherwise, please refer to the next section to create a <code>pytensor</code>-compatible likelihood function.</p> <p>The signature for the likelihood also has to follow a specific pattern. Please refer to this section for more details.</p> </li> <li><p><code>list_params</code>: a list of <code>str</code>s specifying the parameters used in the model. Note the order in which the parameters are defined in this list is very important! In principle it has to be consistent with the order in which parameters are passed to the <code>Op</code> or <code>Callable</code> specified with <code>loglik</code>.</p> </li> <li><p><code>bounds</code>: a <code>dict</code> of <code>param : (lower, upper)</code> specifying the range of values that each parameter can take. HSSM currently does not support specifying <code>inf</code> or <code>-inf</code> as bounds but will in an update in the near future.</p> </li> </ul> <p>Once the distribution is created, you can use it with <code>PyMC</code> just as the <code>DDM</code> class created above.</p> In\u00a0[6]: Copied! <pre>from hssm.distribution_utils import (\n    make_distribution,\n    make_likelihood_callable,\n)\n\nloglik_op = make_likelihood_callable(\n    loglik=\"ddm.onnx\",  # will be downloaded from huggingface\n    loglik_kind=\"approx_differentiable\",\n    backend=\"jax\",  # the onnx will be translated to JAX\n    params_is_reg=[False] * 4,  # required if backend is JAX.\n    # Since the model below has no regression, we provide a list of 4 `False`s\n)\n\nDDM_JAX = make_distribution(\n    rv=\"ddm\",\n    loglik=loglik_op,\n    list_params=[\"v\", \"a\", \"z\", \"t\"],\n    bounds={\"t\": (0, 2), \"a\": (0, 2.5)},\n)\n\nwith pm.Model() as ddm_jax_model:\n    v = pm.Uniform(\"v\", lower=-3, upper=3)\n    a = pm.Gamma(\"a\", mu=0.5, sigma=1.0)\n    z = pm.Uniform(\"z\", lower=0.1, upper=0.9)\n    t = pm.Uniform(\"t\", lower=0.01, upper=1.0, initval=0.1)\n\n    ddm = DDM_JAX(\"ddm\", v=v, a=a, z=z, t=t, observed=dataset.values)\n\n    ddm_jax_trace = pm.sample(mp_ctx=\"spawn\", tune=500, draws=200)\n\naz.plot_trace(ddm_jax_trace)\nplt.tight_layout()\n</pre> from hssm.distribution_utils import (     make_distribution,     make_likelihood_callable, )  loglik_op = make_likelihood_callable(     loglik=\"ddm.onnx\",  # will be downloaded from huggingface     loglik_kind=\"approx_differentiable\",     backend=\"jax\",  # the onnx will be translated to JAX     params_is_reg=[False] * 4,  # required if backend is JAX.     # Since the model below has no regression, we provide a list of 4 `False`s )  DDM_JAX = make_distribution(     rv=\"ddm\",     loglik=loglik_op,     list_params=[\"v\", \"a\", \"z\", \"t\"],     bounds={\"t\": (0, 2), \"a\": (0, 2.5)}, )  with pm.Model() as ddm_jax_model:     v = pm.Uniform(\"v\", lower=-3, upper=3)     a = pm.Gamma(\"a\", mu=0.5, sigma=1.0)     z = pm.Uniform(\"z\", lower=0.1, upper=0.9)     t = pm.Uniform(\"t\", lower=0.01, upper=1.0, initval=0.1)      ddm = DDM_JAX(\"ddm\", v=v, a=a, z=z, t=t, observed=dataset.values)      ddm_jax_trace = pm.sample(mp_ctx=\"spawn\", tune=500, draws=200)  az.plot_trace(ddm_jax_trace) plt.tight_layout() <pre>Initializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [v, a, z, t]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 500 tune and 200 draw iterations (2_000 + 800 draws total) took 87 seconds.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n</pre>"},{"location":"tutorials/pymc/#using-the-low-level-api-from-hssm-directly-with-pymc","title":"Using the low-level API from HSSM directly with PyMC\u00b6","text":"<p>This is a tutorial for advanced users who prefer to use the convenience functions and classes in HSSM to create models and sample in PyMC without using <code>bambi</code>. We assume that the readers of this tutorial are familiar with the internals of <code>PyMC</code>, <code>pytensor</code>, and/or <code>JAX</code>.</p> <p>In addition to the high-level API that relies on <code>bambi</code> for model creation, HSSM also features a low-level API that it internally calls for creating <code>pytensor</code> <code>Op</code>s and <code>pm.Distribution</code>s. Experienced users can use the low-level API directly with <code>PyMC</code> to create even more customized models. This tutorial shows how advanced users can utilize the low-level API and convenience functions that HSSM provides to interface with PyMC.</p>"},{"location":"tutorials/pymc/#colab-instructions","title":"Colab Instructions\u00b6","text":"<p>If you would like to run this tutorial on Google colab, please click this link.</p> <p>Once you are in the colab, follow the installation instructions below and then restart your runtime.</p> <p>Just uncomment the code in the next code cell and run it!</p> <p>NOTE:</p> <p>You may want to switch your runtime to have a GPU or TPU. To do so, go to Runtime &gt; Change runtime type and select the desired hardware accelerator.</p> <p>Note that if you switch your runtime you have to follow the installation instructions again.</p>"},{"location":"tutorials/pymc/#load-modules","title":"Load Modules\u00b6","text":""},{"location":"tutorials/pymc/#build-a-model-directly-from-pymc","title":"Build a model directly from PyMC\u00b6","text":"<p>With <code>DDM</code> and <code>DDM_SDV</code> which are <code>pm.Distribution</code>s, we can create models directly with PyMC. We first simulate some data using <code>ssm_simulators</code> and then use <code>PyMC</code> to specify the model and sample from the it.</p>"},{"location":"tutorials/pymc/#building-custom-top-level-distributions-with-make_distribution","title":"Building custom top-level distributions with <code>make_distribution</code>\u00b6","text":"<p>Note: This tutorial has undergone major updates in HSSM 0.2.1+ following breaking changes in the <code>distribution_utils</code> api. Please follow this tutorial closely if your previous code no longer works.</p>"},{"location":"tutorials/pymc/#make_distribution","title":"<code>make_distribution</code>\u00b6","text":"<p>The above example shows that, as long as the top-level distribution is known, modeling can be done in <code>PyMC</code> as well without using <code>Bambi</code>. However, as this official <code>PyMC</code> tutorial shows, creating a distribution in PyMC can be a time consuming-task. You will need to create a <code>RandomVariable</code> first and then define your custom <code>Distribution</code> by extending <code>pm.Distribution</code> class. From <code>PyMC 5.0.0</code> on, <code>pm.CustomDist</code> simplifies this process, but the use case is not applicable to complex likelihoods in HSSM. Therefore, HSSM provides convenience functions in its <code>distribution_utils</code> submodule that make this process easy. Next, we use another example to show how we can use these functions to create custom <code>pm.Distribution</code>s to be used with <code>PyMC</code>.</p> <p>Use Case: Suppose we have a likelihood function for DDM models with standard deviations for <code>v</code> written. This model has 5 parameters: <code>v, a, z, t, sv</code>, and we want to use this function as the likelihood to create a <code>pm.Distribution</code> for modeling with <code>PyMC</code>. We can use <code>make_distribution</code> for this purpose.</p> <p>Note: This distribution is already available in HSSM at <code>hssm.likelihoods.DDM_SDV</code>. For illustration purposes, we go through the same process in which this distribution is created. We can use the same procedure for other distributions not currently available in HSSM.</p>"},{"location":"tutorials/pymc/#creating-pytensor-compatible-likelihood-functions-with-make_likelihood_callable","title":"Creating <code>pytensor</code>-compatible likelihood functions with <code>make_likelihood_callable</code>\u00b6","text":"<p>Update notice: Up until HSSM 0.2.0, there used to be a <code>make_distribution_from_onnx</code> function for creating <code>pm.Distribution</code>s directly from <code>onnx</code> files. That function is now deprecated in favor of a more general and customizable process described below.</p> <p>Sometimes users might have written a custom likelihood in <code>JAX</code>, <code>Cython</code>, or <code>numba</code> that they would like to incorporate into their <code>PyMC</code> modeling workflows. They might also want to incorporate a neural network as an approximate likelihood function. In all of these cases, they can use <code>make_likelihood_callable</code> to wrap these non-<code>pytensor</code>-compatible likelihoods into a <code>pytensor</code> <code>Op</code>, so that it can be used in the sampling process. We now describe both of these use cases:</p> <ol> <li>I have a function written in <code>JAX</code>, <code>Cython</code>, <code>numba</code>, or even pure Python:</li> </ol> <p>In this case, we assume that these are \"black box\" likelihoods that are not differentiable. We do know that JAX functions are differentiable, and we are planning to support differentiable <code>JAX</code> likelihoods soon, but for now, we assume that if this function is written in <code>JAX</code>, it is a <code>jit</code>ted <code>JAX</code> function that is not differentiable.</p> <pre>from hssm.distribution_utils import make_likelihood_callable\n\n# Suppose you have a non-differentiable function called my_awesome_likelihood_func\nloglik_op = make_likelihood_callable(\n    loglik=my_awesome_likelihood_func,\n    loglik_kind=\"blackbox\",\n)\n\nMY_DIST = make_distribution(\n    loglik=loglik_op,\n    ...\n)\n</pre> <p>What happens under the hood is that <code>make_likelihood_callable</code> will construct an <code>Op</code> and wrap this <code>Op</code> around the function. Please note that this does NOT make the function differentiable. When using the <code>MY_DIST</code> distribution created above, please use a sampler that does not make use of the gradients such as the slice sampler.</p> <ol> <li>I have a neural network saved in an <code>onnx</code> file that I want to use as the likelihood function.</li> </ol> <p>This use case is very similar to the one above. All you need to do is to provide a <code>str</code> or a <code>Path</code> to the onnx file. <code>make_likelihood_callable</code> will look for the file locally first. If the file is not available locally, it will also check the HSSM hugging face model repository to look for the model.</p> <p>HSSM will translate the <code>onnx</code> file either into a <code>JAX</code> function or a <code>pytensor</code> function, depending on which <code>backend</code> you specify. When the <code>jax</code> backend is chosen, please also provide a list of <code>bool</code>s to <code>param_is_reg</code> indicating which of the parameters will be the target of a regression. This tells JAX how to vectorize the comptuation for maximum performance.</p>"},{"location":"tutorials/pymc_to_hssm/","title":"HSSM MathPsych 2025","text":"<p>Welcome to the PyMC to HSSM tutorial:</p> <p>This is an advanced tutorial, taught as part of a workshop session during the conference for Mathematical Psychology, held in Columbus Ohio, 2025.</p> <p>In this short tutorial we will explore some of the features of HSSM that are geared towards users who want to do one of three things:</p> <ol> <li>Build custom PyMC models around observation models build via HSSM low-level utilities (to e.g. break out of the hierarchical regression corsett imposed via the Bambi interface)</li> <li>Work with custom models that are not provided by HSSM out of the box via the High-Level Interface (please consider contributing directly to the HSSM ecosystem eventually :))</li> <li>Break our of PyMC/HSSM on the tail end to apply your own samplers!</li> </ol> In\u00a0[1]: Copied! <pre># If running this on Colab, please uncomment the next line\n# !pip install hssm\n# !pip install onnxruntime\n# !pip install \"zeus-mcmc&gt;=2.5.4\"\n</pre> # If running this on Colab, please uncomment the next line # !pip install hssm # !pip install onnxruntime # !pip install \"zeus-mcmc&gt;=2.5.4\" In\u00a0[2]: Copied! <pre># # Data Files\n# !wget -P  data/mathpsych_workshop_2025_data https://raw.githubusercontent.com/lnccbrown/HSSM/main/docs/tutorials/pymc_to_hssm/mathpsych_workshop_2025_data/race_3_no_bias_lan_batch.onnx\n# !wget -P  data/mathpsych_workshop_2025_data https://raw.githubusercontent.com/lnccbrown/HSSM/main/docs/tutorials/pymc_to_hssm/mathpsych_workshop_2025_data/ddm_lan_batch.onnx\n</pre> # # Data Files # !wget -P  data/mathpsych_workshop_2025_data https://raw.githubusercontent.com/lnccbrown/HSSM/main/docs/tutorials/pymc_to_hssm/mathpsych_workshop_2025_data/race_3_no_bias_lan_batch.onnx # !wget -P  data/mathpsych_workshop_2025_data https://raw.githubusercontent.com/lnccbrown/HSSM/main/docs/tutorials/pymc_to_hssm/mathpsych_workshop_2025_data/ddm_lan_batch.onnx  In\u00a0[3]: Copied! <pre>import os\nfrom pathlib import Path\nfrom copy import deepcopy\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport hssm\nimport ssms\nimport arviz as az\n\nimport onnx\nimport onnxruntime as ort\n</pre> import os from pathlib import Path from copy import deepcopy import matplotlib.pyplot as plt import numpy as np  import hssm import ssms import arviz as az  import onnx import onnxruntime as ort <p>As you will see over the course of this tutorial, ultimately we don't care about where networks might come from. Here we rely on our own pipeline to train a LAN, however the conceptual part of this tutorial translates to likelihood functions that are derived from really any source, such as e.g. coming directly out of the BayesFlow package.</p> <p>Our pipeline relies on two support packages:</p> <ol> <li>ssm-simulators: A package with focus on fast simulation of sequential sampling models</li> <li>LanFactory: A package to train simple neural networks geared to work seemlessly with training data generated from ssm-simulators</li> </ol> <p><code>ssm-simulators</code> is designed to be simple to contribute to (we continuously improve that aspect), so if you design new simulators, you should find it quite easy to add them to the package.</p> In\u00a0[4]: Copied! <pre># !torchtrain --config-path pymc_to_hssm/network_config.yaml --training-data-folder torch_nb_data/training_data --dl-workers 4 --networks-path-base pymc_to_hssm/example_network/\n</pre> # !torchtrain --config-path pymc_to_hssm/network_config.yaml --training-data-folder torch_nb_data/training_data --dl-workers 4 --networks-path-base pymc_to_hssm/example_network/ In\u00a0[5]: Copied! <pre>def make_batchable(onnx_model,\n                   folder_path = \"pymc_to_hssm/mathpsych_2025_data\",\n                   file_name = \"ddm_lan_batch.onnx\"):\n\n    # Change input and output dimensions to be dynamic to allow for batching\n    # (in case this is not already done)\n    \n    for input_tensor in onnx_model.graph.input:\n        dim_proto = input_tensor.type.tensor_type.shape.dim[0]\n        if not dim_proto.dim_param == \"None\":\n            dim_proto.dim_param = \"None\"\n\n    for output_tensor in onnx_model.graph.output:\n        dim_proto = output_tensor.type.tensor_type.shape.dim[0]\n        if not dim_proto.dim_param == \"None\":\n            dim_proto.dim_param = \"None\"\n    os.makedirs(folder_path, exist_ok = True)\n    onnx.save(onnx_model, Path(folder_path, file_name))\n</pre> def make_batchable(onnx_model,                    folder_path = \"pymc_to_hssm/mathpsych_2025_data\",                    file_name = \"ddm_lan_batch.onnx\"):      # Change input and output dimensions to be dynamic to allow for batching     # (in case this is not already done)          for input_tensor in onnx_model.graph.input:         dim_proto = input_tensor.type.tensor_type.shape.dim[0]         if not dim_proto.dim_param == \"None\":             dim_proto.dim_param = \"None\"      for output_tensor in onnx_model.graph.output:         dim_proto = output_tensor.type.tensor_type.shape.dim[0]         if not dim_proto.dim_param == \"None\":             dim_proto.dim_param = \"None\"     os.makedirs(folder_path, exist_ok = True)     onnx.save(onnx_model, Path(folder_path, file_name)) In\u00a0[6]: Copied! <pre>ddm_network_path = Path(\"pymc_to_hssm\",\n                        \"mathpsych_workshop_2025_data\",\n                        \"ddm_lan_batch.onnx\")\n</pre> ddm_network_path = Path(\"pymc_to_hssm\",                         \"mathpsych_workshop_2025_data\",                         \"ddm_lan_batch.onnx\") In\u00a0[7]: Copied! <pre># Load onnx model\nonnx_model = onnx.load(ddm_network_path)\ninput_name = onnx_model.graph.input[0].name\nort_session = ort.InferenceSession(onnx_model.SerializeToString())\n\n# Test inference speed\nimport time\n\nstart = time.time()\nfor i in range(100):\n    ort_session.run(\n        None, {input_name: np.random.uniform(size=(1000, 6)).astype(np.float32)}\n    )\nend = time.time()\nprint(f\"Time taken: {(end - start) / 100} seconds\")\n</pre> # Load onnx model onnx_model = onnx.load(ddm_network_path) input_name = onnx_model.graph.input[0].name ort_session = ort.InferenceSession(onnx_model.SerializeToString())  # Test inference speed import time  start = time.time() for i in range(100):     ort_session.run(         None, {input_name: np.random.uniform(size=(1000, 6)).astype(np.float32)}     ) end = time.time() print(f\"Time taken: {(end - start) / 100} seconds\") <pre>Time taken: 0.0002436685562133789 seconds\n</pre> <p>We will first work with <code>.onnx</code> files by directly refering to the filepath and letting our utilities take care of the rest. However, we will see one example at the end, where we manually construct a likelihood from an instantiated <code>.onnx</code> runtime.</p> In\u00a0[8]: Copied! <pre># Set parameters\nv0 = 1.5\nv1 = 0.5\nv2 = 0.5\na = 1.5\nt = 0.3\nz = 0.5\n\n# simulate some data from the model\nobs_race3 = hssm.simulate_data(\n    theta=dict(v0=v0, v1=v1, v2=v2, a=a, t=t, z=z), model=\"race_no_bias_3\", size=750\n)\n</pre> # Set parameters v0 = 1.5 v1 = 0.5 v2 = 0.5 a = 1.5 t = 0.3 z = 0.5  # simulate some data from the model obs_race3 = hssm.simulate_data(     theta=dict(v0=v0, v1=v1, v2=v2, a=a, t=t, z=z), model=\"race_no_bias_3\", size=750 ) In\u00a0[9]: Copied! <pre>ssms.config.model_config[\"race_no_bias_3\"]\n</pre> ssms.config.model_config[\"race_no_bias_3\"] Out[9]: <pre>{'name': 'race_no_bias_3',\n 'params': ['v0', 'v1', 'v2', 'a', 'z', 't'],\n 'param_bounds': [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n  [2.5, 2.5, 2.5, 3.0, 0.9, 2.0]],\n 'boundary_name': 'constant',\n 'boundary': &lt;function ssms.basic_simulators.boundary_functions.constant(t: float | numpy.ndarray = 0) -&gt; float | numpy.ndarray&gt;,\n 'n_params': 6,\n 'n_particles': 3,\n 'default_params': [0.0, 0.0, 0.0, 2.0, 0.5, 0.001],\n 'nchoices': 3,\n 'choices': [0, 1, 2],\n 'simulator': &lt;cyfunction race_model at 0x2ac625a80&gt;}</pre> In\u00a0[10]: Copied! <pre>race_3_model_config = deepcopy(ssms.config.model_config[\"race_no_bias_3\"])\n</pre> race_3_model_config = deepcopy(ssms.config.model_config[\"race_no_bias_3\"]) In\u00a0[11]: Copied! <pre># Little utilitiy to transform the list entry for parameter bounds into a dictionary\ndef make_param_bound_dict(config):\n    param_bounds = config[\"param_bounds\"]\n    print(param_bounds)\n    param_names = config['params']\n    print(param_names)\n    return {param_names[i]: (param_bounds[0][i],\n                             param_bounds[1][i]) for i in range(len(param_names))}\n</pre> # Little utilitiy to transform the list entry for parameter bounds into a dictionary def make_param_bound_dict(config):     param_bounds = config[\"param_bounds\"]     print(param_bounds)     param_names = config['params']     print(param_names)     return {param_names[i]: (param_bounds[0][i],                              param_bounds[1][i]) for i in range(len(param_names))} In\u00a0[12]: Copied! <pre># Networks\nnetwork_path = Path(\"pymc_to_hssm\",\n                    \"mathpsych_workshop_2025_data\",\n                    \"race_3_no_bias_lan_batch.onnx\")\n</pre> # Networks network_path = Path(\"pymc_to_hssm\",                     \"mathpsych_workshop_2025_data\",                     \"race_3_no_bias_lan_batch.onnx\") In\u00a0[13]: Copied! <pre>from hssm.distribution_utils.dist import (\n    make_distribution,\n    make_likelihood_callable,\n)\n\n# Step 1: Define a likelihood function\nlogp_jax_op = make_likelihood_callable(\n    loglik=network_path,\n    loglik_kind=\"approx_differentiable\",\n    backend=\"jax\",\n    params_is_reg=[True for _ in range(len(race_3_model_config[\"params\"]))],\n    # params_only=False,\n)\n\n# Step 2: Define a distribution\nCustomDistribution = make_distribution(\n    rv=\"race_no_bias_3\", # CustomRV,\n    loglik=logp_jax_op,\n    list_params=race_3_model_config[\"params\"],\n    bounds=make_param_bound_dict(race_3_model_config),\n)\n</pre> from hssm.distribution_utils.dist import (     make_distribution,     make_likelihood_callable, )  # Step 1: Define a likelihood function logp_jax_op = make_likelihood_callable(     loglik=network_path,     loglik_kind=\"approx_differentiable\",     backend=\"jax\",     params_is_reg=[True for _ in range(len(race_3_model_config[\"params\"]))],     # params_only=False, )  # Step 2: Define a distribution CustomDistribution = make_distribution(     rv=\"race_no_bias_3\", # CustomRV,     loglik=logp_jax_op,     list_params=race_3_model_config[\"params\"],     bounds=make_param_bound_dict(race_3_model_config), ) <pre>[[0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [2.5, 2.5, 2.5, 3.0, 0.9, 2.0]]\n['v0', 'v1', 'v2', 'a', 'z', 't']\n</pre> <p>Note</p> <p><code>make_likelihood_callable()</code> has a parameter <code>params_is_reg</code> to which we pass a list of <code>boolean</code> variables (all set to <code>True</code>) in our case.</p> <p>This argument sets parameters of our likelihood to scalars or vectors (trial-wise, <code>is_reg</code> for whether the parameter could be the outcome of a regression). For simplicity we set all parameters to vectors as a default.</p> <p>The conseuqence is that we will define our PyMC model below with that expectation, and multiple all our parameters by a fixed vector of ones to establish the trial-wise relation.</p> In\u00a0[14]: Copied! <pre>import pymc as pm\nimport pytensor.tensor as pt\n\n\nparam_bounds_race_3 = make_param_bound_dict(race_3_model_config)\n\nwith pm.Model() as race3_pymc:\n    # Data\n    obs_pt = pm.Data(\"obs\", obs_race3[[\"rt\", \"response\"]].values)\n\n    # Priors\n    v0 = pm.Uniform(\"v0\", \n                   lower=param_bounds_race_3[\"v0\"][0],\n                   upper=param_bounds_race_3[\"v0\"][1],\n                   initval=1.0)\n    v1 = pm.Uniform(\"v1\", \n                   lower=param_bounds_race_3[\"v1\"][0],\n                   upper=param_bounds_race_3[\"v1\"][1],\n                   initval=1.0)\n    v2 = pm.Uniform(\"v2\", \n                   lower=param_bounds_race_3[\"v2\"][0],\n                   upper=param_bounds_race_3[\"v2\"][1],\n                   initval=1.0)\n    a = pm.Uniform(\"a\",\n                      lower=param_bounds_race_3[\"a\"][0],\n                      upper=param_bounds_race_3[\"a\"][1],\n                      initval=2.0)\n    z = 0.5 # We fix this parameter because it is extremely hard to jointly identify both z and a\n    t = pm.Uniform(\"t\",\n                   lower=param_bounds_race_3[\"t\"][0],\n                   upper=param_bounds_race_3[\"t\"][1],\n                   initval=0.1)\n\n    # Vectorize parameters (Deterministics)\n    v0_reg = pm.Deterministic(\"v0_reg\", v0 * pt.ones_like(obs_race3.values[:, 0]))\n    v1_reg = pm.Deterministic(\"v1_reg\", v1 * pt.ones_like(obs_race3.values[:, 0]))\n    v2_reg = pm.Deterministic(\"v2_reg\", v2 * pt.ones_like(obs_race3.values[:, 0]))\n    a_reg = pm.Deterministic(\"a_reg\", a * pt.ones_like(obs_race3.values[:, 0]))\n    t_reg = pm.Deterministic(\"t_reg\", t * pt.ones_like(obs_race3.values[:, 0]))\n    z_reg = pm.Deterministic(\"z_reg\", z * pt.ones_like(obs_race3.values[:, 0]))\n\n    # Likelihood\n    race3_obs = CustomDistribution(\n        \"Race3\", v0=v0_reg, v1=v1_reg, v2=v2_reg, a=a_reg, z=z_reg, t=t_reg, observed=obs_pt\n    )\n</pre> import pymc as pm import pytensor.tensor as pt   param_bounds_race_3 = make_param_bound_dict(race_3_model_config)  with pm.Model() as race3_pymc:     # Data     obs_pt = pm.Data(\"obs\", obs_race3[[\"rt\", \"response\"]].values)      # Priors     v0 = pm.Uniform(\"v0\",                     lower=param_bounds_race_3[\"v0\"][0],                    upper=param_bounds_race_3[\"v0\"][1],                    initval=1.0)     v1 = pm.Uniform(\"v1\",                     lower=param_bounds_race_3[\"v1\"][0],                    upper=param_bounds_race_3[\"v1\"][1],                    initval=1.0)     v2 = pm.Uniform(\"v2\",                     lower=param_bounds_race_3[\"v2\"][0],                    upper=param_bounds_race_3[\"v2\"][1],                    initval=1.0)     a = pm.Uniform(\"a\",                       lower=param_bounds_race_3[\"a\"][0],                       upper=param_bounds_race_3[\"a\"][1],                       initval=2.0)     z = 0.5 # We fix this parameter because it is extremely hard to jointly identify both z and a     t = pm.Uniform(\"t\",                    lower=param_bounds_race_3[\"t\"][0],                    upper=param_bounds_race_3[\"t\"][1],                    initval=0.1)      # Vectorize parameters (Deterministics)     v0_reg = pm.Deterministic(\"v0_reg\", v0 * pt.ones_like(obs_race3.values[:, 0]))     v1_reg = pm.Deterministic(\"v1_reg\", v1 * pt.ones_like(obs_race3.values[:, 0]))     v2_reg = pm.Deterministic(\"v2_reg\", v2 * pt.ones_like(obs_race3.values[:, 0]))     a_reg = pm.Deterministic(\"a_reg\", a * pt.ones_like(obs_race3.values[:, 0]))     t_reg = pm.Deterministic(\"t_reg\", t * pt.ones_like(obs_race3.values[:, 0]))     z_reg = pm.Deterministic(\"z_reg\", z * pt.ones_like(obs_race3.values[:, 0]))      # Likelihood     race3_obs = CustomDistribution(         \"Race3\", v0=v0_reg, v1=v1_reg, v2=v2_reg, a=a_reg, z=z_reg, t=t_reg, observed=obs_pt     ) <pre>[[0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [2.5, 2.5, 2.5, 3.0, 0.9, 2.0]]\n['v0', 'v1', 'v2', 'a', 'z', 't']\n</pre> In\u00a0[15]: Copied! <pre>race3_pymc\n</pre> race3_pymc Out[15]:  $$             \\begin{array}{rcl}             \\text{v0} &amp;\\sim &amp; \\operatorname{Uniform}(0,~2.5)\\\\\\text{v1} &amp;\\sim &amp; \\operatorname{Uniform}(0,~2.5)\\\\\\text{v2} &amp;\\sim &amp; \\operatorname{Uniform}(0,~2.5)\\\\\\text{a} &amp;\\sim &amp; \\operatorname{Uniform}(1,~3)\\\\\\text{t} &amp;\\sim &amp; \\operatorname{Uniform}(0,~2)\\\\\\text{v0\\_reg} &amp;\\sim &amp; \\operatorname{Deterministic}(f(\\text{v0}))\\\\\\text{v1\\_reg} &amp;\\sim &amp; \\operatorname{Deterministic}(f(\\text{v1}))\\\\\\text{v2\\_reg} &amp;\\sim &amp; \\operatorname{Deterministic}(f(\\text{v2}))\\\\\\text{a\\_reg} &amp;\\sim &amp; \\operatorname{Deterministic}(f(\\text{a}))\\\\\\text{t\\_reg} &amp;\\sim &amp; \\operatorname{Deterministic}(f(\\text{t}))\\\\\\text{z\\_reg} &amp;\\sim &amp; \\operatorname{Deterministic}(f())\\\\\\text{Race3} &amp;\\sim &amp; \\operatorname{SSM}(\\text{v0\\_reg},~\\text{v1\\_reg},~\\text{v2\\_reg},~\\text{a\\_reg},~\\text{z\\_reg},~\\text{t\\_reg})             \\end{array}             $$  In\u00a0[16]: Copied! <pre>pm.model_to_graphviz(model=race3_pymc)\n</pre> pm.model_to_graphviz(model=race3_pymc) Out[16]: In\u00a0[17]: Copied! <pre>with race3_pymc:\n    # Sample Posterior\n    race3_pymc_trace = pm.sample(nuts_sampler = \"numpyro\",\n                                 chains = 2,\n                                 tune = 500,\n                                 draws = 500)\n    # Get Posterior Predictive Samples\n    pm.sample_posterior_predictive(race3_pymc_trace,\n                                   extend_inferencedata=True)\n</pre> with race3_pymc:     # Sample Posterior     race3_pymc_trace = pm.sample(nuts_sampler = \"numpyro\",                                  chains = 2,                                  tune = 500,                                  draws = 500)     # Get Posterior Predictive Samples     pm.sample_posterior_predictive(race3_pymc_trace,                                    extend_inferencedata=True) <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>We recommend running at least 4 chains for robust computation of convergence diagnostics\nSampling: [Race3]\n</pre> <pre>Output()</pre> <pre></pre> In\u00a0[18]: Copied! <pre># Simple Regression Setup\nbeta = 0.5\ndifficulty = np.random.uniform(low = -1,\n                               high = 1,\n                               size = 750)\n\n# Set parameters\nv0 = 1.75 - beta * difficulty\nv1 = 1.75 - beta * difficulty\nv2 = 1.2 - beta * difficulty\na = 2.0 + beta * difficulty\nt = 0.3\nz = 0.5\n\n# simulate some data from the model\nobs_race3_reg = hssm.simulate_data(\n    theta=dict(v0=v0, v1=v1, v2=v2, a=a, t=t, z=z), model=\"race_no_bias_3\", size=1\n)\n</pre> # Simple Regression Setup beta = 0.5 difficulty = np.random.uniform(low = -1,                                high = 1,                                size = 750)  # Set parameters v0 = 1.75 - beta * difficulty v1 = 1.75 - beta * difficulty v2 = 1.2 - beta * difficulty a = 2.0 + beta * difficulty t = 0.3 z = 0.5  # simulate some data from the model obs_race3_reg = hssm.simulate_data(     theta=dict(v0=v0, v1=v1, v2=v2, a=a, t=t, z=z), model=\"race_no_bias_3\", size=1 ) In\u00a0[19]: Copied! <pre>obs_race3_reg\n</pre> obs_race3_reg Out[19]: rt response 0 0.531644 1.0 1 0.674176 0.0 2 1.245017 0.0 3 1.267111 1.0 4 0.808519 0.0 ... ... ... 745 0.527132 0.0 746 0.566118 1.0 747 1.824016 1.0 748 0.525823 0.0 749 0.440037 1.0 <p>750 rows \u00d7 2 columns</p> In\u00a0[20]: Copied! <pre>import pymc as pm\nfrom pytensor import tensor as pt\n\n# Turn config parameter bounds into dictionary\nparam_bounds_race_3 = make_param_bound_dict(race_3_model_config)\n\nwith pm.Model() as race3_pymc2:\n    # Data\n    obs_pt = pm.Data(\"obs\", obs_race3_reg[[\"rt\", \"response\"]].values)\n\n    # Priors\n    beta = pm.Normal(\"beta\", mu=0, sigma=0.5, initval=0.0)\n    v0 = pm.Uniform(\"v0\", \n                   lower=param_bounds_race_3[\"v0\"][0],\n                   upper=param_bounds_race_3[\"v0\"][1],\n                   initval=1.0)\n    v1 = pm.Uniform(\"v1\", \n                   lower=param_bounds_race_3[\"v1\"][0],\n                   upper=param_bounds_race_3[\"v1\"][1],\n                   initval=1.0)\n    v2 = pm.Uniform(\"v2\", \n                   lower=param_bounds_race_3[\"v2\"][0],\n                   upper=param_bounds_race_3[\"v2\"][1],\n                   initval=1.0)\n    a = pm.Uniform(\"a\",\n                      lower=param_bounds_race_3[\"a\"][0],\n                      upper=param_bounds_race_3[\"a\"][1],\n                      initval=2.0)\n    t = pm.Uniform(\"t\",\n                   lower=0.0,\n                   upper=2.0,\n                   initval=0.1)\n    \n    z = 0.5 # We fix this parameter because it is extremely hard to jointly identify both z and a\n    \n    # Deterministics Block\n    # Compute Regressions\n    reg_a = pm.Deterministic(\"reg_a\", a + beta * difficulty)\n    reg_v0 = pm.Deterministic(\"reg_v0\", v0 - beta * difficulty)\n    reg_v1 = pm.Deterministic(\"reg_v1\", v1 - beta * difficulty)\n    reg_v2 = pm.Deterministic(\"reg_v2\", v2 - beta * difficulty)\n\n    # Vectorize remaining parameters\n    reg_t = pm.Deterministic(\"reg_t\", t * pt.ones_like(obs_race3_reg.values[:, 0]))\n    reg_z = pm.Deterministic(\"reg_z\", z * pt.ones_like(obs_race3_reg.values[:, 0]))\n\n    # Likelihood\n    race3_obs = CustomDistribution(\n        \"Race3\", v0=reg_v0, v1=reg_v1, v2=reg_v2, a=reg_a, z=reg_z, t=reg_t, observed=obs_pt\n    )\n</pre> import pymc as pm from pytensor import tensor as pt  # Turn config parameter bounds into dictionary param_bounds_race_3 = make_param_bound_dict(race_3_model_config)  with pm.Model() as race3_pymc2:     # Data     obs_pt = pm.Data(\"obs\", obs_race3_reg[[\"rt\", \"response\"]].values)      # Priors     beta = pm.Normal(\"beta\", mu=0, sigma=0.5, initval=0.0)     v0 = pm.Uniform(\"v0\",                     lower=param_bounds_race_3[\"v0\"][0],                    upper=param_bounds_race_3[\"v0\"][1],                    initval=1.0)     v1 = pm.Uniform(\"v1\",                     lower=param_bounds_race_3[\"v1\"][0],                    upper=param_bounds_race_3[\"v1\"][1],                    initval=1.0)     v2 = pm.Uniform(\"v2\",                     lower=param_bounds_race_3[\"v2\"][0],                    upper=param_bounds_race_3[\"v2\"][1],                    initval=1.0)     a = pm.Uniform(\"a\",                       lower=param_bounds_race_3[\"a\"][0],                       upper=param_bounds_race_3[\"a\"][1],                       initval=2.0)     t = pm.Uniform(\"t\",                    lower=0.0,                    upper=2.0,                    initval=0.1)          z = 0.5 # We fix this parameter because it is extremely hard to jointly identify both z and a          # Deterministics Block     # Compute Regressions     reg_a = pm.Deterministic(\"reg_a\", a + beta * difficulty)     reg_v0 = pm.Deterministic(\"reg_v0\", v0 - beta * difficulty)     reg_v1 = pm.Deterministic(\"reg_v1\", v1 - beta * difficulty)     reg_v2 = pm.Deterministic(\"reg_v2\", v2 - beta * difficulty)      # Vectorize remaining parameters     reg_t = pm.Deterministic(\"reg_t\", t * pt.ones_like(obs_race3_reg.values[:, 0]))     reg_z = pm.Deterministic(\"reg_z\", z * pt.ones_like(obs_race3_reg.values[:, 0]))      # Likelihood     race3_obs = CustomDistribution(         \"Race3\", v0=reg_v0, v1=reg_v1, v2=reg_v2, a=reg_a, z=reg_z, t=reg_t, observed=obs_pt     ) <pre>[[0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [2.5, 2.5, 2.5, 3.0, 0.9, 2.0]]\n['v0', 'v1', 'v2', 'a', 'z', 't']\n</pre> In\u00a0[21]: Copied! <pre>pm.model_to_graphviz(model=race3_pymc2)\n</pre> pm.model_to_graphviz(model=race3_pymc2) Out[21]: In\u00a0[22]: Copied! <pre>with race3_pymc2:\n    # Sample Posterior\n    race3_pymc_trace2 = pm.sample(nuts_sampler = \"numpyro\",\n                                  chains = 2,\n                                  tune = 500,\n                                  draws = 500)\n    \n    # Get Posterior Predictive Samples\n    pm.sample_posterior_predictive(race3_pymc_trace2,\n                                   extend_inferencedata=True)\n</pre> with race3_pymc2:     # Sample Posterior     race3_pymc_trace2 = pm.sample(nuts_sampler = \"numpyro\",                                   chains = 2,                                   tune = 500,                                   draws = 500)          # Get Posterior Predictive Samples     pm.sample_posterior_predictive(race3_pymc_trace2,                                    extend_inferencedata=True) <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>There were 17 divergences after tuning. Increase `target_accept` or reparameterize.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\nSampling: [Race3]\n</pre> <pre>Output()</pre> <pre></pre> In\u00a0[23]: Copied! <pre>az.summary(race3_pymc_trace2, var_names = [\"~reg\"], filter_vars = \"like\")\n</pre> az.summary(race3_pymc_trace2, var_names = [\"~reg\"], filter_vars = \"like\") Out[23]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat beta 0.495 0.035 0.421 0.552 0.002 0.001 352.0 344.0 1.00 v0 1.466 0.130 1.231 1.708 0.009 0.004 189.0 517.0 1.00 v1 1.631 0.127 1.395 1.857 0.010 0.004 181.0 429.0 1.00 v2 1.263 0.135 1.011 1.536 0.011 0.004 159.0 364.0 1.01 a 1.806 0.095 1.643 1.990 0.009 0.004 108.0 330.0 1.01 t 0.305 0.008 0.291 0.320 0.001 0.000 147.0 304.0 1.01 In\u00a0[24]: Copied! <pre># DDM models (the Wiener First-Passage Time distribution)\nfrom hssm.likelihoods import DDM\n</pre> # DDM models (the Wiener First-Passage Time distribution) from hssm.likelihoods import DDM In\u00a0[25]: Copied! <pre># Simulate\nparam_dict_pymc = dict(v=0.5,\n                       a=1.5,\n                       z=0.5,\n                       t=0.5,\n                       theta=0.0)\ndataset_pymc = hssm.simulate_data(model=\"ddm\", theta=param_dict_pymc, size=1000)\n</pre> # Simulate param_dict_pymc = dict(v=0.5,                        a=1.5,                        z=0.5,                        t=0.5,                        theta=0.0) dataset_pymc = hssm.simulate_data(model=\"ddm\", theta=param_dict_pymc, size=1000) In\u00a0[26]: Copied! <pre>dataset_pymc\n</pre> dataset_pymc Out[26]: rt response 0 2.255000 1.0 1 1.575909 1.0 2 2.692775 1.0 3 2.252525 1.0 4 1.627746 1.0 ... ... ... 995 5.772747 -1.0 996 6.422646 -1.0 997 3.223192 1.0 998 1.756524 1.0 999 2.404412 1.0 <p>1000 rows \u00d7 2 columns</p> In\u00a0[27]: Copied! <pre>import pymc as pm\n\nwith pm.Model() as ddm_pymc:\n    # Data\n    obs_pt = pm.Data(\"obs\", dataset_pymc[[\"rt\", \"response\"]].values)\n\n    # Priors\n    v = pm.Uniform(\"v\", \n                   lower=-10.0,\n                   upper=10.0)\n    a = pm.HalfNormal(\"a\",\n                      sigma=2.0)\n    z = pm.Uniform(\"z\",\n                   lower=0.01,\n                   upper=0.99)\n    t = pm.Uniform(\"t\",\n                   lower=0.0,\n                   upper=0.6)\n    \n    # Likelihood\n    ddm = DDM(\n        \"DDM\", v=v, a=a, z=z, t=t, observed=obs_pt\n    )\n</pre> import pymc as pm  with pm.Model() as ddm_pymc:     # Data     obs_pt = pm.Data(\"obs\", dataset_pymc[[\"rt\", \"response\"]].values)      # Priors     v = pm.Uniform(\"v\",                     lower=-10.0,                    upper=10.0)     a = pm.HalfNormal(\"a\",                       sigma=2.0)     z = pm.Uniform(\"z\",                    lower=0.01,                    upper=0.99)     t = pm.Uniform(\"t\",                    lower=0.0,                    upper=0.6)          # Likelihood     ddm = DDM(         \"DDM\", v=v, a=a, z=z, t=t, observed=obs_pt     ) In\u00a0[28]: Copied! <pre>pm.model_to_graphviz(model=ddm_pymc)\n</pre> pm.model_to_graphviz(model=ddm_pymc) Out[28]: In\u00a0[29]: Copied! <pre>with ddm_pymc:\n    # Sample Posterior\n    ddm_pymc_trace = pm.sample(chains = 2,\n                               draws = 500,\n                               tune = 500)\n    # Get Posterior Predictive Samples\n    pm.sample_posterior_predictive(ddm_pymc_trace,\n                                   extend_inferencedata=True)\n</pre> with ddm_pymc:     # Sample Posterior     ddm_pymc_trace = pm.sample(chains = 2,                                draws = 500,                                tune = 500)     # Get Posterior Predictive Samples     pm.sample_posterior_predictive(ddm_pymc_trace,                                    extend_inferencedata=True) <pre>Initializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [v, a, z, t]\n/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n</pre> <pre>Output()</pre> <pre>/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n</pre> <pre></pre> <pre>Sampling 2 chains for 500 tune and 500 draw iterations (1_000 + 1_000 draws total) took 12 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nSampling: [DDM]\n</pre> <pre>Output()</pre> <pre></pre> In\u00a0[30]: Copied! <pre>az.plot_trace(\n    ddm_pymc_trace,\n    lines=[(key_, {}, param_dict_pymc[key_]) \\\n           for key_ in param_dict_pymc],\n)\n\nplt.tight_layout()\n</pre> az.plot_trace(     ddm_pymc_trace,     lines=[(key_, {}, param_dict_pymc[key_]) \\            for key_ in param_dict_pymc], )  plt.tight_layout() <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/arviz/plots/backends/matplotlib/traceplot.py:218: UserWarning: A valid var_name should be provided, found {'theta'} expected from {'v', 'a', 't', 'z'}\n  warnings.warn(\n</pre> In\u00a0[31]: Copied! <pre># Networks\nnetwork_path = Path(\"pymc_to_hssm\",\n                    \"mathpsych_workshop_2025_data\",\n                    \"race_3_no_bias_lan_batch.onnx\")\n</pre> # Networks network_path = Path(\"pymc_to_hssm\",                     \"mathpsych_workshop_2025_data\",                     \"race_3_no_bias_lan_batch.onnx\") In\u00a0[32]: Copied! <pre># Set parameters\nv0 = 1.0\nv1 = 0.5\nv2 = 0.25\na = 1.5\nt = 0.3\nz = 0.5\n\n# simulate some data from the model\nobs_race3 = hssm.simulate_data(\n    theta=dict(v0=v0, v1=v1, v2=v2, a=a, t=t, z=z), model=\"race_no_bias_3\", size=1000\n)\n</pre> # Set parameters v0 = 1.0 v1 = 0.5 v2 = 0.25 a = 1.5 t = 0.3 z = 0.5  # simulate some data from the model obs_race3 = hssm.simulate_data(     theta=dict(v0=v0, v1=v1, v2=v2, a=a, t=t, z=z), model=\"race_no_bias_3\", size=1000 ) In\u00a0[33]: Copied! <pre>param_bounds_race_3 = make_param_bound_dict(race_3_model_config)\nparameters_race_3 = race_3_model_config[\"params\"]\n\nhssm_race3 = hssm.HSSM(\n    data=obs_race3,\n    model=\"race_no_bias_3\",  # some name for the model\n    model_config={\n        \"list_params\": parameters_race_3,\n        \"bounds\": param_bounds_race_3,\n        \"backend\": \"jax\", # can choose \"jax\" or \"pytensor\" here\n    },  # minimal specification of model parameters and parameter bounds\n    loglik_kind=\"approx_differentiable\",  # use the blackbox loglik\n    loglik=network_path,\n    choices=[0, 1, 2],  # list the legal choice options\n    z=0.5,\n    # p_outlier=0,\n)\n</pre> param_bounds_race_3 = make_param_bound_dict(race_3_model_config) parameters_race_3 = race_3_model_config[\"params\"]  hssm_race3 = hssm.HSSM(     data=obs_race3,     model=\"race_no_bias_3\",  # some name for the model     model_config={         \"list_params\": parameters_race_3,         \"bounds\": param_bounds_race_3,         \"backend\": \"jax\", # can choose \"jax\" or \"pytensor\" here     },  # minimal specification of model parameters and parameter bounds     loglik_kind=\"approx_differentiable\",  # use the blackbox loglik     loglik=network_path,     choices=[0, 1, 2],  # list the legal choice options     z=0.5,     # p_outlier=0, ) <pre>[[0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [2.5, 2.5, 2.5, 3.0, 0.9, 2.0]]\n['v0', 'v1', 'v2', 'a', 'z', 't']\nModel initialized successfully.\n</pre> In\u00a0[34]: Copied! <pre>hssm_race3.graph()\n</pre> hssm_race3.graph() Out[34]: In\u00a0[35]: Copied! <pre>hssm_race3_idata = hssm_race3.sample(draws=500,\n                                     tune=200,\n                                     chains = 2,\n                                     discard_tuned_samples=False)\n</pre> hssm_race3_idata = hssm_race3.sample(draws=500,                                      tune=200,                                      chains = 2,                                      discard_tuned_samples=False) <pre>Using default initvals. \n\n</pre> <pre>  0%|          | 0/700 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/700 [00:00&lt;?, ?it/s]</pre> <pre>We recommend running at least 4 chains for robust computation of convergence diagnostics\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/pymc/pytensorf.py:958: FutureWarning: compile_pymc was renamed to compile. Old name will be removed in a future release of PyMC\n  warnings.warn(\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:01&lt;00:00, 642.33it/s]\n</pre> In\u00a0[36]: Copied! <pre>az.summary(hssm_race3_idata)\n</pre> az.summary(hssm_race3_idata) Out[36]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat a 1.431 0.068 1.304 1.555 0.004 0.003 304.0 368.0 1.01 t 0.302 0.007 0.289 0.315 0.000 0.000 342.0 459.0 1.01 v0 1.072 0.099 0.882 1.243 0.005 0.003 427.0 624.0 1.00 v2 0.367 0.113 0.154 0.583 0.006 0.004 392.0 439.0 1.01 v1 0.589 0.108 0.397 0.799 0.006 0.004 357.0 461.0 1.01 In\u00a0[37]: Copied! <pre>az.plot_trace(hssm_race3.traces,\n             var_names=[\"~v0_mean\"],\n             filter_vars = 'like')\nplt.tight_layout()\n</pre> az.plot_trace(hssm_race3.traces,              var_names=[\"~v0_mean\"],              filter_vars = 'like') plt.tight_layout() <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/arviz/utils.py:146: UserWarning: Items starting with ~: ['v0_mean'] have not been found and will be ignored\n  warnings.warn(\n</pre> In\u00a0[38]: Copied! <pre>az.plot_pair(hssm_race3.traces,\n             var_names=[\"~v0_mean\"],\n             filter_vars= 'like',\n             kind = \"kde\",\n             marginals = True)\nplt.tight_layout()\n</pre> az.plot_pair(hssm_race3.traces,              var_names=[\"~v0_mean\"],              filter_vars= 'like',              kind = \"kde\",              marginals = True) plt.tight_layout() <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/arviz/utils.py:146: UserWarning: Items starting with ~: ['v0_mean'] have not been found and will be ignored\n  warnings.warn(\n</pre> In\u00a0[39]: Copied! <pre>logp_fun = hssm_race3.compile_logp()\nprint(logp_fun(hssm_race3.initial_point(transformed=False)))\n</pre> logp_fun = hssm_race3.compile_logp() print(logp_fun(hssm_race3.initial_point(transformed=False))) <pre>-5648.685684239808\n</pre> In\u00a0[40]: Copied! <pre># time\nimport time\n\nmy_start_point = hssm_race3.initial_point(transformed=False)\nstart_time = time.time()\nfor i in range(1000):\n    logp_fun(my_start_point)\nprint((time.time() - start_time) / 1000)\n</pre> # time import time  my_start_point = hssm_race3.initial_point(transformed=False) start_time = time.time() for i in range(1000):     logp_fun(my_start_point) print((time.time() - start_time) / 1000) <pre>0.00163793683052063\n</pre> In\u00a0[41]: Copied! <pre>def mylogp(theta: list[float]) -&gt; float:\n    \"\"\"Wrap function for compiled log probability function to work with zeus sampler.\n\n    Args\n    ----\n        theta: List of model parameters [v, a, z, t] where:\n            v: Drift rate\n            a: Boundary separation\n            z: Starting point\n            t: Non-decision time\n\n    Returns\n    -------\n        float: Log probability value for the given parameters\n    \"\"\"\n    v0, v1, v2, a, t = theta\n    return logp_fun({\"v0\": v0, \"v1\": v1, \"v2\": v2, \"a\": a, \"t\": t})\n</pre> def mylogp(theta: list[float]) -&gt; float:     \"\"\"Wrap function for compiled log probability function to work with zeus sampler.      Args     ----         theta: List of model parameters [v, a, z, t] where:             v: Drift rate             a: Boundary separation             z: Starting point             t: Non-decision time      Returns     -------         float: Log probability value for the given parameters     \"\"\"     v0, v1, v2, a, t = theta     return logp_fun({\"v0\": v0, \"v1\": v1, \"v2\": v2, \"a\": a, \"t\": t}) In\u00a0[42]: Copied! <pre>import zeus\n\nstart = np.random.uniform(low=-0.2,\n                          high=0.2,\n                          size=(10, 5)) + np.tile([0.5, 1.5, 0.5, 1.5, 0.3], (10, 1)\n)\nsampler = zeus.EnsembleSampler(10, 5, mylogp)\n</pre> import zeus  start = np.random.uniform(low=-0.2,                           high=0.2,                           size=(10, 5)) + np.tile([0.5, 1.5, 0.5, 1.5, 0.3], (10, 1) ) sampler = zeus.EnsembleSampler(10, 5, mylogp) In\u00a0[43]: Copied! <pre>sampler.run_mcmc(start, 1000)\n</pre> sampler.run_mcmc(start, 1000) <pre>Initialising ensemble of 10 walkers...\nSampling progress : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:21&lt;00:00, 12.28it/s]\n</pre> In\u00a0[44]: Copied! <pre>plt.figure(figsize=(16, 1.5 * 5))\nfor n in range(5):\n    plt.subplot2grid((5, 1), (n, 0))\n    plt.plot(sampler.get_chain()[:, :, n], alpha=0.5)\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(16, 1.5 * 5)) for n in range(5):     plt.subplot2grid((5, 1), (n, 0))     plt.plot(sampler.get_chain()[:, :, n], alpha=0.5) plt.tight_layout() plt.show()"},{"location":"tutorials/pymc_to_hssm/#hssm-mathpsych-2025","title":"HSSM MathPsych 2025\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#colab-instructions","title":"Colab Instructions\u00b6","text":"<p>If you would like to run this tutorial on Google colab, please click this link.</p> <p>Once you are in the colab:</p> <ol> <li>Follow the installation instructions below  (uncomment the respective code)</li> <li>restart your runtime.</li> </ol> <p>NOTE:</p> <p>You may want to switch your runtime to have a GPU or TPU. To do so, go to Runtime &gt; Change runtime type and select the desired hardware accelerator. Note that if you switch your runtime you have to follow the installation instructions again.</p>"},{"location":"tutorials/pymc_to_hssm/#start-of-tutorial","title":"Start of Tutorial\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#load-modules","title":"Load Modules\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#getting-ourselves-a-lan-likelihood-network","title":"Getting ourselves a LAN / Likelihood-Network\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#training-data","title":"Training Data\u00b6","text":"<p>Once you have your simulator implemented, you can specify a simple <code>.yaml</code> file, with a variation of the following contents:</p> <pre><code>MODEL: 'race_3_no_bias'\nN_SAMPLES: 2000\nN_PARAMETER_SETS: 100\nDELTA_T: 0.001\nN_TRAINING_SAMPLES_BY_PARAMETER_SET: 200\nN_SUBRUNS: 20\nGENERATOR_APPROACH: 'lan'\n</code></pre> <p>You can use the following command to generate training data:</p> <pre><code>generate --config-path &lt;path/to/config.yaml&gt; \\\n         --output &lt;output/directory&gt;\n</code></pre> <p>(Please see the basic Readme for a more complete explanation)</p>"},{"location":"tutorials/pymc_to_hssm/#network-training","title":"Network Training\u00b6","text":"<p>Once we have our training data, we can again use a single command to train the network. We only need to specify a few network features in a <code>network_config.yaml</code> file and train</p> <pre><code>NETWORK_TYPE: \"lan\" \nCPU_BATCH_SIZE: 1000\nN_EPOCHS: 20\nMODEL: \"ddm\"\nLAYER_SIZES: [[100, 100, 100, 1]]\nACTIVATIONS: [['tanh', 'tanh', 'tanh']]\nLABELS_LOWER_BOUND: np.log(1e-7)\n</code></pre> <p>Now we simply call:</p> <pre><code>torchtrain --config-path &lt;path/to/network_config.yaml&gt; \\\n           --training-data-folder &lt;path/to/training-data&gt; \\\n           --dl-workers 3 \\\n           --network-path-base &lt;my_trained_network&gt;`\n</code></pre> <p>(Same thing, this is explained in the baseid Readme on the github page of the package)</p>"},{"location":"tutorials/pymc_to_hssm/#working-directly-with-onnx-networks","title":"Working Directly with <code>.onnx</code> networks\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#utilities","title":"Utilities\u00b6","text":"<p>The below utility turns a network which may be defined for single datapoints into a batchable version of itself (so that call can work in parallel over the trial-dimension). We assume below that all our networks are already batchable, but keep this utility here for your convenience.</p>"},{"location":"tutorials/pymc_to_hssm/#network-path","title":"Network Path\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#load-and-time-network","title":"Load and time Network\u00b6","text":"<p>We will work natively with with neural networks that are encoded as <code>.onnx</code> files. In this part , but here we first focus on loading an <code>.onnx</code> file into an <code>onnx runtime</code>, just to give an idea what <code>.onnx</code> is about.</p> <p>Below we load a network, and time the forward pass, which should translate to the evaluation time we can expect for a single likelihood. We will illustrate this approach a bit later.</p>"},{"location":"tutorials/pymc_to_hssm/#hssm-low-level-interface","title":"HSSM: Low-level Interface\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#example-1-construct-your-own-random-variable","title":"Example (1): Construct your own random variable\u00b6","text":"<p>Under the hood, a lot of the heavy lifting done by HSSM boils down to the creating of valid Random Variables that are compatible with PyMC specifications.</p> <p>In broad strokes: In probabilist programming, a minimal specification for a random variable that let's us sample all the quantities of interest for a standard Bayesian Analysis are:</p> <ul> <li>A likelihood function</li> <li>A valid simulator</li> </ul> <p>If we have both of these, we can do everything. We can sample from the posterior, from the prior predictive and from the posterior predictive,which will cover essentially 99.9% of our needs.</p> <p>Below, we use HSSM low level functionality to construct such a random variable in three simple steps. Once constructed we use it in a PyMC model natively!</p> <p>Note:</p> <p>Truth be told, we can run MCMC without ever supplying a valid simulator and other tutorials showcase that scenario. However in such cases you are on your own once you want to sample from posterior predictives and/or prior predictives.</p>"},{"location":"tutorials/pymc_to_hssm/#model","title":"Model\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#data","title":"Data\u00b6","text":"<p>We simulate a simple dataset consisting fo $750$ trials from the Race 3 model as depicted above. HSSM has a <code>simulate_data()</code> function which comes in handy for this purpose. We can access all named models which are implemented in the ssm-simulators package.</p>"},{"location":"tutorials/pymc_to_hssm/#utilities","title":"Utilities\u00b6","text":"<p>While we can use fully custom simulators in the context of this tutorial, we are sticking to simulators implemented in ssm-simulators ssm-simulators has model configuration dictionaries for every model that is implemented in the package.</p> <p>Let's take a peak below, since we have to pass various elements of this dictionary to downstream utility functions.</p>"},{"location":"tutorials/pymc_to_hssm/#setting-the-network-path","title":"Setting the Network Path\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#networks-to-random-variables","title":"Networks to Random Variables\u00b6","text":"<p>Remember that the <code>.onnx</code> file is just a representation for a neural network... this Network is actually a LAN, and hence, by definition differentiable with respect to it's inputs (the parameters of the model).</p> <p>HSSM will help you construct costom PyMC random variables starting from <code>.onnx</code> files. We will see two routes to work with custom distributions in PYMC and HSSM, however more routes exist, and are in the making. Here is a list:</p> <ol> <li>Any valid Python function (we will see this later): The <code>blackbox</code> regime</li> <li><code>.onnx</code> function with Network signature: <code>approx_differentiable</code></li> <li>JAX function with Network signature: <code>appox_differentiable</code></li> <li>PyTensor function / JAX function with valid likelihood signature: <code>analytical</code></li> </ol> <p>We will ow focus on route 2., directly through PyMC, using HSSM only as a help in constructing random variabels. We will then showcase how the HSSM high-level (main) interface still allows us to work with option 2. very conveniently.</p> <p>Last, we are going to showcase option 1. via the HSSM high-level interface.</p>"},{"location":"tutorials/pymc_to_hssm/#constructing-the-random-variable","title":"Constructing the Random Variable\u00b6","text":"<p>To construct out PyMC random variables we make use of two utilities HSSM provides.</p> <ol> <li><code>make_likelihood_callable()</code> allows us to pass the network path, as well as a few configuration arguments, and will construct a valid callable likelihood function (duh..) for us</li> <li><code>make_distribution()</code> attaches a simulator (if it is part of the <code>ssms-simulators</code> package we can pass a string here) and assembles a valid PyMC distribution for us</li> </ol>"},{"location":"tutorials/pymc_to_hssm/#first-pymc-model","title":"First PyMC model\u00b6","text":"<p>We could (and have) spend a whole workshop, or many, to introduce the basics of PyMC itself. It is a rich library, connects to a deep eco-system around probabilistic programming and allows us to construct generative models around our core SSM, in ways that allows us to test a plethora of hypothesis about a given dataset.</p> <p>We will, however, not get distracted too much by the intricacies of PyMC, and instead just use this occasion to showcase that in general all doors are now open.</p> <p>As a very basic principle, consider the construction of a PyMC model as:</p> <ol> <li>Setting data (Data Block)</li> <li>Defining priors over parameters (Priors Block)</li> <li>Defining any kind of transformation on the parameters (Deterministics Block)</li> <li>Defining our observation model that observes our data and ingests trial level transformed parameters (Likelihood Block)</li> </ol>"},{"location":"tutorials/pymc_to_hssm/#string-representation-of-model","title":"String Representation of Model\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#graph-representation-of-model","title":"Graph Representation of Model\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#sample","title":"Sample\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#example-2-slightly-more-complicate-pymc-model","title":"Example (2): Slightly more complicate PyMC model\u00b6","text":"<p>Once we are in the PyMC-universe directly, we can build models very flexibly. This includes</p> <ol> <li>Introducing hierarchies</li> <li>Complicated non-linear transformations on parameters</li> <li>Time series approaches as processes over parameters (more on that by Krishn later)</li> <li>and much much more!</li> </ol> <p>Obviously this allows for things to go wrong, but you can have great fun along the way :)</p> <p>Below we will build a model that is easy enough in principle, but specifically hard (impossible) to construct via the HSSM user interface directly. But more on that a bit later!</p>"},{"location":"tutorials/pymc_to_hssm/#simulate-data","title":"Simulate Data\u00b6","text":"<p>Notice in the data simulation below: We are applying a regresion to all of <code>v0</code>, <code>v1</code>, <code>v2</code> and <code>a</code> and de facto the <code>beta</code> parameter is shared amongst all four.</p>"},{"location":"tutorials/pymc_to_hssm/#second-pymc-model","title":"Second PyMC Model\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#graph-representation-of-model","title":"Graph Representation of Model\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#sample","title":"Sample\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#hssm-medium-level-interface","title":"HSSM: Medium-level Interface\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#load-module","title":"Load Module\u00b6","text":"<p>HSSM Provides a (growing) set of pre-assembled random variables out of the box, which we can load directly from the <code>likelihoods</code> submodule. For these pre-assmebled random variables, the workflow is even simpler!</p>"},{"location":"tutorials/pymc_to_hssm/#simulate-data","title":"Simulate Data\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#pymc-model","title":"PyMC Model\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#graph-representation-of-model","title":"Graph Representation of Model\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#sample","title":"Sample\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#hssm-high-level-interface","title":"HSSM: High-Level Interface\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#re-defining-the-network-path","title":"(Re) Defining the Network Path\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#data-and-underlying-model","title":"Data and Underlying Model\u00b6","text":"<p>This example focuses on a <code>Race</code> model with three choice options. See the picture below for an illustration:</p>"},{"location":"tutorials/pymc_to_hssm/#simulate-data","title":"Simulate Data\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#hssm-class","title":"HSSM class\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#graph-representation-of-the-model","title":"Graph Representation of the Model\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#sample","title":"Sample\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#dont-like-any-of-the-samplers-in-pymc","title":"Don't like any of the samplers in PyMC?\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#help-yourself","title":"Help yourself!\u00b6","text":"<p>If you don't like the samplers HSSM or even PyMC can provide out of the box, we have a remedy. You can just compile the likelihood, and use it as a simple function downstream, however you like!</p>"},{"location":"tutorials/pymc_to_hssm/#compiling-the-likelihood","title":"Compiling the likelihood\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#timing-the-compiled-likelihood","title":"Timing the compiled likelihood\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#define-a-simple-wrapper-and-sample-via-third-party-library","title":"Define a simple wrapper and sample via third party library\u00b6","text":""},{"location":"tutorials/pymc_to_hssm/#the-end","title":"The End:\u00b6","text":"<p>We have seen how to use introduce custom models into HSSM workflows, how to do posterior sampling with HSSM models via third party libraries and how to use the low-level functionality of HSSM to construct random variables that can be used via PyMC!</p> <p>This tutorial ends here, but please feel invited to take occasion and check out more advanced topics from the HSSM docs!</p>"},{"location":"tutorials/pymc_to_hssm/#pointers-to-more-advanced-topics","title":"Pointers to more advanced Topics\u00b6","text":"<p>We are scratching only the surface of what cann be done with HSSM, let alone the broader eco-system supporting simulation based inference (SBI).</p> <p>Check out our simulator package, ssm-simulators as well as our our little neural network library for training LANs, lanfactory.</p> <p>Exciting work is being done (more on this in the next tutorial) on connecting to other packages in the wider eco-system, such as BayesFlow as well as the sbi package.</p> <p>Here is a taste of advanced topics with links to corresponding tutorials:</p> <ul> <li>Variational Inference with HSSM</li> <li>Build PyMC models with HSSM random variables</li> <li>Connect compiled models to third party MCMC libraries</li> <li>Construct custom models from simulators and contributed likelihoods</li> <li>Using link functions to transform parameters</li> </ul> <p>you will find this and a lot more information in the official documentation</p>"},{"location":"tutorials/rlssm_rlwm_model/","title":"Tutorial for hierarchical Bayesian inference for Reinforcement Learning - Sequential Sampling Models.","text":"In\u00a0[1]: Copied! <pre># Import necessary libraries\nimport numpy as np\nimport arviz as az\nimport pickle\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom functools import partial\nfrom scipy.stats import spearmanr\n\n# Import HSSM and simulator package\nimport hssm\nfrom hssm.utils import decorate_atomic_simulator\nfrom hssm.likelihoods.rldm import make_rldm_logp_op\nfrom hssm.distribution_utils.dist import make_hssm_rv\nfrom ssms.basic_simulators.simulator import simulator\n</pre> # Import necessary libraries import numpy as np import arviz as az import pickle import matplotlib.pyplot as plt import matplotlib.ticker as ticker from functools import partial from scipy.stats import spearmanr  # Import HSSM and simulator package import hssm from hssm.utils import decorate_atomic_simulator from hssm.likelihoods.rldm import make_rldm_logp_op from hssm.distribution_utils.dist import make_hssm_rv from ssms.basic_simulators.simulator import simulator In\u00a0[2]: Copied! <pre># Set the style for the plots\nplt.style.use('seaborn-v0_8-dark-palette')\n</pre> # Set the style for the plots plt.style.use('seaborn-v0_8-dark-palette') In\u00a0[3]: Copied! <pre># load pickle file\n\nwith open(\"../../tests/fixtures/rlwm_data.pickle\", \"rb\") as f:\n    datafile = pickle.load(f) \n</pre> # load pickle file  with open(\"../../tests/fixtures/rlwm_data.pickle\", \"rb\") as f:     datafile = pickle.load(f)  <pre>/var/folders/x0/fmky6rx50nlb2gv47r2586k80000gn/T/ipykernel_27284/1621081835.py:4: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n  datafile = pickle.load(f)\n</pre> In\u00a0[4]: Copied! <pre>dataset = datafile[\"sim_data\"]\ndataset\n</pre> dataset = datafile[\"sim_data\"] dataset Out[4]: participant_id block_id stimulus_id response feedback rt acc stim_ctr set_size unidim_mask new_block_start correct_response 0 0.0 0.0 1.0 1.0 1.0 0.905485 1.0 1.0 5.0 0.0 1.0 1.0 1 0.0 0.0 4.0 0.0 0.0 0.687666 0.0 1.0 5.0 0.0 0.0 2.0 2 0.0 0.0 2.0 1.0 0.0 0.708184 0.0 1.0 5.0 0.0 0.0 0.0 3 0.0 0.0 1.0 1.0 1.0 0.514008 1.0 2.0 5.0 0.0 0.0 1.0 4 0.0 0.0 4.0 2.0 1.0 0.836765 1.0 2.0 5.0 0.0 0.0 2.0 ... ... ... ... ... ... ... ... ... ... ... ... ... 31315 86.0 9.0 0.0 2.0 1.0 0.830150 1.0 9.0 3.0 0.0 0.0 2.0 31316 86.0 9.0 0.0 2.0 1.0 0.827908 1.0 10.0 3.0 0.0 0.0 2.0 31317 86.0 9.0 2.0 2.0 1.0 0.611198 1.0 9.0 3.0 0.0 0.0 2.0 31318 86.0 9.0 1.0 2.0 1.0 1.029457 1.0 10.0 3.0 0.0 0.0 2.0 31319 86.0 9.0 2.0 2.0 1.0 0.473282 1.0 10.0 3.0 0.0 0.0 2.0 <p>31320 rows \u00d7 12 columns</p> In\u00a0[5]: Copied! <pre>dataset = dataset[dataset['participant_id'] &lt; 20]\nn_participants = 20\nn_trials = 360\n\ndataset\n</pre> dataset = dataset[dataset['participant_id'] &lt; 20] n_participants = 20 n_trials = 360  dataset Out[5]: participant_id block_id stimulus_id response feedback rt acc stim_ctr set_size unidim_mask new_block_start correct_response 0 0.0 0.0 1.0 1.0 1.0 0.905485 1.0 1.0 5.0 0.0 1.0 1.0 1 0.0 0.0 4.0 0.0 0.0 0.687666 0.0 1.0 5.0 0.0 0.0 2.0 2 0.0 0.0 2.0 1.0 0.0 0.708184 0.0 1.0 5.0 0.0 0.0 0.0 3 0.0 0.0 1.0 1.0 1.0 0.514008 1.0 2.0 5.0 0.0 0.0 1.0 4 0.0 0.0 4.0 2.0 1.0 0.836765 1.0 2.0 5.0 0.0 0.0 2.0 ... ... ... ... ... ... ... ... ... ... ... ... ... 7195 19.0 9.0 0.0 2.0 1.0 0.524149 1.0 9.0 3.0 0.0 0.0 2.0 7196 19.0 9.0 2.0 0.0 1.0 0.391745 1.0 9.0 3.0 0.0 0.0 0.0 7197 19.0 9.0 0.0 2.0 1.0 0.559724 1.0 10.0 3.0 0.0 0.0 2.0 7198 19.0 9.0 2.0 0.0 1.0 0.495000 1.0 10.0 3.0 0.0 0.0 0.0 7199 19.0 9.0 1.0 0.0 1.0 0.323350 1.0 10.0 3.0 0.0 0.0 0.0 <p>7200 rows \u00d7 12 columns</p> In\u00a0[6]: Copied! <pre># # Load synthetic RLSSM dataset containing both behavioral data and ground truth parameters\n# savefile = np.load(\"../../tests/fixtures/rldm_data.npy\", allow_pickle=True).item()\n# dataset = savefile['data']\n\n# # Rename trial column to match HSSM conventions\n# dataset.rename(columns={'trial': 'trial_id'}, inplace=True)\n\n# # Examine the dataset structure\n# dataset.head()\n\n# # Validate data structure and extract dataset configuration \n# dataset, n_participants, n_trials = hssm.check_data_for_rl(dataset)\n\n# print(f\"Number of participants: {n_participants}\")\n# print(f\"Number of trials: {n_trials}\")\n</pre> # # Load synthetic RLSSM dataset containing both behavioral data and ground truth parameters # savefile = np.load(\"../../tests/fixtures/rldm_data.npy\", allow_pickle=True).item() # dataset = savefile['data']  # # Rename trial column to match HSSM conventions # dataset.rename(columns={'trial': 'trial_id'}, inplace=True)  # # Examine the dataset structure # dataset.head()  # # Validate data structure and extract dataset configuration  # dataset, n_participants, n_trials = hssm.check_data_for_rl(dataset)  # print(f\"Number of participants: {n_participants}\") # print(f\"Number of trials: {n_trials}\") In\u00a0[7]: Copied! <pre># Define parameters for the RLSSM model (RL + decision model parameters)\nlist_params = ['a', 'z', 'theta', 'alpha', 'phi', 'rho', 'gamma', 'epsilon', 'C', 'eta']\n\n# Create a dummy simulator for generating synthetic data (used for posterior predictives)\n# This bypasses the need for a full RLSSM simulator implementation\ndef create_dummy_simulator():\n    \"\"\"Create a dummy simulator function for RLSSM model.\"\"\"\n    def sim_wrapper(simulator_fun, theta, model, n_samples, random_state, **kwargs):\n        # Generate random RT and choice data as placeholders\n        sim_rt = np.random.uniform(0.2, 0.6, n_samples)\n        sim_ch = np.random.randint(0, 3, n_samples)\n        \n        return np.column_stack([sim_rt, sim_ch])\n\n    # Wrap the simulator function with required metadata\n    wrapped_simulator = partial(sim_wrapper, simulator_fun=simulator, model=\"custom\", n_samples=1)\n\n    # Decorate the simulator to make it compatible with HSSM\n    return decorate_atomic_simulator(model_name=\"custom\", choices=[0, 1], obs_dim=2)(wrapped_simulator)\n\n# Create the simulator and RandomVariable\ndecorated_simulator = create_dummy_simulator()\n\n# Create a PyTensor RandomVariable using `make_hssm_rv` for use in the PyMC model\nCustomRV = make_hssm_rv(\n    simulator_fun=decorated_simulator, list_params=list_params\n)\n</pre> # Define parameters for the RLSSM model (RL + decision model parameters) list_params = ['a', 'z', 'theta', 'alpha', 'phi', 'rho', 'gamma', 'epsilon', 'C', 'eta']  # Create a dummy simulator for generating synthetic data (used for posterior predictives) # This bypasses the need for a full RLSSM simulator implementation def create_dummy_simulator():     \"\"\"Create a dummy simulator function for RLSSM model.\"\"\"     def sim_wrapper(simulator_fun, theta, model, n_samples, random_state, **kwargs):         # Generate random RT and choice data as placeholders         sim_rt = np.random.uniform(0.2, 0.6, n_samples)         sim_ch = np.random.randint(0, 3, n_samples)                  return np.column_stack([sim_rt, sim_ch])      # Wrap the simulator function with required metadata     wrapped_simulator = partial(sim_wrapper, simulator_fun=simulator, model=\"custom\", n_samples=1)      # Decorate the simulator to make it compatible with HSSM     return decorate_atomic_simulator(model_name=\"custom\", choices=[0, 1], obs_dim=2)(wrapped_simulator)  # Create the simulator and RandomVariable decorated_simulator = create_dummy_simulator()  # Create a PyTensor RandomVariable using `make_hssm_rv` for use in the PyMC model CustomRV = make_hssm_rv(     simulator_fun=decorated_simulator, list_params=list_params ) In\u00a0[8]: Copied! <pre># Create a Pytensor Op for the likelihood function.\n# The `make_rldm_logp_op` function is a utility that wraps the base JAX likelihood function into a HSSM/PyMC-compatible callable.\n\nlogp_jax_op = make_rldm_logp_op(\n    n_participants=n_participants,\n    n_trials=n_trials,\n    n_params=10\n)\n</pre> # Create a Pytensor Op for the likelihood function. # The `make_rldm_logp_op` function is a utility that wraps the base JAX likelihood function into a HSSM/PyMC-compatible callable.  logp_jax_op = make_rldm_logp_op(     n_participants=n_participants,     n_trials=n_trials,     n_params=10 ) In\u00a0[9]: Copied! <pre># Test the likelihood function\n\ndef extract_data_columns(dataset):\n    \"\"\"Extract required data columns from dataset.\"\"\"\n    return {\n        'rt': dataset[\"rt\"].values,\n        'response': dataset[\"response\"].values,\n        'participant_id': dataset[\"participant_id\"].values,\n        'set_size': dataset[\"set_size\"].values,\n        'stimulus_id': dataset[\"stimulus_id\"].values,\n        'feedback': dataset[\"feedback\"].values,\n        'new_block_start': dataset[\"new_block_start\"].values,\n        'unidim_mask': dataset[\"unidim_mask\"].values,\n    }\n\ndef create_test_parameters(n_trials):\n    \"\"\"Create dummy parameters for testing the likelihood function.\"\"\"\n    return {\n        'a': np.ones(n_trials) * 1.5,\n        'z': np.ones(n_trials) * 0.4,\n        'theta': np.ones(n_trials) * 0.1,\n        'alpha': np.ones(n_trials) * 0.003,\n        'phi': np.ones(n_trials) * 0.3,\n        'rho': np.ones(n_trials) * 0.7,\n        'gamma': np.ones(n_trials) * 0.3,\n        'epsilon': np.ones(n_trials) * 0.2,\n        'C': np.ones(n_trials) * 3.5,\n        'eta': np.ones(n_trials) * 0.8,\n}\n\n# Extract data and create test parameters\ndata_columns = extract_data_columns(dataset)\nnum_subj = len(np.unique(data_columns['participant_id']))\nn_trials_total = num_subj * 360\n\ntest_params = create_test_parameters(n_trials_total)\n\n# Evaluate the likelihood function\ntest_logp_out = logp_jax_op(\n    np.column_stack((data_columns['rt'], data_columns['response'])),\n    test_params['a'],\n    test_params['z'],\n    test_params['theta'],\n    test_params['alpha'],\n    test_params['phi'],\n    test_params['rho'],\n    test_params['gamma'],\n    test_params['epsilon'],\n    test_params['C'],\n    test_params['eta'],\n    data_columns['participant_id'],\n    data_columns['set_size'],\n    data_columns['stimulus_id'],\n    data_columns['feedback'],\n    data_columns['new_block_start'],\n    data_columns['unidim_mask'],\n)\n\nLL = test_logp_out.eval()\nprint(f\"Log likelihood: {np.sum(LL):.4f}\")\n</pre> # Test the likelihood function  def extract_data_columns(dataset):     \"\"\"Extract required data columns from dataset.\"\"\"     return {         'rt': dataset[\"rt\"].values,         'response': dataset[\"response\"].values,         'participant_id': dataset[\"participant_id\"].values,         'set_size': dataset[\"set_size\"].values,         'stimulus_id': dataset[\"stimulus_id\"].values,         'feedback': dataset[\"feedback\"].values,         'new_block_start': dataset[\"new_block_start\"].values,         'unidim_mask': dataset[\"unidim_mask\"].values,     }  def create_test_parameters(n_trials):     \"\"\"Create dummy parameters for testing the likelihood function.\"\"\"     return {         'a': np.ones(n_trials) * 1.5,         'z': np.ones(n_trials) * 0.4,         'theta': np.ones(n_trials) * 0.1,         'alpha': np.ones(n_trials) * 0.003,         'phi': np.ones(n_trials) * 0.3,         'rho': np.ones(n_trials) * 0.7,         'gamma': np.ones(n_trials) * 0.3,         'epsilon': np.ones(n_trials) * 0.2,         'C': np.ones(n_trials) * 3.5,         'eta': np.ones(n_trials) * 0.8, }  # Extract data and create test parameters data_columns = extract_data_columns(dataset) num_subj = len(np.unique(data_columns['participant_id'])) n_trials_total = num_subj * 360  test_params = create_test_parameters(n_trials_total)  # Evaluate the likelihood function test_logp_out = logp_jax_op(     np.column_stack((data_columns['rt'], data_columns['response'])),     test_params['a'],     test_params['z'],     test_params['theta'],     test_params['alpha'],     test_params['phi'],     test_params['rho'],     test_params['gamma'],     test_params['epsilon'],     test_params['C'],     test_params['eta'],     data_columns['participant_id'],     data_columns['set_size'],     data_columns['stimulus_id'],     data_columns['feedback'],     data_columns['new_block_start'],     data_columns['unidim_mask'], )  LL = test_logp_out.eval() print(f\"Log likelihood: {np.sum(LL):.4f}\") <pre>Log likelihood: -95428.7823\n</pre> In\u00a0[10]: Copied! <pre># Step 3: Define the model config\n\n# Configure the HSSM model \nmodel_config = hssm.ModelConfig(\n    response=[\"rt\", \"response\"],        # Dependent variables (RT and choice)\n    list_params=                        # List of model parameters\n        ['a', 'z', 'theta', 'alpha', 'phi', 'rho', 'gamma', 'epsilon', 'C', 'eta'],   \n    choices=[0, 1, 2],                     # Possible choice options\n    default_priors={},                  # Use custom priors (defined below)\n    bounds=dict(                        # Parameter bounds for optimization\n        a=(0.1, 6),\n        z=(0.0, 0.9),                   \n        theta=(0.0, 1.2),                \n        alpha=(-7.0, -4.6),\n        phi=(0.0, 1.0),\n        rho=(0.0, 1.0),\n        gamma=(0.0, 1.0),\n        epsilon=(0.0, 0.5),\n        C=(1.0, 5.0),\n        eta=(0.1, 2.0)\n        ),\n    rv=CustomRV,                        # Custom RandomVariable that we created earlier\n    extra_fields=[                      # Additional data columns to be passed to the likelihood function as extra_fields\n        \"participant_id\", \n        \"set_size\",\n        \"stimulus_id\",\n        \"feedback\",\n        \"new_block_start\",\n        \"unidim_mask\",\n        ],  \n    backend=\"jax\"                       # Use JAX for computation\n)\n</pre> # Step 3: Define the model config  # Configure the HSSM model  model_config = hssm.ModelConfig(     response=[\"rt\", \"response\"],        # Dependent variables (RT and choice)     list_params=                        # List of model parameters         ['a', 'z', 'theta', 'alpha', 'phi', 'rho', 'gamma', 'epsilon', 'C', 'eta'],        choices=[0, 1, 2],                     # Possible choice options     default_priors={},                  # Use custom priors (defined below)     bounds=dict(                        # Parameter bounds for optimization         a=(0.1, 6),         z=(0.0, 0.9),                            theta=(0.0, 1.2),                         alpha=(-7.0, -4.6),         phi=(0.0, 1.0),         rho=(0.0, 1.0),         gamma=(0.0, 1.0),         epsilon=(0.0, 0.5),         C=(1.0, 5.0),         eta=(0.1, 2.0)         ),     rv=CustomRV,                        # Custom RandomVariable that we created earlier     extra_fields=[                      # Additional data columns to be passed to the likelihood function as extra_fields         \"participant_id\",          \"set_size\",         \"stimulus_id\",         \"feedback\",         \"new_block_start\",         \"unidim_mask\",         ],       backend=\"jax\"                       # Use JAX for computation ) In\u00a0[38]: Copied! <pre># Create a hierarchical HSSM model with custom likelihood function\nhssm_model = hssm.HSSM(\n    data=dataset,                        # Input dataset\n    model_config=model_config,           # Model configuration\n    p_outlier=0,                         # No outlier modeling\n    lapse=None,                          # No lapse rate modeling\n    loglik=logp_jax_op,                  # Custom RLDM likelihood function\n    loglik_kind=\"approx_differentiable\", # Use approximate gradients\n    noncentered=False,                    # Use non-centered parameterization\n    process_initvals=False,              # Skip initial value processing in HSSM\n    include=[\n        # Define hierarchical priors: group-level intercepts + subject-level random effects\n        hssm.Param(\"a\", \n                formula=\"a ~ 1 + (1|participant_id)\", \n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.1, upper=6, mu=0.5, initval=0.5)}),\n        hssm.Param(\"z\", \n                formula=\"z ~ 1 + (1|participant_id)\", \n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.0, upper=0.9, mu=0.2, initval=0.2)}),\n        hssm.Param(\"theta\", \n                formula=\"theta ~ 1 + (1|participant_id)\", \n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.00, upper=1.2, mu=0.3, initval=0.3)}),\n        hssm.Param(\"alpha\", \n                formula=\"alpha ~ 1 + (1|participant_id)\",\n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=-7.0, upper=-4.6, mu=-5.5, initval=-5.5)}),\n        hssm.Param(\"phi\", \n                formula=\"phi ~ 1 + (1|participant_id)\",\n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.0, upper=1.0, mu=0.2, initval=0.2)}),\n        hssm.Param(\"rho\", \n                formula=\"rho ~ 1 + (1|participant_id)\", \n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.0, upper=1.0, mu=0.5, initval=0.5)}),\n        hssm.Param(\"gamma\", \n                formula=\"gamma ~ 1 + (1|participant_id)\",\n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.0, upper=1.0, mu=0.1, initval=0.1)}),\n        hssm.Param(\"epsilon\", \n                formula=\"epsilon ~ 1 + (1|participant_id)\",\n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.0, upper=0.1, mu=0.02, initval=0.02)}),\n        hssm.Param(\"C\", \n                formula=\"C ~ 1 + (1|participant_id)\",\n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=1.0, upper=5.0, mu=2.5, initval=2.5)}),\n        hssm.Param(\"eta\", \n                formula=\"eta ~ 1 + (1|participant_id)\",\n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.1, upper=2.0, mu=1.0, initval=1.0)}),\n    ]\n)\n</pre> # Create a hierarchical HSSM model with custom likelihood function hssm_model = hssm.HSSM(     data=dataset,                        # Input dataset     model_config=model_config,           # Model configuration     p_outlier=0,                         # No outlier modeling     lapse=None,                          # No lapse rate modeling     loglik=logp_jax_op,                  # Custom RLDM likelihood function     loglik_kind=\"approx_differentiable\", # Use approximate gradients     noncentered=False,                    # Use non-centered parameterization     process_initvals=False,              # Skip initial value processing in HSSM     include=[         # Define hierarchical priors: group-level intercepts + subject-level random effects         hssm.Param(\"a\",                  formula=\"a ~ 1 + (1|participant_id)\",                  prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.1, upper=6, mu=0.5, initval=0.5)}),         hssm.Param(\"z\",                  formula=\"z ~ 1 + (1|participant_id)\",                  prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.0, upper=0.9, mu=0.2, initval=0.2)}),         hssm.Param(\"theta\",                  formula=\"theta ~ 1 + (1|participant_id)\",                  prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.00, upper=1.2, mu=0.3, initval=0.3)}),         hssm.Param(\"alpha\",                  formula=\"alpha ~ 1 + (1|participant_id)\",                 prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=-7.0, upper=-4.6, mu=-5.5, initval=-5.5)}),         hssm.Param(\"phi\",                  formula=\"phi ~ 1 + (1|participant_id)\",                 prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.0, upper=1.0, mu=0.2, initval=0.2)}),         hssm.Param(\"rho\",                  formula=\"rho ~ 1 + (1|participant_id)\",                  prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.0, upper=1.0, mu=0.5, initval=0.5)}),         hssm.Param(\"gamma\",                  formula=\"gamma ~ 1 + (1|participant_id)\",                 prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.0, upper=1.0, mu=0.1, initval=0.1)}),         hssm.Param(\"epsilon\",                  formula=\"epsilon ~ 1 + (1|participant_id)\",                 prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.0, upper=0.1, mu=0.02, initval=0.02)}),         hssm.Param(\"C\",                  formula=\"C ~ 1 + (1|participant_id)\",                 prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=1.0, upper=5.0, mu=2.5, initval=2.5)}),         hssm.Param(\"eta\",                  formula=\"eta ~ 1 + (1|participant_id)\",                 prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.1, upper=2.0, mu=1.0, initval=1.0)}),     ] ) <pre>No common intercept. Bounds for parameter a is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter z is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter theta is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter alpha is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter phi is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter rho is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter gamma is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter epsilon is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter C is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter eta is not applied due to a current limitation of Bambi. This will change in the future.\nModel initialized successfully.\n</pre> In\u00a0[39]: Copied! <pre>hssm_model.initvals\n</pre> hssm_model.initvals Out[39]: <pre>{'a_Intercept': array(0.5),\n 'a_1|participant_id_mu': array(0.),\n 'a_1|participant_id_sigma': array(0.27082359),\n 'a_1|participant_id': array([-0.00609534,  0.00711659,  0.00100053,  0.00640197,  0.00584437,\n        -0.00809237, -0.00865371,  0.00365292, -0.00515377,  0.00916522,\n         0.00616267, -0.00328771, -0.00040869,  0.00885343, -0.00068692,\n        -0.00089339,  0.00168498,  0.00336153,  0.00758564, -0.00723777]),\n 'z_Intercept': array(0.2),\n 'z_1|participant_id_mu': array(0.),\n 'z_1|participant_id_sigma': array(0.27082359),\n 'z_1|participant_id': array([ 9.58198030e-03,  6.10256940e-03,  7.99741223e-03, -7.81630115e-06,\n        -7.73580000e-03, -2.81746918e-03, -4.84167924e-03,  1.67717098e-03,\n        -5.05552022e-03, -6.47253531e-04,  5.98031608e-03,  9.68790241e-03,\n         6.34669606e-03,  9.75689571e-03,  1.94374891e-03, -8.73824395e-03,\n         8.78173951e-03,  1.14631257e-03, -4.99753049e-03,  9.78373573e-04]),\n 'theta_Intercept': array(0.3),\n 'theta_1|participant_id_mu': array(0.),\n 'theta_1|participant_id_sigma': array(0.27082359),\n 'theta_1|participant_id': array([-0.00079413, -0.00696315, -0.00213439, -0.00124098, -0.0084155 ,\n         0.00461855, -0.00995485,  0.00473805,  0.00690034,  0.0081845 ,\n         0.00743397, -0.00055535, -0.00624872,  0.00356975,  0.00891706,\n         0.00859457,  0.00676896,  0.00734498, -0.00110408,  0.00358596]),\n 'alpha_Intercept': array(-5.5),\n 'alpha_1|participant_id_mu': array(0.),\n 'alpha_1|participant_id_sigma': array(0.27082359),\n 'alpha_1|participant_id': array([-0.00264203, -0.00802546,  0.00452972,  0.00814388,  0.00362155,\n        -0.00271722,  0.00366344, -0.00260807, -0.00156272, -0.00749841,\n         0.007465  , -0.00743459,  0.00506185,  0.00030553,  0.00046728,\n        -0.0092764 ,  0.00060364,  0.003263  ,  0.00267338, -0.00293383]),\n 'phi_Intercept': array(0.2),\n 'phi_1|participant_id_mu': array(0.),\n 'phi_1|participant_id_sigma': array(0.27082359),\n 'phi_1|participant_id': array([ 0.00303146,  0.00233754,  0.00918529,  0.00266861,  0.00772109,\n        -0.00651544,  0.00997751, -0.00169334, -0.00692044, -0.00011506,\n         0.00558664, -0.00074128, -0.00930365,  0.00545811, -0.00667883,\n        -0.00955408,  0.00559593,  0.0011453 ,  0.00880885, -0.00591214]),\n 'rho_Intercept': array(0.5),\n 'rho_1|participant_id_mu': array(0.),\n 'rho_1|participant_id_sigma': array(0.27082359),\n 'rho_1|participant_id': array([ 0.00501482,  0.00076743, -0.00894825, -0.00479669,  0.00118744,\n         0.00232758,  0.00938712,  0.00957815, -0.0032994 , -0.00318853,\n         0.00889771, -0.00646618, -0.0061075 , -0.00642642, -0.00309801,\n         0.0003528 ,  0.00015528,  0.00392793, -0.00506898,  0.00092777]),\n 'gamma_Intercept': array(0.1),\n 'gamma_1|participant_id_mu': array(0.),\n 'gamma_1|participant_id_sigma': array(0.27082359),\n 'gamma_1|participant_id': array([ 0.00882582,  0.00642957,  0.00412469,  0.00926778,  0.0028835 ,\n        -0.00156566, -0.00833628, -0.00724594,  0.00527448,  0.00685873,\n         0.00778395,  0.0043443 , -0.00571756, -0.00127361, -0.00766991,\n         0.0076729 , -0.00696409, -0.0011734 , -0.0075441 ,  0.00762667]),\n 'epsilon_Intercept': array(0.02),\n 'epsilon_1|participant_id_mu': array(0.),\n 'epsilon_1|participant_id_sigma': array(0.27082359),\n 'epsilon_1|participant_id': array([ 0.00890905, -0.00209825,  0.00780422, -0.0056049 , -0.00533383,\n         0.00198637, -0.0089169 ,  0.00621421, -0.00758207,  0.00660938,\n        -0.00312083,  0.00354209,  0.00491573,  0.0055547 ,  0.00115861,\n        -0.00737579, -0.0008666 , -0.00514344, -0.00682944, -0.00926962]),\n 'C_Intercept': array(2.5),\n 'C_1|participant_id_mu': array(0.),\n 'C_1|participant_id_sigma': array(0.27082359),\n 'C_1|participant_id': array([ 0.00322316,  0.00293094,  0.00599011, -0.00563202,  0.00597329,\n        -0.00591479,  0.00153422, -0.00476739, -0.00518996,  0.00515959,\n        -0.00234183,  0.00327491,  0.00291407, -0.00069963,  0.0059097 ,\n         0.0010478 , -0.00664892,  0.00787738, -0.00431384, -0.00244423]),\n 'eta_Intercept': array(1.),\n 'eta_1|participant_id_mu': array(0.),\n 'eta_1|participant_id_sigma': array(0.27082359),\n 'eta_1|participant_id': array([ 0.00394991,  0.00568354,  0.00020684, -0.00077258, -0.00214244,\n        -0.00765622,  0.00514104,  0.00806712,  0.00051069,  0.00741062,\n        -0.00060719,  0.00795197, -0.00657538, -0.00183795,  0.00592442,\n         0.00938788, -0.00436759, -0.00250615,  0.00745219,  0.00585218])}</pre> In\u00a0[40]: Copied! <pre># Run MCMC sampling using NUTS sampler with JAX backend\n# Note: Using small number of samples for demonstration (increase for real analysis)\nidata_mcmc = hssm_model.sample(\n    sampler='nuts_numpyro',  # JAX-based NUTS sampler for efficiency\n    chains=1,                # Number of parallel chains\n    draws=500,                # Number of posterior samples\n    tune=500,                 # Number of tuning/warmup samples\n)\n</pre> # Run MCMC sampling using NUTS sampler with JAX backend # Note: Using small number of samples for demonstration (increase for real analysis) idata_mcmc = hssm_model.sample(     sampler='nuts_numpyro',  # JAX-based NUTS sampler for efficiency     chains=1,                # Number of parallel chains     draws=500,                # Number of posterior samples     tune=500,                 # Number of tuning/warmup samples ) <pre>Using default initvals. \n\n</pre> <pre>sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [07:39&lt;00:00,  2.18it/s, 23 steps of size 3.72e-02. acc. prob=0.78]\nThere were 500 divergences after tuning. Increase `target_accept` or reparameterize.\nOnly one chain was sampled, this makes it impossible to run some convergence checks\n/Users/krishnbera/Documents/revert_rldm/HSSM/.venv/lib/python3.12/site-packages/pymc/pytensorf.py:958: FutureWarning: compile_pymc was renamed to compile. Old name will be removed in a future release of PyMC\n  warnings.warn(\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:04&lt;00:00, 106.94it/s]\n</pre> In\u00a0[41]: Copied! <pre>idata_mcmc\n</pre> idata_mcmc Out[41]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 925kB\nDimensions:                         (chain: 1, draw: 500,\n                                     participant_id__factor_dim: 20,\n                                     a_1|participant_id__factor_dim: 20)\nCoordinates:\n  * chain                           (chain) int64 8B 0\n  * draw                            (draw) int64 4kB 0 1 2 3 ... 496 497 498 499\n  * a_1|participant_id__factor_dim  (a_1|participant_id__factor_dim) &lt;U4 320B ...\n  * participant_id__factor_dim      (participant_id__factor_dim) &lt;U4 320B '0....\nData variables: (12/40)\n    epsilon_1|participant_id        (chain, draw, participant_id__factor_dim) float64 80kB ...\n    epsilon_1|participant_id_sigma  (chain, draw) float64 4kB 0.1045 ... 0.06631\n    theta_1|participant_id          (chain, draw, participant_id__factor_dim) float64 80kB ...\n    eta_1|participant_id            (chain, draw, participant_id__factor_dim) float64 80kB ...\n    rho_1|participant_id            (chain, draw, participant_id__factor_dim) float64 80kB ...\n    theta_1|participant_id_mu       (chain, draw) float64 4kB -0.08804 ... -0...\n    ...                              ...\n    z_1|participant_id              (chain, draw, participant_id__factor_dim) float64 80kB ...\n    z_1|participant_id_sigma        (chain, draw) float64 4kB 0.2547 ... 0.2363\n    rho_1|participant_id_sigma      (chain, draw) float64 4kB 0.1926 ... 0.1611\n    eta_1|participant_id_mu         (chain, draw) float64 4kB 0.003616 ... -0...\n    gamma_1|participant_id          (chain, draw, participant_id__factor_dim) float64 80kB ...\n    alpha_1|participant_id          (chain, draw, participant_id__factor_dim) float64 80kB ...\nAttributes:\n    created_at:                  2025-07-16T20:49:22.630978+00:00\n    arviz_version:               0.22.0\n    inference_library:           numpyro\n    inference_library_version:   0.18.0\n    sampling_time:               460.059804\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 500</li><li>participant_id__factor_dim: 20</li><li>a_1|participant_id__factor_dim: 20</li></ul></li><li>Coordinates: (4)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>a_1|participant_id__factor_dim(a_1|participant_id__factor_dim)&lt;U4'0.0' '1.0' '2.0' ... '18.0' '19.0'<pre>array(['0.0', '1.0', '2.0', '3.0', '4.0', '5.0', '6.0', '7.0', '8.0', '9.0',\n       '10.0', '11.0', '12.0', '13.0', '14.0', '15.0', '16.0', '17.0', '18.0',\n       '19.0'], dtype='&lt;U4')</pre></li><li>participant_id__factor_dim(participant_id__factor_dim)&lt;U4'0.0' '1.0' '2.0' ... '18.0' '19.0'<pre>array(['0.0', '1.0', '2.0', '3.0', '4.0', '5.0', '6.0', '7.0', '8.0', '9.0',\n       '10.0', '11.0', '12.0', '13.0', '14.0', '15.0', '16.0', '17.0', '18.0',\n       '19.0'], dtype='&lt;U4')</pre></li></ul></li><li>Data variables: (40)<ul><li>epsilon_1|participant_id(chain, draw, participant_id__factor_dim)float640.04388 0.05131 ... 0.01847 0.08358<pre>array([[[ 0.04387956,  0.05130743,  0.1105224 , ...,  0.06089406,\n          0.00122794, -0.00124098],\n        [ 0.05102171,  0.04488086,  0.1029292 , ...,  0.06357102,\n          0.00441038,  0.00214604],\n        [ 0.06954365,  0.05429405,  0.09244542, ...,  0.07727429,\n         -0.00939144,  0.03821564],\n        ...,\n        [ 0.1187009 ,  0.17415199,  0.07270448, ...,  0.02042003,\n          0.02879614,  0.0868519 ],\n        [ 0.12000253,  0.17628945,  0.07103362, ...,  0.017397  ,\n          0.02711463,  0.08870727],\n        [ 0.12937074,  0.17331219,  0.05560118, ...,  0.02012467,\n          0.01846935,  0.08357568]]], shape=(1, 500, 20))</pre></li><li>epsilon_1|participant_id_sigma(chain, draw)float640.1045 0.1059 ... 0.06633 0.06631<pre>array([[0.10447051, 0.10591883, 0.11267977, 0.11005284, 0.11755471,\n        0.11773461, 0.11123034, 0.11522475, 0.11621029, 0.11674283,\n        0.11121363, 0.11217215, 0.11169599, 0.11430662, 0.11537388,\n        0.09529234, 0.09668726, 0.09646707, 0.09563313, 0.09671641,\n        0.09722989, 0.09731922, 0.09672063, 0.10036908, 0.10072448,\n        0.08566437, 0.09698101, 0.09265245, 0.09514988, 0.09478155,\n        0.09435905, 0.09432317, 0.09474927, 0.09474927, 0.09346267,\n        0.09487039, 0.09291028, 0.08961385, 0.0875491 , 0.08914148,\n        0.08952295, 0.0877404 , 0.0879038 , 0.08528488, 0.08962411,\n        0.09004497, 0.09135747, 0.09331481, 0.09351473, 0.09261821,\n        0.09271086, 0.09347484, 0.0931304 , 0.09284766, 0.09008965,\n        0.0914292 , 0.09204764, 0.09204764, 0.09204764, 0.09055454,\n        0.0905408 , 0.09081647, 0.08755743, 0.08711726, 0.08711726,\n        0.08711108, 0.08711108, 0.08680837, 0.08589969, 0.08589969,\n        0.08630529, 0.0868031 , 0.0868031 , 0.0868031 , 0.08696568,\n        0.08653938, 0.08646022, 0.08544856, 0.08594213, 0.08665923,\n        0.08665923, 0.08665923, 0.08665923, 0.09484929, 0.0950319 ,\n        0.0952614 , 0.09477855, 0.0966316 , 0.09541922, 0.09565493,\n        0.09525767, 0.09189372, 0.09374613, 0.09612357, 0.0954623 ,\n        0.09537346, 0.09454305, 0.09512573, 0.09722058, 0.09722058,\n...\n        0.05983744, 0.05989159, 0.06003806, 0.06156031, 0.05921592,\n        0.05823508, 0.05823508, 0.05823508, 0.05695968, 0.05695968,\n        0.05695968, 0.0568621 , 0.05670329, 0.05629857, 0.05584151,\n        0.05642375, 0.05643927, 0.05522205, 0.05492741, 0.05401853,\n        0.05361938, 0.05342343, 0.05630057, 0.05964914, 0.05967502,\n        0.05956054, 0.05952378, 0.05965037, 0.05992683, 0.05957264,\n        0.06135081, 0.06163504, 0.06247684, 0.0624647 , 0.0624647 ,\n        0.06243348, 0.06243348, 0.06243348, 0.06248231, 0.0625098 ,\n        0.06227727, 0.06219627, 0.06195888, 0.0619855 , 0.0619855 ,\n        0.0619855 , 0.06211888, 0.06211888, 0.06217987, 0.06254361,\n        0.06273941, 0.06336871, 0.06195807, 0.06232334, 0.06146026,\n        0.06151621, 0.06055119, 0.06054936, 0.06063567, 0.06195652,\n        0.06153658, 0.06135805, 0.06163979, 0.0600581 , 0.05938894,\n        0.05862732, 0.0588515 , 0.05903203, 0.05889007, 0.05905297,\n        0.05906812, 0.05901742, 0.05860159, 0.05853893, 0.05894858,\n        0.06074606, 0.0597229 , 0.05741114, 0.05751925, 0.05651514,\n        0.06010003, 0.06119528, 0.06070013, 0.06186282, 0.06316446,\n        0.06375653, 0.06353   , 0.06435765, 0.06462831, 0.06619492,\n        0.06442603, 0.06472617, 0.06696507, 0.06639012, 0.06477418,\n        0.06452683, 0.06542885, 0.06555345, 0.06632627, 0.06631019]])</pre></li><li>theta_1|participant_id(chain, draw, participant_id__factor_dim)float64-0.08208 -0.01541 ... -0.114<pre>array([[[-0.08207647, -0.01540882, -0.06179026, ..., -0.11615549,\n         -0.10871544, -0.07259136],\n        [-0.08446646, -0.01506638, -0.06151483, ..., -0.11072114,\n         -0.10982148, -0.0749429 ],\n        [-0.08978116, -0.0050856 , -0.06291648, ..., -0.12304587,\n         -0.11258955, -0.06021489],\n        ...,\n        [-0.09205404,  0.00800404, -0.07735991, ..., -0.12247285,\n         -0.0782173 , -0.12526099],\n        [-0.09128062,  0.00737874, -0.07901369, ..., -0.1250991 ,\n         -0.0804653 , -0.12270003],\n        [-0.09463993, -0.00094152, -0.07538872, ..., -0.12451734,\n         -0.07473622, -0.11400908]]], shape=(1, 500, 20))</pre></li><li>eta_1|participant_id(chain, draw, participant_id__factor_dim)float64-0.008303 -0.08464 ... -0.06685<pre>array([[[-0.00830317, -0.08464037, -0.21614629, ...,  0.03839864,\n         -0.13576783, -0.14608827],\n        [-0.00379612, -0.0714562 , -0.21546151, ...,  0.03787923,\n         -0.13726166, -0.14226117],\n        [ 0.01338591, -0.06903285, -0.20445279, ...,  0.02857247,\n         -0.14317592, -0.14754859],\n        ...,\n        [ 0.02373042, -0.12339414, -0.17375446, ..., -0.01282426,\n         -0.27032003, -0.06939633],\n        [ 0.02544071, -0.12269907, -0.17442596, ..., -0.01178245,\n         -0.27025533, -0.06838184],\n        [ 0.0168939 , -0.12515055, -0.17142496, ..., -0.01722703,\n         -0.27086456, -0.06684876]]], shape=(1, 500, 20))</pre></li><li>rho_1|participant_id(chain, draw, participant_id__factor_dim)float64-0.04883 -0.1025 ... -0.004804<pre>array([[[-0.04882968, -0.10252377, -0.33142287, ...,  0.10105216,\n         -0.1480869 ,  0.03921897],\n        [-0.05210732, -0.10530069, -0.31891562, ...,  0.10374417,\n         -0.15010795,  0.04562958],\n        [-0.08750924, -0.12697343, -0.34641954, ...,  0.17161117,\n         -0.14480141,  0.04508325],\n        ...,\n        [-0.07449871, -0.05394621, -0.32804118, ...,  0.21646162,\n         -0.17761913,  0.00598731],\n        [-0.07795931, -0.05146624, -0.32786345, ...,  0.21754411,\n         -0.18292616,  0.00356589],\n        [-0.0817308 , -0.05349837, -0.32183443, ...,  0.20439244,\n         -0.1826268 , -0.00480419]]], shape=(1, 500, 20))</pre></li><li>theta_1|participant_id_mu(chain, draw)float64-0.08804 -0.09094 ... -0.05844<pre>array([[-0.08803757, -0.09094445, -0.09650796, -0.09718169, -0.07897126,\n        -0.07739379, -0.07840107, -0.0794176 , -0.08420434, -0.08873311,\n        -0.07888701, -0.08033426, -0.06148001, -0.05303532, -0.0552691 ,\n        -0.03361826, -0.03403331, -0.03364072, -0.03407667, -0.03059572,\n        -0.03473645, -0.03448434, -0.03222633, -0.03717513, -0.03584065,\n        -0.04289012, -0.06905137, -0.0477274 , -0.05906671, -0.05845077,\n        -0.0571223 , -0.05656502, -0.05600827, -0.05600827, -0.05844814,\n        -0.04458195, -0.04491668, -0.03814283, -0.05344474, -0.0554195 ,\n        -0.05568828, -0.05639818, -0.05534348, -0.05600147, -0.05268217,\n        -0.05265876, -0.05047638, -0.05112651, -0.05236813, -0.0595    ,\n        -0.05995717, -0.06293463, -0.06377597, -0.06774415, -0.06225687,\n        -0.06886379, -0.07038195, -0.07038195, -0.07038195, -0.07679886,\n        -0.07674706, -0.07652336, -0.05261989, -0.05361808, -0.05361808,\n        -0.05420185, -0.05420185, -0.05484143, -0.05411475, -0.05411475,\n        -0.05484373, -0.05560453, -0.05560453, -0.05560453, -0.05482904,\n        -0.05541439, -0.05612846, -0.05570889, -0.05619316, -0.05476805,\n        -0.05476805, -0.05476805, -0.05476805, -0.0673789 , -0.06636785,\n        -0.0660842 , -0.06678898, -0.06256451, -0.0564546 , -0.05558844,\n        -0.05706499, -0.04923898, -0.04976097, -0.05645683, -0.05387697,\n        -0.0513308 , -0.05441178, -0.05510929, -0.05752733, -0.05752733,\n...\n        -0.06860433, -0.06916898, -0.07003363, -0.07042607, -0.07987347,\n        -0.08344939, -0.08344939, -0.08344939, -0.08236613, -0.08236613,\n        -0.08236613, -0.08146382, -0.08081225, -0.08118985, -0.08111105,\n        -0.08118226, -0.07939626, -0.08061898, -0.08368351, -0.08519997,\n        -0.08492657, -0.08513446, -0.08291438, -0.07234387, -0.07420968,\n        -0.07311031, -0.07419255, -0.07342473, -0.0764474 , -0.07504518,\n        -0.07826506, -0.0780519 , -0.08007598, -0.08081239, -0.08081239,\n        -0.08012971, -0.08012971, -0.08012971, -0.08035856, -0.08177413,\n        -0.08222272, -0.08159266, -0.0821304 , -0.0822277 , -0.0822277 ,\n        -0.0822277 , -0.0833196 , -0.0833196 , -0.08288782, -0.08094246,\n        -0.08161043, -0.08104757, -0.08381452, -0.08386595, -0.08656091,\n        -0.08771965, -0.08854967, -0.09018966, -0.09061047, -0.08791728,\n        -0.08464652, -0.08331032, -0.0823667 , -0.07853156, -0.07617139,\n        -0.07106117, -0.07091172, -0.07285402, -0.07293658, -0.07258745,\n        -0.07344965, -0.07428468, -0.07390691, -0.07371964, -0.07935719,\n        -0.07727852, -0.08044994, -0.07714791, -0.07581022, -0.07153429,\n        -0.06103569, -0.06035391, -0.05568472, -0.08275206, -0.08099495,\n        -0.0794131 , -0.07855913, -0.07946399, -0.08007785, -0.08216541,\n        -0.07794642, -0.07761029, -0.0748063 , -0.0772595 , -0.0719179 ,\n        -0.07180327, -0.06458671, -0.0636362 , -0.06335482, -0.05844023]])</pre></li><li>z_Intercept(chain, draw)float640.3939 0.3936 ... 0.4135 0.4147<pre>array([[0.3938959 , 0.39361746, 0.39611701, 0.39488443, 0.39746067,\n        0.39851954, 0.39676427, 0.39770845, 0.39547142, 0.39398358,\n        0.40126299, 0.40137788, 0.39945272, 0.39746309, 0.39579983,\n        0.39660448, 0.39848718, 0.39843816, 0.39845889, 0.39814822,\n        0.40106768, 0.40158656, 0.39964786, 0.39534354, 0.39440413,\n        0.3951123 , 0.39605404, 0.40312642, 0.40128222, 0.40203992,\n        0.401836  , 0.40254746, 0.40276315, 0.40276315, 0.39865663,\n        0.40037915, 0.40031579, 0.40154936, 0.40287514, 0.40373162,\n        0.40285469, 0.40130572, 0.40127485, 0.4049988 , 0.39712833,\n        0.39677598, 0.39766091, 0.39811891, 0.39844593, 0.39984096,\n        0.40015832, 0.39980521, 0.40033409, 0.4007631 , 0.39690792,\n        0.39519757, 0.39467179, 0.39467179, 0.39467179, 0.39809935,\n        0.39803031, 0.39763343, 0.40146802, 0.40175052, 0.40175052,\n        0.40190982, 0.40190982, 0.40122859, 0.39958855, 0.39958855,\n        0.39974617, 0.40016143, 0.40016143, 0.40016143, 0.4006244 ,\n        0.40054538, 0.4001373 , 0.40152762, 0.4012934 , 0.39951463,\n        0.39951463, 0.39951463, 0.39951463, 0.39807975, 0.39875515,\n        0.39812503, 0.401203  , 0.40098207, 0.40582018, 0.40534367,\n        0.40412543, 0.40346864, 0.40638851, 0.40909882, 0.40834903,\n        0.40858242, 0.40844782, 0.40856391, 0.40748985, 0.40748985,\n...\n        0.41202382, 0.412285  , 0.41157507, 0.41017167, 0.41084215,\n        0.41376841, 0.41376841, 0.41376841, 0.4149538 , 0.4149538 ,\n        0.4149538 , 0.41491971, 0.41446058, 0.41493265, 0.41418187,\n        0.4149883 , 0.41545621, 0.41381812, 0.41163782, 0.41248095,\n        0.41340984, 0.41328871, 0.41141525, 0.40770726, 0.4078025 ,\n        0.40803825, 0.40715167, 0.40693591, 0.40688387, 0.40844523,\n        0.40873479, 0.4086744 , 0.41117693, 0.41106013, 0.41106013,\n        0.41160505, 0.41160505, 0.41160505, 0.41151301, 0.41083242,\n        0.4112332 , 0.41080395, 0.41042373, 0.40944683, 0.40944683,\n        0.40944683, 0.40928582, 0.40928582, 0.40999342, 0.4078296 ,\n        0.40797052, 0.40892971, 0.40825633, 0.40941963, 0.40776751,\n        0.40673446, 0.40779305, 0.40701464, 0.40643569, 0.40926443,\n        0.40943492, 0.40945977, 0.41045877, 0.41076346, 0.41191029,\n        0.4147176 , 0.41404228, 0.41405462, 0.41369502, 0.41380415,\n        0.41375545, 0.41249732, 0.41304955, 0.41355478, 0.41488378,\n        0.41426763, 0.41233972, 0.41486765, 0.41552291, 0.4137843 ,\n        0.41601255, 0.41744773, 0.41878958, 0.41448404, 0.41577064,\n        0.41474457, 0.41558624, 0.41636678, 0.4178749 , 0.41694383,\n        0.41440574, 0.41431387, 0.41964992, 0.41764951, 0.4147282 ,\n        0.41293541, 0.41473474, 0.41456015, 0.41346387, 0.41470507]])</pre></li><li>theta_1|participant_id_sigma(chain, draw)float640.2253 0.2254 ... 0.1439 0.1455<pre>array([[0.22531676, 0.22544397, 0.22025817, 0.21959657, 0.21909043,\n        0.21547558, 0.21752332, 0.21818094, 0.21703976, 0.21685678,\n        0.21778642, 0.21841931, 0.21765845, 0.21707999, 0.21490941,\n        0.21770949, 0.21785565, 0.21760269, 0.21798172, 0.2152397 ,\n        0.21411652, 0.21437118, 0.21355054, 0.20954329, 0.21068109,\n        0.20785076, 0.2094984 , 0.21060685, 0.20717872, 0.20650608,\n        0.20631275, 0.20633733, 0.20638422, 0.20638422, 0.20638204,\n        0.20888064, 0.20838449, 0.20491825, 0.20343237, 0.20252286,\n        0.20160995, 0.20109948, 0.20111827, 0.20052787, 0.19307689,\n        0.19307323, 0.19590665, 0.19578826, 0.19569798, 0.19573637,\n        0.19590228, 0.19667694, 0.19702793, 0.19680893, 0.19586051,\n        0.19498158, 0.19466754, 0.19466754, 0.19466754, 0.19594709,\n        0.19608438, 0.1962348 , 0.19367702, 0.19397136, 0.19397136,\n        0.19425077, 0.19425077, 0.19418406, 0.19437825, 0.19437825,\n        0.19384051, 0.19392984, 0.19392984, 0.19392984, 0.19417068,\n        0.19440342, 0.1944405 , 0.19472205, 0.19486869, 0.19593708,\n        0.19593708, 0.19593708, 0.19593708, 0.19774375, 0.1971265 ,\n        0.1970083 , 0.19731242, 0.19801956, 0.19934376, 0.19922147,\n        0.20066002, 0.2053863 , 0.20709201, 0.20584825, 0.20661311,\n        0.20674714, 0.20676506, 0.20574497, 0.20527767, 0.20527767,\n...\n        0.15383288, 0.15370913, 0.15364849, 0.15313614, 0.15366154,\n        0.15393271, 0.15393271, 0.15393271, 0.15324351, 0.15324351,\n        0.15324351, 0.15326157, 0.15341927, 0.15369295, 0.15360592,\n        0.15321377, 0.1532467 , 0.15339103, 0.15228065, 0.15224045,\n        0.15260991, 0.15247281, 0.15255594, 0.15142543, 0.15150981,\n        0.15177483, 0.15178061, 0.15174466, 0.1519088 , 0.15167441,\n        0.15128102, 0.15122941, 0.15103304, 0.15103073, 0.15103073,\n        0.15123908, 0.15123908, 0.15123908, 0.15129357, 0.15091312,\n        0.15086622, 0.15091362, 0.15108781, 0.15106796, 0.15106796,\n        0.15106796, 0.15094747, 0.15094747, 0.15053307, 0.15081866,\n        0.15096414, 0.1505625 , 0.15000933, 0.15003692, 0.14835293,\n        0.14827389, 0.14792531, 0.14760694, 0.14767175, 0.146729  ,\n        0.14692367, 0.14682426, 0.14690103, 0.14676193, 0.14703948,\n        0.14561438, 0.14568712, 0.14539588, 0.14548265, 0.14569309,\n        0.14584261, 0.14593365, 0.14568331, 0.14560558, 0.14488805,\n        0.14565506, 0.14506972, 0.14319327, 0.14344391, 0.14486648,\n        0.15061416, 0.15167325, 0.15150634, 0.1537654 , 0.15090197,\n        0.15102412, 0.15061699, 0.15009044, 0.14950295, 0.14895367,\n        0.14564637, 0.14635266, 0.1451298 , 0.14608784, 0.14560694,\n        0.14465149, 0.14413593, 0.14404162, 0.1439198 , 0.14547679]])</pre></li><li>alpha_1|participant_id_sigma(chain, draw)float640.2363 0.2386 ... 0.218 0.2186<pre>array([[0.23632504, 0.23858101, 0.23449654, 0.23390806, 0.2316522 ,\n        0.23509234, 0.23747826, 0.23836692, 0.24034744, 0.2410209 ,\n        0.24106844, 0.24122683, 0.24079065, 0.23996121, 0.23994995,\n        0.23704508, 0.23619368, 0.23612827, 0.23585207, 0.23620811,\n        0.23641688, 0.23604779, 0.23647268, 0.23709679, 0.23730749,\n        0.23283237, 0.23334494, 0.23360109, 0.23106566, 0.23125308,\n        0.23126906, 0.23117658, 0.2313092 , 0.2313092 , 0.22963516,\n        0.22898914, 0.22946981, 0.22910443, 0.22990841, 0.23013188,\n        0.23002467, 0.23024626, 0.23032001, 0.22943439, 0.22586331,\n        0.2261537 , 0.22366261, 0.22376352, 0.22360151, 0.22419152,\n        0.22413378, 0.22411763, 0.22425005, 0.22436443, 0.22473272,\n        0.22362996, 0.22378248, 0.22378248, 0.22378248, 0.22394182,\n        0.22417559, 0.22459368, 0.22482549, 0.22489791, 0.22489791,\n        0.2248929 , 0.2248929 , 0.22494745, 0.22468397, 0.22468397,\n        0.22495165, 0.22492379, 0.22492379, 0.22492379, 0.22477281,\n        0.22486691, 0.22497776, 0.22457887, 0.22469081, 0.2258518 ,\n        0.2258518 , 0.2258518 , 0.2258518 , 0.22433521, 0.22427348,\n        0.22433244, 0.2240127 , 0.22399275, 0.2243201 , 0.22455567,\n        0.22365399, 0.22345511, 0.22409864, 0.22406162, 0.22446757,\n        0.22498734, 0.22479536, 0.22490451, 0.22439051, 0.22439051,\n...\n        0.20991489, 0.2099082 , 0.20986096, 0.21073426, 0.21104325,\n        0.21043692, 0.21043692, 0.21043692, 0.20970206, 0.20970206,\n        0.20970206, 0.20963913, 0.20963906, 0.2095879 , 0.20947714,\n        0.20928864, 0.20938572, 0.20982276, 0.20971084, 0.20971832,\n        0.21004796, 0.20995289, 0.20974634, 0.21478645, 0.21491028,\n        0.21508704, 0.21544554, 0.2155516 , 0.2151212 , 0.21460678,\n        0.21470564, 0.21475559, 0.2147152 , 0.21471837, 0.21471837,\n        0.21457533, 0.21457533, 0.21457533, 0.21464907, 0.21495096,\n        0.21505685, 0.21510244, 0.21502841, 0.21488329, 0.21488329,\n        0.21488329, 0.2150896 , 0.2150896 , 0.21524112, 0.2151705 ,\n        0.21526847, 0.21539222, 0.21516032, 0.2151262 , 0.21535822,\n        0.21519111, 0.21566732, 0.21568727, 0.21564238, 0.21633014,\n        0.21650007, 0.21686014, 0.21686015, 0.2167656 , 0.21717061,\n        0.21659405, 0.21655437, 0.21638953, 0.21645251, 0.21651781,\n        0.21649843, 0.21645201, 0.21653799, 0.21643829, 0.21725344,\n        0.21631252, 0.21607304, 0.21620695, 0.21600406, 0.21702515,\n        0.2154009 , 0.215074  , 0.21455548, 0.21531807, 0.21503172,\n        0.2149445 , 0.21380334, 0.21370436, 0.21360231, 0.21427151,\n        0.21634538, 0.21557942, 0.2182844 , 0.21840512, 0.2190049 ,\n        0.21851212, 0.21764992, 0.2177254 , 0.21803171, 0.21855478]])</pre></li><li>phi_1|participant_id_sigma(chain, draw)float640.1802 0.1792 ... 0.19 0.1974<pre>array([[0.18024907, 0.17919277, 0.16513972, 0.16412476, 0.15499416,\n        0.15265527, 0.14994772, 0.15835812, 0.15366233, 0.15453917,\n        0.14871705, 0.14788979, 0.15542977, 0.14384245, 0.13562997,\n        0.1209672 , 0.1221599 , 0.12227816, 0.12486815, 0.12250254,\n        0.11962455, 0.1207153 , 0.12491423, 0.12005345, 0.12216402,\n        0.11970317, 0.11853526, 0.12945043, 0.12719434, 0.12769432,\n        0.12787473, 0.1281384 , 0.12792858, 0.12792858, 0.13400965,\n        0.12722029, 0.12430371, 0.12338042, 0.12629868, 0.12493921,\n        0.12547306, 0.12288431, 0.12126318, 0.12440331, 0.12743487,\n        0.12714269, 0.11981067, 0.12272519, 0.12243526, 0.12498829,\n        0.12563601, 0.12641052, 0.12700438, 0.12542525, 0.12471908,\n        0.12196194, 0.12249448, 0.12249448, 0.12249448, 0.11546306,\n        0.11600768, 0.11640798, 0.11497435, 0.11383398, 0.11383398,\n        0.11376667, 0.11376667, 0.11348563, 0.11459627, 0.11459627,\n        0.11370944, 0.11427572, 0.11427572, 0.11427572, 0.11376704,\n        0.11389541, 0.11442328, 0.11448754, 0.11429823, 0.1113795 ,\n        0.1113795 , 0.1113795 , 0.1113795 , 0.11178405, 0.11355816,\n        0.11356556, 0.11120315, 0.1093257 , 0.10969485, 0.10930555,\n        0.10840797, 0.10850578, 0.11394896, 0.11747442, 0.11579726,\n        0.1140712 , 0.11451115, 0.11651132, 0.11401276, 0.11401276,\n...\n        0.11952662, 0.11958762, 0.1186841 , 0.10893402, 0.11984191,\n        0.11739466, 0.11739466, 0.11739466, 0.11658127, 0.11658127,\n        0.11658127, 0.11533177, 0.11548269, 0.11553817, 0.11602051,\n        0.11593555, 0.11590554, 0.11493843, 0.11650711, 0.11808802,\n        0.11837271, 0.11779739, 0.118258  , 0.11221674, 0.11261479,\n        0.11248259, 0.11176908, 0.11236925, 0.11081044, 0.11022006,\n        0.10779938, 0.10797455, 0.10753786, 0.10735011, 0.10735011,\n        0.10752513, 0.10752513, 0.10752513, 0.10782499, 0.10647174,\n        0.10711708, 0.10748197, 0.10723475, 0.10710816, 0.10710816,\n        0.10710816, 0.10732001, 0.10732001, 0.10719489, 0.10724376,\n        0.10849348, 0.10879934, 0.10834527, 0.10804175, 0.10828469,\n        0.10752535, 0.10622175, 0.10750716, 0.10738028, 0.10656158,\n        0.10633527, 0.10674506, 0.10685263, 0.10864474, 0.11369713,\n        0.11138159, 0.11345826, 0.11258738, 0.11179377, 0.11140462,\n        0.11236504, 0.11193523, 0.11241063, 0.11239658, 0.11789577,\n        0.11723895, 0.11863526, 0.12248745, 0.12429704, 0.12481607,\n        0.14440384, 0.1408586 , 0.14215311, 0.14713124, 0.16592471,\n        0.16451857, 0.16133864, 0.16265298, 0.1639352 , 0.16377753,\n        0.16917766, 0.17027231, 0.18733173, 0.19115611, 0.1932247 ,\n        0.19046442, 0.18896937, 0.18994097, 0.1899521 , 0.19739229]])</pre></li><li>epsilon_Intercept(chain, draw)float640.03263 0.03217 ... 0.05341 0.0531<pre>array([[0.03262626, 0.03216646, 0.03323739, 0.03315453, 0.0334253 ,\n        0.03276489, 0.03409517, 0.03455748, 0.03582919, 0.03584876,\n        0.03802635, 0.03785308, 0.03802709, 0.03804712, 0.03871839,\n        0.037729  , 0.03744365, 0.03736969, 0.03721267, 0.03716627,\n        0.03737816, 0.03733871, 0.03715299, 0.03557317, 0.03586445,\n        0.03844183, 0.03856096, 0.03761599, 0.03646986, 0.03658149,\n        0.0365722 , 0.03640656, 0.03634766, 0.03634766, 0.03602647,\n        0.03543708, 0.03522113, 0.03526745, 0.03413011, 0.03433227,\n        0.03425843, 0.03475375, 0.03488695, 0.03505941, 0.03821397,\n        0.03830105, 0.03763173, 0.03780812, 0.03777993, 0.03775587,\n        0.03780332, 0.03776704, 0.03776751, 0.03792839, 0.03547563,\n        0.03491614, 0.03473725, 0.03473725, 0.03473725, 0.03440916,\n        0.03451821, 0.03467986, 0.03488656, 0.03490257, 0.03490257,\n        0.03493997, 0.03493997, 0.03492517, 0.03494627, 0.03494627,\n        0.03482126, 0.03494451, 0.03494451, 0.03494451, 0.03502798,\n        0.03498955, 0.03488879, 0.03511384, 0.03505379, 0.03524143,\n        0.03524143, 0.03524143, 0.03524143, 0.03436295, 0.03431462,\n        0.03429485, 0.03444929, 0.03442986, 0.03470694, 0.03504984,\n        0.0350042 , 0.03512099, 0.03564205, 0.03605358, 0.0359801 ,\n        0.03571499, 0.0356558 , 0.03579308, 0.03597358, 0.03597358,\n...\n        0.04124394, 0.0412197 , 0.04109731, 0.04079165, 0.03987212,\n        0.03987167, 0.03987167, 0.03987167, 0.03986783, 0.03986783,\n        0.03986783, 0.03985501, 0.03991167, 0.03997251, 0.04013101,\n        0.04001353, 0.04007929, 0.04027659, 0.04043783, 0.04033546,\n        0.04033989, 0.04028217, 0.04072444, 0.04253762, 0.04254005,\n        0.04301706, 0.042861  , 0.04304825, 0.0431064 , 0.04329521,\n        0.04328429, 0.04331987, 0.04311   , 0.04311785, 0.04311785,\n        0.04307099, 0.04307099, 0.04307099, 0.0429179 , 0.04257595,\n        0.04269821, 0.04261958, 0.04250862, 0.04249834, 0.04249834,\n        0.04249834, 0.04256013, 0.04256013, 0.04259519, 0.0424921 ,\n        0.04256957, 0.04273481, 0.04270767, 0.04266364, 0.04229406,\n        0.04218857, 0.04246224, 0.04239584, 0.0426681 , 0.04288859,\n        0.04284895, 0.04283998, 0.04298926, 0.04280201, 0.04354185,\n        0.04387528, 0.04379807, 0.04380976, 0.0441322 , 0.04406408,\n        0.04427019, 0.04454735, 0.04415232, 0.04420026, 0.04431011,\n        0.04429269, 0.04515446, 0.04536395, 0.04516362, 0.04464792,\n        0.04567885, 0.04555051, 0.0470817 , 0.0460867 , 0.04692792,\n        0.04663126, 0.04745678, 0.04739148, 0.04734961, 0.04705003,\n        0.0508077 , 0.05071338, 0.05161394, 0.05178002, 0.05159413,\n        0.05147817, 0.05310161, 0.05311546, 0.05340603, 0.05310449]])</pre></li><li>phi_1|participant_id_mu(chain, draw)float640.03043 0.02883 ... -0.04125<pre>array([[ 0.03043352,  0.02882914, -0.00690989, -0.01182103, -0.00232885,\n         0.02187222,  0.000698  ,  0.00636379,  0.02480303,  0.02544949,\n         0.00595263,  0.00157044, -0.00213425, -0.01445856, -0.01363449,\n         0.00176178,  0.00248694,  0.00043426, -0.00953066, -0.01416569,\n        -0.01871089, -0.01968897, -0.01651888, -0.01375312, -0.01382502,\n        -0.00345695, -0.02251964, -0.0038116 , -0.01131344, -0.00886945,\n        -0.01054852, -0.01122085, -0.01050256, -0.01050256, -0.01792364,\n        -0.02861512, -0.02616221, -0.02640277, -0.02370757, -0.02322512,\n        -0.01926701, -0.01575183, -0.01918694, -0.02601002, -0.02710128,\n        -0.02926148, -0.03580071, -0.0347595 , -0.03474036, -0.03308949,\n        -0.03374544, -0.03460522, -0.03345807, -0.03394281, -0.05792107,\n        -0.04336485, -0.03913926, -0.03913926, -0.03913926, -0.04451094,\n        -0.04596924, -0.04591047, -0.04506339, -0.04451597, -0.04451597,\n        -0.04355267, -0.04355267, -0.0442992 , -0.04482451, -0.04482451,\n        -0.04817969, -0.04807161, -0.04807161, -0.04807161, -0.04711282,\n        -0.04728902, -0.04697765, -0.04819595, -0.04934546, -0.04840573,\n        -0.04840573, -0.04840573, -0.04840573, -0.0530327 , -0.05314879,\n        -0.05393304, -0.0594335 , -0.06723036, -0.05947198, -0.05497796,\n        -0.04641761, -0.0391808 , -0.0292707 , -0.03897892, -0.03828262,\n        -0.03153419, -0.03242883, -0.03271274, -0.02866824, -0.02866824,\n...\n        -0.01605323, -0.0147057 , -0.01331252, -0.02422513, -0.04134843,\n        -0.03706642, -0.03706642, -0.03706642, -0.03410722, -0.03410722,\n        -0.03410722, -0.03469475, -0.03403529, -0.03498026, -0.03375976,\n        -0.03824958, -0.04010977, -0.03217573, -0.02211759, -0.02247869,\n        -0.0260301 , -0.02489268, -0.019514  , -0.05561399, -0.05188443,\n        -0.04943686, -0.04869912, -0.04964248, -0.04878617, -0.05577922,\n        -0.055632  , -0.0541408 , -0.04987367, -0.04785605, -0.04785605,\n        -0.0488189 , -0.0488189 , -0.0488189 , -0.04849502, -0.04859026,\n        -0.04883209, -0.04886919, -0.04885962, -0.04592804, -0.04592804,\n        -0.04592804, -0.04475695, -0.04475695, -0.04386858, -0.04090294,\n        -0.04101065, -0.04069996, -0.0402776 , -0.03916962, -0.04145598,\n        -0.04317898, -0.04742556, -0.04725028, -0.04400379, -0.04940384,\n        -0.05208136, -0.05114978, -0.05213341, -0.05592853, -0.05894406,\n        -0.06113112, -0.05745875, -0.06018134, -0.06025844, -0.06147313,\n        -0.06027365, -0.05648762, -0.05346808, -0.05161487, -0.04385015,\n        -0.04327918, -0.03679494, -0.0203664 , -0.02353769, -0.0225986 ,\n        -0.04107317, -0.04105874, -0.0503005 , -0.0622483 , -0.11263537,\n        -0.11391571, -0.08963749, -0.08850676, -0.09600411, -0.09664206,\n        -0.08986583, -0.08658899, -0.0619733 , -0.06302993, -0.05428474,\n        -0.05489869, -0.05070467, -0.05216163, -0.05362616, -0.04125484]])</pre></li><li>gamma_1|participant_id_mu(chain, draw)float640.08305 0.07637 ... 0.1182 0.1199<pre>array([[0.08304957, 0.07637162, 0.13956239, 0.13797534, 0.17700435,\n        0.1850379 , 0.19480482, 0.18350135, 0.16975442, 0.1666007 ,\n        0.16021187, 0.16436947, 0.14698833, 0.17363055, 0.16636907,\n        0.15127705, 0.14821732, 0.14824601, 0.14166164, 0.14301469,\n        0.15314734, 0.15720231, 0.15918872, 0.13902984, 0.13783153,\n        0.11810996, 0.13543676, 0.09407927, 0.08918094, 0.08672209,\n        0.08647795, 0.08624185, 0.08569896, 0.08569896, 0.08636687,\n        0.08253556, 0.08558285, 0.07930076, 0.12108354, 0.1254897 ,\n        0.13003973, 0.12976197, 0.13012199, 0.13010093, 0.05519958,\n        0.05675271, 0.07080866, 0.07442628, 0.07233847, 0.07059471,\n        0.06890184, 0.06907263, 0.06856332, 0.0671936 , 0.06515504,\n        0.04739179, 0.05072154, 0.05072154, 0.05072154, 0.0671607 ,\n        0.06703243, 0.0684331 , 0.11567423, 0.11638455, 0.11638455,\n        0.11561264, 0.11561264, 0.11658205, 0.11716854, 0.11716854,\n        0.11765169, 0.11598462, 0.11598462, 0.11598462, 0.11536951,\n        0.11544523, 0.11402797, 0.11346037, 0.11423176, 0.1237714 ,\n        0.1237714 , 0.1237714 , 0.1237714 , 0.12075339, 0.11885162,\n        0.11823772, 0.11799747, 0.12416158, 0.11033692, 0.10700954,\n        0.11108211, 0.1549904 , 0.16963523, 0.17762718, 0.17523157,\n        0.17084333, 0.17075867, 0.17114679, 0.16666316, 0.16666316,\n...\n        0.15733577, 0.15800544, 0.1582876 , 0.13475027, 0.13791083,\n        0.14872065, 0.14872065, 0.14872065, 0.14614634, 0.14614634,\n        0.14614634, 0.14570114, 0.14298237, 0.14396998, 0.14297297,\n        0.14409273, 0.14381645, 0.14270661, 0.13025857, 0.13366562,\n        0.13096509, 0.13074835, 0.12776448, 0.11487621, 0.11197652,\n        0.11207734, 0.10701131, 0.10695393, 0.1015524 , 0.09645655,\n        0.10016273, 0.10241544, 0.10516745, 0.10546864, 0.10546864,\n        0.10692555, 0.10692555, 0.10692555, 0.10878702, 0.11181972,\n        0.11059688, 0.11042095, 0.11231098, 0.11400716, 0.11400716,\n        0.11400716, 0.11268425, 0.11268425, 0.11226117, 0.11868756,\n        0.11928833, 0.12538812, 0.12750753, 0.12663083, 0.13166602,\n        0.12918882, 0.12891959, 0.13080589, 0.14108768, 0.14861479,\n        0.14589876, 0.14830834, 0.14517437, 0.14997692, 0.14269617,\n        0.13406646, 0.13475432, 0.13786509, 0.13895259, 0.13986671,\n        0.13985056, 0.14244584, 0.13748251, 0.13659204, 0.14285315,\n        0.14297038, 0.13678889, 0.13659339, 0.12943493, 0.14749945,\n        0.14649522, 0.13790009, 0.15954977, 0.1541356 , 0.1625957 ,\n        0.1603801 , 0.16262735, 0.16480223, 0.16599213, 0.1652605 ,\n        0.10600044, 0.11047888, 0.12432649, 0.11529259, 0.13067826,\n        0.125224  , 0.10887636, 0.11106107, 0.11824783, 0.11991606]])</pre></li><li>C_Intercept(chain, draw)float642.572 2.577 2.625 ... 2.857 2.904<pre>array([[2.57229985, 2.5768163 , 2.62494496, 2.61488762, 2.64865344,\n        2.65798826, 2.62841525, 2.67571539, 2.67839611, 2.66307047,\n        2.68430062, 2.66462779, 2.61049773, 2.62712171, 2.63413129,\n        2.68366124, 2.67386085, 2.67413448, 2.68177279, 2.69820573,\n        2.69574361, 2.69983809, 2.69914207, 2.67170254, 2.66205529,\n        2.57092281, 2.51776294, 2.5617253 , 2.5288427 , 2.54137866,\n        2.54284543, 2.54398406, 2.54049974, 2.54049974, 2.54255163,\n        2.55252728, 2.54398702, 2.56249528, 2.52632307, 2.52595851,\n        2.55438699, 2.56430062, 2.56338891, 2.56646102, 2.51910168,\n        2.51543125, 2.48893928, 2.49118752, 2.48979446, 2.4808811 ,\n        2.48379322, 2.48161882, 2.48680101, 2.48197271, 2.48993562,\n        2.46922539, 2.4616744 , 2.4616744 , 2.4616744 , 2.43442665,\n        2.43139208, 2.4334734 , 2.52067877, 2.51332537, 2.51332537,\n        2.5075454 , 2.5075454 , 2.50817453, 2.50978519, 2.50978519,\n        2.50332429, 2.49764358, 2.49764358, 2.49764358, 2.49772208,\n        2.49747496, 2.49383303, 2.50285651, 2.50502195, 2.51241493,\n        2.51241493, 2.51241493, 2.51241493, 2.52976856, 2.53372063,\n        2.53726078, 2.54167404, 2.5492985 , 2.53115495, 2.52327598,\n        2.50649897, 2.48211935, 2.51261713, 2.48999251, 2.49156817,\n        2.48223675, 2.4950203 , 2.5017896 , 2.49600589, 2.49600589,\n...\n        2.91081958, 2.91000657, 2.91126874, 2.90275906, 2.87087845,\n        2.8559995 , 2.8559995 , 2.8559995 , 2.84533263, 2.84533263,\n        2.84533263, 2.84846696, 2.84734885, 2.84628871, 2.84466525,\n        2.85488518, 2.84549237, 2.8405571 , 2.89237081, 2.8866482 ,\n        2.87643522, 2.88365619, 2.89877263, 2.88374795, 2.87750163,\n        2.87148404, 2.87199795, 2.88027925, 2.89450904, 2.88759044,\n        2.89495085, 2.89495159, 2.88889831, 2.89422281, 2.89422281,\n        2.89277898, 2.89277898, 2.89277898, 2.89958007, 2.91086396,\n        2.91328892, 2.91065885, 2.91391864, 2.91679135, 2.91679135,\n        2.91679135, 2.915498  , 2.915498  , 2.91165161, 2.90194264,\n        2.90022484, 2.91286476, 2.91795546, 2.90799119, 2.89418387,\n        2.90281109, 2.91457422, 2.9136211 , 2.91476908, 2.92416069,\n        2.88730358, 2.88636122, 2.87176095, 2.88065362, 2.87617639,\n        2.95781655, 2.95473288, 2.96667209, 2.96199742, 2.96167478,\n        2.96522361, 2.9693858 , 2.96631927, 2.96652023, 2.95378437,\n        2.93906084, 2.94049646, 2.97614342, 2.97360397, 2.94686525,\n        2.91271016, 2.91008378, 2.85613686, 2.85073415, 2.84551839,\n        2.85282163, 2.84898642, 2.85810278, 2.85956977, 2.85739331,\n        2.86881028, 2.83742327, 2.88313709, 2.86762528, 2.84899697,\n        2.837658  , 2.84089905, 2.83628694, 2.85683577, 2.90440288]])</pre></li><li>C_1|participant_id(chain, draw, participant_id__factor_dim)float640.179 0.09131 ... 0.01049 0.105<pre>array([[[ 1.78993592e-01,  9.13056906e-02, -1.64730601e-01, ...,\n         -4.08915600e-03,  2.17265495e-02,  1.75579870e-01],\n        [ 1.85213789e-01,  9.04790535e-02, -1.62771512e-01, ...,\n          1.16130360e-04,  2.66519707e-02,  1.92193863e-01],\n        [ 2.00244613e-01,  4.86159214e-02, -1.50847318e-01, ...,\n          5.02158691e-03,  3.86298490e-02,  1.72205493e-01],\n        ...,\n        [ 4.39296163e-02,  8.92605645e-02,  1.59395593e-01, ...,\n          7.71273557e-02,  1.08438816e-02,  1.02305711e-01],\n        [ 4.22502311e-02,  9.07168137e-02,  1.64655915e-01, ...,\n          7.40447296e-02,  1.10660543e-02,  1.06975616e-01],\n        [ 5.02997851e-02,  1.03013373e-01,  1.59104019e-01, ...,\n          7.07141010e-02,  1.04859347e-02,  1.05028259e-01]]],\n      shape=(1, 500, 20))</pre></li><li>a_1|participant_id_mu(chain, draw)float640.0406 0.03847 ... 0.04241 0.05612<pre>array([[ 0.04059531,  0.03846868,  0.0288261 ,  0.01458844,  0.00140258,\n         0.0258816 ,  0.02676589,  0.01765454, -0.00344185,  0.00172435,\n         0.02006696,  0.0261306 , -0.00063681,  0.03540149,  0.0189762 ,\n         0.03094254,  0.01588839,  0.01500226,  0.01350069,  0.00655974,\n         0.0229789 ,  0.02257259,  0.00920949,  0.02420225,  0.03179717,\n         0.03940171, -0.03633799,  0.00391467, -0.01518619, -0.02006788,\n        -0.0181294 , -0.01501535, -0.01103592, -0.01103592, -0.00332469,\n        -0.01056947, -0.01319441, -0.02003593,  0.0393817 ,  0.04250733,\n         0.04335107,  0.03442451,  0.03350605,  0.03174272, -0.02247351,\n        -0.02270643, -0.01965485, -0.0183841 , -0.01834794, -0.0090709 ,\n        -0.01614685, -0.01535174, -0.006964  , -0.00990089, -0.02145749,\n        -0.02147008, -0.02541081, -0.02541081, -0.02541081, -0.0128824 ,\n        -0.01139769, -0.01459155,  0.02935206,  0.02795219,  0.02795219,\n         0.02886979,  0.02886979,  0.0271415 ,  0.02380446,  0.02380446,\n         0.02431188,  0.02207893,  0.02207893,  0.02207893,  0.02454223,\n         0.02369886,  0.02278177,  0.01942268,  0.01895982,  0.01660168,\n         0.01660168,  0.01660168,  0.01660168, -0.00715275, -0.01215126,\n        -0.01245939, -0.01099839, -0.01645759, -0.00518376,  0.00186219,\n         0.02177816, -0.00141596, -0.020101  , -0.01571776, -0.01769572,\n        -0.0157126 , -0.01571268, -0.01506203, -0.01407316, -0.01407316,\n...\n         0.00302533,  0.00401099,  0.00356364,  0.01423817,  0.00137234,\n         0.02422491,  0.02422491,  0.02422491,  0.02194194,  0.02194194,\n         0.02194194,  0.0233256 ,  0.02444533,  0.02634958,  0.02806634,\n         0.0259454 ,  0.03038845,  0.02539417,  0.03298319,  0.03242755,\n         0.02949701,  0.02946839,  0.03370445,  0.06348225,  0.06336411,\n         0.06710723,  0.06436562,  0.06190592,  0.05930625,  0.05564429,\n         0.05360783,  0.05428219,  0.05447622,  0.05138367,  0.05138367,\n         0.05135863,  0.05135863,  0.05135863,  0.05346604,  0.05401558,\n         0.05380341,  0.05133193,  0.05143207,  0.05048865,  0.05048865,\n         0.05048865,  0.04925978,  0.04925978,  0.04933858,  0.04664386,\n         0.04468595,  0.046072  ,  0.04072918,  0.03812952,  0.03648805,\n         0.03802101,  0.03619328,  0.03446445,  0.03785988,  0.04632708,\n         0.04486945,  0.04324991,  0.04579258,  0.04307327,  0.04759945,\n         0.06988919,  0.06849313,  0.06365702,  0.06330932,  0.06703825,\n         0.06766085,  0.06750092,  0.06778391,  0.06617905,  0.07732922,\n         0.07658397,  0.09268116,  0.05567435,  0.06526782,  0.05607258,\n         0.0507753 ,  0.03757619,  0.04306533,  0.05048038,  0.08102695,\n         0.07263738,  0.06138115,  0.05615506,  0.05496124,  0.04982767,\n         0.0583163 ,  0.04658076,  0.04129728,  0.0408005 ,  0.04314266,\n         0.04396526,  0.04075265,  0.03902335,  0.04241107,  0.05611976]])</pre></li><li>eta_Intercept(chain, draw)float640.8099 0.8123 ... 0.8277 0.8251<pre>array([[0.80993123, 0.81231517, 0.81887564, 0.82012002, 0.82971172,\n        0.82204151, 0.82117948, 0.82097299, 0.82093755, 0.82312759,\n        0.82069071, 0.81947914, 0.8238088 , 0.8246376 , 0.82580164,\n        0.81868791, 0.82131545, 0.82113442, 0.82088559, 0.81974383,\n        0.83368886, 0.83390668, 0.83124133, 0.83098422, 0.83246878,\n        0.8303052 , 0.83873567, 0.83756192, 0.84316325, 0.84385491,\n        0.84397639, 0.8431753 , 0.84334511, 0.84334511, 0.84145113,\n        0.84279188, 0.84059168, 0.83607377, 0.83683814, 0.83475848,\n        0.83504926, 0.83438998, 0.83464093, 0.83625677, 0.83996503,\n        0.83956435, 0.8455067 , 0.84665728, 0.84683685, 0.84605651,\n        0.84660139, 0.84633999, 0.84683928, 0.84704305, 0.86536151,\n        0.86577797, 0.8665486 , 0.8665486 , 0.8665486 , 0.86597131,\n        0.86527084, 0.8652708 , 0.8664971 , 0.86675979, 0.86675979,\n        0.86710069, 0.86710069, 0.86753439, 0.86856762, 0.86856762,\n        0.86747891, 0.86717052, 0.86717052, 0.86717052, 0.86712786,\n        0.86676057, 0.86710105, 0.86654721, 0.86708936, 0.86807691,\n        0.86807691, 0.86807691, 0.86807691, 0.86558629, 0.86574824,\n        0.86570569, 0.86619142, 0.86309356, 0.86401323, 0.86483963,\n        0.86532308, 0.85426276, 0.85457523, 0.8546349 , 0.85511938,\n        0.85447591, 0.85400967, 0.85609699, 0.85510321, 0.85510321,\n...\n        0.8136117 , 0.81372418, 0.81441998, 0.81328955, 0.81255268,\n        0.82226208, 0.82226208, 0.82226208, 0.82356907, 0.82356907,\n        0.82356907, 0.8238129 , 0.82399786, 0.82312509, 0.82271716,\n        0.8232015 , 0.8223921 , 0.82136595, 0.81995769, 0.82034617,\n        0.81931855, 0.81912771, 0.81545453, 0.81591695, 0.81652232,\n        0.81497593, 0.81531334, 0.81560456, 0.81736256, 0.81632493,\n        0.81536675, 0.81568578, 0.81417871, 0.81399134, 0.81399134,\n        0.81390609, 0.81390609, 0.81390609, 0.81438206, 0.81469103,\n        0.81442698, 0.81430889, 0.81479891, 0.81428736, 0.81428736,\n        0.81428736, 0.81456607, 0.81456607, 0.81497677, 0.81437417,\n        0.81432487, 0.81524418, 0.81462825, 0.81404062, 0.81471764,\n        0.81546382, 0.81413083, 0.81396077, 0.81259058, 0.81146566,\n        0.81096423, 0.81089758, 0.81105215, 0.8109766 , 0.81010576,\n        0.81164045, 0.81144923, 0.81047691, 0.81034166, 0.81003233,\n        0.81013521, 0.81030011, 0.81137192, 0.81127566, 0.81027923,\n        0.81089702, 0.81281807, 0.8086608 , 0.80896269, 0.80683133,\n        0.81963907, 0.82261417, 0.82609749, 0.82123145, 0.81908439,\n        0.81825312, 0.81412446, 0.81389474, 0.81414324, 0.81379821,\n        0.82715   , 0.82690979, 0.82171698, 0.82009314, 0.82136924,\n        0.82425556, 0.82724683, 0.82752256, 0.82769395, 0.82506571]])</pre></li><li>phi_Intercept(chain, draw)float640.2669 0.2688 ... 0.3019 0.2996<pre>array([[0.26692407, 0.2687586 , 0.26513771, 0.2659971 , 0.26491176,\n        0.26121977, 0.26624026, 0.27254852, 0.26718946, 0.26644291,\n        0.27047643, 0.27046673, 0.26918691, 0.2717872 , 0.27402136,\n        0.27453696, 0.27591912, 0.27609506, 0.27692744, 0.27890578,\n        0.27120444, 0.27095812, 0.27111476, 0.27406318, 0.2738573 ,\n        0.27288338, 0.28545677, 0.2940021 , 0.28979274, 0.28933208,\n        0.28929591, 0.28951181, 0.28923201, 0.28923201, 0.28925892,\n        0.28766627, 0.28605398, 0.28756317, 0.29984653, 0.29937071,\n        0.30270446, 0.30226792, 0.30230143, 0.29623541, 0.28288305,\n        0.28266805, 0.28073667, 0.28034389, 0.27991581, 0.28273242,\n        0.28185788, 0.28249792, 0.28268014, 0.28325938, 0.28366033,\n        0.27827943, 0.27857699, 0.27857699, 0.27857699, 0.2813678 ,\n        0.28127716, 0.28240876, 0.27936649, 0.2788537 , 0.2788537 ,\n        0.27859086, 0.27859086, 0.2784853 , 0.27982682, 0.27982682,\n        0.27958092, 0.27983752, 0.27983752, 0.27983752, 0.27982998,\n        0.27972845, 0.2796916 , 0.28010585, 0.27999768, 0.27752231,\n        0.27752231, 0.27752231, 0.27752231, 0.28486334, 0.28486898,\n        0.28557261, 0.28713866, 0.28927949, 0.2923428 , 0.2930469 ,\n        0.29365091, 0.29129466, 0.2945114 , 0.29619433, 0.29527234,\n        0.29515418, 0.29529374, 0.29552227, 0.29687984, 0.29687984,\n...\n        0.32272231, 0.32203874, 0.32167547, 0.32027099, 0.31882608,\n        0.3202941 , 0.3202941 , 0.3202941 , 0.32094097, 0.32094097,\n        0.32094097, 0.32109691, 0.32058789, 0.32046971, 0.32066776,\n        0.32054576, 0.32200255, 0.32008391, 0.31647083, 0.31498474,\n        0.31347021, 0.31421521, 0.31213384, 0.30938211, 0.30913478,\n        0.31240634, 0.31347634, 0.31334075, 0.31244653, 0.31273716,\n        0.31616627, 0.31698242, 0.31538925, 0.31540447, 0.31540447,\n        0.31567716, 0.31567716, 0.31567716, 0.31633886, 0.31577167,\n        0.31608012, 0.3157599 , 0.31642149, 0.31716938, 0.31716938,\n        0.31716938, 0.31744652, 0.31744652, 0.31801462, 0.31885989,\n        0.31921613, 0.32202237, 0.32207303, 0.32275163, 0.32052372,\n        0.32188166, 0.32129006, 0.32060967, 0.32157208, 0.32346685,\n        0.32280195, 0.32495359, 0.32502048, 0.32606878, 0.32958112,\n        0.3187762 , 0.31942236, 0.31842272, 0.31846303, 0.31897652,\n        0.31916481, 0.32001366, 0.321167  , 0.32122369, 0.32329621,\n        0.32260806, 0.32387802, 0.32279745, 0.32122898, 0.3219704 ,\n        0.31390092, 0.31309236, 0.31017207, 0.31246478, 0.30551729,\n        0.30627728, 0.30690099, 0.30894313, 0.30966698, 0.30908524,\n        0.30937124, 0.30800584, 0.31441132, 0.31502697, 0.31537905,\n        0.31383639, 0.30434696, 0.30419852, 0.30193553, 0.29958608]])</pre></li><li>phi_1|participant_id(chain, draw, participant_id__factor_dim)float64-0.07752 -0.09937 ... 0.03576<pre>array([[[-0.0775161 , -0.09937288,  0.11671608, ..., -0.11202562,\n          0.26545088,  0.09151213],\n        [-0.09998224, -0.10801187,  0.12292386, ..., -0.09058865,\n          0.25017743,  0.09674744],\n        [-0.1428098 , -0.09498238,  0.09356625, ..., -0.16718635,\n          0.24550748,  0.05620848],\n        ...,\n        [-0.21542796, -0.05054457,  0.01650046, ..., -0.18700018,\n          0.26156432,  0.03414498],\n        [-0.21469009, -0.04972316,  0.01686886, ..., -0.19205639,\n          0.26273742,  0.03527398],\n        [-0.22843193, -0.06246123,  0.00595709, ..., -0.23487887,\n          0.25136072,  0.03576489]]], shape=(1, 500, 20))</pre></li><li>alpha_1|participant_id_mu(chain, draw)float64-0.05049 -0.05122 ... -0.04379<pre>array([[-5.04928058e-02, -5.12232253e-02, -4.97209681e-02,\n        -4.81183667e-02, -4.26339730e-02, -4.35423892e-02,\n        -4.59901019e-02, -4.07414897e-02, -4.69189860e-02,\n        -4.80980535e-02, -4.76914895e-02, -4.95315304e-02,\n        -4.18041576e-02, -5.22604427e-02, -4.41986600e-02,\n        -4.97379334e-02, -4.68846683e-02, -4.86056965e-02,\n        -4.93897174e-02, -5.40055041e-02, -5.37932798e-02,\n        -5.53042414e-02, -5.28476675e-02, -5.19484658e-02,\n        -5.80878109e-02, -2.67341639e-02, -1.03758131e-02,\n        -5.53102152e-03, -1.93002427e-02, -1.74546234e-02,\n        -1.68352433e-02, -1.73531774e-02, -1.70276237e-02,\n        -1.70276237e-02, -1.64378328e-02, -2.03992305e-02,\n        -1.94555140e-02, -2.32553611e-02, -2.94653530e-02,\n        -3.76370837e-02, -3.69874016e-02, -4.22647732e-02,\n        -4.06906159e-02, -4.42352531e-02, -3.06506349e-02,\n        -3.09374416e-02, -2.19499455e-02, -2.82357848e-02,\n        -2.78180402e-02, -3.04225991e-02, -2.86570714e-02,\n        -2.74258216e-02, -2.57751747e-02, -2.61087273e-02,\n        -2.12329337e-02, -1.21813343e-02, -1.22898378e-02,\n        -1.22898378e-02, -1.22898378e-02, -8.34627905e-03,\n...\n        -1.52166157e-02, -1.48484406e-02, -1.48394827e-02,\n        -1.48394827e-02, -1.48394827e-02, -1.43717092e-02,\n        -1.43717092e-02, -1.53398004e-02, -1.70936385e-02,\n        -1.70090754e-02, -1.36353496e-02, -1.20188708e-02,\n        -1.10563819e-02, -1.25402715e-02, -9.24807852e-03,\n        -3.71838819e-03, -2.91148374e-03, -3.01181433e-03,\n        -1.41683552e-03,  1.20976491e-03,  2.06612254e-03,\n         1.13639131e-03, -1.50850178e-03,  2.29220473e-03,\n        -7.54812402e-03, -6.57237540e-03, -7.29633167e-03,\n        -7.29857776e-03, -6.99824233e-03, -7.27233311e-03,\n        -9.43849459e-03, -9.28375170e-03, -7.73308318e-03,\n        -1.13025337e-02, -8.75554568e-03, -1.04896870e-02,\n        -1.45214330e-03, -1.43451160e-03, -2.88248158e-03,\n        -7.04230243e-03, -7.51004953e-03, -2.81323200e-03,\n        -8.93819305e-03, -2.05314358e-02, -2.07648943e-02,\n        -2.37509479e-02, -2.47013593e-02, -2.37274924e-02,\n        -2.63174915e-02, -4.13780090e-02, -3.54207437e-02,\n        -3.91753789e-02, -2.97214611e-02, -2.67042565e-02,\n        -2.65335131e-02, -3.37782727e-02, -3.45944625e-02,\n        -3.21950166e-02, -4.37883581e-02]])</pre></li><li>C_1|participant_id_sigma(chain, draw)float640.2059 0.2054 ... 0.08577 0.08476<pre>array([[0.20588912, 0.20536388, 0.18721079, 0.18545043, 0.19296946,\n        0.18664624, 0.19131202, 0.1867317 , 0.17414774, 0.17133003,\n        0.1741021 , 0.17340184, 0.18100214, 0.18773204, 0.19154075,\n        0.18302852, 0.18392149, 0.18324364, 0.17988234, 0.17998309,\n        0.1793654 , 0.17899695, 0.17651552, 0.17854478, 0.18025451,\n        0.19378296, 0.19287072, 0.20911603, 0.2038515 , 0.20371249,\n        0.2027635 , 0.20350324, 0.20360818, 0.20360818, 0.20248244,\n        0.19844873, 0.19808669, 0.18907315, 0.184024  , 0.17832212,\n        0.1750056 , 0.17224044, 0.17198356, 0.17426893, 0.1637531 ,\n        0.1641942 , 0.17345907, 0.17743575, 0.17711324, 0.17063517,\n        0.17214577, 0.17334536, 0.17389412, 0.17403791, 0.164888  ,\n        0.16818549, 0.16539194, 0.16539194, 0.16539194, 0.16484156,\n        0.16497632, 0.16387724, 0.16919176, 0.1686396 , 0.1686396 ,\n        0.16969278, 0.16969278, 0.16907939, 0.16868376, 0.16868376,\n        0.17049973, 0.17089471, 0.17089471, 0.17089471, 0.16971458,\n        0.17022336, 0.17045857, 0.17075713, 0.17133017, 0.17153775,\n        0.17153775, 0.17153775, 0.17153775, 0.17069755, 0.17025189,\n        0.17036731, 0.1691527 , 0.17353116, 0.17340916, 0.1742551 ,\n        0.17480943, 0.17086115, 0.17237499, 0.16631942, 0.1662273 ,\n        0.16602608, 0.16705264, 0.16593889, 0.16334556, 0.16334556,\n...\n        0.10649788, 0.10661554, 0.10638523, 0.10194891, 0.09795266,\n        0.09891096, 0.09891096, 0.09891096, 0.09845783, 0.09845783,\n        0.09845783, 0.09885293, 0.09858613, 0.09855064, 0.09829851,\n        0.09818826, 0.09794683, 0.09819037, 0.09864795, 0.09844522,\n        0.09789921, 0.09699061, 0.09674697, 0.09920538, 0.09908843,\n        0.0974014 , 0.09654331, 0.09684975, 0.09753531, 0.09782477,\n        0.09965538, 0.09957804, 0.09885876, 0.09908898, 0.09908898,\n        0.09878684, 0.09878684, 0.09878684, 0.09852383, 0.0982105 ,\n        0.0976814 , 0.09751119, 0.09733569, 0.09777887, 0.09777887,\n        0.09777887, 0.09740925, 0.09740925, 0.09657408, 0.09753963,\n        0.09732995, 0.09841485, 0.09934297, 0.09910522, 0.09797822,\n        0.0970059 , 0.09739563, 0.09665534, 0.09711502, 0.09821407,\n        0.09924074, 0.09853294, 0.10022799, 0.10156155, 0.09890714,\n        0.09884586, 0.09858584, 0.09861588, 0.09826042, 0.09876001,\n        0.09922286, 0.09844201, 0.09729702, 0.09776619, 0.09646783,\n        0.09505977, 0.09216145, 0.09260027, 0.09262497, 0.09206422,\n        0.09798875, 0.09520581, 0.09484523, 0.08065044, 0.08496803,\n        0.08483685, 0.08444594, 0.08475904, 0.08370494, 0.08405802,\n        0.08922651, 0.08885207, 0.08811566, 0.0868193 , 0.08707048,\n        0.08634384, 0.08618633, 0.0861358 , 0.08577432, 0.08476382]])</pre></li><li>gamma_1|participant_id_sigma(chain, draw)float640.2522 0.2535 0.261 ... 0.21 0.2137<pre>array([[0.25223059, 0.25352665, 0.26101993, 0.25927824, 0.27071711,\n        0.27038771, 0.27072363, 0.26408637, 0.27707288, 0.27654494,\n        0.26132742, 0.26066148, 0.26250911, 0.26479638, 0.26846152,\n        0.26154303, 0.25627907, 0.25632647, 0.25795223, 0.25850133,\n        0.26855617, 0.26787334, 0.26742429, 0.27443713, 0.2761384 ,\n        0.25053708, 0.24971667, 0.25357238, 0.24656415, 0.24761167,\n        0.2466808 , 0.24650428, 0.24675793, 0.24675793, 0.2452482 ,\n        0.24789648, 0.24690811, 0.24928159, 0.25285117, 0.24914836,\n        0.2501129 , 0.24688138, 0.24701721, 0.2522191 , 0.25183256,\n        0.25182748, 0.25750687, 0.25808962, 0.2575995 , 0.25729249,\n        0.256431  , 0.25594386, 0.25549119, 0.25477327, 0.25325676,\n        0.25463994, 0.25521082, 0.25521082, 0.25521082, 0.2551265 ,\n        0.25497644, 0.25589398, 0.25185663, 0.25149457, 0.25149457,\n        0.25163545, 0.25163545, 0.25131861, 0.25137836, 0.25137836,\n        0.25111719, 0.25157408, 0.25157408, 0.25157408, 0.25104916,\n        0.25122686, 0.25193448, 0.25004754, 0.25010381, 0.25107481,\n        0.25107481, 0.25107481, 0.25107481, 0.25595316, 0.25499192,\n        0.25550002, 0.25421866, 0.25263876, 0.24982311, 0.2504714 ,\n        0.24771964, 0.24727174, 0.2481334 , 0.24525794, 0.24652879,\n        0.24658307, 0.2464032 , 0.24546871, 0.24428522, 0.24428522,\n...\n        0.19189846, 0.19160392, 0.19109542, 0.19343139, 0.18811399,\n        0.19458292, 0.19458292, 0.19458292, 0.19398237, 0.19398237,\n        0.19398237, 0.19423867, 0.19439395, 0.19513759, 0.19556235,\n        0.19453163, 0.1950794 , 0.19513216, 0.19027884, 0.19033887,\n        0.19044928, 0.19057698, 0.19008628, 0.19097516, 0.19090251,\n        0.19089785, 0.18969645, 0.18927298, 0.18898702, 0.18839571,\n        0.1881416 , 0.18757118, 0.18884912, 0.18828538, 0.18828538,\n        0.188653  , 0.188653  , 0.188653  , 0.1881632 , 0.18797677,\n        0.18815496, 0.18824553, 0.18765711, 0.18675495, 0.18675495,\n        0.18675495, 0.18671218, 0.18671218, 0.18799116, 0.18784599,\n        0.18788677, 0.1882636 , 0.18669888, 0.18606279, 0.18974227,\n        0.19103053, 0.19104356, 0.19078135, 0.19155705, 0.19094911,\n        0.1901176 , 0.19005037, 0.19000752, 0.19098064, 0.1924352 ,\n        0.19295164, 0.19199982, 0.19297247, 0.19293989, 0.19288738,\n        0.19304867, 0.19499504, 0.19400963, 0.19415076, 0.19547581,\n        0.1940993 , 0.19433188, 0.19244762, 0.1938135 , 0.19313073,\n        0.19594306, 0.19609846, 0.19466867, 0.19308534, 0.19469205,\n        0.19550301, 0.20422264, 0.20404733, 0.2031769 , 0.20555466,\n        0.21648305, 0.2166824 , 0.21360236, 0.21353067, 0.21122569,\n        0.21090302, 0.21094332, 0.21109457, 0.210045  , 0.21374569]])</pre></li><li>rho_1|participant_id_mu(chain, draw)float64-0.0689 -0.07059 ... -0.02217<pre>array([[-6.89042274e-02, -7.05856689e-02, -6.40006709e-02,\n        -6.07669052e-02, -9.49720409e-02, -9.72572652e-02,\n        -9.98244694e-02, -9.82614419e-02, -9.02500845e-02,\n        -9.34748999e-02, -8.38706923e-02, -7.86883357e-02,\n        -7.79860752e-02, -8.02678028e-02, -7.78869380e-02,\n        -7.18017151e-02, -6.87323078e-02, -6.87487524e-02,\n        -7.03867089e-02, -6.45073100e-02, -5.48401293e-02,\n        -5.62959705e-02, -5.54927257e-02, -5.51719812e-02,\n        -5.94137886e-02, -5.27942607e-02, -2.77052151e-02,\n        -2.07503909e-02, -1.12646741e-02, -1.31304542e-02,\n        -1.34878711e-02, -1.26759512e-02, -1.31516144e-02,\n        -1.31516144e-02, -1.39087901e-02, -1.37625733e-03,\n        -1.63129654e-03,  6.95455951e-03,  1.15928914e-02,\n         1.09437397e-02,  7.63846652e-03, -1.67100075e-04,\n        -2.04401503e-03, -6.00671641e-03,  1.14480110e-02,\n         1.06273132e-02,  4.59583359e-03, -9.05741911e-06,\n         2.67694715e-04,  2.25698102e-03,  4.36486753e-03,\n         7.97802921e-03,  8.60714682e-03,  7.87088197e-03,\n         1.32611443e-02,  2.56382581e-02,  2.94798734e-02,\n         2.94798734e-02,  2.94798734e-02,  2.61597896e-02,\n...\n         1.83488091e-02,  1.89743169e-02,  2.04536064e-02,\n         2.04536064e-02,  2.04536064e-02,  2.08544344e-02,\n         2.08544344e-02,  2.09441833e-02,  2.49593004e-02,\n         2.40431815e-02,  2.47097687e-02,  2.43975439e-02,\n         2.51779622e-02,  3.38039356e-02,  3.44379424e-02,\n         3.68019401e-02,  3.91326702e-02,  3.71942327e-02,\n         3.66869506e-02,  3.58767107e-02,  3.81622478e-02,\n         3.52873686e-02,  3.40878580e-02,  3.90836528e-02,\n         4.34586738e-02,  4.55638147e-02,  4.55574136e-02,\n         4.67940084e-02,  4.79086316e-02,  4.88798562e-02,\n         4.49460228e-02,  4.31451224e-02,  4.24473065e-02,\n         4.07424006e-02,  4.14270428e-02,  4.55964235e-02,\n         7.19542870e-02,  7.55107779e-02,  7.17255865e-02,\n         5.17133088e-02,  5.71169196e-02,  4.62953466e-02,\n         5.24688354e-02,  3.77553861e-02,  3.73079350e-02,\n         2.71084639e-02,  2.95265765e-02,  2.67531922e-02,\n         3.22216026e-02,  1.32775981e-03, -3.33904600e-03,\n        -1.79531295e-02, -1.80007034e-02, -1.66517372e-02,\n        -1.53770601e-02, -1.85616907e-02, -1.94128497e-02,\n        -2.07451793e-02, -2.21743332e-02]])</pre></li><li>z_1|participant_id_mu(chain, draw)float640.1943 0.2033 ... 0.04548 0.03898<pre>array([[0.19432589, 0.20328346, 0.22278311, 0.2294798 , 0.21816975,\n        0.2131487 , 0.21147433, 0.2047835 , 0.20513584, 0.2122713 ,\n        0.21487477, 0.21212251, 0.22039133, 0.18396285, 0.18193661,\n        0.19500786, 0.19879106, 0.19772079, 0.20024219, 0.20090145,\n        0.21079904, 0.21104375, 0.21660335, 0.21472499, 0.21878432,\n        0.19821732, 0.18424149, 0.18155935, 0.17641308, 0.17321476,\n        0.17271635, 0.17476885, 0.17420042, 0.17420042, 0.15879613,\n        0.13188097, 0.13021822, 0.12725781, 0.11411688, 0.12799807,\n        0.12624697, 0.12236713, 0.12228173, 0.12310699, 0.1149645 ,\n        0.11588871, 0.11288616, 0.11870049, 0.11798327, 0.11573172,\n        0.11554458, 0.11450745, 0.1138401 , 0.11811598, 0.10908461,\n        0.10737971, 0.10784849, 0.10784849, 0.10784849, 0.10397492,\n        0.10256494, 0.10111132, 0.10318199, 0.10265278, 0.10265278,\n        0.10343012, 0.10343012, 0.10258115, 0.10450435, 0.10450435,\n        0.10763184, 0.10801481, 0.10801481, 0.10801481, 0.10894651,\n        0.11092264, 0.11116979, 0.10758388, 0.10785106, 0.10369167,\n        0.10369167, 0.10369167, 0.10369167, 0.11517445, 0.11461579,\n        0.11271516, 0.10826309, 0.10049941, 0.10152357, 0.1001628 ,\n        0.09804423, 0.11732841, 0.11281141, 0.11521618, 0.11315861,\n        0.11697309, 0.11654702, 0.11655245, 0.11380297, 0.11380297,\n...\n        0.13923746, 0.13886042, 0.13893238, 0.13065837, 0.11438802,\n        0.12165186, 0.12165186, 0.12165186, 0.12016775, 0.12016775,\n        0.12016775, 0.11941406, 0.11913184, 0.11803685, 0.11885908,\n        0.11763048, 0.1191989 , 0.11772088, 0.12475306, 0.12610478,\n        0.12520428, 0.12500666, 0.12756735, 0.13711044, 0.13840292,\n        0.12896154, 0.13158789, 0.12985846, 0.13350513, 0.13335387,\n        0.1359138 , 0.13429255, 0.1317949 , 0.13212669, 0.13212669,\n        0.13246158, 0.13246158, 0.13246158, 0.13493373, 0.13502282,\n        0.13551361, 0.13459258, 0.1349401 , 0.13212342, 0.13212342,\n        0.13212342, 0.13304325, 0.13304325, 0.13407862, 0.13935607,\n        0.13938855, 0.13373873, 0.13279592, 0.13219809, 0.13295335,\n        0.13198492, 0.13413513, 0.13346775, 0.13151971, 0.13324882,\n        0.1335423 , 0.13438143, 0.13219933, 0.1333929 , 0.13345263,\n        0.13814299, 0.13699244, 0.13761335, 0.13822037, 0.13853725,\n        0.13974511, 0.14967482, 0.14674339, 0.14669048, 0.1435781 ,\n        0.14430085, 0.1352692 , 0.12634045, 0.12893667, 0.13249067,\n        0.12436642, 0.12871513, 0.13710106, 0.10900063, 0.11711854,\n        0.11543505, 0.11199461, 0.11151729, 0.10701688, 0.10736878,\n        0.06366406, 0.06361611, 0.05314846, 0.0550998 , 0.04641716,\n        0.04352231, 0.04775518, 0.0489708 , 0.04548429, 0.03897506]])</pre></li><li>a_Intercept(chain, draw)float640.539 0.5398 ... 0.5176 0.5159<pre>array([[0.53904806, 0.53975373, 0.54716979, 0.55132825, 0.55427997,\n        0.54815642, 0.54528752, 0.54730681, 0.54416375, 0.5427562 ,\n        0.54750427, 0.54933443, 0.55005332, 0.54519335, 0.54696003,\n        0.5374001 , 0.54023073, 0.53940981, 0.5394073 , 0.54027141,\n        0.5446705 , 0.54472482, 0.54556679, 0.54740558, 0.54759468,\n        0.55007409, 0.55443617, 0.55574297, 0.55466176, 0.55436172,\n        0.55455523, 0.55471727, 0.55442074, 0.55442074, 0.55201459,\n        0.55761932, 0.5588358 , 0.55718313, 0.55826939, 0.56162007,\n        0.55655866, 0.56039387, 0.56090538, 0.5576657 , 0.5454031 ,\n        0.54538919, 0.54357876, 0.54477648, 0.54444057, 0.54526504,\n        0.54525244, 0.54384922, 0.54555818, 0.54634379, 0.55471807,\n        0.55162619, 0.55176915, 0.55176915, 0.55176915, 0.55476084,\n        0.55487557, 0.55567231, 0.55243726, 0.55292009, 0.55292009,\n        0.5529052 , 0.5529052 , 0.5529608 , 0.55344472, 0.55344472,\n        0.55324064, 0.55277007, 0.55277007, 0.55277007, 0.55323106,\n        0.5531115 , 0.55342926, 0.55297642, 0.55275579, 0.5546925 ,\n        0.5546925 , 0.5546925 , 0.5546925 , 0.55812504, 0.55956827,\n        0.55947955, 0.55931576, 0.55856039, 0.56018782, 0.55972677,\n        0.55749534, 0.55968787, 0.56364211, 0.56274937, 0.56231514,\n        0.56306894, 0.56230066, 0.56202433, 0.5620359 , 0.5620359 ,\n...\n        0.51820053, 0.51833048, 0.51797742, 0.51770148, 0.51742903,\n        0.51872315, 0.51872315, 0.51872315, 0.51856884, 0.51856884,\n        0.51856884, 0.51862775, 0.51910033, 0.51901934, 0.5191005 ,\n        0.5184994 , 0.51827698, 0.51815085, 0.51539531, 0.51487536,\n        0.51505735, 0.5154553 , 0.51434864, 0.51739444, 0.51750652,\n        0.51712324, 0.51618084, 0.51566137, 0.51591983, 0.51531972,\n        0.51074104, 0.51068528, 0.51007752, 0.50990382, 0.50990382,\n        0.50998106, 0.50998106, 0.50998106, 0.51015795, 0.5095545 ,\n        0.50908897, 0.50879394, 0.50798893, 0.50890102, 0.50890102,\n        0.50890102, 0.50932235, 0.50932235, 0.50856493, 0.50995693,\n        0.51033814, 0.51129433, 0.50979758, 0.51131557, 0.50959419,\n        0.50874533, 0.50766068, 0.50715483, 0.5073199 , 0.50809645,\n        0.50761692, 0.50683293, 0.50778942, 0.51025033, 0.51002567,\n        0.51231593, 0.51159793, 0.51218575, 0.51209099, 0.51178699,\n        0.51172713, 0.51292795, 0.51316305, 0.51323813, 0.50958281,\n        0.50906317, 0.50774247, 0.51037573, 0.51026856, 0.50805811,\n        0.51481903, 0.51663601, 0.51336211, 0.51770111, 0.51407151,\n        0.51461604, 0.51301226, 0.5136057 , 0.51221906, 0.51182406,\n        0.51631492, 0.51569971, 0.51600955, 0.51613203, 0.51608659,\n        0.51579868, 0.51902313, 0.51915976, 0.51758811, 0.51593675]])</pre></li><li>a_1|participant_id_sigma(chain, draw)float640.09007 0.08947 ... 0.08652 0.0871<pre>array([[0.09006588, 0.08947449, 0.09546302, 0.09586597, 0.09865594,\n        0.08819503, 0.0875489 , 0.08702119, 0.09211654, 0.08873752,\n        0.09561321, 0.09269709, 0.08886084, 0.09889851, 0.09718966,\n        0.08693299, 0.09009659, 0.09054213, 0.09201638, 0.09075514,\n        0.09813196, 0.09867916, 0.09959553, 0.10075748, 0.09923474,\n        0.09142814, 0.09096841, 0.08874849, 0.08892492, 0.08838707,\n        0.08855979, 0.08811345, 0.08846901, 0.08846901, 0.09115385,\n        0.08405548, 0.08437184, 0.08348135, 0.08014799, 0.07645392,\n        0.07981423, 0.07809117, 0.0783641 , 0.08274134, 0.0968733 ,\n        0.09780347, 0.10063199, 0.10341849, 0.10370251, 0.10681992,\n        0.1053093 , 0.1040678 , 0.10351819, 0.10772204, 0.09065892,\n        0.08730724, 0.08727634, 0.08727634, 0.08727634, 0.08523317,\n        0.08559316, 0.08531064, 0.08515503, 0.08502585, 0.08502585,\n        0.08478423, 0.08478423, 0.08529844, 0.08487138, 0.08487138,\n        0.08436354, 0.0843191 , 0.0843191 , 0.0843191 , 0.08413435,\n        0.08376508, 0.08373509, 0.08386803, 0.08398711, 0.08284567,\n        0.08284567, 0.08284567, 0.08284567, 0.07602177, 0.07621704,\n        0.07572917, 0.07399781, 0.07173766, 0.06780057, 0.06655962,\n        0.06910941, 0.07024416, 0.07279441, 0.07200027, 0.07070623,\n        0.07176997, 0.07171564, 0.07016491, 0.07042407, 0.07042407,\n...\n        0.09272625, 0.09243781, 0.09152772, 0.08942271, 0.10335179,\n        0.10205437, 0.10205437, 0.10205437, 0.10197796, 0.10197796,\n        0.10197796, 0.10253677, 0.10323969, 0.10443285, 0.10539979,\n        0.10376489, 0.10284809, 0.10308092, 0.10540914, 0.10658239,\n        0.10471809, 0.10441465, 0.1057439 , 0.10314617, 0.10264975,\n        0.10501712, 0.10524774, 0.10446842, 0.10693231, 0.10339135,\n        0.10052802, 0.1009405 , 0.1011994 , 0.10195154, 0.10195154,\n        0.10221616, 0.10221616, 0.10221616, 0.10200947, 0.10197764,\n        0.1020687 , 0.10175415, 0.10201642, 0.10321397, 0.10321397,\n        0.10321397, 0.10332691, 0.10332691, 0.10540225, 0.1041269 ,\n        0.105117  , 0.10400995, 0.10477719, 0.10336698, 0.09950256,\n        0.09778075, 0.0985505 , 0.09836699, 0.09774767, 0.09744336,\n        0.1001379 , 0.10120479, 0.1040135 , 0.10505419, 0.10290687,\n        0.10140196, 0.10007712, 0.1006654 , 0.100477  , 0.10071599,\n        0.10057553, 0.09855449, 0.09714602, 0.09828512, 0.10019946,\n        0.10168782, 0.09929132, 0.10966005, 0.11022986, 0.10911316,\n        0.09910087, 0.09544214, 0.0957159 , 0.0991611 , 0.08476506,\n        0.08479264, 0.08193582, 0.0830968 , 0.08467669, 0.08135554,\n        0.08880404, 0.08643247, 0.08663496, 0.08663132, 0.0839323 ,\n        0.0839415 , 0.08430617, 0.08403602, 0.08651861, 0.08710237]])</pre></li><li>epsilon_1|participant_id_mu(chain, draw)float640.07859 0.08626 ... 0.04547 0.03986<pre>array([[0.07858688, 0.08625864, 0.08409156, 0.08700365, 0.09145565,\n        0.09068546, 0.08991813, 0.09410375, 0.09201123, 0.09332475,\n        0.10037684, 0.09917406, 0.10322397, 0.09781572, 0.0914794 ,\n        0.08852163, 0.0931799 , 0.09219862, 0.09462192, 0.09337796,\n        0.06724434, 0.0663577 , 0.06479454, 0.07201361, 0.07197848,\n        0.06442757, 0.05719509, 0.05675171, 0.065329  , 0.06621207,\n        0.06558915, 0.06490196, 0.06416001, 0.06416001, 0.06188521,\n        0.06221132, 0.06041095, 0.05971756, 0.06449463, 0.05612494,\n        0.0572602 , 0.06040593, 0.06102773, 0.06087593, 0.05976963,\n        0.06039263, 0.06508903, 0.06459604, 0.06388063, 0.06657783,\n        0.06570887, 0.06502285, 0.06490498, 0.06690107, 0.07850576,\n        0.07421896, 0.07407141, 0.07407141, 0.07407141, 0.07633673,\n        0.07619688, 0.07838486, 0.06993524, 0.06952037, 0.06952037,\n        0.06949972, 0.06949972, 0.06880845, 0.06819827, 0.06819827,\n        0.06843748, 0.06822894, 0.06822894, 0.06822894, 0.06856937,\n        0.06846399, 0.06782802, 0.06746824, 0.06668102, 0.06901712,\n        0.06901712, 0.06901712, 0.06901712, 0.06979794, 0.06834581,\n        0.06924891, 0.07190493, 0.07330416, 0.07383244, 0.07596708,\n        0.08013313, 0.09858055, 0.09736089, 0.09788565, 0.09694128,\n        0.09553182, 0.09361917, 0.09552109, 0.09371905, 0.09371905,\n...\n        0.05098866, 0.04971132, 0.04925712, 0.05215354, 0.05717651,\n        0.05588431, 0.05588431, 0.05588431, 0.05715312, 0.05715312,\n        0.05715312, 0.05766619, 0.05787882, 0.05797547, 0.0580871 ,\n        0.05762503, 0.05709911, 0.05897262, 0.0578559 , 0.05725821,\n        0.05843609, 0.05798904, 0.05898619, 0.05691472, 0.05725594,\n        0.05647309, 0.0572645 , 0.05672034, 0.05418483, 0.05091585,\n        0.05117788, 0.05169355, 0.05137253, 0.05078094, 0.05078094,\n        0.04996077, 0.04996077, 0.04996077, 0.05043542, 0.04954616,\n        0.04925474, 0.04965613, 0.05012153, 0.0457669 , 0.0457669 ,\n        0.0457669 , 0.04427976, 0.04427976, 0.04462968, 0.04355931,\n        0.04415287, 0.04638529, 0.05051422, 0.05153562, 0.0510127 ,\n        0.05245566, 0.05291227, 0.05049155, 0.0513565 , 0.05262225,\n        0.04968322, 0.05248665, 0.052754  , 0.05336238, 0.05443728,\n        0.05222809, 0.05145407, 0.05111352, 0.0514506 , 0.05168896,\n        0.05102991, 0.0507436 , 0.0508057 , 0.05092641, 0.04679419,\n        0.04895805, 0.05370522, 0.05766284, 0.0565073 , 0.05524557,\n        0.0642933 , 0.06477971, 0.06554367, 0.05411435, 0.05750019,\n        0.05778291, 0.06604252, 0.06518052, 0.06912964, 0.07031835,\n        0.04723812, 0.04941634, 0.04465568, 0.04292483, 0.04517336,\n        0.04348633, 0.04688013, 0.04702783, 0.04546721, 0.039857  ]])</pre></li><li>theta_Intercept(chain, draw)float640.1917 0.1919 ... 0.1842 0.1858<pre>array([[0.19168642, 0.19185378, 0.18857306, 0.19007486, 0.19207618,\n        0.19292725, 0.19531332, 0.19545626, 0.1939403 , 0.19426209,\n        0.1946598 , 0.19438734, 0.19470159, 0.19082826, 0.19102407,\n        0.18580736, 0.18398859, 0.18400486, 0.18277795, 0.18393857,\n        0.18137384, 0.18147445, 0.18118638, 0.18451253, 0.18472292,\n        0.18381032, 0.18734856, 0.18748493, 0.187222  , 0.18702233,\n        0.18730208, 0.1871652 , 0.18704302, 0.18704302, 0.18700626,\n        0.19123094, 0.19157782, 0.19367728, 0.19354511, 0.19394384,\n        0.19409026, 0.19481481, 0.19471662, 0.19425079, 0.19359604,\n        0.19406488, 0.19307591, 0.19473973, 0.19467109, 0.19330527,\n        0.19322892, 0.1934166 , 0.19330121, 0.19349387, 0.19401744,\n        0.19538809, 0.1957979 , 0.1957979 , 0.1957979 , 0.1962363 ,\n        0.19626037, 0.19598247, 0.19411339, 0.193795  , 0.193795  ,\n        0.19349938, 0.19349938, 0.19291573, 0.19264796, 0.19264796,\n        0.19273021, 0.19329194, 0.19329194, 0.19329194, 0.19354581,\n        0.19367023, 0.19389886, 0.19383808, 0.19395904, 0.1936502 ,\n        0.1936502 , 0.1936502 , 0.1936502 , 0.19197879, 0.1919626 ,\n        0.19215529, 0.19262206, 0.19142806, 0.19111729, 0.19075105,\n        0.19172493, 0.19263688, 0.19327853, 0.19249851, 0.19237422,\n        0.19146972, 0.19157645, 0.19186707, 0.19109695, 0.19109695,\n...\n        0.19100875, 0.1914281 , 0.19147283, 0.19135127, 0.18862115,\n        0.18447121, 0.18447121, 0.18447121, 0.18503957, 0.18503957,\n        0.18503957, 0.18524149, 0.18533553, 0.18563854, 0.18594055,\n        0.18562285, 0.18539265, 0.18595731, 0.18695182, 0.18786205,\n        0.18789824, 0.18821886, 0.18893041, 0.18788833, 0.18768842,\n        0.18736877, 0.18660487, 0.1866023 , 0.18708765, 0.18777071,\n        0.18747761, 0.18739404, 0.1868986 , 0.18673174, 0.18673174,\n        0.18668569, 0.18668569, 0.18668569, 0.18644445, 0.18713405,\n        0.18706898, 0.18714454, 0.18747586, 0.18763033, 0.18763033,\n        0.18763033, 0.18779626, 0.18779626, 0.18804755, 0.18840944,\n        0.18839129, 0.18894174, 0.18875063, 0.18855873, 0.18739483,\n        0.18779996, 0.18802228, 0.18822752, 0.18864887, 0.18800358,\n        0.1885968 , 0.18905511, 0.18943617, 0.19067544, 0.18999938,\n        0.19165301, 0.19189081, 0.19095356, 0.19097157, 0.19092074,\n        0.19095217, 0.19073354, 0.19012492, 0.19027688, 0.18959533,\n        0.18913478, 0.18769711, 0.1916934 , 0.19148662, 0.19208534,\n        0.18978773, 0.18936329, 0.18778381, 0.19058549, 0.1879441 ,\n        0.18789582, 0.18665008, 0.1864596 , 0.18621019, 0.18678423,\n        0.18270972, 0.18329087, 0.18313625, 0.18355569, 0.18309026,\n        0.18391467, 0.18423455, 0.18461081, 0.18421844, 0.18575914]])</pre></li><li>C_1|participant_id_mu(chain, draw)float640.01578 0.0142 ... 0.07321 0.05443<pre>array([[ 0.01578436,  0.01419709, -0.00514173, -0.00543944, -0.01859869,\n        -0.00763707, -0.02681349, -0.02973511, -0.01421386, -0.02569824,\n        -0.01951918, -0.01332818, -0.01369471, -0.02181389, -0.01460048,\n        -0.03379977, -0.03483448, -0.03400243, -0.03492585, -0.02393016,\n        -0.02401249, -0.02343966, -0.02929283,  0.00583133,  0.00578422,\n         0.03089633,  0.05806972,  0.04703395,  0.07079563,  0.07246403,\n         0.07267012,  0.07264296,  0.07223444,  0.07223444,  0.0816222 ,\n         0.10425219,  0.10380096,  0.10867478,  0.13472561,  0.1268915 ,\n         0.1162046 ,  0.12295821,  0.12152219,  0.11707478,  0.07675325,\n         0.07679847,  0.08220704,  0.07736455,  0.07777124,  0.07456265,\n         0.07174245,  0.07102137,  0.07367372,  0.06907768,  0.0578559 ,\n         0.04631747,  0.04607826,  0.04607826,  0.04607826,  0.05367217,\n         0.05202043,  0.05172115,  0.07347809,  0.07378724,  0.07378724,\n         0.07459417,  0.07459417,  0.07689404,  0.07927054,  0.07927054,\n         0.08599674,  0.08392871,  0.08392871,  0.08392871,  0.08187389,\n         0.0827772 ,  0.08508141,  0.08497424,  0.0839911 ,  0.08693079,\n         0.08693079,  0.08693079,  0.08693079,  0.09621769,  0.09433966,\n         0.0950345 ,  0.08375728,  0.08275253,  0.07208094,  0.06993273,\n         0.06673355,  0.09355008,  0.07487572,  0.07251783,  0.07267911,\n         0.07187655,  0.07241295,  0.06689988,  0.06997054,  0.06997054,\n...\n         0.06698349,  0.06505746,  0.06452414,  0.06339664,  0.07075037,\n         0.07133305,  0.07133305,  0.07133305,  0.06746287,  0.06746287,\n         0.06746287,  0.06754582,  0.0661351 ,  0.06774426,  0.06723222,\n         0.06651894,  0.07015602,  0.06992285,  0.05696358,  0.05671593,\n         0.05479383,  0.05553146,  0.06177435,  0.08190123,  0.08131009,\n         0.08778315,  0.0826076 ,  0.08436697,  0.0847597 ,  0.0798821 ,\n         0.08230783,  0.08304592,  0.08050599,  0.07967812,  0.07967812,\n         0.08040538,  0.08040538,  0.08040538,  0.08006566,  0.07484957,\n         0.07494058,  0.07411369,  0.07191679,  0.07121137,  0.07121137,\n         0.07121137,  0.0728716 ,  0.0728716 ,  0.07341262,  0.06369364,\n         0.06340664,  0.06472363,  0.06853885,  0.07135865,  0.06738841,\n         0.06639859,  0.06832558,  0.07133575,  0.07033184,  0.07148032,\n         0.06684106,  0.06433046,  0.06282376,  0.065321  ,  0.07082663,\n         0.0771048 ,  0.07478181,  0.07868308,  0.08257998,  0.0833656 ,\n         0.08654108,  0.08495038,  0.08499727,  0.08418393,  0.07083844,\n         0.07030796,  0.06960476,  0.07085155,  0.07499344,  0.07912533,\n         0.08406142,  0.07727316,  0.04781004,  0.03291878,  0.07451546,\n         0.07291925,  0.07045998,  0.06860434,  0.06635229,  0.07100056,\n         0.10039966,  0.09582858,  0.07809281,  0.07090788,  0.07104187,\n         0.07426574,  0.0710479 ,  0.07028664,  0.07320609,  0.05443298]])</pre></li><li>alpha_Intercept(chain, draw)float64-5.709 -5.712 ... -5.539 -5.547<pre>array([[-5.70927994, -5.71219485, -5.71329386, -5.71335807, -5.69836103,\n        -5.70017284, -5.70982342, -5.71226891, -5.70551891, -5.70290719,\n        -5.69041913, -5.69087814, -5.68628365, -5.68968059, -5.69776594,\n        -5.68338794, -5.6832258 , -5.68261791, -5.68173884, -5.67915365,\n        -5.67735668, -5.67726027, -5.67537499, -5.67498191, -5.67756926,\n        -5.67092944, -5.64997243, -5.65959937, -5.66768046, -5.66678548,\n        -5.66630098, -5.66734414, -5.667143  , -5.667143  , -5.66501745,\n        -5.67651504, -5.6797796 , -5.68038977, -5.67826661, -5.67562229,\n        -5.66880478, -5.66954597, -5.66918778, -5.66190652, -5.65501282,\n        -5.65490768, -5.65428194, -5.6554156 , -5.65610407, -5.64956811,\n        -5.64841792, -5.64994078, -5.65135828, -5.65388872, -5.66332169,\n        -5.66625577, -5.6663719 , -5.6663719 , -5.6663719 , -5.66282111,\n        -5.66227241, -5.66082316, -5.65966371, -5.65911508, -5.65911508,\n        -5.65969654, -5.65969654, -5.65987041, -5.66031581, -5.66031581,\n        -5.6596965 , -5.66045632, -5.66045632, -5.66045632, -5.66127271,\n        -5.66232365, -5.66193552, -5.66071845, -5.66141475, -5.66107276,\n        -5.66107276, -5.66107276, -5.66107276, -5.66113443, -5.65947462,\n        -5.65856679, -5.65572581, -5.65624593, -5.6554276 , -5.65711446,\n        -5.65988199, -5.66505893, -5.67313286, -5.67679971, -5.67772826,\n        -5.67599339, -5.67651388, -5.6795356 , -5.68096033, -5.68096033,\n...\n        -5.56007817, -5.5603112 , -5.55934605, -5.56789106, -5.56643363,\n        -5.57512853, -5.57512853, -5.57512853, -5.57309864, -5.57309864,\n        -5.57309864, -5.57208548, -5.57274492, -5.5730281 , -5.57272715,\n        -5.5735575 , -5.57198231, -5.57117314, -5.57532243, -5.57483769,\n        -5.57261157, -5.57246924, -5.57571649, -5.58784103, -5.58839056,\n        -5.58681129, -5.58569706, -5.58684355, -5.58976927, -5.59226517,\n        -5.59059848, -5.59025429, -5.59303861, -5.59274851, -5.59274851,\n        -5.59269908, -5.59269908, -5.59269908, -5.59278965, -5.59371027,\n        -5.59334362, -5.59348912, -5.5931217 , -5.59359001, -5.59359001,\n        -5.59359001, -5.59330426, -5.59330426, -5.59441805, -5.5925795 ,\n        -5.59098434, -5.5905265 , -5.58938831, -5.58936249, -5.58772364,\n        -5.58886416, -5.58931861, -5.58909745, -5.58978652, -5.59200787,\n        -5.59248613, -5.59233765, -5.59311212, -5.59257756, -5.5953118 ,\n        -5.59251179, -5.59312079, -5.59464969, -5.59540168, -5.59390533,\n        -5.59312401, -5.59455512, -5.59692199, -5.59635791, -5.59660284,\n        -5.59651791, -5.59427681, -5.58730806, -5.58495933, -5.57747646,\n        -5.56326993, -5.56847011, -5.58222524, -5.56352712, -5.56308853,\n        -5.56253218, -5.56147505, -5.56168552, -5.56193385, -5.56269917,\n        -5.5478639 , -5.55221504, -5.53964726, -5.53714106, -5.5351195 ,\n        -5.53756124, -5.5389435 , -5.53976459, -5.53892246, -5.54672922]])</pre></li><li>gamma_Intercept(chain, draw)float640.1306 0.1292 ... 0.157 0.1578<pre>array([[0.13055284, 0.12920968, 0.13357314, 0.13462549, 0.13646345,\n        0.13251246, 0.13257955, 0.13185515, 0.1267559 , 0.12642612,\n        0.12717614, 0.12784682, 0.13050146, 0.13232555, 0.13024898,\n        0.12831351, 0.12778193, 0.12739812, 0.12728375, 0.12814213,\n        0.13018841, 0.13049501, 0.13154557, 0.13504185, 0.13686168,\n        0.1324678 , 0.1328249 , 0.13447123, 0.13303611, 0.13317504,\n        0.13303538, 0.13276326, 0.13256831, 0.13256831, 0.13092959,\n        0.12780579, 0.12741573, 0.12701745, 0.12406169, 0.12520945,\n        0.12640998, 0.12701937, 0.12685629, 0.12573728, 0.11464505,\n        0.11460153, 0.11674841, 0.11839047, 0.11854224, 0.11751293,\n        0.11684574, 0.11668471, 0.11673641, 0.1162366 , 0.1173824 ,\n        0.11605202, 0.11615963, 0.11615963, 0.11615963, 0.11593134,\n        0.11579082, 0.11612355, 0.11821081, 0.118245  , 0.118245  ,\n        0.11860559, 0.11860559, 0.11872524, 0.11863944, 0.11863944,\n        0.11949471, 0.11955078, 0.11955078, 0.11955078, 0.11948665,\n        0.11959317, 0.11988095, 0.11944698, 0.1192125 , 0.11995826,\n        0.11995826, 0.11995826, 0.11995826, 0.12307636, 0.12326939,\n        0.12319094, 0.12137617, 0.12162474, 0.12260717, 0.12131541,\n        0.12007683, 0.11730535, 0.11655168, 0.11665073, 0.11623562,\n        0.1159023 , 0.11623611, 0.11616995, 0.11614136, 0.11614136,\n...\n        0.14896298, 0.14930124, 0.149285  , 0.15092186, 0.15173611,\n        0.14820158, 0.14820158, 0.14820158, 0.14859671, 0.14859671,\n        0.14859671, 0.14883742, 0.148723  , 0.14877332, 0.14898555,\n        0.14877283, 0.14829503, 0.14811147, 0.15133961, 0.15032026,\n        0.15150004, 0.1515627 , 0.15140551, 0.14962933, 0.1499994 ,\n        0.149808  , 0.15047111, 0.15046454, 0.14972624, 0.15060675,\n        0.15255276, 0.15273141, 0.15283828, 0.15262824, 0.15262824,\n        0.15244996, 0.15244996, 0.15244996, 0.15213078, 0.15202082,\n        0.15231665, 0.1517314 , 0.15170835, 0.15152109, 0.15152109,\n        0.15152109, 0.15146643, 0.15146643, 0.15094442, 0.15050386,\n        0.15079755, 0.14923316, 0.14922964, 0.14915642, 0.14951945,\n        0.14969246, 0.14963973, 0.15029545, 0.15034094, 0.15077254,\n        0.15078894, 0.15045778, 0.15107591, 0.14910353, 0.14753605,\n        0.14898683, 0.1499724 , 0.15007086, 0.15024402, 0.15048048,\n        0.15087307, 0.15177551, 0.15318205, 0.15329916, 0.15139113,\n        0.15190337, 0.15245411, 0.1512308 , 0.15151234, 0.15221477,\n        0.1555333 , 0.15713815, 0.15706872, 0.15431859, 0.14920701,\n        0.14902663, 0.14586046, 0.14661847, 0.14635111, 0.14342379,\n        0.15707554, 0.15747347, 0.15636834, 0.15811611, 0.15815316,\n        0.15933872, 0.15674626, 0.15696628, 0.1570165 , 0.15779063]])</pre></li><li>rho_Intercept(chain, draw)float640.5944 0.5907 ... 0.5311 0.5429<pre>array([[0.59440467, 0.59071223, 0.58578353, 0.56870837, 0.56421251,\n        0.56523805, 0.5748404 , 0.55967906, 0.57821576, 0.56968083,\n        0.57959769, 0.58924521, 0.58549006, 0.57727869, 0.56908251,\n        0.5432229 , 0.56050314, 0.5606202 , 0.55554819, 0.55752221,\n        0.54379123, 0.54215435, 0.5428301 , 0.54264773, 0.55253065,\n        0.55056008, 0.55016828, 0.57111811, 0.5604702 , 0.55816912,\n        0.55917747, 0.55741606, 0.558176  , 0.558176  , 0.55629369,\n        0.5564916 , 0.55590976, 0.5724306 , 0.57995445, 0.58189593,\n        0.57658823, 0.55773788, 0.55699798, 0.55558312, 0.54122269,\n        0.54351756, 0.54204823, 0.54381337, 0.5437911 , 0.55038831,\n        0.54737855, 0.54399945, 0.54276069, 0.53888066, 0.52559979,\n        0.54032466, 0.54178858, 0.54178858, 0.54178858, 0.55276731,\n        0.55252716, 0.55241702, 0.52884583, 0.52855284, 0.52855284,\n        0.52988242, 0.52988242, 0.53002963, 0.52533417, 0.52533417,\n        0.52236635, 0.51795051, 0.51795051, 0.51795051, 0.51452647,\n        0.51343917, 0.51423669, 0.51599553, 0.51853269, 0.50775477,\n        0.50775477, 0.50775477, 0.50775477, 0.54942927, 0.54901022,\n        0.54599678, 0.54480099, 0.5548543 , 0.55695437, 0.55762938,\n        0.56598886, 0.54533465, 0.55869327, 0.5735963 , 0.57383793,\n        0.57619716, 0.57347913, 0.57275522, 0.57468851, 0.57468851,\n...\n        0.53739784, 0.53664134, 0.53680365, 0.56116054, 0.5350101 ,\n        0.54446496, 0.54446496, 0.54446496, 0.54409019, 0.54409019,\n        0.54409019, 0.5432917 , 0.54337654, 0.54395771, 0.54160349,\n        0.53931078, 0.53488439, 0.53254185, 0.53203156, 0.5301936 ,\n        0.53135148, 0.53243256, 0.53172329, 0.54619885, 0.54876634,\n        0.54883733, 0.5447521 , 0.54364376, 0.54895236, 0.53852434,\n        0.5350477 , 0.53713634, 0.53644602, 0.53635138, 0.53635138,\n        0.53548093, 0.53548093, 0.53548093, 0.53240424, 0.53199576,\n        0.5305669 , 0.53599883, 0.53371059, 0.53742581, 0.53742581,\n        0.53742581, 0.53817561, 0.53817561, 0.53423574, 0.54475221,\n        0.54625681, 0.55604386, 0.54839919, 0.55379938, 0.55853004,\n        0.55630308, 0.56332088, 0.55750633, 0.56045754, 0.55911377,\n        0.55650458, 0.56160145, 0.56266252, 0.56259533, 0.55878065,\n        0.56195644, 0.56057375, 0.55692198, 0.55517978, 0.55318247,\n        0.55358861, 0.55947334, 0.5595323 , 0.56141519, 0.56581825,\n        0.56247052, 0.55979686, 0.55562463, 0.5528317 , 0.5432901 ,\n        0.55066685, 0.55617288, 0.55255029, 0.56316978, 0.55809686,\n        0.55139366, 0.53416388, 0.53899827, 0.54269916, 0.53811329,\n        0.53825326, 0.53206499, 0.54736506, 0.54436543, 0.5414487 ,\n        0.53858311, 0.5319978 , 0.53117995, 0.53107369, 0.54289172]])</pre></li><li>eta_1|participant_id_sigma(chain, draw)float640.2273 0.2272 ... 0.202 0.2039<pre>array([[0.22731942, 0.22719432, 0.22102405, 0.22301573, 0.22150184,\n        0.22150787, 0.2229514 , 0.21626542, 0.21316896, 0.21222906,\n        0.21194625, 0.21127125, 0.2103746 , 0.20602444, 0.20686118,\n        0.2063072 , 0.20659044, 0.20648229, 0.20868889, 0.20991307,\n        0.21360252, 0.21364978, 0.21321333, 0.21337629, 0.21371713,\n        0.20028241, 0.19858386, 0.2006692 , 0.20143731, 0.20165115,\n        0.20181252, 0.20193778, 0.20187688, 0.20187688, 0.20154979,\n        0.20293548, 0.20342585, 0.20055902, 0.19995464, 0.19746816,\n        0.19809491, 0.19870399, 0.19870628, 0.19816915, 0.18910377,\n        0.18901266, 0.18600855, 0.1858518 , 0.18571275, 0.18558343,\n        0.18534269, 0.18509893, 0.18520937, 0.18499297, 0.18309419,\n        0.18186133, 0.18170771, 0.18170771, 0.18170771, 0.18358628,\n        0.18351035, 0.18391722, 0.18553458, 0.18556708, 0.18556708,\n        0.18552874, 0.18552874, 0.18578064, 0.18606356, 0.18606356,\n        0.18721576, 0.18739089, 0.18739089, 0.18739089, 0.18730414,\n        0.18715455, 0.18705729, 0.18713132, 0.18699753, 0.18848034,\n        0.18848034, 0.18848034, 0.18848034, 0.18748083, 0.18758178,\n        0.18755181, 0.18682101, 0.18814119, 0.18793655, 0.18858732,\n        0.18969249, 0.18183915, 0.18240505, 0.18325476, 0.18341093,\n        0.18329762, 0.18324971, 0.18322006, 0.18259287, 0.18259287,\n...\n        0.20154492, 0.20187217, 0.20158336, 0.20321291, 0.20133741,\n        0.20354544, 0.20354544, 0.20354544, 0.20401637, 0.20401637,\n        0.20401637, 0.20383279, 0.203853  , 0.20388703, 0.20417038,\n        0.20421945, 0.20433861, 0.20462818, 0.20262697, 0.20277017,\n        0.20211943, 0.20201353, 0.20131423, 0.19911247, 0.1993815 ,\n        0.19994378, 0.19999476, 0.20053315, 0.20025746, 0.19853167,\n        0.19847914, 0.19857158, 0.19728751, 0.19697736, 0.19697736,\n        0.19683057, 0.19683057, 0.19683057, 0.19690817, 0.19698515,\n        0.19708715, 0.19736816, 0.19727402, 0.19757973, 0.19757973,\n        0.19757973, 0.19753093, 0.19753093, 0.19819576, 0.198613  ,\n        0.19837996, 0.19861153, 0.19748431, 0.19777172, 0.19885704,\n        0.19844965, 0.19862192, 0.19880767, 0.19841629, 0.19710386,\n        0.19691317, 0.19651431, 0.19585834, 0.19528197, 0.19496344,\n        0.19463734, 0.19483213, 0.19533546, 0.19549436, 0.19553096,\n        0.19528538, 0.19478667, 0.19535189, 0.19529792, 0.19743225,\n        0.1975537 , 0.19709681, 0.19620989, 0.19595962, 0.19627442,\n        0.19558385, 0.19635823, 0.19372303, 0.19768705, 0.19580791,\n        0.19593807, 0.19553239, 0.19555708, 0.19590522, 0.1967554 ,\n        0.19713126, 0.19707932, 0.19991302, 0.19994129, 0.20112186,\n        0.20222787, 0.20297053, 0.20279495, 0.2019607 , 0.20391331]])</pre></li><li>a_1|participant_id(chain, draw, a_1|participant_id__factor_dim)float64-0.05049 -0.1089 ... 0.1145 0.01574<pre>array([[[-0.05048662, -0.10885403, -0.08399289, ..., -0.00509862,\n          0.09798332,  0.01740595],\n        [-0.05106856, -0.11154311, -0.09148353, ..., -0.00637629,\n          0.10198886,  0.01839951],\n        [-0.06691206, -0.10620439, -0.07983043, ..., -0.02159439,\n          0.12714115,  0.02091129],\n        ...,\n        [-0.0413251 , -0.09607123, -0.0568945 , ...,  0.02900587,\n          0.1147857 ,  0.01529557],\n        [-0.04061325, -0.09918203, -0.05747897, ...,  0.02896483,\n          0.1136221 ,  0.01452411],\n        [-0.03931342, -0.0958111 , -0.04245727, ...,  0.02727242,\n          0.11450276,  0.01573909]]], shape=(1, 500, 20))</pre></li><li>z_1|participant_id(chain, draw, participant_id__factor_dim)float640.1538 0.1751 ... 0.1541 0.1644<pre>array([[[0.15378953, 0.17512536, 0.21938288, ..., 0.19333116,\n         0.17591   , 0.17205034],\n        [0.15928462, 0.1715394 , 0.21602213, ..., 0.19325397,\n         0.17844855, 0.16712971],\n        [0.14998501, 0.16189296, 0.22084839, ..., 0.19343275,\n         0.19183855, 0.18192124],\n        ...,\n        [0.11721935, 0.13410816, 0.20202511, ..., 0.19035755,\n         0.15353313, 0.16900337],\n        [0.11702445, 0.13624227, 0.20471372, ..., 0.18996008,\n         0.15379776, 0.16916314],\n        [0.12430674, 0.12917456, 0.20647585, ..., 0.18586487,\n         0.15408537, 0.16444637]]], shape=(1, 500, 20))</pre></li><li>z_1|participant_id_sigma(chain, draw)float640.2547 0.2553 ... 0.2363 0.2363<pre>array([[0.25471208, 0.25530382, 0.25390845, 0.25670863, 0.25719299,\n        0.2584308 , 0.25803722, 0.25487905, 0.25451352, 0.25437075,\n        0.25262708, 0.2524091 , 0.25217171, 0.25079102, 0.25157718,\n        0.25057658, 0.24996098, 0.24991988, 0.24873931, 0.24887644,\n        0.2547039 , 0.2545061 , 0.25675344, 0.25673204, 0.25580362,\n        0.25329287, 0.25733974, 0.25428939, 0.25213219, 0.25183001,\n        0.25190711, 0.25180061, 0.25167417, 0.25167417, 0.25057122,\n        0.24657766, 0.24568018, 0.24645001, 0.24399449, 0.24371921,\n        0.24237028, 0.24298884, 0.24289301, 0.24228839, 0.2524139 ,\n        0.2522654 , 0.25190222, 0.25184841, 0.2520182 , 0.25199381,\n        0.25238539, 0.25231527, 0.25192112, 0.25193123, 0.25390339,\n        0.25521861, 0.2549252 , 0.2549252 , 0.2549252 , 0.25689763,\n        0.25662743, 0.25672594, 0.25560704, 0.25547573, 0.25547573,\n        0.2555949 , 0.2555949 , 0.25560118, 0.25577695, 0.25577695,\n        0.25552908, 0.25571386, 0.25571386, 0.25571386, 0.25550794,\n        0.25536924, 0.25532038, 0.2561756 , 0.25637096, 0.25655911,\n        0.25655911, 0.25655911, 0.25655911, 0.25713616, 0.25713802,\n        0.25703968, 0.25682526, 0.25694607, 0.25705606, 0.25786549,\n        0.25845192, 0.26047633, 0.25918753, 0.25847843, 0.25932203,\n        0.25880736, 0.25864977, 0.25815676, 0.25797888, 0.25797888,\n...\n        0.23418645, 0.23398186, 0.23394544, 0.23360845, 0.23431407,\n        0.23295472, 0.23295472, 0.23295472, 0.23246893, 0.23246893,\n        0.23246893, 0.23206374, 0.23203958, 0.23197585, 0.2317834 ,\n        0.23199997, 0.23154038, 0.23130023, 0.2322606 , 0.2318461 ,\n        0.23212114, 0.23206234, 0.23279914, 0.23313813, 0.23335094,\n        0.23375635, 0.23448473, 0.23366439, 0.23431285, 0.23475608,\n        0.23453309, 0.2345315 , 0.23413533, 0.23420953, 0.23420953,\n        0.23451468, 0.23451468, 0.23451468, 0.2343932 , 0.23432412,\n        0.23424417, 0.23424448, 0.23439084, 0.23418051, 0.23418051,\n        0.23418051, 0.23410622, 0.23410622, 0.23376296, 0.23362374,\n        0.23376663, 0.23375333, 0.23396219, 0.23438132, 0.23333286,\n        0.23372692, 0.23397232, 0.23383127, 0.23447077, 0.23472244,\n        0.23510871, 0.2347327 , 0.234305  , 0.23332571, 0.23351265,\n        0.23540088, 0.2358075 , 0.23517293, 0.234902  , 0.2347352 ,\n        0.23518879, 0.23554746, 0.23525318, 0.23531724, 0.23651662,\n        0.23700478, 0.2367541 , 0.23388233, 0.23459254, 0.23423801,\n        0.23787688, 0.23784329, 0.23958284, 0.23634952, 0.23639196,\n        0.2366375 , 0.24013076, 0.24004828, 0.23827964, 0.23920823,\n        0.2368357 , 0.2367859 , 0.23534402, 0.23632029, 0.23648517,\n        0.23660275, 0.23589737, 0.23580623, 0.23633188, 0.23633749]])</pre></li><li>rho_1|participant_id_sigma(chain, draw)float640.1926 0.1865 ... 0.1586 0.1611<pre>array([[0.19258751, 0.18647424, 0.21071009, 0.2151877 , 0.21889395,\n        0.22116425, 0.22260593, 0.21439793, 0.22293416, 0.22318553,\n        0.22459714, 0.22327587, 0.22670921, 0.22399203, 0.22553474,\n        0.2182976 , 0.21694367, 0.2173485 , 0.21735145, 0.21886345,\n        0.21928713, 0.21800562, 0.22083187, 0.20484491, 0.20307396,\n        0.1942928 , 0.19680483, 0.19229121, 0.2000585 , 0.19991951,\n        0.20006307, 0.19963709, 0.19928277, 0.19928277, 0.19640821,\n        0.19875497, 0.19758159, 0.19347121, 0.19388193, 0.18855828,\n        0.19143612, 0.19444301, 0.19447947, 0.19909711, 0.18917544,\n        0.18950793, 0.19054616, 0.19031301, 0.19057945, 0.19067625,\n        0.19017928, 0.18932556, 0.18780708, 0.18753677, 0.17828917,\n        0.1751563 , 0.17560287, 0.17560287, 0.17560287, 0.17376777,\n        0.17338467, 0.17347127, 0.17056861, 0.17052552, 0.17052552,\n        0.17063598, 0.17063598, 0.17102899, 0.17180561, 0.17180561,\n        0.17042418, 0.16998008, 0.16998008, 0.16998008, 0.1699179 ,\n        0.17007138, 0.1700213 , 0.1692142 , 0.16972107, 0.16933282,\n        0.16933282, 0.16933282, 0.16933282, 0.16523554, 0.16530111,\n        0.16523826, 0.16471452, 0.16761799, 0.16951699, 0.16988666,\n        0.16947299, 0.17591972, 0.17466086, 0.16957808, 0.16987398,\n        0.1696826 , 0.17024489, 0.16995715, 0.17125699, 0.17125699,\n...\n        0.16345646, 0.16351182, 0.16322868, 0.16619723, 0.16028566,\n        0.16191486, 0.16191486, 0.16191486, 0.16158947, 0.16158947,\n        0.16158947, 0.16146807, 0.16163392, 0.16188489, 0.16171743,\n        0.16193281, 0.1619677 , 0.16192016, 0.16328562, 0.16435806,\n        0.16468338, 0.16484751, 0.16211912, 0.16684733, 0.16633402,\n        0.16611407, 0.16713502, 0.16673794, 0.16524485, 0.1671241 ,\n        0.16742477, 0.16731459, 0.16882203, 0.16835693, 0.16835693,\n        0.16853228, 0.16853228, 0.16853228, 0.1682664 , 0.16784429,\n        0.16754026, 0.16816711, 0.16795492, 0.16829609, 0.16829609,\n        0.16829609, 0.16836248, 0.16836248, 0.16830516, 0.16868349,\n        0.1678258 , 0.1684457 , 0.16814272, 0.16821733, 0.16946118,\n        0.16892553, 0.16913105, 0.16873556, 0.16852432, 0.16838257,\n        0.16745984, 0.16719638, 0.16744464, 0.16680697, 0.1674538 ,\n        0.16946298, 0.16979768, 0.1706618 , 0.17021023, 0.17016991,\n        0.17052711, 0.17049193, 0.16976341, 0.16971504, 0.1667063 ,\n        0.16661434, 0.16650451, 0.16588326, 0.16626099, 0.1643863 ,\n        0.17782661, 0.17657818, 0.17613942, 0.17645392, 0.16975017,\n        0.16949495, 0.16846297, 0.16918434, 0.17012107, 0.17050098,\n        0.16690904, 0.16591968, 0.15935451, 0.15848757, 0.15889336,\n        0.15731169, 0.15735884, 0.15794361, 0.15860106, 0.16107535]])</pre></li><li>eta_1|participant_id_mu(chain, draw)float640.003616 0.004675 ... -0.08247<pre>array([[ 0.00361595,  0.00467543, -0.01473062, -0.01471479, -0.01581183,\n        -0.03887454, -0.02383386,  0.01002023,  0.01880541,  0.01345935,\n         0.00393147,  0.00207231, -0.00280657, -0.0029225 , -0.0051495 ,\n        -0.00320307, -0.00779798, -0.00684566, -0.01840435, -0.0325089 ,\n        -0.0280994 , -0.0268875 , -0.02489074, -0.00582601, -0.00240596,\n        -0.05443498, -0.05151899, -0.04609513, -0.06897202, -0.07004397,\n        -0.0703207 , -0.07027842, -0.07051378, -0.07051378, -0.06903556,\n        -0.06125281, -0.06051479, -0.04081901, -0.03026746, -0.03947561,\n        -0.01561589, -0.01328782, -0.01409783, -0.00753734, -0.04269502,\n        -0.04606414, -0.0475243 , -0.04393926, -0.0419327 , -0.04234316,\n        -0.04694063, -0.04886674, -0.04810043, -0.05329422, -0.02737815,\n        -0.0457696 , -0.04843496, -0.04843496, -0.04843496, -0.0402127 ,\n        -0.03790885, -0.03628124, -0.05041219, -0.04899895, -0.04899895,\n        -0.04746938, -0.04746938, -0.04898668, -0.047301  , -0.047301  ,\n        -0.04837469, -0.04765575, -0.04765575, -0.04765575, -0.04759788,\n        -0.04549933, -0.04308731, -0.04478587, -0.04513936, -0.0406077 ,\n        -0.0406077 , -0.0406077 , -0.0406077 , -0.04534546, -0.04579276,\n        -0.04679822, -0.05018828, -0.04013375, -0.04259089, -0.03584449,\n        -0.03438213, -0.04784975, -0.06760758, -0.07727949, -0.07684258,\n        -0.07416049, -0.07336429, -0.07412423, -0.08272384, -0.08272384,\n...\n        -0.10009898, -0.10062208, -0.10242644, -0.1132128 , -0.11770625,\n        -0.14687331, -0.14687331, -0.14687331, -0.14475952, -0.14475952,\n        -0.14475952, -0.14647187, -0.1468357 , -0.1501898 , -0.14973516,\n        -0.14852066, -0.14832505, -0.14709116, -0.14753657, -0.1470141 ,\n        -0.14904612, -0.14822233, -0.15212483, -0.14420455, -0.14305798,\n        -0.13759249, -0.14224423, -0.1408427 , -0.13254689, -0.13914749,\n        -0.12979574, -0.13028564, -0.12909567, -0.12897493, -0.12897493,\n        -0.12865213, -0.12865213, -0.12865213, -0.12913515, -0.13058174,\n        -0.13019531, -0.12834209, -0.12886102, -0.13165667, -0.13165667,\n        -0.13165667, -0.13163463, -0.13163463, -0.12497871, -0.12260552,\n        -0.12253178, -0.11951855, -0.11774476, -0.11836307, -0.11748518,\n        -0.11541261, -0.11914238, -0.12175178, -0.12332001, -0.13347932,\n        -0.13407898, -0.13259034, -0.13208738, -0.13288961, -0.11967976,\n        -0.13021707, -0.12738481, -0.12687404, -0.12779387, -0.12900277,\n        -0.12938005, -0.13012279, -0.12887362, -0.12960891, -0.122848  ,\n        -0.12612294, -0.14345186, -0.12899052, -0.12946481, -0.13854198,\n        -0.11218805, -0.1146273 , -0.12100425, -0.17118298, -0.16343088,\n        -0.16313431, -0.14786087, -0.14549674, -0.14796021, -0.15084294,\n        -0.12285156, -0.12399476, -0.10154413, -0.09116318, -0.09423433,\n        -0.09073928, -0.08104906, -0.07846319, -0.08435025, -0.08247286]])</pre></li><li>gamma_1|participant_id(chain, draw, participant_id__factor_dim)float640.2468 0.08872 ... -0.03203 -0.1173<pre>array([[[ 0.24677627,  0.08871814, -0.01450579, ...,  0.11985633,\n          0.05079528, -0.08370043],\n        [ 0.23262517,  0.0836787 , -0.01437528, ...,  0.12723577,\n          0.04932076, -0.08245508],\n        [ 0.22738125,  0.07997162, -0.01743648, ...,  0.09617044,\n          0.04369918, -0.07146386],\n        ...,\n        [ 0.34347418,  0.24441944, -0.07125266, ...,  0.08116183,\n         -0.04015995, -0.10835499],\n        [ 0.33831465,  0.24501078, -0.0781171 , ...,  0.08167632,\n         -0.0407438 , -0.10919174],\n        [ 0.34303167,  0.25643535, -0.07599499, ...,  0.08111235,\n         -0.03202681, -0.11733937]]], shape=(1, 500, 20))</pre></li><li>alpha_1|participant_id(chain, draw, participant_id__factor_dim)float64-0.2055 -0.2025 ... 0.1944 0.1<pre>array([[[-0.20545216, -0.20246893, -0.20069485, ..., -0.08456042,\n         -0.02859338,  0.09680472],\n        [-0.20894052, -0.21813709, -0.20326689, ..., -0.08498481,\n         -0.04567356,  0.09269474],\n        [-0.24004026, -0.2073548 , -0.20137039, ..., -0.07344454,\n         -0.11335545,  0.12362292],\n        ...,\n        [-0.0398603 , -0.17727362, -0.11031095, ...,  0.07031773,\n          0.20976511,  0.10269696],\n        [-0.04110379, -0.16912441, -0.11214507, ...,  0.06938337,\n          0.21345874,  0.10128373],\n        [-0.04618131, -0.1739038 , -0.11775938, ...,  0.0751991 ,\n          0.19439052,  0.10000947]]], shape=(1, 500, 20))</pre></li></ul></li><li>Indexes: (4)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>a_1|participant_id__factor_dimPandasIndex<pre>PandasIndex(Index(['0.0', '1.0', '2.0', '3.0', '4.0', '5.0', '6.0', '7.0', '8.0', '9.0',\n       '10.0', '11.0', '12.0', '13.0', '14.0', '15.0', '16.0', '17.0', '18.0',\n       '19.0'],\n      dtype='object', name='a_1|participant_id__factor_dim'))</pre></li><li>participant_id__factor_dimPandasIndex<pre>PandasIndex(Index(['0.0', '1.0', '2.0', '3.0', '4.0', '5.0', '6.0', '7.0', '8.0', '9.0',\n       '10.0', '11.0', '12.0', '13.0', '14.0', '15.0', '16.0', '17.0', '18.0',\n       '19.0'],\n      dtype='object', name='participant_id__factor_dim'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-07-16T20:49:22.630978+00:00arviz_version :0.22.0inference_library :numpyroinference_library_version :0.18.0sampling_time :460.059804tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 29MB\nDimensions:      (chain: 1, draw: 500, __obs__: 7200)\nCoordinates:\n  * chain        (chain) int64 8B 0\n  * draw         (draw) int64 4kB 0 1 2 3 4 5 6 ... 493 494 495 496 497 498 499\n  * __obs__      (__obs__) int64 58kB 0 1 2 3 4 5 ... 7195 7196 7197 7198 7199\nData variables:\n    rt,response  (chain, draw, __obs__) float64 29MB -0.9687 -0.1167 ... 0.1321\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 500</li><li>__obs__: 7200</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 ... 7196 7197 7198 7199<pre>array([   0,    1,    2, ..., 7197, 7198, 7199], shape=(7200,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-0.9687 -0.1167 ... 0.833 0.1321<pre>array([[[-0.96870901, -0.11665493, -0.27212845, ...,  0.81840048,\n          0.77629105, -0.44277145],\n        [-0.99340202, -0.12971503, -0.28725214, ...,  0.82543719,\n          0.78173367, -0.53849374],\n        [-1.02511805, -0.11891327, -0.27720492, ...,  0.78920148,\n          0.74572081, -0.34671054],\n        ...,\n        [-0.98095713, -0.08818411, -0.24206957, ...,  0.81750828,\n          0.7966787 , -0.02718026],\n        [-0.99310803, -0.08628929, -0.240801  , ...,  0.82371238,\n          0.80849869,  0.02858707],\n        [-0.98092768, -0.10502516, -0.25741369, ...,  0.83688832,\n          0.83300393,  0.13210151]]], shape=(1, 500, 7200))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       7190, 7191, 7192, 7193, 7194, 7195, 7196, 7197, 7198, 7199],\n      dtype='int64', name='__obs__', length=7200))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 29kB\nDimensions:          (chain: 1, draw: 500)\nCoordinates:\n  * chain            (chain) int64 8B 0\n  * draw             (draw) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\nData variables:\n    acceptance_rate  (chain, draw) float64 4kB 0.935 0.9496 ... 0.8487 0.9455\n    step_size        (chain, draw) float64 4kB 0.03717 0.03717 ... 0.03717\n    diverging        (chain, draw) bool 500B True True True ... True True True\n    energy           (chain, draw) float64 4kB 1.638e+03 1.622e+03 ... 1.616e+03\n    n_steps          (chain, draw) int64 4kB 18 20 37 22 43 32 ... 11 23 7 7 23\n    tree_depth       (chain, draw) int64 4kB 5 5 6 5 6 6 5 5 ... 6 4 6 4 5 3 3 5\n    lp               (chain, draw) float64 4kB 1.526e+03 1.525e+03 ... 1.507e+03\nAttributes:\n    created_at:                  2025-07-16T20:49:22.642832+00:00\n    arviz_version:               0.22.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (7)<ul><li>acceptance_rate(chain, draw)float640.935 0.9496 ... 0.8487 0.9455<pre>array([[0.93498601, 0.94963512, 0.96841856, 0.88892291, 0.97626096,\n        0.96867657, 0.9468348 , 0.9531597 , 0.95763319, 0.95307469,\n        0.95944622, 0.92835628, 0.95232594, 0.97906961, 0.96074584,\n        0.93336629, 0.95239463, 0.90008825, 0.93107197, 0.91423488,\n        0.9565078 , 0.66387069, 0.88958154, 0.96581741, 0.96301331,\n        0.9779673 , 0.95550903, 0.96316153, 0.96965178, 0.74384408,\n        0.5       , 0.66666667, 0.66666667, 0.        , 0.92021361,\n        0.97517169, 0.82848951, 0.95825421, 0.93592573, 0.98070637,\n        0.94935141, 0.92593337, 0.96023903, 0.91666667, 0.95335845,\n        0.66666667, 0.87996781, 0.91565631, 0.83615095, 0.92786946,\n        0.83228373, 0.85445395, 0.8538915 , 0.89434925, 0.96503528,\n        0.87066402, 0.85714286, 0.        , 0.        , 0.89977847,\n        0.88528867, 0.84995873, 0.93095662, 0.66666667, 0.        ,\n        0.5       , 0.        , 0.5       , 0.78845167, 0.        ,\n        0.75      , 0.66374211, 0.        , 0.        , 0.66383084,\n        0.49796439, 0.5       , 0.74830721, 0.88248955, 0.875     ,\n        0.        , 0.        , 0.        , 0.93636133, 0.79869517,\n        0.66497698, 0.90042845, 0.92719159, 0.94312038, 0.85685299,\n        0.87998733, 0.96385632, 0.95208549, 0.86786699, 0.74997167,\n        0.84652358, 0.85614324, 0.88917338, 0.74765736, 0.        ,\n...\n        0.        , 0.5       , 0.66634928, 0.93777836, 0.94649705,\n        0.94128175, 0.        , 0.        , 0.74991996, 0.        ,\n        0.        , 0.6654883 , 0.5       , 0.5       , 0.66536981,\n        0.7384502 , 0.74720551, 0.74633298, 0.92399117, 0.8670765 ,\n        0.74987565, 0.5       , 0.88699438, 0.92971628, 0.5       ,\n        0.75      , 0.77535409, 0.75      , 0.91541972, 0.88597676,\n        0.90788538, 0.49738781, 0.74465558, 0.74469616, 0.        ,\n        0.49497532, 0.        , 0.        , 0.5       , 0.74402866,\n        0.49171026, 0.65225663, 0.48639133, 0.74498394, 0.        ,\n        0.        , 0.49922298, 0.        , 0.78718038, 0.75      ,\n        0.7986167 , 0.92307692, 0.89955182, 0.83922035, 0.88296698,\n        0.74671106, 0.78065525, 0.92020977, 0.85683852, 0.87170752,\n        0.89961138, 0.79369143, 0.88459531, 0.89158497, 0.89652077,\n        0.9478353 , 0.87492189, 0.84415491, 0.66572935, 0.87693168,\n        0.5       , 0.9120336 , 0.88504327, 0.66572707, 0.92857143,\n        0.87977588, 0.89651783, 0.93351493, 0.90909091, 0.95958671,\n        0.96513443, 0.92302923, 0.93687969, 0.95961705, 0.97426345,\n        0.85617114, 0.94376169, 0.75      , 0.86988583, 0.93323355,\n        0.9727585 , 0.90702737, 0.96570454, 0.926783  , 0.976112  ,\n        0.9072868 , 0.95403836, 0.8531662 , 0.84868512, 0.94546513]])</pre></li><li>step_size(chain, draw)float640.03717 0.03717 ... 0.03717 0.03717<pre>array([[0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n...\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838,\n        0.03716838, 0.03716838, 0.03716838, 0.03716838, 0.03716838]])</pre></li><li>diverging(chain, draw)boolTrue True True ... True True True<pre>array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n...\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True]])</pre></li><li>energy(chain, draw)float641.638e+03 1.622e+03 ... 1.616e+03<pre>array([[1637.64145764, 1622.39279193, 1637.54902093, 1644.80386592,\n        1630.25134895, 1628.47651787, 1632.50140261, 1638.26443118,\n        1637.4347004 , 1647.88019417, 1623.85787876, 1629.62084235,\n        1648.68727695, 1640.91585588, 1642.4756719 , 1637.30047337,\n        1618.85057806, 1637.68127023, 1641.44276211, 1634.91213989,\n        1646.02485534, 1672.75941524, 1664.49708159, 1641.31870745,\n        1626.78295553, 1669.2327935 , 1650.46341577, 1628.17359762,\n        1641.87530451, 1618.28272046, 1650.00125246, 1634.23644339,\n        1645.35677858, 1631.9385957 , 1633.46215019, 1647.93389485,\n        1642.555108  , 1642.42947803, 1641.38556242, 1640.96905089,\n        1641.89506291, 1650.74896122, 1654.33049104, 1642.57562988,\n        1631.5665228 , 1649.19868304, 1645.3508066 , 1632.16321614,\n        1642.78182965, 1643.80582984, 1619.56254016, 1633.92035364,\n        1640.76269637, 1617.20572832, 1644.49822366, 1646.00620662,\n        1642.91267335, 1644.20108693, 1656.43766887, 1626.98097605,\n        1632.30052426, 1623.77537465, 1621.06254727, 1644.46750925,\n        1626.15924835, 1638.22812514, 1627.06044712, 1627.40277924,\n        1626.70356537, 1633.75699363, 1625.50635493, 1657.21256318,\n        1617.50412503, 1620.03400004, 1645.49718786, 1642.14337056,\n        1641.49867519, 1634.38531024, 1620.47031493, 1622.95613578,\n...\n        1608.50797222, 1611.0222878 , 1601.54024513, 1582.58422658,\n        1616.79218542, 1594.83436068, 1604.27110483, 1610.80877593,\n        1605.27717832, 1604.17955394, 1607.4739541 , 1583.90574086,\n        1614.40513103, 1604.14435314, 1612.51244312, 1616.10875724,\n        1592.28305186, 1589.68874047, 1609.949244  , 1621.76806498,\n        1600.78993452, 1616.73114511, 1612.30120844, 1610.20818589,\n        1616.20759472, 1600.97494353, 1585.46706993, 1599.18666185,\n        1609.3419889 , 1611.26527197, 1599.63754845, 1591.64019466,\n        1601.85143468, 1597.26881666, 1602.17225542, 1613.13595669,\n        1609.48613442, 1603.42894801, 1607.38595736, 1617.29379395,\n        1590.26182507, 1599.40662735, 1591.89669954, 1610.93929845,\n        1595.99710917, 1592.53518908, 1607.27338022, 1630.21972226,\n        1602.01784228, 1590.50981478, 1609.09571343, 1592.68735012,\n        1601.84287667, 1591.93901358, 1609.55651157, 1599.70319467,\n        1610.52528657, 1614.78770212, 1608.46725245, 1603.73935307,\n        1620.38812014, 1593.88141661, 1618.67250472, 1607.64223591,\n        1613.13911007, 1604.23015161, 1591.38164397, 1612.26835385,\n        1590.68734374, 1608.55606075, 1606.05346971, 1588.95928601,\n        1601.17889033, 1613.96559544, 1614.57445761, 1625.12325999,\n        1643.1977578 , 1626.00952732, 1607.43407886, 1616.05529307]])</pre></li><li>n_steps(chain, draw)int6418 20 37 22 43 32 ... 11 23 7 7 23<pre>array([[18, 20, 37, 22, 43, 32, 25, 30, 47, 30, 28, 14, 26, 55, 26, 38,\n        23, 14, 23, 12, 25,  3, 10, 37, 28, 50, 47, 28, 33,  4,  2,  3,\n         3,  1, 14, 41,  6, 26, 23, 52, 25, 14, 29, 12, 33,  3, 20, 14,\n         7, 14,  6,  7,  7, 10, 53, 11,  7,  1,  1, 13,  9,  7, 16,  3,\n         1,  2,  1,  2,  5,  1,  4,  3,  1,  1,  3,  2,  2,  4,  9,  8,\n         1,  1,  1, 17,  5,  3, 12, 15, 19,  7,  9, 33, 21,  9,  4,  7,\n         7, 10,  4,  1,  2,  2,  7,  9,  4,  3,  1,  3,  8,  7, 12, 11,\n        11, 12, 17,  9, 16, 59, 11, 27,  6, 33,  7,  9, 11,  3,  4,  5,\n        50, 39, 19, 20,  5, 35, 16, 14, 30, 30, 21, 43, 41, 15,  4, 19,\n         1,  3, 98, 24, 61, 48, 54, 62, 13, 41, 11, 21,  2,  1,  3,  1,\n        14, 33, 25, 30, 19, 35, 15,  6, 25, 21, 51,  9,  5,  2,  7,  3,\n        21,  8,  1,  1,  1,  7,  2,  8,  4,  1,  1,  1,  2, 19, 42, 29,\n        18, 10, 19, 40, 31, 19, 16, 11, 41, 17,  6, 16, 16, 10, 23, 35,\n        42, 28, 72, 34, 27, 51, 15, 23, 29, 43, 64, 10, 38, 40, 22, 51,\n        47,  3,  7,  1,  4,  9,  3,  3,  7,  3,  6, 18, 23, 16, 16,  5,\n        10, 31,  2,  3, 16,  8,  5, 13, 14, 23,  9,  8,  2,  8, 34, 11,\n        10,  7, 20,  7, 10, 12, 32, 70, 27, 39, 24, 29, 15, 38, 35, 31,\n        30, 25, 20, 15, 17, 26, 49, 51, 62, 33, 24, 63, 17, 15, 26, 41,\n        27, 12, 18, 16, 19, 28,  5, 54,  7, 21,  1,  1,  2,  1,  9,  1,\n         8, 18, 28,  8,  9, 11, 32, 22, 18, 45, 24, 43, 45, 26, 37, 22,\n        27, 14, 21, 27,  7, 29, 43, 40, 19, 21,  8,  6,  4,  9,  5,  6,\n         6,  2,  4,  8,  1,  5,  7, 16,  2,  5,  2,  3, 10, 15, 11,  6,\n         6,  9, 18,  5, 35,  7, 14,  3, 10, 33,  2, 49,  8,  6,  7,  5,\n         4, 21,  9, 37, 12,  9, 26,  5,  2,  3, 10, 21, 21,  5,  4,  2,\n         3,  9,  7, 23,  8,  6,  7,  9,  7,  1,  1,  1,  2, 12,  2,  1,\n         1,  2,  3, 18, 21, 23,  1,  1,  4,  1,  1,  3,  2,  2,  3,  4,\n         4,  4, 18,  9,  4,  2,  9, 17,  2,  4,  5,  4, 12,  9, 13,  2,\n         4,  4,  1,  2,  1,  1,  2,  4,  2,  3,  2,  4,  1,  1,  2,  1,\n         5,  4,  5, 13, 10,  7,  9,  4,  5, 13,  7,  8, 10,  5,  9, 14,\n        10, 22,  8,  8,  3,  9,  2, 12,  9,  3, 14, 10, 10, 22, 11, 30,\n        37, 13, 19, 57, 49,  7, 29,  4,  8, 18, 40, 20, 42, 14, 62, 11,\n        23,  7,  7, 23]])</pre></li><li>tree_depth(chain, draw)int645 5 6 5 6 6 5 5 ... 6 4 6 4 5 3 3 5<pre>array([[5, 5, 6, 5, 6, 6, 5, 5, 6, 5, 5, 4, 5, 6, 5, 6, 5, 4, 5, 4, 5, 2,\n        4, 6, 5, 6, 6, 5, 6, 3, 2, 2, 2, 1, 4, 6, 3, 5, 5, 6, 5, 4, 5, 4,\n        6, 2, 5, 4, 3, 4, 3, 3, 3, 4, 6, 4, 3, 1, 1, 4, 4, 3, 5, 2, 1, 2,\n        1, 2, 3, 1, 3, 2, 1, 1, 2, 2, 2, 3, 4, 4, 1, 1, 1, 5, 3, 2, 4, 4,\n        5, 3, 4, 6, 5, 4, 3, 3, 3, 4, 3, 1, 2, 2, 3, 4, 3, 2, 1, 2, 4, 3,\n        4, 4, 4, 4, 5, 4, 5, 6, 4, 5, 3, 6, 3, 4, 4, 2, 3, 3, 6, 6, 5, 5,\n        3, 6, 5, 4, 5, 5, 5, 6, 6, 4, 3, 5, 1, 2, 7, 5, 6, 6, 6, 6, 4, 6,\n        4, 5, 2, 1, 2, 1, 4, 6, 5, 5, 5, 6, 4, 3, 5, 5, 6, 4, 3, 2, 3, 2,\n        5, 4, 1, 1, 1, 3, 2, 4, 3, 1, 1, 1, 2, 5, 6, 5, 5, 4, 5, 6, 5, 5,\n        5, 4, 6, 5, 3, 5, 5, 4, 5, 6, 6, 5, 7, 6, 5, 6, 4, 5, 5, 6, 7, 4,\n        6, 6, 5, 6, 6, 2, 3, 1, 3, 4, 2, 2, 3, 2, 3, 5, 5, 5, 5, 3, 4, 5,\n        2, 2, 5, 4, 3, 4, 4, 5, 4, 4, 2, 4, 6, 4, 4, 3, 5, 3, 4, 4, 6, 7,\n        5, 6, 5, 5, 4, 6, 6, 5, 5, 5, 5, 4, 5, 5, 6, 6, 6, 6, 5, 6, 5, 4,\n        5, 6, 5, 4, 5, 5, 5, 5, 3, 6, 3, 5, 1, 1, 2, 1, 4, 1, 4, 5, 5, 4,\n        4, 4, 6, 5, 5, 6, 5, 6, 6, 5, 6, 5, 5, 4, 5, 5, 3, 5, 6, 6, 5, 5,\n        4, 3, 3, 4, 3, 3, 3, 2, 3, 4, 1, 3, 3, 5, 2, 3, 2, 2, 4, 4, 4, 3,\n        3, 4, 5, 3, 6, 3, 4, 2, 4, 6, 2, 6, 4, 3, 3, 3, 3, 5, 4, 6, 4, 4,\n        5, 3, 2, 2, 4, 5, 5, 3, 3, 2, 2, 4, 3, 5, 4, 3, 3, 4, 3, 1, 1, 1,\n        2, 4, 2, 1, 1, 2, 2, 5, 5, 5, 1, 1, 3, 1, 1, 2, 2, 2, 2, 3, 3, 3,\n        5, 4, 3, 2, 4, 5, 2, 3, 3, 3, 4, 4, 4, 2, 3, 3, 1, 2, 1, 1, 2, 3,\n        2, 2, 2, 3, 1, 1, 2, 1, 3, 3, 3, 4, 4, 3, 4, 3, 3, 4, 3, 4, 4, 3,\n        4, 4, 4, 5, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 4, 5, 4, 5, 6, 4, 5, 6,\n        6, 3, 5, 3, 4, 5, 6, 5, 6, 4, 6, 4, 5, 3, 3, 5]])</pre></li><li>lp(chain, draw)float641.526e+03 1.525e+03 ... 1.507e+03<pre>array([[1525.7293548 , 1525.24259568, 1520.26128757, 1522.18636256,\n        1522.7513256 , 1520.22886916, 1519.58897113, 1515.49964625,\n        1530.55882485, 1535.75249779, 1527.77014606, 1527.16798231,\n        1528.85697316, 1522.77250241, 1517.8834471 , 1511.87190252,\n        1514.32004824, 1514.1952119 , 1517.81546143, 1520.72210944,\n        1524.13353419, 1525.58206943, 1527.2730936 , 1522.73373736,\n        1523.02917232, 1527.5216424 , 1526.99610365, 1528.21423378,\n        1524.2399713 , 1522.99904977, 1522.72478591, 1521.95239004,\n        1520.20639408, 1520.20639408, 1525.54212257, 1519.73538773,\n        1520.72427354, 1522.46960131, 1534.60851221, 1530.23171038,\n        1534.45283147, 1534.9313648 , 1537.1648426 , 1525.90739237,\n        1518.13268041, 1517.81996156, 1518.01437576, 1516.03664574,\n        1515.12810446, 1515.43445667, 1515.68062574, 1515.53127953,\n        1513.85629577, 1515.09964041, 1525.97253152, 1528.08225251,\n        1524.96005346, 1524.96005346, 1524.96005346, 1519.7519268 ,\n        1518.93730766, 1519.61011857, 1517.03734573, 1518.05341831,\n        1518.05341831, 1519.13850847, 1519.13850847, 1518.9538595 ,\n        1519.44472253, 1519.44472253, 1518.71576044, 1519.53877962,\n        1519.53877962, 1519.53877962, 1519.95689828, 1520.06166853,\n        1519.51486906, 1520.05399411, 1520.07909783, 1520.16633996,\n...\n        1486.78224576, 1485.41378327, 1483.38187348, 1494.64869359,\n        1493.73994903, 1492.53088077, 1494.66393998, 1494.18796766,\n        1495.20854596, 1489.34968903, 1489.63824652, 1490.36845119,\n        1491.68048803, 1492.00160637, 1492.00160637, 1492.73178811,\n        1492.73178811, 1492.73178811, 1491.06799095, 1491.10211021,\n        1491.46089176, 1491.58337838, 1491.55582695, 1487.56777578,\n        1487.56777578, 1487.56777578, 1488.28817186, 1488.28817186,\n        1490.06537346, 1488.51113071, 1488.10906423, 1487.50974182,\n        1486.72480945, 1488.81421534, 1488.74770965, 1488.86023048,\n        1489.96043834, 1489.62104731, 1489.70788401, 1483.84889436,\n        1484.16433347, 1486.20046822, 1483.2942659 , 1488.92040595,\n        1487.42827919, 1487.46788056, 1488.31108363, 1487.84391442,\n        1488.76004616, 1489.37560969, 1489.05492953, 1487.19934019,\n        1487.23171706, 1486.86758944, 1489.41192567, 1489.2281076 ,\n        1490.65916029, 1496.26735475, 1492.19884683, 1495.78425208,\n        1499.8918339 , 1494.99282716, 1491.27635381, 1496.66679657,\n        1494.76181623, 1493.43482007, 1489.6885926 , 1488.23687831,\n        1488.08073509, 1494.41275857, 1486.97483414, 1493.20676551,\n        1503.84658748, 1500.62297948, 1507.44107998, 1503.59558069,\n        1498.11738182, 1497.89884804, 1503.8437497 , 1506.7209778 ]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (4)created_at :2025-07-16T20:49:22.642832+00:00arviz_version :0.22.0modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 173kB\nDimensions:                  (__obs__: 7200, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 58kB 0 1 2 3 ... 7197 7198 7199\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 115kB ...\nAttributes:\n    created_at:                  2025-07-16T20:49:22.643458+00:00\n    arviz_version:               0.22.0\n    inference_library:           numpyro\n    inference_library_version:   0.18.0\n    sampling_time:               460.059804\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 7200</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 ... 7196 7197 7198 7199<pre>array([   0,    1,    2, ..., 7197, 7198, 7199], shape=(7200,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float640.9055 1.0 0.6877 ... 0.3234 0.0<pre>array([[0.90548512, 1.        ],\n       [0.68766596, 0.        ],\n       [0.70818379, 1.        ],\n       ...,\n       [0.55972397, 2.        ],\n       [0.49500015, 0.        ],\n       [0.32335044, 0.        ]], shape=(7200, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       7190, 7191, 7192, 7193, 7194, 7195, 7196, 7197, 7198, 7199],\n      dtype='int64', name='__obs__', length=7200))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-07-16T20:49:22.643458+00:00arviz_version :0.22.0inference_library :numpyroinference_library_version :0.18.0sampling_time :460.059804tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[42]: Copied! <pre># Define parameter names for analysis\n\nlist_group_mean_params = [\n    \"a_Intercept\",\n    \"z_Intercept\",\n    \"theta_Intercept\",\n    \"alpha_Intercept\",\n    \"phi_Intercept\",\n    \"rho_Intercept\",\n    \"gamma_Intercept\",\n    \"epsilon_Intercept\",\n    \"C_Intercept\",\n    \"eta_Intercept\",\n]\n\nlist_group_sd_params = [\n    \"a_1|participant_id_sigma\",\n    \"z_1|participant_id_sigma\",\n    \"theta_1|participant_id_sigma\",\n    \"alpha_1|participant_id_sigma\",\n    \"phi_1|participant_id_sigma\",\n    \"rho_1|participant_id_sigma\",\n    \"gamma_1|participant_id_sigma\",\n    \"epsilon_1|participant_id_sigma\",\n    \"C_1|participant_id_sigma\",\n    \"eta_1|participant_id_sigma\",\n]\n</pre> # Define parameter names for analysis  list_group_mean_params = [     \"a_Intercept\",     \"z_Intercept\",     \"theta_Intercept\",     \"alpha_Intercept\",     \"phi_Intercept\",     \"rho_Intercept\",     \"gamma_Intercept\",     \"epsilon_Intercept\",     \"C_Intercept\",     \"eta_Intercept\", ]  list_group_sd_params = [     \"a_1|participant_id_sigma\",     \"z_1|participant_id_sigma\",     \"theta_1|participant_id_sigma\",     \"alpha_1|participant_id_sigma\",     \"phi_1|participant_id_sigma\",     \"rho_1|participant_id_sigma\",     \"gamma_1|participant_id_sigma\",     \"epsilon_1|participant_id_sigma\",     \"C_1|participant_id_sigma\",     \"eta_1|participant_id_sigma\", ] In\u00a0[45]: Copied! <pre># plot the posterior pair plots of the group-level parameters\n# this will show the joint distributions and correlations between the group-level parameters.\naz.plot_trace(idata_mcmc, var_names=list_group_mean_params, ) # kind='kde', point_estimate='mean'\nplt.tight_layout()\n</pre> # plot the posterior pair plots of the group-level parameters # this will show the joint distributions and correlations between the group-level parameters. az.plot_trace(idata_mcmc, var_names=list_group_mean_params, ) # kind='kde', point_estimate='mean' plt.tight_layout() In\u00a0[46]: Copied! <pre># Extract ground truth subject-level parameters from the synthetic dataset\n# Reshape from dictionary format to matrix (subjects x parameters)\nsim_param_list = datafile['sim_param_list']\n</pre> # Extract ground truth subject-level parameters from the synthetic dataset # Reshape from dictionary format to matrix (subjects x parameters) sim_param_list = datafile['sim_param_list'] In\u00a0[47]: Copied! <pre>sim_param_list = sim_param_list[0:20, :]\nprint(sim_param_list.shape)\n</pre> sim_param_list = sim_param_list[0:20, :] print(sim_param_list.shape) <pre>(20, 10)\n</pre> In\u00a0[48]: Copied! <pre>idata_mcmc.posterior['alpha_Intercept'].values[0].shape\n</pre> idata_mcmc.posterior['alpha_Intercept'].values[0].shape Out[48]: <pre>(500,)</pre> In\u00a0[49]: Copied! <pre># Function to extract subject-level parameters from inference data.\ndef extract_subject_parameters(idata, param_names):\n    \n    n_subjects = idata.posterior[f'{param_names[0]}_1|participant_id'].shape[-1]\n    n_params = len(param_names)\n    \n    subject_params = np.zeros((n_subjects, n_params))\n    \n    for i, param in enumerate(param_names):\n        intercept = np.mean(idata.posterior[f'{param}_Intercept'].values[0])\n        random_effects = np.mean(idata.posterior[f'{param}_1|participant_id'].values[0], axis=0)\n        subject_params[:, i] = intercept + random_effects\n    \n    return subject_params\n\n# Extract recovered parameters\nrecov_param_list = extract_subject_parameters(idata_mcmc, model_config.list_params)\n</pre> # Function to extract subject-level parameters from inference data. def extract_subject_parameters(idata, param_names):          n_subjects = idata.posterior[f'{param_names[0]}_1|participant_id'].shape[-1]     n_params = len(param_names)          subject_params = np.zeros((n_subjects, n_params))          for i, param in enumerate(param_names):         intercept = np.mean(idata.posterior[f'{param}_Intercept'].values[0])         random_effects = np.mean(idata.posterior[f'{param}_1|participant_id'].values[0], axis=0)         subject_params[:, i] = intercept + random_effects          return subject_params  # Extract recovered parameters recov_param_list = extract_subject_parameters(idata_mcmc, model_config.list_params) In\u00a0[50]: Copied! <pre>print(recov_param_list.shape)\n</pre> print(recov_param_list.shape) <pre>(20, 10)\n</pre> In\u00a0[51]: Copied! <pre>plot_param_ranges = [[0.1, 1.5], [0, 1], [0, 1.2], [-7, -4], [0, 1], [0, 1], [0, 1], [0, 0.5], [1, 5], [0.1, 2]]\nplot_param_names = [\"a\", \"z\", \"theta\", \"alpha\", \"phi\", \"rho\", \"gamma\", \"epsilon\", \"C\", \"eta\"]\n</pre> plot_param_ranges = [[0.1, 1.5], [0, 1], [0, 1.2], [-7, -4], [0, 1], [0, 1], [0, 1], [0, 0.5], [1, 5], [0.1, 2]] plot_param_names = [\"a\", \"z\", \"theta\", \"alpha\", \"phi\", \"rho\", \"gamma\", \"epsilon\", \"C\", \"eta\"] In\u00a0[52]: Copied! <pre># Function to create parameter recovery plots comparing simulated vs recovered values\ndef plot_parameter_recovery(sim_params, recov_params, param_names, axes_limits, \n                          additional_data=None, additional_label=None, show_correlation=False):\n\n    fig, axes = plt.subplots(3, 4, figsize=(12, 8))\n    axes = axes.flatten()\n\n    for i, ax in enumerate(axes):\n        if i &gt;= len(param_names):\n            ax.set_visible(False)\n            continue\n\n        x, y = sim_params[:, i], recov_params[:, i]\n        \n        # Scatter plot showing parameter recovery \n        ax.scatter(x, y, alpha=0.6, label='MCMC' if additional_data is None else 'MCMC')\n            \n        # Additional data if provided\n        if additional_data is not None:\n            z = additional_data[:, i]\n            ax.scatter(x, z, alpha=0.6, marker='x', label=additional_label or 'Additional')\n            ax.legend(loc='lower right')\n\n        # Calculate and display correlation between true and recovered parameters\n        if show_correlation:\n            spearman_r, _ = spearmanr(x, y)\n            ax.text(0.05, 0.88, f\"R: {spearman_r:.2f}\", transform=ax.transAxes, \n                        fontsize=12, verticalalignment='bottom')\n\n        # Formatting subplot\n        ax.set_title(param_names[i], fontsize=16)\n        ax.set_xlim(axes_limits[i])\n        ax.set_ylim(axes_limits[i])\n        ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=5))\n        ax.yaxis.set_major_locator(ticker.MaxNLocator(nbins=5))\n        ax.grid(True, linestyle='--', alpha=0.6)\n        ax.axline((0, 0), linestyle='--', slope=1, c='k', alpha=0.8)  # Perfect recovery line\n\n    # Add axis labels\n    fig.text(0.5, 0.02, 'Simulated', ha='center', fontsize=20)\n    fig.text(0.02, 0.5, 'Recovered', va='center', rotation='vertical', fontsize=20)\n\n    plt.tight_layout(rect=[0.05, 0.05, 1, 1])\n    return fig\n\n# Plot parameter recovery\nplot_parameter_recovery(sim_param_list, recov_param_list, plot_param_names, plot_param_ranges, show_correlation=True)\nplt.show()\n</pre> # Function to create parameter recovery plots comparing simulated vs recovered values def plot_parameter_recovery(sim_params, recov_params, param_names, axes_limits,                            additional_data=None, additional_label=None, show_correlation=False):      fig, axes = plt.subplots(3, 4, figsize=(12, 8))     axes = axes.flatten()      for i, ax in enumerate(axes):         if i &gt;= len(param_names):             ax.set_visible(False)             continue          x, y = sim_params[:, i], recov_params[:, i]                  # Scatter plot showing parameter recovery          ax.scatter(x, y, alpha=0.6, label='MCMC' if additional_data is None else 'MCMC')                      # Additional data if provided         if additional_data is not None:             z = additional_data[:, i]             ax.scatter(x, z, alpha=0.6, marker='x', label=additional_label or 'Additional')             ax.legend(loc='lower right')          # Calculate and display correlation between true and recovered parameters         if show_correlation:             spearman_r, _ = spearmanr(x, y)             ax.text(0.05, 0.88, f\"R: {spearman_r:.2f}\", transform=ax.transAxes,                          fontsize=12, verticalalignment='bottom')          # Formatting subplot         ax.set_title(param_names[i], fontsize=16)         ax.set_xlim(axes_limits[i])         ax.set_ylim(axes_limits[i])         ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=5))         ax.yaxis.set_major_locator(ticker.MaxNLocator(nbins=5))         ax.grid(True, linestyle='--', alpha=0.6)         ax.axline((0, 0), linestyle='--', slope=1, c='k', alpha=0.8)  # Perfect recovery line      # Add axis labels     fig.text(0.5, 0.02, 'Simulated', ha='center', fontsize=20)     fig.text(0.02, 0.5, 'Recovered', va='center', rotation='vertical', fontsize=20)      plt.tight_layout(rect=[0.05, 0.05, 1, 1])     return fig  # Plot parameter recovery plot_parameter_recovery(sim_param_list, recov_param_list, plot_param_names, plot_param_ranges, show_correlation=True) plt.show() In\u00a0[\u00a0]: Copied! <pre># Run variational inference (VI) as a faster alternative to MCMC\n# VI approximates the posterior with a simpler distribution family\nidata_vi = hssm_model.vi(\n    niter=30000,    # Number of optimization iterations\n    draws=1000,     # Number of samples from approximate posterior\n    method=\"advi\"   # Automatic Differentiation Variational Inference\n)\n</pre> # Run variational inference (VI) as a faster alternative to MCMC # VI approximates the posterior with a simpler distribution family idata_vi = hssm_model.vi(     niter=30000,    # Number of optimization iterations     draws=1000,     # Number of samples from approximate posterior     method=\"advi\"   # Automatic Differentiation Variational Inference ) <p>We now examine the VI loss over iterations. In general, this looks good with the caveat that there are oscillations during the initial iterations. In principle, this could arise from the model geometry, priors or simply because of an aggressive learning rate. We recommend users experiment with different settings to figure out what works best for their model.</p> In\u00a0[\u00a0]: Copied! <pre>plt.plot(hssm_model.vi_approx.hist)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\n</pre> plt.plot(hssm_model.vi_approx.hist) plt.xlabel(\"Iteration\") plt.ylabel(\"Loss\") In\u00a0[\u00a0]: Copied! <pre>idata_vi\n</pre> idata_vi In\u00a0[\u00a0]: Copied! <pre># Extract VI recovered parameters\nrecov_param_list_vi = extract_subject_parameters(idata_vi, model_config.list_params)\n</pre> # Extract VI recovered parameters recov_param_list_vi = extract_subject_parameters(idata_vi, model_config.list_params) In\u00a0[\u00a0]: Copied! <pre># Plot comparison between MCMC and VI recovery\nplot_parameter_recovery(\n    sim_param_list, \n    recov_param_list, \n    plot_param_names, \n    plot_param_ranges,\n    additional_data=recov_param_list_vi,\n    additional_label='VI',\n    show_correlation=False\n)\nplt.show()\n</pre> # Plot comparison between MCMC and VI recovery plot_parameter_recovery(     sim_param_list,      recov_param_list,      plot_param_names,      plot_param_ranges,     additional_data=recov_param_list_vi,     additional_label='VI',     show_correlation=False ) plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/rlssm_rlwm_model/#tutorial-for-hierarchical-bayesian-inference-for-reinforcement-learning-sequential-sampling-models","title":"Tutorial for hierarchical Bayesian inference for Reinforcement Learning - Sequential Sampling Models.\u00b6","text":"<p>This is a (preview) tutorial for using the HSSM Python package to simultaneously estimate reinforcement learning parameters and decision parameters within a fully hierarchical bayesian estimation framework, including steps for constructing HSSM-compatible likelihoods/distributions and sampling from the posterior. Further, the plots to assess the recovery of model parameters are also shown.</p> <p>The module uses the reinforcement learning sequential sampling model (RLSSM), a reinforcement learning model that replaces the standard \u201csoftmax\u201d choice function with a drift diffusion process with collapsing bounds (referred to as the 'angle' model hereon). The softmax and sequential sampling process is equivalent for capturing choice proportions, but the angle model also takes RT distributions into account; options are provided to also only fit RL parameters without RT. The RLSSM estimates trial-by-trial drift rate as a scaled difference in expected rewards (expected reward for upper bound alternative minus expected reward for lower bound alternative). Expected rewards are updated with a delta learning rule using either a single learning rate or with separate learning rates for positive and negative prediction errors. The model also includes the standard angle parameters. The broader RLSSM framework is described in detail in Pedersen, Frank &amp; Biele (2017) and Fengler, Bera, Pedersen &amp; Frank (2022).</p>"},{"location":"tutorials/rlssm_rlwm_model/#load-and-prepare-the-demo-dataset","title":"Load and prepare the demo dataset\u00b6","text":"<p>This data file contains (synthetic) data from a simulated 2-armed bandit task. We examine the dataset -- it contains the typical columns that are expected from a canonical instrumental learning task. <code>participant_id</code> identifies the subject id, <code>trial</code> identifies the sequence of trials within the subject data, <code>response</code> and <code>rt</code> are the data columns recorded for each trial, <code>feedback</code> column shows the reward obtained on a given trial and <code>correct</code> records whether the response was correct.</p>"},{"location":"tutorials/rlssm_rlwm_model/#construct-hssm-compatible-pymc-distribution-from-a-simulator-and-jax-likelihood-callable","title":"Construct HSSM-compatible PyMC distribution from a simulator and JAX likelihood callable\u00b6","text":"<p>We now construct a custom model that is compatible with HSSM and PyMC. Note that HSSM internally constructs a PyMC object (which is used for sampling) based on the user-specified HSSM model. In other words, we are peeling the abstration layers conveniently afforded by HSSM to directly use the core machinery of HSSM. This advanced HSSM tutorial explains how to use HSSM when starting from the very basics of a model -- a simulator and a JAX likelihood callable.</p> <p>The simulator function is used for generating samples from the model (for posterior predictives, etc.) and the likelihood callable is employed for sampling/inference. This preview tutorial exposes the key flexibility of the HSSM for use in fitting RLSSM models. Therefore, the subsequent tutorial will focus only on the sampling/inference aspect. We create a dummy simulator function to bypass the need for defining the actual simulator.</p>"},{"location":"tutorials/rlssm_rlwm_model/#step-1-define-a-pytensor-randomvariable","title":"Step 1: Define a pytensor RandomVariable\u00b6","text":""},{"location":"tutorials/rlssm_rlwm_model/#step-2-define-a-likelihood-function","title":"Step 2: Define a likelihood function\u00b6","text":""},{"location":"tutorials/rlssm_rlwm_model/#step-3-define-a-model-config-and-hssm-model","title":"Step 3: Define a model config and HSSM model\u00b6","text":""},{"location":"tutorials/rlssm_rlwm_model/#sample-using-nuts-mcmc","title":"Sample using NUTS MCMC\u00b6","text":""},{"location":"tutorials/rlssm_rlwm_model/#assess-the-model-fits","title":"Assess the model fits\u00b6","text":"<p>We examine the quality of fits by comparing the recovered parameters with the ground-truth data generating parameters of the simulated dataset. We examine the quality of fits both at group-level as well as subject-level.</p>"},{"location":"tutorials/rlssm_rlwm_model/#examining-group-level-posteriors","title":"Examining group-level posteriors\u00b6","text":""},{"location":"tutorials/rlssm_rlwm_model/#examining-participant-level-posteriors","title":"Examining participant-level posteriors\u00b6","text":""},{"location":"tutorials/rlssm_rlwm_model/#estimating-the-posterior-using-variation-inference-vi","title":"Estimating the posterior using Variation Inference (VI)\u00b6","text":""},{"location":"tutorials/rlssm_tutorial/","title":"Tutorial for hierarchical Bayesian inference for Reinforcement Learning - Sequential Sampling Models.","text":"In\u00a0[1]: Copied! <pre># Import necessary libraries\nimport numpy as np\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom functools import partial\nfrom scipy.stats import spearmanr\n\n# Import HSSM and simulator package\nimport hssm\nfrom hssm.utils import decorate_atomic_simulator\nfrom hssm.likelihoods.rldm import make_rldm_logp_op\nfrom hssm.distribution_utils.dist import make_hssm_rv\nfrom ssms.basic_simulators.simulator import simulator\n</pre> # Import necessary libraries import numpy as np import arviz as az import matplotlib.pyplot as plt import matplotlib.ticker as ticker from functools import partial from scipy.stats import spearmanr  # Import HSSM and simulator package import hssm from hssm.utils import decorate_atomic_simulator from hssm.likelihoods.rldm import make_rldm_logp_op from hssm.distribution_utils.dist import make_hssm_rv from ssms.basic_simulators.simulator import simulator In\u00a0[2]: Copied! <pre># Set the style for the plots\nplt.style.use('seaborn-v0_8-dark-palette')\n</pre> # Set the style for the plots plt.style.use('seaborn-v0_8-dark-palette') In\u00a0[3]: Copied! <pre># Load synthetic RLSSM dataset containing both behavioral data and ground truth parameters\nsavefile = np.load(\"../../tests/fixtures/rldm_data.npy\", allow_pickle=True).item()\ndataset = savefile['data']\n\n# Rename trial column to match HSSM conventions\ndataset.rename(columns={'trial': 'trial_id'}, inplace=True)\n\n# Examine the dataset structure\ndataset.head()\n</pre> # Load synthetic RLSSM dataset containing both behavioral data and ground truth parameters savefile = np.load(\"../../tests/fixtures/rldm_data.npy\", allow_pickle=True).item() dataset = savefile['data']  # Rename trial column to match HSSM conventions dataset.rename(columns={'trial': 'trial_id'}, inplace=True)  # Examine the dataset structure dataset.head() Out[3]: participant_id trial_id response rt feedback correct 0 0 0 0.0 0.935602 0.126686 0.0 1 0 1 0.0 1.114379 0.173100 0.0 2 0 2 0.0 0.564311 0.444935 0.0 3 0 3 0.0 2.885860 0.307207 0.0 4 0 4 0.0 0.532113 0.177911 0.0 In\u00a0[4]: Copied! <pre># Validate data structure and extract dataset configuration \ndataset, n_participants, n_trials = hssm.check_data_for_rl(dataset)\n\nprint(f\"Number of participants: {n_participants}\")\nprint(f\"Number of trials: {n_trials}\")\n</pre> # Validate data structure and extract dataset configuration  dataset, n_participants, n_trials = hssm.check_data_for_rl(dataset)  print(f\"Number of participants: {n_participants}\") print(f\"Number of trials: {n_trials}\") <pre>Number of participants: 20\nNumber of trials: 200\n</pre> In\u00a0[5]: Copied! <pre># Define parameters for the RLSSM model (RL + decision model parameters)\nlist_params = ['rl.alpha', 'scaler', 'a', 'z', 't', 'theta']\n\n# Create a dummy simulator for generating synthetic data (used for posterior predictives)\n# This bypasses the need for a full RLSSM simulator implementation\ndef create_dummy_simulator():\n    \"\"\"Create a dummy simulator function for RLSSM model.\"\"\"\n    def sim_wrapper(simulator_fun, theta, model, n_samples, random_state, **kwargs):\n        # Generate random RT and choice data as placeholders\n        sim_rt = np.random.uniform(0.2, 0.6, n_samples)\n        sim_ch = np.random.randint(0, 2, n_samples)\n        \n        return np.column_stack([sim_rt, sim_ch])\n\n    # Wrap the simulator function with required metadata\n    wrapped_simulator = partial(sim_wrapper, simulator_fun=simulator, model=\"custom\", n_samples=1)\n\n    # Decorate the simulator to make it compatible with HSSM\n    return decorate_atomic_simulator(model_name=\"custom\", choices=[0, 1], obs_dim=2)(wrapped_simulator)\n\n# Create the simulator and RandomVariable\ndecorated_simulator = create_dummy_simulator()\n\n# Create a PyTensor RandomVariable using `make_hssm_rv` for use in the PyMC model\nCustomRV = make_hssm_rv(\n    simulator_fun=decorated_simulator, list_params=list_params\n)\n</pre> # Define parameters for the RLSSM model (RL + decision model parameters) list_params = ['rl.alpha', 'scaler', 'a', 'z', 't', 'theta']  # Create a dummy simulator for generating synthetic data (used for posterior predictives) # This bypasses the need for a full RLSSM simulator implementation def create_dummy_simulator():     \"\"\"Create a dummy simulator function for RLSSM model.\"\"\"     def sim_wrapper(simulator_fun, theta, model, n_samples, random_state, **kwargs):         # Generate random RT and choice data as placeholders         sim_rt = np.random.uniform(0.2, 0.6, n_samples)         sim_ch = np.random.randint(0, 2, n_samples)                  return np.column_stack([sim_rt, sim_ch])      # Wrap the simulator function with required metadata     wrapped_simulator = partial(sim_wrapper, simulator_fun=simulator, model=\"custom\", n_samples=1)      # Decorate the simulator to make it compatible with HSSM     return decorate_atomic_simulator(model_name=\"custom\", choices=[0, 1], obs_dim=2)(wrapped_simulator)  # Create the simulator and RandomVariable decorated_simulator = create_dummy_simulator()  # Create a PyTensor RandomVariable using `make_hssm_rv` for use in the PyMC model CustomRV = make_hssm_rv(     simulator_fun=decorated_simulator, list_params=list_params ) In\u00a0[6]: Copied! <pre># Create a Pytensor Op for the likelihood function.\n# The `make_rldm_logp_op` function is a utility that wraps the base JAX likelihood function into a HSSM/PyMC-compatible callable.\n\nlogp_jax_op = make_rldm_logp_op(\n    n_participants=n_participants,\n    n_trials=n_trials,\n    n_params=6 # the number of parameters in the RLSSM model. Should match the length of `list_params`\n)\n</pre> # Create a Pytensor Op for the likelihood function. # The `make_rldm_logp_op` function is a utility that wraps the base JAX likelihood function into a HSSM/PyMC-compatible callable.  logp_jax_op = make_rldm_logp_op(     n_participants=n_participants,     n_trials=n_trials,     n_params=6 # the number of parameters in the RLSSM model. Should match the length of `list_params` ) In\u00a0[7]: Copied! <pre># Test the likelihood function by evaluating it with dummy parameters and data extracted from the dataset.\n\ndef extract_data_columns(dataset):\n    \"\"\"Extract required data columns from dataset.\"\"\"\n    return {\n        'participant_id': dataset[\"participant_id\"].values,\n        'trial': dataset[\"trial_id\"].values,\n        'response': dataset[\"response\"].values,\n        'feedback': dataset[\"feedback\"].values,\n        'rt': dataset[\"rt\"].values\n    }\n\ndef create_test_parameters(n_trials):\n    \"\"\"Create dummy parameters for testing the likelihood function.\"\"\"\n    return {\n        'rl_alpha': np.ones(n_trials) * 0.60,\n        'scaler': np.ones(n_trials) * 3.2,\n        'a': np.ones(n_trials) * 1.2,\n        'z': np.ones(n_trials) * 0.1,\n        't': np.ones(n_trials) * 0.1,\n        'theta': np.ones(n_trials) * 0.1\n}\n\n# Extract data and create test parameters\ndata_columns = extract_data_columns(dataset)\nnum_subj = len(np.unique(data_columns['participant_id']))\nn_trials_total = num_subj * 200\n\ntest_params = create_test_parameters(n_trials_total)\n\n# Evaluate the likelihood function\ntest_logp_out = logp_jax_op(\n    np.column_stack((data_columns['rt'], data_columns['response'])),\n    test_params['rl_alpha'],\n    test_params['scaler'],\n    test_params['a'], \n    test_params['z'],\n    test_params['t'],\n    test_params['theta'],\n    data_columns['participant_id'],\n    data_columns['trial'],\n    data_columns['feedback'],\n)\n\nLL = test_logp_out.eval()\nprint(f\"Log likelihood: {np.sum(LL):.4f}\")\n</pre> # Test the likelihood function by evaluating it with dummy parameters and data extracted from the dataset.  def extract_data_columns(dataset):     \"\"\"Extract required data columns from dataset.\"\"\"     return {         'participant_id': dataset[\"participant_id\"].values,         'trial': dataset[\"trial_id\"].values,         'response': dataset[\"response\"].values,         'feedback': dataset[\"feedback\"].values,         'rt': dataset[\"rt\"].values     }  def create_test_parameters(n_trials):     \"\"\"Create dummy parameters for testing the likelihood function.\"\"\"     return {         'rl_alpha': np.ones(n_trials) * 0.60,         'scaler': np.ones(n_trials) * 3.2,         'a': np.ones(n_trials) * 1.2,         'z': np.ones(n_trials) * 0.1,         't': np.ones(n_trials) * 0.1,         'theta': np.ones(n_trials) * 0.1 }  # Extract data and create test parameters data_columns = extract_data_columns(dataset) num_subj = len(np.unique(data_columns['participant_id'])) n_trials_total = num_subj * 200  test_params = create_test_parameters(n_trials_total)  # Evaluate the likelihood function test_logp_out = logp_jax_op(     np.column_stack((data_columns['rt'], data_columns['response'])),     test_params['rl_alpha'],     test_params['scaler'],     test_params['a'],      test_params['z'],     test_params['t'],     test_params['theta'],     data_columns['participant_id'],     data_columns['trial'],     data_columns['feedback'], )  LL = test_logp_out.eval() print(f\"Log likelihood: {np.sum(LL):.4f}\") <pre>Log likelihood: -6879.1526\n</pre> In\u00a0[8]: Copied! <pre># Step 3: Define the model config\n\n# Configure the HSSM model \nmodel_config = hssm.ModelConfig(\n    response=[\"rt\", \"response\"],        # Dependent variables (RT and choice)\n    list_params=                        # List of model parameters\n        ['rl.alpha', 'scaler', 'a', 'z', 't', 'theta'],            \n    choices=[0, 1],                     # Possible choice options\n    default_priors={},                  # Use custom priors (defined below)\n    bounds=dict(                        # Parameter bounds for optimization\n        rl_alpha=(0.01, 1),             # Learning rate bounds\n        scaler=(1, 4),                  # Scaler bounds\n        a=(0.3, 2.5),                   # Boundary separation bounds\n        z=(0.1, 0.9),                   # Bias bounds\n        t=(0.1, 2.0),                   # Non-decision time bounds\n        theta=(0.0, 1.2)                # Collapse rate bounds\n        ),\n    rv=CustomRV,                        # Custom RandomVariable that we created earlier\n    extra_fields=[                      # Additional data columns to be passed to the likelihood function as extra_fields\n        \"participant_id\", \n        \"trial_id\", \n        \"feedback\"],  \n    backend=\"jax\"                       # Use JAX for computation\n)\n</pre> # Step 3: Define the model config  # Configure the HSSM model  model_config = hssm.ModelConfig(     response=[\"rt\", \"response\"],        # Dependent variables (RT and choice)     list_params=                        # List of model parameters         ['rl.alpha', 'scaler', 'a', 'z', 't', 'theta'],                 choices=[0, 1],                     # Possible choice options     default_priors={},                  # Use custom priors (defined below)     bounds=dict(                        # Parameter bounds for optimization         rl_alpha=(0.01, 1),             # Learning rate bounds         scaler=(1, 4),                  # Scaler bounds         a=(0.3, 2.5),                   # Boundary separation bounds         z=(0.1, 0.9),                   # Bias bounds         t=(0.1, 2.0),                   # Non-decision time bounds         theta=(0.0, 1.2)                # Collapse rate bounds         ),     rv=CustomRV,                        # Custom RandomVariable that we created earlier     extra_fields=[                      # Additional data columns to be passed to the likelihood function as extra_fields         \"participant_id\",          \"trial_id\",          \"feedback\"],       backend=\"jax\"                       # Use JAX for computation ) In\u00a0[9]: Copied! <pre># Create a hierarchical HSSM model with custom likelihood function\nhssm_model = hssm.HSSM(\n    data=dataset,                        # Input dataset\n    model_config=model_config,           # Model configuration\n    p_outlier=0,                         # No outlier modeling\n    lapse=None,                          # No lapse rate modeling\n    loglik=logp_jax_op,                  # Custom RLDM likelihood function\n    loglik_kind=\"approx_differentiable\", # Use approximate gradients\n    noncentered=True,                    # Use non-centered parameterization\n    process_initvals=False,              # Skip initial value processing in HSSM\n    include=[\n        # Define hierarchical priors: group-level intercepts + subject-level random effects\n        hssm.Param(\"rl.alpha\", \n                formula=\"rl_alpha ~ 1 + (1|participant_id)\", \n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.01, upper=1, mu=0.3)}),\n        hssm.Param(\"scaler\", \n                formula=\"scaler ~ 1 + (1|participant_id)\", \n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=1, upper=4, mu=1.5)}),\n        hssm.Param(\"a\", \n                formula=\"a ~ 1 + (1|participant_id)\", \n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.3, upper=2.5, mu=1.0)}),\n        hssm.Param(\"z\", \n                formula=\"z ~ 1 + (1|participant_id)\", \n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.1, upper=0.9, mu=0.2)}),\n        hssm.Param(\"t\", \n                formula=\"t ~ 1 + (1|participant_id)\", \n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.01, upper=2, mu=0.2, initval=0.1)}),\n        hssm.Param(\"theta\", \n                formula=\"theta ~ 1 + (1|participant_id)\", \n                prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.00, upper=1.2, mu=0.3)}),\n    ]\n)\n</pre> # Create a hierarchical HSSM model with custom likelihood function hssm_model = hssm.HSSM(     data=dataset,                        # Input dataset     model_config=model_config,           # Model configuration     p_outlier=0,                         # No outlier modeling     lapse=None,                          # No lapse rate modeling     loglik=logp_jax_op,                  # Custom RLDM likelihood function     loglik_kind=\"approx_differentiable\", # Use approximate gradients     noncentered=True,                    # Use non-centered parameterization     process_initvals=False,              # Skip initial value processing in HSSM     include=[         # Define hierarchical priors: group-level intercepts + subject-level random effects         hssm.Param(\"rl.alpha\",                  formula=\"rl_alpha ~ 1 + (1|participant_id)\",                  prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.01, upper=1, mu=0.3)}),         hssm.Param(\"scaler\",                  formula=\"scaler ~ 1 + (1|participant_id)\",                  prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=1, upper=4, mu=1.5)}),         hssm.Param(\"a\",                  formula=\"a ~ 1 + (1|participant_id)\",                  prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.3, upper=2.5, mu=1.0)}),         hssm.Param(\"z\",                  formula=\"z ~ 1 + (1|participant_id)\",                  prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.1, upper=0.9, mu=0.2)}),         hssm.Param(\"t\",                  formula=\"t ~ 1 + (1|participant_id)\",                  prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.01, upper=2, mu=0.2, initval=0.1)}),         hssm.Param(\"theta\",                  formula=\"theta ~ 1 + (1|participant_id)\",                  prior={\"Intercept\": hssm.Prior(\"TruncatedNormal\", lower=0.00, upper=1.2, mu=0.3)}),     ] ) <pre>No common intercept. Bounds for parameter rl.alpha is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter scaler is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter a is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter z is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter t is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter theta is not applied due to a current limitation of Bambi. This will change in the future.\nModel initialized successfully.\n</pre> In\u00a0[10]: Copied! <pre># Run MCMC sampling using NUTS sampler with JAX backend\n# Note: Using small number of samples for demonstration (increase for real analysis)\nidata_mcmc = hssm_model.sample(\n    sampler='nuts_numpyro',  # JAX-based NUTS sampler for efficiency\n    chains=1,                # Number of parallel chains\n    draws=1000,                # Number of posterior samples\n    tune=1000,                 # Number of tuning/warmup samples\n)\n</pre> # Run MCMC sampling using NUTS sampler with JAX backend # Note: Using small number of samples for demonstration (increase for real analysis) idata_mcmc = hssm_model.sample(     sampler='nuts_numpyro',  # JAX-based NUTS sampler for efficiency     chains=1,                # Number of parallel chains     draws=1000,                # Number of posterior samples     tune=1000,                 # Number of tuning/warmup samples ) <pre>Using default initvals. \n\n</pre> <pre>sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [08:03&lt;00:00,  4.14it/s, 31 steps of size 1.50e-01. acc. prob=0.91]   \nOnly one chain was sampled, this makes it impossible to run some convergence checks\n/Users/krishnbera/Documents/revert_rldm/HSSM/.venv/lib/python3.12/site-packages/pymc/pytensorf.py:958: FutureWarning: compile_pymc was renamed to compile. Old name will be removed in a future release of PyMC\n  warnings.warn(\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:02&lt;00:00, 374.33it/s]\n</pre> In\u00a0[11]: Copied! <pre>idata_mcmc\n</pre> idata_mcmc Out[11]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 2MB\nDimensions:                                (chain: 1, draw: 1000,\n                                            participant_id__factor_dim: 20,\n                                            rl.alpha_1|participant_id__factor_dim: 20)\nCoordinates:\n  * chain                                  (chain) int64 8B 0\n  * draw                                   (draw) int64 8kB 0 1 2 ... 998 999\n  * rl.alpha_1|participant_id__factor_dim  (rl.alpha_1|participant_id__factor_dim) &lt;U2 160B ...\n  * participant_id__factor_dim             (participant_id__factor_dim) &lt;U2 160B ...\nData variables: (12/30)\n    a_1|participant_id                     (chain, draw, participant_id__factor_dim) float64 160kB ...\n    rl.alpha_1|participant_id_offset       (chain, draw, rl.alpha_1|participant_id__factor_dim) float64 160kB ...\n    a_1|participant_id_offset              (chain, draw, participant_id__factor_dim) float64 160kB ...\n    rl.alpha_Intercept                     (chain, draw) float64 8kB 0.6899 ....\n    t_1|participant_id                     (chain, draw, participant_id__factor_dim) float64 160kB ...\n    t_1|participant_id_mu                  (chain, draw) float64 8kB 0.196 .....\n    ...                                     ...\n    z_1|participant_id_mu                  (chain, draw) float64 8kB 0.1362 ....\n    scaler_1|participant_id                (chain, draw, participant_id__factor_dim) float64 160kB ...\n    theta_1|participant_id_mu              (chain, draw) float64 8kB -0.173 ....\n    a_1|participant_id_mu                  (chain, draw) float64 8kB -0.5176 ...\n    t_1|participant_id_offset              (chain, draw, participant_id__factor_dim) float64 160kB ...\n    rl.alpha_1|participant_id              (chain, draw, rl.alpha_1|participant_id__factor_dim) float64 160kB ...\nAttributes:\n    created_at:                  2025-07-20T18:47:18.671108+00:00\n    arviz_version:               0.22.0\n    inference_library:           numpyro\n    inference_library_version:   0.18.0\n    sampling_time:               487.226472\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 1000</li><li>participant_id__factor_dim: 20</li><li>rl.alpha_1|participant_id__factor_dim: 20</li></ul></li><li>Coordinates: (4)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li><li>rl.alpha_1|participant_id__factor_dim(rl.alpha_1|participant_id__factor_dim)&lt;U2'0' '1' '2' '3' ... '17' '18' '19'<pre>array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n       '13', '14', '15', '16', '17', '18', '19'], dtype='&lt;U2')</pre></li><li>participant_id__factor_dim(participant_id__factor_dim)&lt;U2'0' '1' '2' '3' ... '17' '18' '19'<pre>array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n       '13', '14', '15', '16', '17', '18', '19'], dtype='&lt;U2')</pre></li></ul></li><li>Data variables: (30)<ul><li>a_1|participant_id(chain, draw, participant_id__factor_dim)float640.01861 -0.03705 ... -0.001481<pre>array([[[ 0.01861116, -0.0370549 ,  0.05113337, ...,  0.00134526,\n         -0.03787274,  0.00410579],\n        [ 0.03764668, -0.01605418,  0.02394244, ..., -0.0511902 ,\n          0.00325148, -0.00244049],\n        [-0.02502225, -0.04072418, -0.05736606, ...,  0.13634183,\n         -0.02686471, -0.01010333],\n        ...,\n        [ 0.02532685,  0.03414944, -0.01610708, ...,  0.02550458,\n         -0.00364289,  0.01390091],\n        [-0.0018852 , -0.01567338, -0.04393237, ...,  0.00734417,\n         -0.04355092, -0.00946759],\n        [-0.01590274,  0.00183839, -0.00138223, ...,  0.00663717,\n         -0.01153597, -0.00148096]]], shape=(1, 1000, 20))</pre></li><li>rl.alpha_1|participant_id_offset(chain, draw, rl.alpha_1|participant_id__factor_dim)float64-0.5592 -0.3717 ... 1.836 0.5258<pre>array([[[-0.55918923, -0.3717073 , -0.46217591, ...,  1.23284732,\n         -0.44826054, -0.51520984],\n        [ 1.47309483,  1.25882103,  0.31191628, ...,  0.4948667 ,\n         -0.6700568 ,  0.09549374],\n        [-0.48501988, -1.49118515, -0.87832149, ..., -0.12829999,\n          1.02311319, -0.44834817],\n        ...,\n        [ 0.68438526,  0.86251767,  0.78593285, ..., -0.34975522,\n         -0.06857723,  0.606667  ],\n        [ 1.0045533 ,  1.05322145, -1.25482706, ...,  1.27541835,\n         -0.34576813, -1.10765115],\n        [ 0.83184682,  0.28017212, -0.45385096, ...,  1.10443057,\n          1.83586669,  0.5258484 ]]], shape=(1, 1000, 20))</pre></li><li>a_1|participant_id_offset(chain, draw, participant_id__factor_dim)float640.4218 -0.8398 ... -1.023 -0.1313<pre>array([[[ 0.42178939, -0.83978466,  1.15884876, ...,  0.03048807,\n         -0.85831966,  0.09305047],\n        [ 1.21950185, -0.5200486 ,  0.77557559, ..., -1.65822146,\n          0.10532626, -0.07905578],\n        [-0.30216839, -0.49178476, -0.6927518 , ...,  1.64646225,\n         -0.32441793, -0.12200764],\n        ...,\n        [ 0.96738657,  1.30437511, -0.6152273 , ...,  0.9741753 ,\n         -0.13914416,  0.5309603 ],\n        [-0.04457356, -0.37057987, -1.03873268, ...,  0.17364478,\n         -1.02971357, -0.22385072],\n        [-1.40967529,  0.16296148, -0.12252562, ...,  0.58834259,\n         -1.02258882, -0.13127729]]], shape=(1, 1000, 20))</pre></li><li>rl.alpha_Intercept(chain, draw)float640.6899 0.7054 ... 0.6841 0.6748<pre>array([[0.6898689 , 0.70542156, 0.65171418, 0.66838756, 0.65658813,\n        0.65177559, 0.72622803, 0.67990515, 0.72189628, 0.62581855,\n        0.68801917, 0.67451444, 0.70435296, 0.69329708, 0.69185202,\n        0.67595306, 0.69969936, 0.73200454, 0.74325545, 0.68549869,\n        0.69254085, 0.73644601, 0.7373668 , 0.66644881, 0.71613001,\n        0.64791645, 0.68776339, 0.7241337 , 0.65783728, 0.7012908 ,\n        0.69638739, 0.70909527, 0.69254451, 0.69360227, 0.72712847,\n        0.64533525, 0.68340874, 0.68947526, 0.73307904, 0.68377227,\n        0.69922135, 0.73401541, 0.71636398, 0.69064717, 0.70713315,\n        0.67203324, 0.69880006, 0.70902208, 0.66413729, 0.72952476,\n        0.66532925, 0.733624  , 0.70517142, 0.67106909, 0.74339662,\n        0.70647598, 0.70579719, 0.70062407, 0.72885545, 0.62737947,\n        0.76648488, 0.70678841, 0.75726143, 0.75848997, 0.6754042 ,\n        0.69678997, 0.6781504 , 0.70111434, 0.65976788, 0.71802072,\n        0.70672999, 0.64475587, 0.69780608, 0.73463199, 0.63149922,\n        0.72060412, 0.71335252, 0.73583886, 0.7527452 , 0.65190949,\n        0.68735256, 0.69127474, 0.69056938, 0.70217423, 0.68454464,\n        0.71437331, 0.73697502, 0.6539357 , 0.71365423, 0.74140236,\n        0.67443544, 0.70059813, 0.77868131, 0.6903235 , 0.70638484,\n        0.7311553 , 0.66205823, 0.69687923, 0.69331518, 0.66287576,\n...\n        0.6472951 , 0.7378786 , 0.63516211, 0.73683893, 0.7251086 ,\n        0.65918101, 0.69582391, 0.69181793, 0.71022926, 0.70213131,\n        0.70054131, 0.7225495 , 0.71561971, 0.66492906, 0.65813566,\n        0.71296007, 0.66355454, 0.68375619, 0.7523662 , 0.66482155,\n        0.72789969, 0.68179192, 0.62558022, 0.73305377, 0.70890249,\n        0.80300564, 0.689709  , 0.7029444 , 0.69456586, 0.72283419,\n        0.65819227, 0.71260509, 0.71427124, 0.68933882, 0.64635354,\n        0.70877979, 0.67072222, 0.73769633, 0.67815274, 0.72889348,\n        0.6853009 , 0.74886452, 0.73118094, 0.76630548, 0.7419709 ,\n        0.78274597, 0.75485473, 0.66738747, 0.81778025, 0.58557141,\n        0.76011266, 0.76175669, 0.67276165, 0.78034184, 0.74265782,\n        0.73500505, 0.73804984, 0.67442811, 0.7355781 , 0.6905038 ,\n        0.69994215, 0.72520935, 0.68608403, 0.74891734, 0.72661061,\n        0.7277246 , 0.68697253, 0.69920151, 0.71315704, 0.73261662,\n        0.71273019, 0.75207654, 0.76696198, 0.71023923, 0.79045961,\n        0.75104594, 0.74589814, 0.66949989, 0.71746961, 0.63476494,\n        0.71981846, 0.67671173, 0.68590756, 0.68958465, 0.68495845,\n        0.73512355, 0.67928838, 0.69658996, 0.70119733, 0.70509999,\n        0.72086974, 0.73830959, 0.68716778, 0.71750045, 0.6994412 ,\n        0.64067542, 0.70147475, 0.7105578 , 0.6840505 , 0.67477865]])</pre></li><li>t_1|participant_id(chain, draw, participant_id__factor_dim)float64-0.0008976 -0.004043 ... -0.0008195<pre>array([[[-0.00089757, -0.00404258, -0.01190943, ..., -0.00382649,\n         -0.00722686, -0.00838207],\n        [-0.0073719 , -0.00047155, -0.00731944, ..., -0.00248009,\n         -0.00664574,  0.00210665],\n        [ 0.00431957,  0.00162841, -0.00025765, ...,  0.0127741 ,\n          0.00074   , -0.0054829 ],\n        ...,\n        [ 0.00321602, -0.00034532, -0.00447669, ..., -0.00077426,\n          0.00308164,  0.00257446],\n        [-0.01117376,  0.00264887, -0.01365607, ...,  0.00832442,\n          0.01209843, -0.01051081],\n        [ 0.00100116, -0.01339445,  0.0078637 , ...,  0.00604669,\n         -0.00112873, -0.0008195 ]]], shape=(1, 1000, 20))</pre></li><li>t_1|participant_id_mu(chain, draw)float640.196 -0.3898 ... 0.3054 -0.1098<pre>array([[ 1.95991737e-01, -3.89762566e-01,  3.91052689e-01,\n         3.23917728e-01,  1.76983297e-01, -2.97546485e-02,\n         4.44182934e-02,  1.12885476e-01, -8.10624892e-02,\n         1.05842143e-01,  2.26686282e-01, -8.51281713e-02,\n        -4.04971887e-01,  3.54669722e-01,  3.81379795e-02,\n        -1.37277596e-01, -2.14381429e-02,  1.70590406e-01,\n         3.33205724e-01, -3.89163358e-01,  3.04742640e-01,\n        -1.33422906e-01, -2.30789723e-01,  3.25558334e-01,\n        -2.13812877e-01,  7.04299818e-02,  1.48707031e-01,\n        -2.21590928e-01,  2.74273437e-01,  1.99518755e-01,\n         8.75452985e-02, -1.97739923e-01,  1.60598874e-01,\n        -1.47048119e-01,  1.32071284e-01, -5.37094849e-02,\n        -1.28223811e-01, -2.08665506e-01,  3.27772752e-01,\n         5.85008982e-01, -5.60725643e-01,  5.63820331e-01,\n         6.54165590e-01,  2.27500854e-01,  1.17912360e-01,\n        -7.04324207e-02,  1.33277258e-01,  2.59093222e-01,\n        -5.88988523e-01,  4.70133445e-01,  1.03252406e-01,\n        -9.72211322e-02, -2.70843698e-01, -8.64122752e-02,\n         2.21784657e-01, -1.35420421e-01,  1.34148500e-01,\n        -3.14638565e-01,  3.27521560e-01, -1.72846155e-01,\n...\n        -2.52009036e-01,  2.35788967e-01,  3.30862999e-01,\n         1.19591055e-01,  2.39252970e-01,  6.08340573e-02,\n        -6.99339426e-02,  1.24826620e-01, -3.94750949e-01,\n        -1.70326719e-01,  9.87858123e-02, -9.65097875e-02,\n        -2.68289032e-02,  1.97646389e-02,  1.31084646e-01,\n        -1.42072872e-01,  7.43752487e-02,  1.82092756e-01,\n        -5.96984720e-01,  6.85219549e-01, -8.16761924e-01,\n         6.73889668e-01, -1.03552244e-01,  2.02974453e-01,\n         1.29718959e-01, -1.34111103e-01, -1.02371994e-01,\n        -2.69540143e-01, -4.31373723e-01, -2.83374740e-01,\n        -1.88238709e-01,  2.12373918e-02, -1.02250000e-01,\n         1.20952785e-01,  2.74791407e-01, -2.73930230e-01,\n         3.36407036e-01, -2.77306259e-01,  2.09542719e-01,\n        -1.10976045e-01, -6.25334924e-02, -4.92360620e-02,\n        -1.52365802e-01, -4.29025952e-01,  4.45981432e-01,\n        -4.54422445e-01, -4.77159283e-01, -9.28862499e-02,\n        -1.72323792e-01, -1.64971242e-01,  1.43838804e-02,\n        -1.89214111e-02, -1.08715283e-01,  4.23504614e-01,\n         1.90477476e-01, -1.94492731e-01,  3.05386437e-01,\n        -1.09781595e-01]])</pre></li><li>scaler_1|participant_id_sigma(chain, draw)float640.1315 0.08893 ... 0.06364 0.06715<pre>array([[0.13152475, 0.08893392, 0.11892824, 0.14307341, 0.11882414,\n        0.16143191, 0.09105589, 0.19045667, 0.1037298 , 0.23605449,\n        0.09817969, 0.15089925, 0.09931196, 0.13229316, 0.1729546 ,\n        0.06656953, 0.03639988, 0.10225504, 0.08410208, 0.11105679,\n        0.10022816, 0.10623643, 0.06332121, 0.0493169 , 0.0577635 ,\n        0.06369923, 0.11171113, 0.0606922 , 0.05937798, 0.16636507,\n        0.16218031, 0.15925944, 0.06063186, 0.11671286, 0.23759442,\n        0.13254745, 0.13718487, 0.15763507, 0.10899923, 0.1286298 ,\n        0.18094278, 0.11205839, 0.10118102, 0.15402255, 0.15732663,\n        0.17883486, 0.18811008, 0.15512371, 0.10590606, 0.16630638,\n        0.07885293, 0.08198613, 0.11216544, 0.10938287, 0.1754932 ,\n        0.12638839, 0.18623635, 0.15548327, 0.05666933, 0.21979413,\n        0.15629362, 0.11839033, 0.07381183, 0.19732864, 0.02258193,\n        0.05856287, 0.12032132, 0.19937223, 0.20076303, 0.17317696,\n        0.17392451, 0.14275275, 0.11144417, 0.03820725, 0.16409934,\n        0.09538243, 0.10300519, 0.15226989, 0.19589947, 0.14808793,\n        0.1240025 , 0.1739936 , 0.14032688, 0.13470514, 0.14489796,\n        0.13809644, 0.14717577, 0.12518115, 0.14345363, 0.10793239,\n        0.14881606, 0.14643411, 0.05755984, 0.09945329, 0.2430928 ,\n        0.1562313 , 0.19983562, 0.10424305, 0.07716125, 0.04961602,\n...\n        0.16650105, 0.20388456, 0.15443073, 0.18389906, 0.13549287,\n        0.15511236, 0.08289772, 0.09097284, 0.12020086, 0.11702985,\n        0.09559883, 0.13624607, 0.18278015, 0.18021473, 0.14185958,\n        0.11738347, 0.14617404, 0.12000788, 0.15710719, 0.18492311,\n        0.27426535, 0.16938677, 0.13444336, 0.15720513, 0.05247771,\n        0.06336135, 0.06398583, 0.02742354, 0.07976396, 0.14279692,\n        0.09739384, 0.07697044, 0.12678193, 0.25762659, 0.22595339,\n        0.14160979, 0.11501334, 0.1519597 , 0.12248305, 0.24730286,\n        0.17089922, 0.17191705, 0.16108949, 0.10246795, 0.11300791,\n        0.10570788, 0.214999  , 0.08780664, 0.0805969 , 0.1304894 ,\n        0.10251601, 0.20471619, 0.1090079 , 0.06387516, 0.05356273,\n        0.1106315 , 0.00798266, 0.0703945 , 0.13064387, 0.05458254,\n        0.10162704, 0.15064265, 0.14697127, 0.17507961, 0.29068558,\n        0.16383813, 0.12829969, 0.32271037, 0.20638939, 0.16922809,\n        0.11342382, 0.17000665, 0.16006245, 0.10364142, 0.17394257,\n        0.14963017, 0.10207802, 0.14413732, 0.1269364 , 0.04339691,\n        0.10878018, 0.04859782, 0.11433933, 0.28526037, 0.17155577,\n        0.18183472, 0.14482887, 0.17370141, 0.18348679, 0.15320766,\n        0.18014052, 0.15919676, 0.16224068, 0.10278698, 0.09159357,\n        0.13997663, 0.16030249, 0.14415664, 0.06364325, 0.06714581]])</pre></li><li>rl.alpha_1|participant_id_mu(chain, draw)float64-0.1259 -0.05456 ... 0.4473<pre>array([[-0.12588838, -0.05455911,  0.05287671, -0.11359861,  0.17250876,\n         0.11144056, -0.1098533 ,  0.15499479, -0.30497033,  0.30418483,\n        -0.14854143, -0.18771712, -0.41621022,  0.44935915, -0.3214163 ,\n        -0.13417248,  0.14019467, -0.23561849,  0.13093826,  0.00779994,\n        -0.01275652,  0.15222189, -0.28095513,  0.2797647 , -0.03516408,\n        -0.21235425, -0.41735281,  0.02495916,  0.00745742,  0.36006582,\n        -0.3506    ,  0.36993076, -0.34117075,  0.23598363, -0.13800864,\n         0.2322351 , -0.34923033,  0.40187826, -0.42562541, -0.17983876,\n         0.18016038, -0.09878536, -0.23605116,  0.39908264, -0.35473202,\n         0.37125248, -0.25875352,  0.020193  , -0.40805188,  0.44421027,\n         0.5918608 , -0.58440791,  0.01548097, -0.15678098,  0.25484931,\n        -0.25116459,  0.22054568,  0.16003159, -0.2496668 , -0.22621718,\n         0.38370277, -0.06335472, -0.22674465, -0.01250476, -0.05066011,\n         0.17418953,  0.04900671,  0.27377994,  0.43140394, -0.09696102,\n         0.10276504,  0.1210555 , -0.13745726, -0.34540194,  0.01287204,\n        -0.25878065, -0.05068736,  0.1874874 , -0.03788357,  0.04110062,\n        -0.15060699,  0.384563  , -0.11142678, -0.18405931,  0.17975916,\n        -0.03888342, -0.28772372,  0.0989687 ,  0.00640652,  0.05919624,\n         0.28866152, -0.23997639,  0.0600526 , -0.64707064,  0.51402663,\n        -0.51421356,  0.49469043, -0.18647901, -0.0531964 ,  0.23035564,\n...\n         0.36588451, -0.68099401,  0.42394817, -0.26426929,  0.1065193 ,\n         0.08744543, -0.08706805, -0.06718841,  0.28023452, -0.32566627,\n         0.09640911,  0.08443912,  0.02482141,  0.16605373, -0.30784704,\n         0.39416422, -0.33137224,  0.35095377, -0.41883774,  0.43099812,\n        -0.03942516, -0.04776935,  0.28054599, -0.4171443 ,  0.32555331,\n        -0.53278604,  0.49454173, -0.23808922,  0.21360407, -0.27138704,\n         0.54065361, -0.12783797,  0.1280389 ,  0.13050833,  0.3197397 ,\n        -0.5183579 ,  0.30250636, -0.3365684 ,  0.33318544, -0.19360783,\n        -0.00339955,  0.18399506,  0.01217869, -0.09659137,  0.25209324,\n         0.00440586,  0.13246508, -0.24720599,  0.20345008, -0.22564103,\n        -0.09466111, -0.05511237,  0.00278511,  0.00878857, -0.08361982,\n         0.14565925, -0.27183387,  0.22887953, -0.10042411,  0.08015172,\n        -0.02167547,  0.19064657, -0.239255  ,  0.54865442, -0.31500917,\n         0.21970082, -0.39403416,  0.31822297, -0.05594442,  0.03410835,\n         0.41947986,  0.36164553,  0.05305817, -0.11020641,  0.14166072,\n        -0.14618032,  0.06156506, -0.08240721,  0.18452713,  0.00674023,\n        -0.08978149,  0.10678519,  0.13874385, -0.14517344,  0.08869926,\n        -0.728774  ,  0.71601417, -0.62845754, -0.48134336,  0.10885605,\n         0.09093716,  0.11691547, -0.32378002, -0.06717534, -0.28840329,\n        -0.06047865, -0.2022403 ,  0.15417829,  0.00311124,  0.44728738]])</pre></li><li>theta_Intercept(chain, draw)float640.3891 0.3701 ... 0.396 0.3738<pre>array([[0.38912376, 0.37008627, 0.39633679, 0.3779549 , 0.3854276 ,\n        0.36279731, 0.36420562, 0.34182261, 0.37855388, 0.38840644,\n        0.40197375, 0.39966933, 0.39132242, 0.35915944, 0.38629307,\n        0.37635198, 0.3759338 , 0.36308927, 0.38771208, 0.38824669,\n        0.38571059, 0.3748179 , 0.36873673, 0.39466191, 0.37726135,\n        0.39841387, 0.37062688, 0.35031338, 0.38139357, 0.3892441 ,\n        0.36905152, 0.40325732, 0.3650816 , 0.39285046, 0.36864717,\n        0.36943   , 0.3912684 , 0.39664266, 0.37831157, 0.38661666,\n        0.38048438, 0.36324368, 0.37299676, 0.37450463, 0.36442701,\n        0.38392313, 0.3786399 , 0.37746931, 0.3962377 , 0.3704636 ,\n        0.3798473 , 0.37299787, 0.37842259, 0.37582394, 0.36372655,\n        0.37170738, 0.38995705, 0.38558886, 0.33494739, 0.39143918,\n        0.3584778 , 0.3399465 , 0.36670905, 0.37521041, 0.40015357,\n        0.35989617, 0.38068548, 0.37056148, 0.39713531, 0.35350373,\n        0.39644988, 0.3744827 , 0.36838024, 0.38154079, 0.37260285,\n        0.37368489, 0.40149092, 0.39567057, 0.35458098, 0.38960068,\n        0.37925772, 0.36784929, 0.38061349, 0.37621887, 0.36332853,\n        0.37801903, 0.37266393, 0.36307517, 0.36660717, 0.3700159 ,\n        0.35272329, 0.3912989 , 0.35226018, 0.33310194, 0.39147058,\n        0.36696245, 0.37970457, 0.37038077, 0.39355742, 0.39494417,\n...\n        0.36310151, 0.36904657, 0.39822874, 0.36111132, 0.39718904,\n        0.39198591, 0.36529708, 0.37229556, 0.36027816, 0.37541996,\n        0.37185865, 0.37837995, 0.36494184, 0.37330708, 0.35736785,\n        0.38649939, 0.38992195, 0.37923331, 0.39994327, 0.36523784,\n        0.37186223, 0.35729941, 0.3570342 , 0.37116917, 0.38384734,\n        0.3881489 , 0.389485  , 0.36261527, 0.39916305, 0.35414252,\n        0.37071049, 0.37092779, 0.37458829, 0.39901378, 0.39128781,\n        0.38655914, 0.36388019, 0.38139753, 0.37974799, 0.36457395,\n        0.38121151, 0.38466812, 0.39428379, 0.37150862, 0.37381426,\n        0.38218407, 0.38891391, 0.36534525, 0.36593039, 0.38041089,\n        0.3755706 , 0.39434515, 0.38524781, 0.36221262, 0.36671281,\n        0.37462923, 0.389729  , 0.36518171, 0.40270688, 0.36916966,\n        0.37257834, 0.35793076, 0.39529009, 0.37792971, 0.36198437,\n        0.39024571, 0.36610681, 0.38737888, 0.39159407, 0.37822516,\n        0.37109637, 0.37660072, 0.37444873, 0.40249028, 0.379618  ,\n        0.37110491, 0.36829013, 0.38399562, 0.37996629, 0.38688761,\n        0.38460564, 0.38912633, 0.36796039, 0.36482956, 0.36465687,\n        0.35960062, 0.37469642, 0.38136268, 0.3868725 , 0.37183269,\n        0.37873629, 0.37392901, 0.38576931, 0.38499685, 0.37710368,\n        0.38009281, 0.39010555, 0.36772401, 0.39595669, 0.37376619]])</pre></li><li>scaler_Intercept(chain, draw)float642.469 2.531 2.486 ... 2.524 2.552<pre>array([[2.46884679, 2.53112254, 2.48586851, 2.50334837, 2.4137279 ,\n        2.47088161, 2.47466927, 2.55740162, 2.55704842, 2.50898114,\n        2.47099659, 2.44094523, 2.49094549, 2.41000031, 2.48888463,\n        2.5431797 , 2.49819834, 2.44298599, 2.5129002 , 2.4244209 ,\n        2.60320014, 2.41632941, 2.49254337, 2.50398954, 2.51075921,\n        2.53336683, 2.49217169, 2.48593364, 2.60820671, 2.4802456 ,\n        2.50468338, 2.44215021, 2.61779304, 2.47394776, 2.56879137,\n        2.49340276, 2.47712968, 2.56249099, 2.54802376, 2.57863027,\n        2.57227106, 2.44379308, 2.46059633, 2.5400572 , 2.49683873,\n        2.56149126, 2.53022407, 2.52395601, 2.62643874, 2.57838774,\n        2.6102328 , 2.48790628, 2.50351423, 2.53753419, 2.52384826,\n        2.45375701, 2.55271919, 2.58272433, 2.43591364, 2.6480788 ,\n        2.34939345, 2.46847606, 2.47926606, 2.38332638, 2.60157916,\n        2.48585207, 2.59155782, 2.60932893, 2.42785464, 2.58271026,\n        2.59085121, 2.45403872, 2.45226622, 2.50751214, 2.6205654 ,\n        2.49580999, 2.50272923, 2.50825187, 2.42852982, 2.45767162,\n        2.46987807, 2.61211199, 2.51119129, 2.5039388 , 2.5205219 ,\n        2.5003316 , 2.54043613, 2.54075552, 2.48897915, 2.50517518,\n        2.59023766, 2.48504428, 2.48093916, 2.55744034, 2.44471936,\n        2.43854617, 2.58105262, 2.45787086, 2.51304531, 2.55680662,\n...\n        2.4588363 , 2.65177752, 2.44045264, 2.47702421, 2.48934457,\n        2.51438359, 2.48911137, 2.51415013, 2.48780758, 2.4395275 ,\n        2.46491426, 2.5586511 , 2.38436907, 2.53662088, 2.5041308 ,\n        2.48614256, 2.51031602, 2.49934633, 2.50564108, 2.51847334,\n        2.59354297, 2.47734327, 2.5040713 , 2.46919376, 2.51203119,\n        2.50538596, 2.40926457, 2.5052795 , 2.53547279, 2.53851449,\n        2.56460561, 2.5535945 , 2.56411113, 2.52704707, 2.61926498,\n        2.64079042, 2.55576311, 2.48656025, 2.52912864, 2.52471343,\n        2.38320605, 2.45738232, 2.47859079, 2.43593591, 2.47125348,\n        2.46119654, 2.50098964, 2.54192762, 2.43433637, 2.55685031,\n        2.41330659, 2.47576831, 2.54605419, 2.4009983 , 2.37711783,\n        2.41997634, 2.49442769, 2.51312198, 2.48155645, 2.49687222,\n        2.56092262, 2.50158777, 2.6340113 , 2.47184676, 2.58088279,\n        2.47786578, 2.5778915 , 2.48869334, 2.48256938, 2.57716562,\n        2.58494875, 2.6260975 , 2.61781948, 2.54748594, 2.5027431 ,\n        2.4561092 , 2.47749259, 2.5309311 , 2.44775593, 2.54442516,\n        2.47905793, 2.49211757, 2.55960124, 2.44586349, 2.44636495,\n        2.51592292, 2.58000369, 2.55512257, 2.6185371 , 2.53869245,\n        2.55786989, 2.55901547, 2.56149651, 2.52948435, 2.46154643,\n        2.54479131, 2.55076959, 2.47623131, 2.52436732, 2.55155433]])</pre></li><li>theta_1|participant_id(chain, draw, participant_id__factor_dim)float640.04144 0.06688 ... 0.01321<pre>array([[[ 4.14372723e-02,  6.68836976e-02,  4.67533722e-02, ...,\n         -5.67459551e-02, -2.35279045e-02,  6.54503857e-02],\n        [ 9.52736111e-03, -7.06096500e-03,  2.47528353e-02, ...,\n         -8.39087954e-02,  1.52687719e-03,  3.61449658e-03],\n        [ 1.54453040e-03,  3.77444988e-04, -1.59886841e-02, ...,\n          4.24009417e-02,  1.39595863e-02,  6.35113073e-02],\n        ...,\n        [ 2.11146727e-02,  2.11078396e-02, -1.91856072e-02, ...,\n         -1.72343547e-02,  7.53477150e-03,  2.76027793e-02],\n        [ 4.93176685e-03,  1.45264477e-02,  1.69364635e-02, ...,\n         -3.53365272e-02,  1.05270303e-02,  2.45868288e-02],\n        [ 1.46147476e-02,  1.70470474e-02, -2.33119544e-03, ...,\n         -2.59543270e-02,  4.90861848e-07,  1.32060226e-02]]],\n      shape=(1, 1000, 20))</pre></li><li>t_1|participant_id_sigma(chain, draw)float640.00793 0.008283 ... 0.01046<pre>array([[0.00792962, 0.00828271, 0.01309851, 0.00544282, 0.00670041,\n        0.00936533, 0.02040697, 0.03483149, 0.02176444, 0.00355885,\n        0.003239  , 0.00400994, 0.00753473, 0.00346139, 0.00527912,\n        0.00058393, 0.00302313, 0.00132923, 0.00259453, 0.0180774 ,\n        0.01896583, 0.01736463, 0.04654484, 0.01943352, 0.02553114,\n        0.01656344, 0.02039283, 0.01273746, 0.01597424, 0.01679715,\n        0.02351412, 0.0206426 , 0.01547382, 0.022999  , 0.01789278,\n        0.01487897, 0.0049337 , 0.01622376, 0.01634796, 0.01496439,\n        0.00859725, 0.01654599, 0.01600986, 0.01293847, 0.03113852,\n        0.02528108, 0.01698835, 0.02970595, 0.01712925, 0.02183438,\n        0.02368522, 0.02280283, 0.02515662, 0.01396541, 0.01959004,\n        0.01486783, 0.01885468, 0.01699221, 0.00662361, 0.01684058,\n        0.01693884, 0.02885505, 0.05030598, 0.03702632, 0.02972723,\n        0.02033754, 0.01659387, 0.01685872, 0.04044963, 0.02286815,\n        0.02965905, 0.02774818, 0.04334689, 0.01813177, 0.02411989,\n        0.02608958, 0.02028899, 0.03909412, 0.02592057, 0.03037408,\n        0.02037233, 0.029652  , 0.02432409, 0.02343095, 0.02140463,\n        0.02661488, 0.02422373, 0.00454359, 0.01931251, 0.01671438,\n        0.02637787, 0.018401  , 0.02379618, 0.02109798, 0.01921719,\n        0.01982165, 0.01014191, 0.02097682, 0.02293658, 0.01292372,\n...\n        0.03038888, 0.00719266, 0.01902792, 0.00354883, 0.0096981 ,\n        0.00503837, 0.01647079, 0.00493861, 0.01140699, 0.02278241,\n        0.01950445, 0.01536233, 0.02995588, 0.03573225, 0.0382818 ,\n        0.04088304, 0.02105363, 0.02798199, 0.04888485, 0.03654465,\n        0.02400596, 0.01721947, 0.00907695, 0.01235622, 0.00267181,\n        0.01719487, 0.01839875, 0.0295447 , 0.02451703, 0.0159502 ,\n        0.02822218, 0.02876104, 0.02275698, 0.02121095, 0.01261739,\n        0.00693491, 0.02341036, 0.01859423, 0.02417209, 0.01417329,\n        0.02678577, 0.01570201, 0.01957777, 0.04019143, 0.03069745,\n        0.01959609, 0.0222977 , 0.02768058, 0.01730404, 0.02099809,\n        0.01527254, 0.03052616, 0.01997292, 0.01417635, 0.02161657,\n        0.02038554, 0.02402422, 0.03341558, 0.01925727, 0.01582604,\n        0.01090113, 0.03718608, 0.03070486, 0.03866139, 0.031374  ,\n        0.02170693, 0.02185879, 0.02655401, 0.02597307, 0.03750509,\n        0.02542918, 0.01637617, 0.02529095, 0.03147538, 0.0419129 ,\n        0.02550598, 0.01632787, 0.00322294, 0.00842935, 0.0115157 ,\n        0.01398827, 0.00466976, 0.00785511, 0.03457046, 0.02181475,\n        0.04992923, 0.02082953, 0.03693059, 0.02974805, 0.02839905,\n        0.01570793, 0.02102701, 0.01047333, 0.01424581, 0.00685863,\n        0.00683207, 0.00736815, 0.00479349, 0.01454565, 0.01045721]])</pre></li><li>scaler_1|participant_id_offset(chain, draw, participant_id__factor_dim)float64-0.2571 -0.1575 ... -1.045 -1.513<pre>array([[[-0.25712434, -0.15749976,  0.95779845, ..., -0.69560833,\n         -0.30188216,  0.12836076],\n        [-1.22734757,  1.43945172,  0.89526307, ..., -0.47151607,\n         -0.49766211, -0.11272395],\n        [-0.32744925,  1.0303359 , -0.31235437, ...,  1.26562666,\n         -0.27055186, -1.51887852],\n        ...,\n        [ 0.19202387,  0.82748433, -0.86717938, ...,  0.02118865,\n         -0.1399486 , -1.51120018],\n        [ 0.68787574,  0.87364413,  0.26894666, ..., -0.48352105,\n         -0.2287319 , -0.06220204],\n        [-2.92979721,  0.14134841,  0.0485534 , ...,  0.21303869,\n         -1.04499044, -1.51287666]]], shape=(1, 1000, 20))</pre></li><li>a_Intercept(chain, draw)float641.523 1.521 1.551 ... 1.544 1.47<pre>array([[1.52316797, 1.5213083 , 1.55103545, 1.5170321 , 1.51848211,\n        1.52478622, 1.50985948, 1.46899845, 1.54501694, 1.5168451 ,\n        1.55025896, 1.54161869, 1.54394462, 1.48950067, 1.5384951 ,\n        1.52619754, 1.49118008, 1.49861933, 1.51707573, 1.48904913,\n        1.49085157, 1.49591678, 1.49537661, 1.52830094, 1.49698594,\n        1.51040526, 1.49201423, 1.49222078, 1.52263945, 1.54725775,\n        1.49146032, 1.53281868, 1.50044846, 1.51872892, 1.50900056,\n        1.49168689, 1.49090962, 1.50064306, 1.53986085, 1.54047288,\n        1.49848679, 1.50799101, 1.50889125, 1.52537411, 1.48561746,\n        1.52800178, 1.51450011, 1.52212024, 1.5527579 , 1.49542235,\n        1.53021361, 1.49412724, 1.51760149, 1.52797098, 1.48935559,\n        1.49608967, 1.51199666, 1.52401055, 1.4558113 , 1.52261434,\n        1.48747   , 1.45927625, 1.52766954, 1.49849143, 1.52638574,\n        1.50725647, 1.53123768, 1.49425532, 1.52062439, 1.45551392,\n        1.56184254, 1.48582149, 1.48541656, 1.50385762, 1.51913609,\n        1.50819682, 1.52205978, 1.52691953, 1.45061408, 1.52716384,\n        1.51560336, 1.50060577, 1.51705528, 1.51860953, 1.5166272 ,\n        1.51734736, 1.53553775, 1.50812091, 1.51427189, 1.50332545,\n        1.50311503, 1.53676792, 1.49566932, 1.47183199, 1.53686191,\n        1.46567116, 1.5429503 , 1.49337058, 1.56521519, 1.57043014,\n...\n        1.47945211, 1.53430866, 1.51796031, 1.52259557, 1.52334792,\n        1.52138189, 1.50433961, 1.50128608, 1.51306797, 1.50973124,\n        1.48897365, 1.51037301, 1.48291153, 1.48395011, 1.44873191,\n        1.49694952, 1.51234944, 1.4947095 , 1.51732008, 1.51254585,\n        1.50137776, 1.50371937, 1.50450374, 1.48243136, 1.54555707,\n        1.49733582, 1.48642957, 1.51162488, 1.48283561, 1.49081971,\n        1.51791138, 1.52871017, 1.514349  , 1.51446213, 1.50298445,\n        1.54788555, 1.53528799, 1.47075202, 1.53474724, 1.48689009,\n        1.4974676 , 1.51071095, 1.51046293, 1.51393346, 1.50998671,\n        1.50231885, 1.52004614, 1.49961119, 1.5063373 , 1.54138125,\n        1.49867758, 1.50199871, 1.53898736, 1.46248645, 1.46003155,\n        1.46168602, 1.52310103, 1.49084127, 1.53363278, 1.49386749,\n        1.52720928, 1.48904611, 1.54996847, 1.46518337, 1.49812369,\n        1.5329661 , 1.48462142, 1.52626682, 1.52411437, 1.53897593,\n        1.50326473, 1.50837817, 1.53106004, 1.54183822, 1.50285306,\n        1.510325  , 1.48967687, 1.52236036, 1.46093085, 1.60339151,\n        1.51209923, 1.50581873, 1.51154065, 1.48848012, 1.47286117,\n        1.50953863, 1.51051954, 1.52750551, 1.53311885, 1.52485627,\n        1.50074719, 1.5226444 , 1.53031859, 1.53969767, 1.48566435,\n        1.54292041, 1.53395978, 1.49366509, 1.54422259, 1.46969364]])</pre></li><li>a_1|participant_id_sigma(chain, draw)float640.04412 0.03087 ... 0.04229 0.01128<pre>array([[0.04412429, 0.03087054, 0.08280896, 0.05947307, 0.03346394,\n        0.0741193 , 0.05862416, 0.06032153, 0.02355096, 0.0548791 ,\n        0.06911147, 0.07543568, 0.07841245, 0.04809975, 0.06159825,\n        0.07522747, 0.05677178, 0.04536765, 0.03123111, 0.02955771,\n        0.02085356, 0.00413946, 0.00615049, 0.02071988, 0.04216158,\n        0.05041048, 0.02299944, 0.06034883, 0.01653022, 0.0175013 ,\n        0.0130423 , 0.02509722, 0.03589073, 0.03497868, 0.04385835,\n        0.05804876, 0.03603706, 0.0460957 , 0.06696908, 0.07859429,\n        0.09922644, 0.04543601, 0.06008181, 0.09168005, 0.0411139 ,\n        0.0486149 , 0.03945264, 0.07055332, 0.04263184, 0.04044025,\n        0.05260151, 0.02083909, 0.02558234, 0.0291485 , 0.02206854,\n        0.03030671, 0.01965089, 0.018531  , 0.05148291, 0.04702884,\n        0.0551335 , 0.05580417, 0.00990018, 0.01897366, 0.07712992,\n        0.05497993, 0.04472764, 0.04944763, 0.0507471 , 0.07444418,\n        0.04923479, 0.03819939, 0.07790441, 0.04684288, 0.03832871,\n        0.05958126, 0.07090541, 0.05208938, 0.05721319, 0.06376901,\n        0.08153925, 0.02702043, 0.02263448, 0.02124683, 0.06960877,\n        0.08227809, 0.05604551, 0.06709417, 0.03290491, 0.03314229,\n        0.05103741, 0.06785723, 0.02591927, 0.03179124, 0.04623443,\n        0.04307358, 0.0366158 , 0.06108199, 0.06493819, 0.05508686,\n...\n        0.07165941, 0.01818941, 0.04803295, 0.06324923, 0.05485143,\n        0.04302693, 0.06113672, 0.05102149, 0.06180303, 0.03135861,\n        0.02667925, 0.05195471, 0.04566693, 0.05858021, 0.05449701,\n        0.06817423, 0.05631413, 0.06194883, 0.08493003, 0.03775275,\n        0.04729123, 0.09104192, 0.09481071, 0.04154212, 0.08250688,\n        0.04213523, 0.03854706, 0.06107175, 0.08580175, 0.07992748,\n        0.09006815, 0.06902557, 0.02607564, 0.02168   , 0.02742516,\n        0.05219986, 0.04518112, 0.03149007, 0.06747755, 0.03491174,\n        0.06630152, 0.05450074, 0.05637069, 0.10357462, 0.02443444,\n        0.02879535, 0.02354535, 0.04022618, 0.05977914, 0.05974587,\n        0.05620681, 0.08584244, 0.06706711, 0.05810945, 0.05731078,\n        0.07539553, 0.06625354, 0.05418063, 0.04859914, 0.03905819,\n        0.06900268, 0.03948322, 0.06473962, 0.06377441, 0.07189585,\n        0.05004191, 0.08480244, 0.04982916, 0.05917389, 0.08290179,\n        0.04717495, 0.08283934, 0.03445631, 0.04650205, 0.03022531,\n        0.01095188, 0.03681945, 0.05572459, 0.04773059, 0.09182266,\n        0.04381992, 0.01664606, 0.03729876, 0.04150589, 0.05098751,\n        0.02670058, 0.06378526, 0.03982903, 0.06556753, 0.08190422,\n        0.06601286, 0.07061206, 0.0510629 , 0.06923898, 0.03897077,\n        0.03975995, 0.0165972 , 0.02618069, 0.04229421, 0.01128114]])</pre></li><li>z_1|participant_id(chain, draw, participant_id__factor_dim)float640.007408 0.009864 ... -0.01189<pre>array([[[ 0.00740771,  0.00986388,  0.0045233 , ...,  0.0036044 ,\n         -0.01003354, -0.03435536],\n        [-0.00348912,  0.00102828, -0.01617755, ..., -0.01435954,\n          0.00292691, -0.002382  ],\n        [-0.00067322,  0.00029502,  0.00231923, ...,  0.001488  ,\n         -0.00171201, -0.00199647],\n        ...,\n        [-0.01143804, -0.01197453,  0.01748623, ..., -0.00030288,\n         -0.00150441, -0.03592678],\n        [-0.0076144 ,  0.01427217, -0.01892795, ..., -0.00450093,\n         -0.00536368, -0.00209391],\n        [ 0.00481321,  0.00460566,  0.01182806, ..., -0.0006516 ,\n          0.00658681, -0.01189099]]], shape=(1, 1000, 20))</pre></li><li>z_1|participant_id_sigma(chain, draw)float640.0164 0.008931 ... 0.007143<pre>array([[0.01640165, 0.00893091, 0.0011231 , 0.00382561, 0.01105926,\n        0.02518958, 0.034378  , 0.02199631, 0.00874454, 0.0062699 ,\n        0.02552182, 0.01630051, 0.02130991, 0.00651615, 0.02633581,\n        0.02641053, 0.03228041, 0.02782896, 0.01898805, 0.0158868 ,\n        0.01420057, 0.02245743, 0.01663815, 0.02109421, 0.0177732 ,\n        0.01858174, 0.02948691, 0.03027515, 0.01958525, 0.01871704,\n        0.01008646, 0.00151935, 0.00368684, 0.00548025, 0.00634575,\n        0.00882879, 0.02541891, 0.0187907 , 0.01663688, 0.01404937,\n        0.02810614, 0.00913804, 0.01409039, 0.00925099, 0.00767639,\n        0.01763857, 0.00869582, 0.0081575 , 0.01346859, 0.01149278,\n        0.02443754, 0.00438107, 0.01536196, 0.014694  , 0.00967572,\n        0.01157775, 0.01833705, 0.00361936, 0.01588015, 0.006986  ,\n        0.01948853, 0.03013755, 0.02184705, 0.02296811, 0.034701  ,\n        0.02049693, 0.03388148, 0.00219584, 0.00118973, 0.00313499,\n        0.00661862, 0.01253781, 0.02126552, 0.00626429, 0.00837822,\n        0.01588743, 0.01778601, 0.00913376, 0.0059565 , 0.02023302,\n        0.02586846, 0.02127404, 0.02795922, 0.0356247 , 0.00597272,\n        0.0074439 , 0.00996649, 0.00574993, 0.02019617, 0.01193568,\n        0.01029277, 0.01011472, 0.01635963, 0.01162068, 0.00635425,\n        0.0093813 , 0.0137154 , 0.01485888, 0.01433651, 0.01575513,\n...\n        0.00924248, 0.01344469, 0.00295564, 0.01487958, 0.01186572,\n        0.00174197, 0.01339281, 0.01079617, 0.01561017, 0.00957515,\n        0.00711133, 0.01493696, 0.01968974, 0.01087207, 0.00935651,\n        0.02298347, 0.01691541, 0.01507592, 0.01479973, 0.00699436,\n        0.00181112, 0.00723804, 0.00653787, 0.01167452, 0.01846565,\n        0.02234614, 0.02636779, 0.02117935, 0.03261173, 0.03099148,\n        0.02722499, 0.03808622, 0.01776563, 0.0146384 , 0.0132993 ,\n        0.0158909 , 0.01557769, 0.01063555, 0.01097323, 0.00851393,\n        0.01096719, 0.01201147, 0.01271475, 0.02121019, 0.01054477,\n        0.02382141, 0.0149595 , 0.01622826, 0.02357158, 0.00734275,\n        0.01020676, 0.01424963, 0.01310467, 0.01283819, 0.02371858,\n        0.02014539, 0.02106856, 0.03190241, 0.02903775, 0.00639881,\n        0.00696193, 0.0036312 , 0.01880017, 0.00475782, 0.00626143,\n        0.00946576, 0.0142508 , 0.0133859 , 0.01298281, 0.0185677 ,\n        0.00932657, 0.00889121, 0.00490762, 0.0078151 , 0.00521888,\n        0.03316522, 0.01032228, 0.00675737, 0.01534723, 0.02107306,\n        0.02031198, 0.02363417, 0.01046921, 0.00711536, 0.00853819,\n        0.02433317, 0.00289377, 0.01593026, 0.01442287, 0.00517052,\n        0.01135813, 0.0186893 , 0.03045435, 0.02270502, 0.01154605,\n        0.02207425, 0.01758684, 0.01473053, 0.00975821, 0.00714343]])</pre></li><li>scaler_1|participant_id_mu(chain, draw)float64-0.1785 0.4793 ... 0.08154 -0.1065<pre>array([[-1.78480460e-01,  4.79333862e-01, -4.87771743e-01,\n        -1.12180946e-02, -6.37353465e-01, -2.60168075e-01,\n         3.64759869e-01, -6.00002627e-01,  2.97390990e-01,\n        -2.30722048e-01, -3.40105561e-02,  1.18114666e-01,\n        -1.89942868e-04,  2.48036720e-02, -5.06167050e-02,\n         2.54551235e-01, -3.39312100e-01, -6.84183630e-02,\n         5.90987940e-02, -4.34056903e-02,  1.85725736e-02,\n        -7.86341293e-02, -6.45392130e-02,  1.24017467e-01,\n        -3.24912007e-02,  5.12829231e-01, -2.02222079e-01,\n         2.55557605e-01, -1.47717407e-01, -4.71163338e-01,\n         2.27593275e-01, -5.76480294e-02, -2.99749527e-02,\n        -4.74547014e-02, -1.00820932e-01, -4.74457101e-02,\n        -2.11706077e-01, -1.19588740e-01, -2.62384667e-01,\n         1.47596108e-01, -1.21379918e-01,  1.87997922e-01,\n         1.43209868e-01, -1.68608962e-01,  2.62741431e-01,\n        -2.54121049e-01, -1.34900253e-01,  3.74235149e-01,\n        -3.43839860e-01,  3.77105848e-01,  3.30612962e-01,\n        -3.22079322e-01,  1.85903731e-01,  2.99758390e-01,\n        -2.64609772e-01,  2.97773372e-01, -1.89233368e-01,\n         2.67677162e-01, -1.59798708e-01, -7.75646485e-02,\n...\n        -4.95557063e-01,  3.52740444e-01,  8.97982667e-02,\n        -2.85643904e-01, -2.93282168e-01, -1.09014780e-02,\n        -8.26362064e-02,  1.25449934e-01, -1.14087131e-01,\n         1.01836782e-01,  1.45541111e-01, -1.68581538e-01,\n        -1.06572879e-01, -1.71776466e-01, -1.15871127e-01,\n         2.79796315e-01, -2.93886934e-01,  2.87917849e-01,\n        -1.67332662e-02,  1.09744098e-01,  1.26895858e-02,\n         1.04365518e-01, -5.90277254e-02,  2.59282221e-01,\n        -3.50010990e-01,  4.71053945e-01,  1.55037722e-01,\n        -7.20690970e-03,  1.87821029e-01,  4.09680536e-01,\n         1.73918359e-01,  3.48376941e-01,  2.09128772e-01,\n        -2.42443450e-01, -2.91183169e-01,  2.92698225e-01,\n        -3.37180053e-01,  1.54872912e-01, -1.44425031e-01,\n         6.15399264e-02, -2.10716058e-01,  5.65436356e-01,\n         4.91328881e-01,  7.31794563e-02, -4.83153733e-02,\n         1.40648540e-01, -1.66619126e-02,  1.79466521e-01,\n         2.76002256e-01, -5.14642314e-02, -1.45783091e-02,\n         1.36187953e-01, -9.69902056e-03,  8.22138720e-02,\n         2.68565416e-02, -4.73738629e-02,  8.15398406e-02,\n        -1.06527465e-01]])</pre></li><li>theta_1|participant_id_offset(chain, draw, participant_id__factor_dim)float640.7612 1.229 ... 1.486e-05 0.3998<pre>array([[[ 7.61210686e-01,  1.22866643e+00,  8.58868466e-01, ...,\n         -1.04243414e+00, -4.32212145e-01,  1.20233621e+00],\n        [ 3.02065490e-01, -2.23868270e-01,  7.84789957e-01, ...,\n         -2.66033281e+00,  4.84097223e-02,  1.14597806e-01],\n        [ 3.63988385e-02,  8.89497493e-03, -3.76793833e-01, ...,\n          9.99232537e-01,  3.28975545e-01,  1.49672536e+00],\n        ...,\n        [ 9.19994260e-01,  9.19696530e-01, -8.35942320e-01, ...,\n         -7.50923661e-01,  3.28299975e-01,  1.20268966e+00],\n        [ 1.63566206e-01,  4.81781885e-01,  5.61712093e-01, ...,\n         -1.17196572e+00,  3.49137835e-01,  8.15442907e-01],\n        [ 4.42492350e-01,  5.16135361e-01, -7.05818655e-02, ...,\n         -7.85822065e-01,  1.48618791e-05,  3.99840224e-01]]],\n      shape=(1, 1000, 20))</pre></li><li>z_1|participant_id_offset(chain, draw, participant_id__factor_dim)float640.4516 0.6014 ... 0.9221 -1.665<pre>array([[[ 0.45164403,  0.60139549,  0.27578326, ...,  0.21975805,\n         -0.61173946, -2.09462758],\n        [-0.39067903,  0.11513768, -1.81141185, ..., -1.60784741,\n          0.32772822, -0.26671467],\n        [-0.5994317 ,  0.26268744,  2.06503367, ...,  1.32490598,\n         -1.5243632 , -1.77764756],\n        ...,\n        [-0.77648543, -0.8129054 ,  1.18707404, ..., -0.02056128,\n         -0.10212873, -2.43893361],\n        [-0.78030695,  1.46258036, -1.93969453, ..., -0.46124516,\n         -0.54965799, -0.21457882],\n        [ 0.67379532,  0.64474164,  1.65579705, ..., -0.09121626,\n          0.92207974, -1.6646063 ]]], shape=(1, 1000, 20))</pre></li><li>rl.alpha_1|participant_id_sigma(chain, draw)float640.07871 0.04736 ... 0.06107 0.09348<pre>array([[0.07870568, 0.04735583, 0.04484967, 0.07612755, 0.07759606,\n        0.09441799, 0.06276805, 0.08793089, 0.04514132, 0.0085362 ,\n        0.02843923, 0.08650291, 0.03671935, 0.0014162 , 0.00349833,\n        0.09102712, 0.00926216, 0.08450448, 0.10090288, 0.05803459,\n        0.04293717, 0.03896643, 0.07530004, 0.03505882, 0.00838695,\n        0.003369  , 0.01012683, 0.00757226, 0.1977147 , 0.02546142,\n        0.00374276, 0.0093312 , 0.0391591 , 0.07830179, 0.10097766,\n        0.11465767, 0.05125291, 0.08040058, 0.08058154, 0.02405648,\n        0.03864058, 0.12506403, 0.07203149, 0.06900667, 0.03610473,\n        0.07821421, 0.07824529, 0.06668119, 0.15110499, 0.02144455,\n        0.06880485, 0.06298592, 0.06241037, 0.04053275, 0.03405768,\n        0.04438129, 0.02838299, 0.04562554, 0.02143036, 0.10126678,\n        0.0892925 , 0.04885562, 0.07442012, 0.09323874, 0.03116753,\n        0.0551413 , 0.02426423, 0.08614493, 0.03873493, 0.05627188,\n        0.06530878, 0.00154526, 0.00586679, 0.00231905, 0.05985199,\n        0.07266683, 0.03843764, 0.04715202, 0.10403822, 0.10831213,\n        0.05214015, 0.02725073, 0.01646612, 0.02178029, 0.0317389 ,\n        0.02057923, 0.0931455 , 0.02316094, 0.03621121, 0.09148668,\n        0.09442281, 0.0296177 , 0.09657786, 0.04694713, 0.01061108,\n        0.00575356, 0.1002312 , 0.1100468 , 0.06024635, 0.07065379,\n...\n        0.01434973, 0.06472537, 0.0151708 , 0.0291999 , 0.09413679,\n        0.04922536, 0.01291749, 0.01974839, 0.02314394, 0.06646183,\n        0.02647244, 0.07622179, 0.01996956, 0.11614157, 0.08467249,\n        0.06123866, 0.07120746, 0.07371051, 0.08376855, 0.01944653,\n        0.08827768, 0.02763549, 0.01548017, 0.08730943, 0.07341483,\n        0.11797156, 0.07344582, 0.01101538, 0.09062946, 0.02548337,\n        0.04603128, 0.06526221, 0.10234282, 0.06586327, 0.03232228,\n        0.10846194, 0.13379091, 0.14878784, 0.06281653, 0.06480059,\n        0.06487994, 0.06095115, 0.04060095, 0.11918394, 0.07503072,\n        0.14870964, 0.07809306, 0.0087123 , 0.03027919, 0.05597514,\n        0.06601343, 0.04528051, 0.03662188, 0.05528121, 0.08380679,\n        0.15156751, 0.03176245, 0.08235188, 0.01100068, 0.07692872,\n        0.02920534, 0.05375358, 0.08716419, 0.10478254, 0.04619516,\n        0.04086899, 0.06196294, 0.05113167, 0.06029847, 0.08280391,\n        0.0465652 , 0.10181228, 0.08439451, 0.05440551, 0.02348842,\n        0.04311552, 0.05856341, 0.02891526, 0.06735571, 0.0510702 ,\n        0.02477705, 0.06132532, 0.09571541, 0.05142121, 0.0669904 ,\n        0.10363149, 0.04824143, 0.04355293, 0.03853435, 0.05809558,\n        0.1276807 , 0.09166675, 0.10019568, 0.07270311, 0.0687422 ,\n        0.07480062, 0.06429935, 0.04354727, 0.06107339, 0.09348142]])</pre></li><li>t_Intercept(chain, draw)float640.4193 0.4142 ... 0.4219 0.4299<pre>array([[0.41931091, 0.41424852, 0.41688213, 0.41289758, 0.42684625,\n        0.40784868, 0.41864454, 0.43084358, 0.41391409, 0.42274018,\n        0.41360134, 0.41799017, 0.41199886, 0.42191399, 0.41804286,\n        0.42123673, 0.42607569, 0.41654296, 0.41811573, 0.42134472,\n        0.41542694, 0.42049293, 0.44310416, 0.41725418, 0.42528267,\n        0.42263458, 0.42923701, 0.42883023, 0.4269453 , 0.41281148,\n        0.41921457, 0.41821963, 0.42446833, 0.41381737, 0.42747042,\n        0.42151247, 0.42480857, 0.41775008, 0.40429179, 0.40090063,\n        0.43031219, 0.421417  , 0.41964829, 0.4198266 , 0.42217635,\n        0.4275625 , 0.41638905, 0.43491337, 0.41220271, 0.41224431,\n        0.4057272 , 0.41788014, 0.42404007, 0.42336926, 0.42376007,\n        0.41681874, 0.42608299, 0.41089281, 0.4379715 , 0.4120857 ,\n        0.43456556, 0.44364516, 0.40289077, 0.41079095, 0.43138071,\n        0.40990744, 0.4157868 , 0.41611346, 0.43877461, 0.4511669 ,\n        0.42537167, 0.42546063, 0.40755977, 0.42821137, 0.42514548,\n        0.42922   , 0.44009314, 0.43894098, 0.439499  , 0.41921576,\n        0.41663376, 0.41952843, 0.41405475, 0.41644215, 0.43341469,\n        0.42247668, 0.41586594, 0.41504321, 0.41353239, 0.40913251,\n        0.41412831, 0.4279162 , 0.42604658, 0.4212995 , 0.41900395,\n        0.43790062, 0.41191067, 0.42933099, 0.41447239, 0.40991308,\n...\n        0.41479847, 0.41043281, 0.41823144, 0.4252805 , 0.41518647,\n        0.41297987, 0.42110991, 0.43447233, 0.41126565, 0.41628258,\n        0.43629834, 0.42047706, 0.44243492, 0.43833944, 0.44059044,\n        0.44541344, 0.41410758, 0.44074724, 0.40890269, 0.42898131,\n        0.42316947, 0.4258251 , 0.42302226, 0.40754632, 0.41864216,\n        0.42379323, 0.42854289, 0.43338946, 0.41971902, 0.42313853,\n        0.41465098, 0.41145609, 0.42267765, 0.42364041, 0.42922082,\n        0.40823598, 0.41479747, 0.43085358, 0.41706213, 0.43736033,\n        0.42379535, 0.42146248, 0.42636726, 0.42374552, 0.43307793,\n        0.41232007, 0.42937261, 0.40910769, 0.42503312, 0.41219738,\n        0.4259549 , 0.42885117, 0.41048359, 0.42533455, 0.42464884,\n        0.43095555, 0.41086097, 0.43380196, 0.42067927, 0.41129556,\n        0.42533477, 0.42997849, 0.41449076, 0.42562507, 0.42040458,\n        0.42387142, 0.42742197, 0.41963308, 0.41825422, 0.42058834,\n        0.41938916, 0.41997781, 0.41012808, 0.41367773, 0.40903523,\n        0.43094383, 0.43252447, 0.40536237, 0.43501928, 0.40914689,\n        0.41285979, 0.42584675, 0.42539625, 0.42631183, 0.42948835,\n        0.43512941, 0.42576651, 0.40949534, 0.40663802, 0.40880199,\n        0.43214644, 0.4202907 , 0.42708761, 0.40859478, 0.43140521,\n        0.41037885, 0.42362244, 0.42582542, 0.42191101, 0.42986663]])</pre></li><li>z_Intercept(chain, draw)float640.2984 0.3015 ... 0.2964 0.2887<pre>array([[0.29842516, 0.30148358, 0.29492298, 0.29674494, 0.31595935,\n        0.30645085, 0.30334207, 0.30318324, 0.29044893, 0.29585273,\n        0.30269122, 0.3057107 , 0.29114053, 0.30565085, 0.29310079,\n        0.29406575, 0.31489103, 0.30146068, 0.29270256, 0.30755437,\n        0.28432911, 0.30849629, 0.3063778 , 0.3009668 , 0.29599161,\n        0.30033691, 0.28555258, 0.29382261, 0.30477943, 0.30037188,\n        0.29948593, 0.29886074, 0.29055124, 0.30756183, 0.30413677,\n        0.29501881, 0.30643535, 0.29979179, 0.29603117, 0.29700942,\n        0.30519698, 0.30525123, 0.29735765, 0.30380866, 0.29888039,\n        0.30066823, 0.29830323, 0.29162545, 0.29428264, 0.29246374,\n        0.28913845, 0.29436819, 0.30012111, 0.29760724, 0.30677542,\n        0.2943742 , 0.30386401, 0.29115242, 0.30599842, 0.28929223,\n        0.32283442, 0.31101707, 0.30697138, 0.30724437, 0.30871907,\n        0.28809718, 0.30481861, 0.29176861, 0.30832586, 0.29937756,\n        0.30239376, 0.30561385, 0.3055899 , 0.30195594, 0.28746151,\n        0.3086578 , 0.30815747, 0.30740431, 0.30370997, 0.30132802,\n        0.30625114, 0.29310591, 0.2912796 , 0.2902951 , 0.30188158,\n        0.29499462, 0.30705484, 0.29684954, 0.297405  , 0.29695192,\n        0.28813065, 0.30967915, 0.29433614, 0.29180279, 0.30218684,\n        0.31121891, 0.29091055, 0.30633284, 0.29591631, 0.2967244 ,\n...\n        0.29550024, 0.29445822, 0.31029448, 0.29890716, 0.29230646,\n        0.30280864, 0.29710762, 0.29121955, 0.30676889, 0.30158605,\n        0.3044669 , 0.29556249, 0.31334021, 0.29902723, 0.29544246,\n        0.30583779, 0.2921056 , 0.29836954, 0.29828956, 0.29394914,\n        0.29899217, 0.29664005, 0.29610086, 0.29185888, 0.30315902,\n        0.29385498, 0.30426623, 0.29853352, 0.31530376, 0.27766461,\n        0.29458283, 0.28927138, 0.28623737, 0.30986047, 0.30659673,\n        0.29554603, 0.28476429, 0.31038966, 0.29003506, 0.29236253,\n        0.31427213, 0.30068841, 0.30033812, 0.30232584, 0.30255679,\n        0.28695527, 0.28734434, 0.30197772, 0.30333506, 0.30145108,\n        0.30569292, 0.30737608, 0.29034106, 0.31022096, 0.30842422,\n        0.31049089, 0.30137169, 0.30155043, 0.31450264, 0.28575817,\n        0.30355869, 0.29779983, 0.29672517, 0.30794375, 0.29392696,\n        0.30308286, 0.30168353, 0.29211588, 0.2976014 , 0.30212637,\n        0.28981335, 0.28490855, 0.28221273, 0.29346091, 0.28675092,\n        0.30310415, 0.3076711 , 0.28719819, 0.29449707, 0.30355162,\n        0.30313867, 0.30452102, 0.29550036, 0.29581429, 0.30264683,\n        0.30316341, 0.28933603, 0.3001162 , 0.29963148, 0.29507363,\n        0.29472785, 0.29648777, 0.29173685, 0.29414472, 0.30877365,\n        0.29289151, 0.30139633, 0.30699865, 0.29638604, 0.28873078]])</pre></li><li>theta_1|participant_id_sigma(chain, draw)float640.05444 0.03154 ... 0.03015 0.03303<pre>array([[0.05443601, 0.03154071, 0.04243351, 0.02112757, 0.03066847,\n        0.03769248, 0.0418285 , 0.07543957, 0.03707388, 0.01934544,\n        0.02992523, 0.02390756, 0.02630203, 0.05847638, 0.04804583,\n        0.04093503, 0.0322124 , 0.03068929, 0.02877548, 0.03261306,\n        0.04196609, 0.04652891, 0.05398537, 0.03374221, 0.05370068,\n        0.02963529, 0.03214913, 0.03287505, 0.06419269, 0.03334763,\n        0.03926888, 0.06132888, 0.03506759, 0.04222519, 0.01878005,\n        0.05049621, 0.02898153, 0.03619261, 0.03004835, 0.03992255,\n        0.02867478, 0.02440064, 0.02388792, 0.0286011 , 0.01786352,\n        0.0239097 , 0.05202107, 0.03423108, 0.04588522, 0.02829912,\n        0.03380232, 0.02156192, 0.03722561, 0.02850599, 0.04713407,\n        0.03510518, 0.05476376, 0.05197982, 0.06226299, 0.03927269,\n        0.0466887 , 0.0310396 , 0.05245458, 0.03276251, 0.03960969,\n        0.06250518, 0.04302845, 0.03880791, 0.0435359 , 0.01051175,\n        0.00561098, 0.03350762, 0.02130897, 0.02246445, 0.01125181,\n        0.02037127, 0.01539413, 0.01951827, 0.02296799, 0.03107312,\n        0.04671935, 0.03754162, 0.04549627, 0.0498223 , 0.0505518 ,\n        0.04662946, 0.02847981, 0.02696095, 0.02384687, 0.03271111,\n        0.034663  , 0.01826317, 0.05556473, 0.12658472, 0.04056971,\n        0.05267752, 0.03806766, 0.03471646, 0.03794364, 0.02721042,\n...\n        0.01163983, 0.05162965, 0.03232735, 0.05966618, 0.0278111 ,\n        0.05482448, 0.04010863, 0.02777974, 0.02008759, 0.03441155,\n        0.04307419, 0.02016838, 0.04915898, 0.02636332, 0.0261404 ,\n        0.05730024, 0.01782254, 0.03724434, 0.02765182, 0.05459755,\n        0.04480615, 0.02583479, 0.02928241, 0.02388954, 0.03191301,\n        0.02574647, 0.033813  , 0.04565678, 0.0639491 , 0.02018825,\n        0.02737239, 0.01837727, 0.04496442, 0.05323022, 0.04006529,\n        0.04215238, 0.02502867, 0.01534835, 0.02046074, 0.04758458,\n        0.03958592, 0.03605474, 0.0380686 , 0.0608956 , 0.07367522,\n        0.04069073, 0.03370791, 0.05002471, 0.0206889 , 0.05046383,\n        0.01504332, 0.02670385, 0.01896267, 0.01379463, 0.02592272,\n        0.02668761, 0.01435983, 0.02261749, 0.04448776, 0.03684066,\n        0.01990529, 0.03606447, 0.03267202, 0.03763015, 0.03760573,\n        0.0394726 , 0.0596546 , 0.05119629, 0.04862527, 0.05362463,\n        0.03613225, 0.03321334, 0.04392479, 0.02857502, 0.03869949,\n        0.03365839, 0.02476136, 0.04060164, 0.05554778, 0.02711943,\n        0.0234735 , 0.03953539, 0.03222895, 0.02631757, 0.03281085,\n        0.03870605, 0.01977898, 0.0360517 , 0.04056611, 0.0379558 ,\n        0.04566549, 0.0578254 , 0.02807171, 0.03722138, 0.0282094 ,\n        0.05702454, 0.03033278, 0.02295087, 0.0301515 , 0.03302825]])</pre></li><li>z_1|participant_id_mu(chain, draw)float640.1362 -0.1247 ... -0.05881 0.2017<pre>array([[ 1.36209308e-01, -1.24740651e-01,  9.38454169e-02,\n        -3.25325906e-02, -1.89430616e-01, -7.18584471e-03,\n         2.49516809e-02, -8.46597356e-02,  6.17947509e-02,\n        -7.41749359e-02, -8.04471339e-02, -2.19472005e-01,\n        -1.95233393e-01,  5.77360316e-02, -1.24519429e-02,\n        -1.58195032e-02, -2.41979894e-01,  2.09798691e-01,\n         9.31799519e-02, -6.43397683e-02,  2.22850789e-02,\n        -4.86462838e-02, -2.17375902e-01,  2.31773297e-01,\n         9.32533712e-02, -2.91338651e-01,  2.25331514e-01,\n         6.10320106e-01, -6.40663845e-01, -3.04519149e-01,\n         1.81429575e-01, -3.26293130e-01,  2.35611067e-01,\n        -1.99804293e-01,  1.48669593e-01, -5.15161763e-02,\n        -1.62403243e-01, -2.07173629e-02, -4.58919829e-01,\n         4.91607431e-02, -5.27116653e-02,  6.12312588e-03,\n        -1.38023357e-01, -3.25502591e-01,  3.41319893e-01,\n        -3.29221442e-01,  1.41631422e-01, -7.10765858e-02,\n        -1.76125894e-01,  1.70427884e-01, -2.55252302e-01,\n         2.79399656e-01,  6.45788128e-02,  6.66259739e-02,\n        -1.96543751e-01,  1.55239390e-01,  8.03259539e-03,\n         1.79451560e-01, -1.00189057e-01, -8.21574228e-03,\n...\n        -6.37310778e-01,  7.51565110e-01,  5.10551043e-02,\n        -3.69901532e-01, -2.82251757e-01,  3.60436357e-01,\n        -1.97801063e-01,  3.44269170e-01,  2.44050538e-01,\n         2.25155198e-01,  1.81880200e-01, -1.74248292e-01,\n        -1.91386069e-01,  1.17710388e-02,  1.43252684e-01,\n        -1.06175281e-01,  1.12421022e-01, -2.28372374e-01,\n         3.15706323e-01, -3.96677523e-01,  3.86602297e-01,\n        -1.53914882e-01,  2.23525378e-02,  9.75510203e-02,\n         1.91995548e-02, -1.51879108e-01,  1.90997401e-01,\n         3.66405432e-01,  1.65842949e-02, -7.84741742e-02,\n        -3.42756302e-01,  2.57854952e-01, -3.05660350e-01,\n         3.88795655e-01, -1.28223649e-01,  1.42660959e-01,\n        -6.52602009e-02, -1.79294664e-01, -2.86825660e-02,\n        -1.38209355e-01,  4.25834500e-01, -3.21963593e-01,\n        -1.63041266e-01, -2.58454390e-01,  1.50503899e-01,\n        -2.77558999e-01, -2.52873781e-01, -3.38228231e-01,\n        -3.97286562e-03,  4.41708256e-01,  4.15787039e-01,\n        -3.94482714e-01, -4.52653723e-01,  7.41789644e-02,\n        -3.73372276e-02,  7.81950981e-02, -5.88088497e-02,\n         2.01652213e-01]])</pre></li><li>scaler_1|participant_id(chain, draw, participant_id__factor_dim)float64-0.03382 -0.02072 ... -0.1016<pre>array([[[-0.03381821, -0.02071512,  0.1259742 , ..., -0.09148971,\n         -0.03970497,  0.01688262],\n        [-0.10915282,  0.12801608,  0.07961925, ..., -0.04193377,\n         -0.04425904, -0.01002498],\n        [-0.03894296,  0.12253603, -0.03714775, ...,  0.15051875,\n         -0.03217626, -0.18063754],\n        ...,\n        [ 0.02768152,  0.11928736, -0.12500967, ...,  0.00305448,\n         -0.02017452, -0.21784954],\n        [ 0.04377865,  0.05560155,  0.01711664, ..., -0.03077285,\n         -0.01455724, -0.00395874],\n        [-0.19672362,  0.00949095,  0.00326016, ...,  0.01430466,\n         -0.07016673, -0.10158334]]], shape=(1, 1000, 20))</pre></li><li>theta_1|participant_id_mu(chain, draw)float64-0.173 -0.1865 ... -0.06928 0.02014<pre>array([[-1.72988296e-01, -1.86501610e-01,  1.78895643e-01,\n         5.47990241e-02, -2.17509472e-01,  1.72327619e-01,\n        -1.74084837e-01,  2.47432580e-01, -1.26556759e-01,\n         1.32799675e-01,  2.78809385e-02,  1.14430279e-01,\n         6.53740171e-02, -9.36734524e-02,  2.04508786e-01,\n        -1.10540061e-01,  2.55428400e-01,  2.45181546e-02,\n         8.52270069e-02, -3.56337706e-02,  2.26642290e-01,\n        -3.34710771e-01,  5.63156343e-01, -4.69102121e-01,\n        -3.08857096e-02,  5.49334661e-02,  1.73714591e-01,\n        -9.42064468e-02, -1.97012281e-02, -1.27967653e-01,\n        -2.14222497e-01,  1.57202717e-01, -1.80845885e-01,\n         8.83921365e-02,  3.15946691e-02,  1.41145259e-01,\n         1.94581321e-01, -7.42095270e-01,  2.89566550e-01,\n         2.79108919e-01, -2.54516844e-01,  2.96365508e-01,\n         1.48413438e-01, -1.19527672e-01,  2.30307598e-01,\n        -2.20602673e-01,  1.47143063e-01,  5.58008307e-01,\n        -1.62701190e-01,  1.49748064e-01,  3.50884987e-01,\n        -3.25612186e-01, -5.99744099e-02, -2.52035034e-01,\n         1.50861048e-01, -1.12513657e-01,  8.96165684e-02,\n        -1.54560489e-01, -3.38842257e-02,  3.71696558e-03,\n...\n        -7.35176083e-01,  7.19737258e-01, -2.76820448e-01,\n         3.29400578e-01,  7.32786959e-02,  1.26608094e-02,\n        -1.32905917e-01,  1.75104840e-01, -5.43636038e-02,\n        -2.02928144e-01,  1.64843350e-01, -1.97919630e-01,\n        -1.02364438e-01,  6.56474728e-02,  3.30034759e-01,\n        -2.49290445e-01,  2.99181828e-01, -2.98897109e-01,\n         3.26379295e-01, -4.93297337e-01,  5.24268819e-01,\n        -6.83517326e-01,  3.39872848e-01,  8.68225661e-02,\n        -3.85601273e-01,  3.29791933e-01,  2.25299029e-01,\n         3.96602949e-01, -1.38652025e-01, -7.61136870e-02,\n         2.29055627e-01, -6.08990091e-02, -1.33439850e-01,\n         1.81225819e-01,  4.31032655e-01, -4.65012490e-01,\n         4.27459696e-01, -1.38885749e-02,  2.15989223e-03,\n         1.32174678e-01,  1.05982901e-01, -8.16316723e-02,\n        -1.27642516e-01, -6.82936554e-02,  6.25659252e-02,\n         8.61247455e-02, -4.42573047e-02,  3.31412734e-01,\n        -3.69618169e-01, -1.85354451e-01, -5.94095378e-02,\n        -2.12693292e-01, -2.12720175e-01,  2.98720995e-01,\n         9.24525321e-02,  1.94818506e-01, -6.92813016e-02,\n         2.01447870e-02]])</pre></li><li>a_1|participant_id_mu(chain, draw)float64-0.5176 -0.03743 ... -0.07059<pre>array([[-5.17606192e-01, -3.74279078e-02,  2.42247300e-02,\n         4.79491721e-02,  5.26879513e-02, -3.81763797e-01,\n         3.11529306e-01, -4.99778270e-01,  5.14148211e-01,\n        -4.67680915e-01, -2.16545791e-01, -8.73814842e-02,\n        -1.96748388e-01,  2.88046380e-01,  1.00967495e-01,\n         1.54339626e-02, -2.07241056e-01, -9.69528519e-02,\n        -8.46878380e-02,  1.62598009e-01,  2.54168096e-01,\n        -2.66214981e-01,  4.18090517e-01, -4.37887858e-01,\n         2.67990049e-01,  5.58279406e-02,  4.66928665e-01,\n         6.40557115e-03, -2.93366248e-01, -4.90869491e-01,\n         4.13536564e-01, -2.79860487e-01,  2.75148174e-01,\n        -2.41464178e-01,  5.01882801e-02, -1.47560644e-01,\n         2.78745184e-01, -2.71433281e-01,  4.49820797e-01,\n         9.44388449e-02, -8.82550912e-02,  1.00423157e-01,\n         9.13576308e-02,  9.51201489e-02, -2.80620167e-02,\n        -1.37583727e-01,  1.61995727e-01, -2.42260012e-01,\n         4.37823062e-01, -3.43052414e-01, -2.22587095e-01,\n         2.20959548e-01, -9.11324213e-02, -6.09219492e-01,\n         5.40880468e-01, -5.12098842e-01,  4.01514215e-01,\n        -4.88900138e-02,  6.78845295e-02,  3.26775272e-01,\n...\n        -2.47750773e-01,  3.18481181e-01,  9.48293276e-02,\n        -5.16672895e-03, -4.22999079e-01,  1.31542516e-01,\n        -3.81775563e-01,  1.27012170e-01, -1.89689176e-01,\n        -2.71153259e-02,  2.01379298e-02, -1.09588681e-02,\n         7.84585044e-02,  4.13840163e-02,  1.60484941e-01,\n        -2.49053308e-01,  2.46437660e-01, -3.41555371e-01,\n         2.06723508e-01, -2.61266833e-01,  3.28242495e-01,\n        -1.80536058e-01,  3.33148115e-01, -7.16411677e-02,\n         1.43392651e-01, -1.38789113e-01, -4.49194441e-02,\n         1.01564603e-01,  3.73853096e-01,  4.20348335e-01,\n         6.57617079e-01,  1.91825803e-02, -1.10621512e-01,\n         4.05333992e-01,  4.15560321e-01, -4.26953779e-01,\n         2.60867456e-01, -2.03458138e-01,  2.97748090e-01,\n        -1.57542482e-01, -6.16989121e-02,  6.52172696e-02,\n        -3.07865069e-03, -2.32999160e-01,  1.83130612e-01,\n        -1.57238430e-01,  3.86209362e-03, -3.52690255e-01,\n         2.16664462e-01,  2.67222389e-01,  5.74432970e-02,\n        -1.35381207e-01, -2.12329184e-01,  4.37522163e-01,\n         1.50098769e-01,  2.94721610e-02, -1.33617625e-01,\n        -7.05945812e-02]])</pre></li><li>t_1|participant_id_offset(chain, draw, participant_id__factor_dim)float64-0.1132 -0.5098 ... -0.07837<pre>array([[[-0.11319153, -0.50980784, -1.50189206, ..., -0.48255687,\n         -0.91137524, -1.05705763],\n        [-0.89003472, -0.05693196, -0.88370112, ..., -0.2994293 ,\n         -0.80236279,  0.25434259],\n        [ 0.32977578,  0.12432047, -0.01966985, ...,  0.97523294,\n          0.05649516, -0.41858979],\n        ...,\n        [ 0.67091365, -0.07203998, -0.93391015, ..., -0.1615238 ,\n          0.64288162,  0.53707425],\n        [-0.76818575,  0.18210745, -0.93884219, ...,  0.57229573,\n          0.83175562, -0.72260796],\n        [ 0.0957389 , -1.28088148,  0.75198807, ...,  0.57823193,\n         -0.10793749, -0.07836677]]], shape=(1, 1000, 20))</pre></li><li>rl.alpha_1|participant_id(chain, draw, rl.alpha_1|participant_id__factor_dim)float64-0.04401 -0.02926 ... 0.04916<pre>array([[[-0.04401137, -0.02925548, -0.03637587, ...,  0.09703209,\n         -0.03528065, -0.04054994],\n        [ 0.06975963,  0.05961252,  0.01477105, ...,  0.02343482,\n         -0.0317311 ,  0.00452219],\n        [-0.02175298, -0.06687916, -0.03939243, ..., -0.00575421,\n          0.04588629, -0.02010827],\n        ...,\n        [ 0.02980311,  0.03756029,  0.03422523, ..., -0.01523089,\n         -0.00298635,  0.02641869],\n        [ 0.06135148,  0.06432381, -0.07663655, ...,  0.07789413,\n         -0.02111723, -0.06764802],\n        [ 0.07776222,  0.02619089, -0.04242663, ...,  0.10324374,\n          0.17161943,  0.04915706]]], shape=(1, 1000, 20))</pre></li></ul></li><li>Indexes: (4)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li><li>rl.alpha_1|participant_id__factor_dimPandasIndex<pre>PandasIndex(Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n       '13', '14', '15', '16', '17', '18', '19'],\n      dtype='object', name='rl.alpha_1|participant_id__factor_dim'))</pre></li><li>participant_id__factor_dimPandasIndex<pre>PandasIndex(Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n       '13', '14', '15', '16', '17', '18', '19'],\n      dtype='object', name='participant_id__factor_dim'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-07-20T18:47:18.671108+00:00arviz_version :0.22.0inference_library :numpyroinference_library_version :0.18.0sampling_time :487.226472tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 32MB\nDimensions:      (chain: 1, draw: 1000, __obs__: 4000)\nCoordinates:\n  * chain        (chain) int64 8B 0\n  * draw         (draw) int64 8kB 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\n  * __obs__      (__obs__) int64 32kB 0 1 2 3 4 5 ... 3995 3996 3997 3998 3999\nData variables:\n    rt,response  (chain, draw, __obs__) float64 32MB -2.682 -3.113 ... -0.286\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 1000</li><li>__obs__: 4000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 ... 3996 3997 3998 3999<pre>array([   0,    1,    2, ..., 3997, 3998, 3999], shape=(4000,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-2.682 -3.113 ... -0.15 -0.286<pre>array([[[-2.68180738, -3.1131558 , -3.36654846, ..., -0.11400319,\n         -0.13231032, -0.27150678],\n        [-2.94921912, -3.25713017, -3.70359014, ..., -0.10952389,\n         -0.14484487, -0.25907136],\n        [-2.76213118, -3.08369583, -3.61708807, ..., -0.14674063,\n         -0.14837968, -0.29183587],\n        ...,\n        [-2.77744387, -3.13516264, -3.81714216, ..., -0.17997102,\n         -0.1338734 , -0.4233849 ],\n        [-2.79801769, -3.14988559, -3.6202565 , ..., -0.11229849,\n         -0.1417263 , -0.23178192],\n        [-2.62654867, -3.00779024, -3.53766374, ..., -0.12262796,\n         -0.15000616, -0.28601772]]], shape=(1, 1000, 4000))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999],\n      dtype='int64', name='__obs__', length=4000))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 57kB\nDimensions:          (chain: 1, draw: 1000)\nCoordinates:\n  * chain            (chain) int64 8B 0\n  * draw             (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    acceptance_rate  (chain, draw) float64 8kB 0.5476 0.876 ... 0.9808 0.9407\n    step_size        (chain, draw) float64 8kB 0.1505 0.1505 ... 0.1505 0.1505\n    diverging        (chain, draw) bool 1kB False False False ... False False\n    energy           (chain, draw) float64 8kB 4.088e+03 4.108e+03 ... 4.118e+03\n    n_steps          (chain, draw) int64 8kB 31 31 31 31 31 ... 31 31 31 31 31\n    tree_depth       (chain, draw) int64 8kB 5 5 5 5 5 5 5 5 ... 5 5 5 5 5 5 5 5\n    lp               (chain, draw) float64 8kB 4.022e+03 4.055e+03 ... 4.051e+03\nAttributes:\n    created_at:                  2025-07-20T18:47:18.683967+00:00\n    arviz_version:               0.22.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 1000</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li></ul></li><li>Data variables: (7)<ul><li>acceptance_rate(chain, draw)float640.5476 0.876 ... 0.9808 0.9407<pre>array([[0.5475757 , 0.8760392 , 0.99760499, 0.70706411, 0.98580543,\n        0.97514756, 0.91003521, 0.96938031, 0.79946268, 0.86648871,\n        0.92677635, 0.99763491, 0.9936993 , 0.92912045, 0.83300566,\n        0.99771545, 0.77330518, 0.89811744, 0.80356   , 0.94239138,\n        0.99523322, 0.98231126, 0.96072934, 0.89184016, 0.9982426 ,\n        0.9933107 , 0.97626353, 0.88236137, 0.98921112, 0.97780833,\n        0.78310763, 0.96397688, 0.85583606, 0.83847221, 0.81722472,\n        0.91946384, 0.93868761, 0.98693467, 0.92075171, 0.97474369,\n        0.96572287, 0.79709175, 0.95382768, 0.99458397, 0.92695574,\n        0.99460539, 0.96316358, 0.92956645, 0.98313184, 0.86165561,\n        0.87470945, 0.9469521 , 0.99374782, 0.99675238, 0.88621776,\n        0.88071367, 0.99701515, 0.91257752, 0.85220198, 0.99074998,\n        0.96549489, 0.95064145, 0.96405574, 0.97060511, 0.99533944,\n        0.66440107, 0.60743889, 0.97522126, 0.82253604, 0.67850556,\n        0.8554479 , 0.8597237 , 0.95480162, 0.86379808, 0.91724143,\n        0.99975599, 0.77403916, 0.88729076, 0.99986654, 0.99156634,\n        0.92995535, 0.7150027 , 0.94271203, 0.96538187, 0.76929346,\n        0.93943105, 0.81764924, 0.7600536 , 0.97033127, 1.        ,\n        0.99293247, 0.92609995, 0.9869216 , 0.95410847, 0.61264211,\n        0.95169585, 0.79099113, 0.92655421, 0.95196644, 0.99390149,\n...\n        0.92247412, 0.91726073, 0.99977262, 0.96680226, 0.95869989,\n        0.84586318, 0.85158769, 0.82359526, 0.93698604, 0.99717871,\n        0.91328513, 0.99830263, 0.98289924, 0.94776602, 0.69084968,\n        0.97906626, 0.90454312, 0.8602878 , 0.95503058, 0.83339491,\n        0.72938046, 0.91201347, 0.88457197, 0.93429365, 0.92206848,\n        0.9965517 , 0.98381221, 0.8718252 , 0.94868617, 0.94078106,\n        0.99830214, 0.69561586, 0.99434607, 0.89999002, 0.75630073,\n        0.99191183, 0.96717055, 0.85844516, 0.93210565, 0.99655554,\n        0.99710711, 0.94424272, 0.95276019, 0.98000821, 0.98270107,\n        0.9571486 , 0.94758654, 0.91824414, 0.93398463, 0.92110605,\n        0.94774135, 0.98156863, 0.92097905, 0.96430199, 0.99717978,\n        0.98624581, 0.99136044, 0.92027928, 0.94134243, 0.76029856,\n        0.97825698, 0.84204463, 0.98959103, 1.        , 0.82796714,\n        0.57964117, 0.99601189, 0.98292583, 0.97814724, 0.67164418,\n        0.70419726, 0.99651961, 0.90489377, 0.80554925, 0.99075309,\n        0.87866607, 0.87494231, 0.98512477, 0.97581869, 0.96055343,\n        0.84886049, 0.86744354, 0.99210951, 0.96273465, 0.64005096,\n        0.99688487, 0.58290988, 0.95206206, 0.97498534, 0.93891268,\n        0.99134341, 0.74310639, 0.95534338, 0.7943315 , 0.85634575,\n        0.95473641, 0.80394732, 0.98233825, 0.98078256, 0.94073592]])</pre></li><li>step_size(chain, draw)float640.1505 0.1505 ... 0.1505 0.1505<pre>array([[0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n...\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831,\n        0.15046831, 0.15046831, 0.15046831, 0.15046831, 0.15046831]])</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False]])</pre></li><li>energy(chain, draw)float644.088e+03 4.108e+03 ... 4.118e+03<pre>array([[4088.4235022 , 4108.13072092, 4117.23859119, 4124.4500633 ,\n        4109.52346914, 4103.50472132, 4084.6086295 , 4067.08769476,\n        4081.89519732, 4116.46086821, 4103.78833792, 4090.14972741,\n        4083.40006923, 4110.45773738, 4085.73544799, 4076.48120652,\n        4094.23430726, 4114.36856049, 4103.23312492, 4104.24736411,\n        4107.17788322, 4118.86685192, 4115.18139002, 4123.43588063,\n        4108.48274196, 4089.17743284, 4104.78128224, 4103.01764335,\n        4093.4094321 , 4098.67934933, 4099.99465514, 4099.82824577,\n        4089.19139139, 4113.32265517, 4107.19862271, 4097.62555897,\n        4090.29641807, 4100.83105068, 4104.29066989, 4099.07360317,\n        4089.58964134, 4106.30769478, 4106.42289284, 4103.39023235,\n        4089.10100757, 4089.01087473, 4086.80137743, 4089.65239763,\n        4096.59827693, 4105.84196072, 4121.85679139, 4113.57951708,\n        4113.48740017, 4095.26513292, 4096.09674118, 4109.43763849,\n        4097.51151944, 4104.33460965, 4106.25313119, 4101.70441241,\n        4085.28673035, 4100.17802379, 4094.02784714, 4090.97558534,\n        4086.91814333, 4087.75203653, 4105.56844689, 4103.78217655,\n        4105.23954306, 4084.4554359 , 4088.65649754, 4098.13479128,\n        4113.833344  , 4109.61867465, 4119.80719614, 4111.93931752,\n        4105.94156722, 4097.28275667, 4091.45892774, 4070.48281108,\n...\n        4085.69037081, 4095.96384493, 4097.35413507, 4094.80210425,\n        4097.60715717, 4098.20551456, 4100.99673338, 4091.83523832,\n        4099.98331812, 4100.3374485 , 4082.94268882, 4087.62757918,\n        4069.03477259, 4081.97416368, 4099.94557847, 4097.19605874,\n        4105.80649624, 4092.65819811, 4103.14941689, 4108.84007475,\n        4096.14261095, 4094.51133472, 4093.94026827, 4083.09024613,\n        4082.38934795, 4097.38582895, 4103.61462508, 4101.82810904,\n        4112.98127423, 4104.08178084, 4109.11174916, 4103.62738273,\n        4096.05624613, 4108.1397368 , 4104.50945129, 4098.87808533,\n        4098.873938  , 4102.19863657, 4099.58140843, 4102.04275198,\n        4101.90112171, 4117.24666362, 4095.06498874, 4091.84747741,\n        4088.49585595, 4092.77380206, 4075.80287525, 4073.18676222,\n        4070.24175443, 4062.48081512, 4077.42818652, 4100.23944473,\n        4102.96412245, 4113.35914961, 4119.65094028, 4116.04948609,\n        4107.37049234, 4110.10401404, 4093.69610385, 4090.41137647,\n        4098.24535261, 4118.64796792, 4111.11557054, 4106.6075414 ,\n        4102.95114212, 4091.39012748, 4095.42841479, 4104.10001445,\n        4091.03258375, 4096.16745771, 4075.93933689, 4069.29726534,\n        4072.47574137, 4084.5536177 , 4088.05817604, 4097.66834529,\n        4103.11326729, 4114.91387748, 4118.6328643 , 4117.79448249]])</pre></li><li>n_steps(chain, draw)int6431 31 31 31 31 ... 31 31 31 31 31<pre>array([[31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n...\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31]])</pre></li><li>tree_depth(chain, draw)int645 5 5 5 5 5 5 5 ... 5 5 5 5 5 5 5 5<pre>array([[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n...\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n        5, 5, 5, 5, 5, 5, 5, 5, 5, 5]])</pre></li><li>lp(chain, draw)float644.022e+03 4.055e+03 ... 4.051e+03<pre>array([[4021.75567232, 4055.22307157, 4045.90444217, 4028.33552066,\n        4041.77394848, 4023.0354148 , 4014.41524633, 4010.88343618,\n        4030.29665843, 4031.45329644, 4034.69768875, 4025.70285262,\n        4022.76513117, 4030.55893953, 4014.85046536, 4024.57497556,\n        4032.61988959, 4037.37606807, 4041.11090836, 4037.2316297 ,\n        4045.03938556, 4041.76637833, 4051.15442725, 4059.07583688,\n        4019.91145028, 4047.00210295, 4027.24309142, 4041.19572932,\n        4031.67213089, 4029.45722696, 4036.08328069, 4024.76024612,\n        4040.15902162, 4030.54427454, 4028.4397492 , 4023.32615121,\n        4036.55473985, 4030.49944779, 4037.31125393, 4020.4540023 ,\n        4025.5578618 , 4034.08871876, 4038.68431865, 4027.50854477,\n        4029.84159176, 4018.62040872, 4020.89736113, 4030.67968208,\n        4031.37328627, 4048.01918623, 4040.7315708 , 4049.051658  ,\n        4032.44408194, 4028.20920719, 4034.95228257, 4036.08588021,\n        4030.15144334, 4044.56046057, 4047.57530088, 4024.52097655,\n        4028.63839648, 4026.92835837, 4020.87636226, 4028.82094529,\n        4017.51886878, 4022.73565305, 4033.3998352 , 4037.55049052,\n        4021.64481385, 4025.39353092, 4023.08637661, 4044.18656458,\n        4042.57914726, 4042.2073327 , 4054.98641817, 4026.71006538,\n        4032.21799033, 4029.02442224, 4021.90520787, 4013.75753936,\n...\n        4021.2287874 , 4026.5270516 , 4036.54049418, 4020.88744373,\n        4029.74370487, 4033.74156021, 4017.63026427, 4023.01372405,\n        4023.20313897, 4030.21663897, 4019.67735721, 4015.99462854,\n        4009.1895431 , 4020.26928681, 4029.45276161, 4030.00460054,\n        4026.29741036, 4037.77790254, 4037.39437966, 4042.87595151,\n        4023.73979187, 4031.9097643 , 4026.55490423, 4013.87834788,\n        4027.85507008, 4031.10747761, 4032.11299307, 4045.41900422,\n        4045.97213796, 4039.66426744, 4036.95754681, 4028.36562744,\n        4035.00140382, 4035.79938008, 4036.29221682, 4030.73817456,\n        4038.06347775, 4025.33722638, 4028.31311677, 4039.27515598,\n        4038.89468653, 4038.57044386, 4033.14206992, 4024.21769737,\n        4018.50369244, 4015.73742544, 4016.81891762, 4013.72950253,\n        4002.54278644, 3999.68841161, 4028.5571682 , 4029.31516311,\n        4035.8271982 , 4039.14856894, 4042.03082891, 4038.16643332,\n        4036.21511346, 4030.4970066 , 4024.3410193 , 4027.62223094,\n        4041.88437839, 4047.88212546, 4046.06980254, 4025.44967392,\n        4028.98160169, 4015.63782175, 4033.97599789, 4032.14103278,\n        4024.57505786, 4023.79216555, 4012.21694367, 4006.21065011,\n        4021.95645956, 4019.94998271, 4032.23992721, 4031.60511166,\n        4036.32928974, 4043.51975725, 4043.59198945, 4051.44728441]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li></ul></li><li>Attributes: (4)created_at :2025-07-20T18:47:18.683967+00:00arviz_version :0.22.0modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 96kB\nDimensions:                  (__obs__: 4000, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 32kB 0 1 2 3 ... 3997 3998 3999\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 64kB ...\nAttributes:\n    created_at:                  2025-07-20T18:47:18.684574+00:00\n    arviz_version:               0.22.0\n    inference_library:           numpyro\n    inference_library_version:   0.18.0\n    sampling_time:               487.226472\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 4000</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 ... 3996 3997 3998 3999<pre>array([   0,    1,    2, ..., 3997, 3998, 3999], shape=(4000,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float640.9356 0.0 1.114 ... 1.0 1.197 1.0<pre>array([[0.93560183, 0.        ],\n       [1.11437929, 0.        ],\n       [0.56431085, 0.        ],\n       ...,\n       [1.32044899, 1.        ],\n       [1.39262187, 1.        ],\n       [1.19681287, 1.        ]], shape=(4000, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999],\n      dtype='int64', name='__obs__', length=4000))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-07-20T18:47:18.684574+00:00arviz_version :0.22.0inference_library :numpyroinference_library_version :0.18.0sampling_time :487.226472tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[12]: Copied! <pre># Define parameter names for analysis\n\nlist_group_mean_params = [\n    \"rl.alpha_Intercept\",\n    \"scaler_Intercept\",\n    \"a_Intercept\",\n    \"z_Intercept\",\n    \"t_Intercept\",\n    \"theta_Intercept\",\n]\n\nlist_group_sd_params = [\n    \"rl.alpha_1|participant_id_sigma\",\n    \"scaler_1|participant_id_sigma\",\n    \"a_1|participant_id_sigma\", \n    \"z_1|participant_id_sigma\",\n    \"t_1|participant_id_sigma\",\n    \"theta_1|participant_id_sigma\",\n]\n</pre> # Define parameter names for analysis  list_group_mean_params = [     \"rl.alpha_Intercept\",     \"scaler_Intercept\",     \"a_Intercept\",     \"z_Intercept\",     \"t_Intercept\",     \"theta_Intercept\", ]  list_group_sd_params = [     \"rl.alpha_1|participant_id_sigma\",     \"scaler_1|participant_id_sigma\",     \"a_1|participant_id_sigma\",      \"z_1|participant_id_sigma\",     \"t_1|participant_id_sigma\",     \"theta_1|participant_id_sigma\", ] In\u00a0[13]: Copied! <pre># Create mapping from HSSM model parameter names to ground truth values.\ndef create_ground_truth_mapping(savefile):\n\n    return {\n    \"rl.alpha_Intercept\": savefile['params_true_group']['rl_alpha_mean'],\n    \"scaler_Intercept\": savefile['params_true_group']['scaler_mean'],\n    \"a_Intercept\": savefile['params_true_group']['a_mean'],\n    \"z_Intercept\": savefile['params_true_group']['z_mean'],\n    \"t_Intercept\": savefile['params_true_group']['t_mean'],\n    \"theta_Intercept\": savefile['params_true_group']['theta_mean'],\n}\n\nground_truth_params = create_ground_truth_mapping(savefile)\nprint(\"Ground truth group means:\\n\")\nfor param, value in ground_truth_params.items():\n    print(f\"{param}: {value:.3f}\")\n</pre> # Create mapping from HSSM model parameter names to ground truth values. def create_ground_truth_mapping(savefile):      return {     \"rl.alpha_Intercept\": savefile['params_true_group']['rl_alpha_mean'],     \"scaler_Intercept\": savefile['params_true_group']['scaler_mean'],     \"a_Intercept\": savefile['params_true_group']['a_mean'],     \"z_Intercept\": savefile['params_true_group']['z_mean'],     \"t_Intercept\": savefile['params_true_group']['t_mean'],     \"theta_Intercept\": savefile['params_true_group']['theta_mean'], }  ground_truth_params = create_ground_truth_mapping(savefile) print(\"Ground truth group means:\\n\") for param, value in ground_truth_params.items():     print(f\"{param}: {value:.3f}\") <pre>Ground truth group means:\n\nrl.alpha_Intercept: 0.660\nscaler_Intercept: 2.785\na_Intercept: 1.481\nz_Intercept: 0.267\nt_Intercept: 0.443\ntheta_Intercept: 0.312\n</pre> In\u00a0[14]: Copied! <pre># Plot posterior distributions and MCMC traces for group-level parameters\n# Vertical lines show ground truth values for parameter recovery assessment\naz.plot_trace(idata_mcmc, var_names=list_group_mean_params,\n              lines=[(key_, {}, ground_truth_params[key_]) for key_ in ground_truth_params])\nplt.tight_layout()\n</pre> # Plot posterior distributions and MCMC traces for group-level parameters # Vertical lines show ground truth values for parameter recovery assessment az.plot_trace(idata_mcmc, var_names=list_group_mean_params,               lines=[(key_, {}, ground_truth_params[key_]) for key_ in ground_truth_params]) plt.tight_layout() In\u00a0[15]: Copied! <pre># plot the posterior pair plots of the group-level parameters\n# this will show the joint distributions and correlations between the group-level parameters.\naz.plot_pair(idata_mcmc, var_names=list_group_mean_params, kind='kde', point_estimate='mean')\nplt.tight_layout()\n</pre> # plot the posterior pair plots of the group-level parameters # this will show the joint distributions and correlations between the group-level parameters. az.plot_pair(idata_mcmc, var_names=list_group_mean_params, kind='kde', point_estimate='mean') plt.tight_layout() In\u00a0[16]: Copied! <pre># Extract ground truth subject-level parameters from the synthetic dataset\n# Reshape from dictionary format to matrix (subjects x parameters)\nsim_param_list = np.zeros((20, 6))\n\nfor ind, p in enumerate(savefile['params_true_subj'].keys()):\n    sim_param_list[:, ind] = savefile['params_true_subj'][p]\n</pre> # Extract ground truth subject-level parameters from the synthetic dataset # Reshape from dictionary format to matrix (subjects x parameters) sim_param_list = np.zeros((20, 6))  for ind, p in enumerate(savefile['params_true_subj'].keys()):     sim_param_list[:, ind] = savefile['params_true_subj'][p] In\u00a0[17]: Copied! <pre># Function to extract subject-level parameters from inference data.\ndef extract_subject_parameters(idata, param_names):\n    \n    n_subjects = idata.posterior[f'{param_names[0]}_1|participant_id'].shape[-1]\n    n_params = len(param_names)\n    \n    subject_params = np.zeros((n_subjects, n_params))\n    \n    for i, param in enumerate(param_names):\n        intercept = np.mean(idata.posterior[f'{param}_Intercept'].values[0])\n        random_effects = np.mean(idata.posterior[f'{param}_1|participant_id'].values[0], axis=0)\n        subject_params[:, i] = intercept + random_effects\n    \n    return subject_params\n\n# Extract recovered parameters\nrecov_param_list = extract_subject_parameters(idata_mcmc, model_config.list_params)\n</pre> # Function to extract subject-level parameters from inference data. def extract_subject_parameters(idata, param_names):          n_subjects = idata.posterior[f'{param_names[0]}_1|participant_id'].shape[-1]     n_params = len(param_names)          subject_params = np.zeros((n_subjects, n_params))          for i, param in enumerate(param_names):         intercept = np.mean(idata.posterior[f'{param}_Intercept'].values[0])         random_effects = np.mean(idata.posterior[f'{param}_1|participant_id'].values[0], axis=0)         subject_params[:, i] = intercept + random_effects          return subject_params  # Extract recovered parameters recov_param_list = extract_subject_parameters(idata_mcmc, model_config.list_params) In\u00a0[18]: Copied! <pre>plot_param_ranges = [[0.5, 0.9], [2.2, 3.5], [1.2, 2], [0, 0.4], [0.3, 0.7], [0.2, 0.5]]\nplot_param_names = [\"rl.alpha\", \"scaler\", \"a\", \"z\", \"t\", \"theta\"]\n</pre> plot_param_ranges = [[0.5, 0.9], [2.2, 3.5], [1.2, 2], [0, 0.4], [0.3, 0.7], [0.2, 0.5]] plot_param_names = [\"rl.alpha\", \"scaler\", \"a\", \"z\", \"t\", \"theta\"] In\u00a0[19]: Copied! <pre># Function to create parameter recovery plots comparing simulated vs recovered values\ndef plot_parameter_recovery(sim_params, recov_params, param_names, axes_limits, \n                          additional_data=None, additional_label=None, show_correlation=False):\n\n    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n    axes = axes.flatten()\n\n    for i, ax in enumerate(axes):\n        if i &gt;= len(param_names):\n            ax.set_visible(False)\n            continue\n\n        x, y = sim_params[:, i], recov_params[:, i]\n        \n        # Scatter plot showing parameter recovery \n        ax.scatter(x, y, alpha=0.6, label='MCMC' if additional_data is None else 'MCMC')\n            \n        # Additional data if provided\n        if additional_data is not None:\n            z = additional_data[:, i]\n            ax.scatter(x, z, alpha=0.6, marker='x', label=additional_label or 'Additional')\n            ax.legend(loc='lower right')\n\n        # Calculate and display correlation between true and recovered parameters\n        if show_correlation:\n            spearman_r, _ = spearmanr(x, y)\n            ax.text(0.05, 0.93, f\"R: {spearman_r:.2f}\", transform=ax.transAxes, \n                        fontsize=12, verticalalignment='bottom')\n\n        # Formatting subplot\n        ax.set_title(param_names[i], fontsize=16)\n        ax.set_xlim(axes_limits[i])\n        ax.set_ylim(axes_limits[i])\n        ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=5))\n        ax.yaxis.set_major_locator(ticker.MaxNLocator(nbins=5))\n        ax.grid(True, linestyle='--', alpha=0.6)\n        ax.axline((0, 0), linestyle='--', slope=1, c='k', alpha=0.8)  # Perfect recovery line\n\n    # Add axis labels\n    fig.text(0.5, 0.02, 'Simulated', ha='center', fontsize=20)\n    fig.text(0.02, 0.5, 'Recovered', va='center', rotation='vertical', fontsize=20)\n\n    plt.tight_layout(rect=[0.05, 0.05, 1, 1])\n    return fig\n\n# Plot parameter recovery\nplot_parameter_recovery(sim_param_list, recov_param_list, plot_param_names, plot_param_ranges, show_correlation=True)\nplt.show()\n</pre> # Function to create parameter recovery plots comparing simulated vs recovered values def plot_parameter_recovery(sim_params, recov_params, param_names, axes_limits,                            additional_data=None, additional_label=None, show_correlation=False):      fig, axes = plt.subplots(2, 3, figsize=(12, 8))     axes = axes.flatten()      for i, ax in enumerate(axes):         if i &gt;= len(param_names):             ax.set_visible(False)             continue          x, y = sim_params[:, i], recov_params[:, i]                  # Scatter plot showing parameter recovery          ax.scatter(x, y, alpha=0.6, label='MCMC' if additional_data is None else 'MCMC')                      # Additional data if provided         if additional_data is not None:             z = additional_data[:, i]             ax.scatter(x, z, alpha=0.6, marker='x', label=additional_label or 'Additional')             ax.legend(loc='lower right')          # Calculate and display correlation between true and recovered parameters         if show_correlation:             spearman_r, _ = spearmanr(x, y)             ax.text(0.05, 0.93, f\"R: {spearman_r:.2f}\", transform=ax.transAxes,                          fontsize=12, verticalalignment='bottom')          # Formatting subplot         ax.set_title(param_names[i], fontsize=16)         ax.set_xlim(axes_limits[i])         ax.set_ylim(axes_limits[i])         ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=5))         ax.yaxis.set_major_locator(ticker.MaxNLocator(nbins=5))         ax.grid(True, linestyle='--', alpha=0.6)         ax.axline((0, 0), linestyle='--', slope=1, c='k', alpha=0.8)  # Perfect recovery line      # Add axis labels     fig.text(0.5, 0.02, 'Simulated', ha='center', fontsize=20)     fig.text(0.02, 0.5, 'Recovered', va='center', rotation='vertical', fontsize=20)      plt.tight_layout(rect=[0.05, 0.05, 1, 1])     return fig  # Plot parameter recovery plot_parameter_recovery(sim_param_list, recov_param_list, plot_param_names, plot_param_ranges, show_correlation=True) plt.show() In\u00a0[20]: Copied! <pre># Run variational inference (VI) as a faster alternative to MCMC\n# VI approximates the posterior with a simpler distribution family\nidata_vi = hssm_model.vi(\n    niter=30000,    # Number of optimization iterations\n    draws=1000,     # Number of samples from approximate posterior\n    method=\"advi\"   # Automatic Differentiation Variational Inference\n)\n</pre> # Run variational inference (VI) as a faster alternative to MCMC # VI approximates the posterior with a simpler distribution family idata_vi = hssm_model.vi(     niter=30000,    # Number of optimization iterations     draws=1000,     # Number of samples from approximate posterior     method=\"advi\"   # Automatic Differentiation Variational Inference ) <pre>Using MCMC starting point defaults.\n</pre> <pre>Output()</pre> <pre></pre> <pre>Finished [100%]: Average Loss = 3,948\n</pre> <p>We now examine the VI loss over iterations. In general, this looks good with the caveat that there are oscillations during the initial iterations. In principle, this could arise from the model geometry, priors or simply because of an aggressive learning rate. We recommend users experiment with different settings to figure out what works best for their model.</p> In\u00a0[21]: Copied! <pre>plt.plot(hssm_model.vi_approx.hist)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\n</pre> plt.plot(hssm_model.vi_approx.hist) plt.xlabel(\"Iteration\") plt.ylabel(\"Loss\") Out[21]: <pre>Text(0, 0.5, 'Loss')</pre> In\u00a0[22]: Copied! <pre>idata_vi\n</pre> idata_vi Out[22]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 2MB\nDimensions:                                (chain: 1, draw: 1000,\n                                            participant_id__factor_dim: 20,\n                                            rl.alpha_1|participant_id__factor_dim: 20)\nCoordinates:\n  * chain                                  (chain) int64 8B 0\n  * draw                                   (draw) int64 8kB 0 1 2 ... 998 999\n  * rl.alpha_1|participant_id__factor_dim  (rl.alpha_1|participant_id__factor_dim) &lt;U2 160B ...\n  * participant_id__factor_dim             (participant_id__factor_dim) &lt;U2 160B ...\nData variables: (12/30)\n    a_1|participant_id                     (chain, draw, participant_id__factor_dim) float64 160kB ...\n    t_1|participant_id                     (chain, draw, participant_id__factor_dim) float64 160kB ...\n    t_1|participant_id_mu                  (chain, draw) float64 8kB -0.4449 ...\n    scaler_Intercept                       (chain, draw) float64 8kB 2.467 .....\n    scaler_1|participant_id_offset         (chain, draw, participant_id__factor_dim) float64 160kB ...\n    z_1|participant_id                     (chain, draw, participant_id__factor_dim) float64 160kB ...\n    ...                                     ...\n    t_Intercept                            (chain, draw) float64 8kB 0.2921 ....\n    theta_1|participant_id_sigma           (chain, draw) float64 8kB 0.02817 ...\n    z_1|participant_id_mu                  (chain, draw) float64 8kB -0.2323 ...\n    theta_1|participant_id_mu              (chain, draw) float64 8kB -0.5682 ...\n    a_1|participant_id_mu                  (chain, draw) float64 8kB -0.4297 ...\n    t_1|participant_id_offset              (chain, draw, participant_id__factor_dim) float64 160kB ...\nAttributes:\n    created_at:                 2025-07-20T18:52:02.063675+00:00\n    arviz_version:              0.22.0\n    inference_library:          pymc\n    inference_library_version:  5.23.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 1000</li><li>participant_id__factor_dim: 20</li><li>rl.alpha_1|participant_id__factor_dim: 20</li></ul></li><li>Coordinates: (4)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li><li>rl.alpha_1|participant_id__factor_dim(rl.alpha_1|participant_id__factor_dim)&lt;U2'0' '1' '2' '3' ... '17' '18' '19'<pre>array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n       '13', '14', '15', '16', '17', '18', '19'], dtype='&lt;U2')</pre></li><li>participant_id__factor_dim(participant_id__factor_dim)&lt;U2'0' '1' '2' '3' ... '17' '18' '19'<pre>array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n       '13', '14', '15', '16', '17', '18', '19'], dtype='&lt;U2')</pre></li></ul></li><li>Data variables: (30)<ul><li>a_1|participant_id(chain, draw, participant_id__factor_dim)float640.009811 -0.01279 ... -0.01175<pre>array([[[ 0.00981125, -0.01279248,  0.01253079, ..., -0.0031118 ,\n          0.00869498, -0.02628247],\n        [ 0.00676756, -0.01928146,  0.02148899, ..., -0.01067237,\n         -0.01324734,  0.03438993],\n        [-0.01652379,  0.05173379, -0.01328357, ...,  0.026884  ,\n          0.00511407, -0.03402523],\n        ...,\n        [ 0.00180163,  0.00869004,  0.02823356, ..., -0.01460367,\n         -0.00536896,  0.01822814],\n        [-0.01771572, -0.0145507 ,  0.00997734, ...,  0.00820832,\n         -0.01368857, -0.01040565],\n        [-0.00656103,  0.00494276, -0.00668855, ...,  0.0148023 ,\n          0.00781951, -0.01174938]]], shape=(1, 1000, 20))</pre></li><li>t_1|participant_id(chain, draw, participant_id__factor_dim)float640.06699 0.1197 ... 0.1085 0.06855<pre>array([[[0.06698918, 0.11973837, 0.0976882 , ..., 0.18683392,\n         0.14170698, 0.06658513],\n        [0.10783791, 0.11467953, 0.09676167, ..., 0.17559194,\n         0.12779506, 0.11146702],\n        [0.10019031, 0.09957611, 0.10013011, ..., 0.15353915,\n         0.13189925, 0.07064564],\n        ...,\n        [0.1228905 , 0.11829122, 0.09936955, ..., 0.20232355,\n         0.15667759, 0.10671404],\n        [0.09207142, 0.06697767, 0.12099268, ..., 0.14576104,\n         0.15171934, 0.09816796],\n        [0.09464787, 0.12023255, 0.13738661, ..., 0.14428041,\n         0.10846253, 0.06854773]]], shape=(1, 1000, 20))</pre></li><li>t_1|participant_id_mu(chain, draw)float64-0.4449 -0.1261 ... -0.2494 -0.3705<pre>array([[-4.44869236e-01, -1.26131990e-01,  1.27514016e-01,\n        -1.70356422e-01,  1.25053305e-01,  8.27546109e-02,\n        -8.82349390e-02,  1.16328153e-02,  3.72657005e-01,\n        -3.45901721e-01, -1.70268528e-01,  1.35449166e-01,\n         1.42889203e-01,  4.53000639e-02,  1.67983150e-01,\n         2.99523452e-01, -2.73186790e-01, -9.44633652e-02,\n         4.08966058e-01, -9.49973582e-02, -3.66158800e-02,\n         1.99886060e-01,  2.55610924e-01, -8.46390946e-02,\n         3.12714534e-01,  3.75030365e-02, -3.43633179e-01,\n        -1.10380263e-01, -2.19967247e-01, -4.20391663e-01,\n         2.86596256e-01,  2.61095100e-01, -9.62664789e-02,\n        -1.63651638e-01,  1.32337090e-01,  4.02906433e-02,\n         4.14866123e-01, -4.11085845e-02,  2.40157295e-01,\n        -2.25549566e-01, -6.81396732e-03,  4.50544722e-01,\n        -1.50149542e-01,  1.18931798e-01, -2.71717720e-02,\n         1.28051697e-02, -1.11006051e-01, -3.71478990e-01,\n        -1.98258029e-01, -1.41571744e-01,  7.49368488e-02,\n         4.00028418e-01,  3.53502899e-01,  1.82873549e-01,\n        -4.99333215e-01, -1.18449806e-01,  2.44021727e-01,\n         1.64850413e-02, -1.84016873e-01, -1.48586183e-01,\n...\n         4.33278546e-01,  2.67483708e-01,  1.99090310e-01,\n        -2.22166406e-01,  2.78838861e-01, -4.88902678e-02,\n        -1.87948377e-02,  2.55632678e-01, -8.61427943e-02,\n         3.05210131e-01,  2.68908432e-01,  3.48745087e-01,\n         4.33887172e-01,  2.26884244e-01, -3.46049520e-01,\n        -4.88907452e-02,  6.40878846e-01,  1.30141913e-01,\n        -3.87361212e-01,  3.20309167e-02, -1.01025674e-01,\n        -3.45973340e-02,  2.22098212e-01,  5.09332637e-02,\n        -1.63003490e-01,  3.18651567e-02,  1.37655749e-01,\n        -1.29943879e-01,  6.84845446e-02, -3.94874482e-01,\n        -1.59096615e-01,  1.50541759e-01,  2.59380929e-01,\n        -3.34631001e-02, -4.29048555e-01, -5.40626059e-03,\n         6.04147425e-01, -1.21170882e-01,  2.91038077e-01,\n         4.35069044e-01, -2.16492077e-01, -2.69698547e-01,\n         4.37104606e-01, -1.57718927e-01, -3.21560358e-02,\n         1.37973216e-01,  2.55775543e-01, -3.22912418e-01,\n         1.77084952e-01,  9.35963822e-02,  4.78730406e-01,\n         9.10645013e-02, -1.52381830e-01,  9.79493089e-02,\n         2.66703821e-01, -3.18621190e-01, -2.49447980e-01,\n        -3.70473127e-01]])</pre></li><li>scaler_Intercept(chain, draw)float642.467 2.537 2.511 ... 2.511 2.51<pre>array([[2.46668664, 2.53694506, 2.51133663, 2.45209689, 2.48316485,\n        2.55916935, 2.49995907, 2.55771751, 2.53810768, 2.55991284,\n        2.43161909, 2.45425828, 2.49190207, 2.61309122, 2.48968322,\n        2.55078397, 2.46134276, 2.54187861, 2.5121303 , 2.52958401,\n        2.53983043, 2.4699692 , 2.5327444 , 2.52889376, 2.53606407,\n        2.57813393, 2.53628793, 2.48775922, 2.55896193, 2.50522596,\n        2.56698731, 2.54280336, 2.53144465, 2.57905263, 2.57510069,\n        2.44598727, 2.54519388, 2.5232067 , 2.51817285, 2.49390742,\n        2.56617243, 2.50601605, 2.60605192, 2.5013306 , 2.52617941,\n        2.54705725, 2.53282999, 2.49164552, 2.5114923 , 2.56080322,\n        2.56321507, 2.51223187, 2.49399085, 2.53599115, 2.53607984,\n        2.46810545, 2.52219701, 2.52814729, 2.56723609, 2.51681019,\n        2.5405405 , 2.54460465, 2.50954769, 2.44986647, 2.5233318 ,\n        2.55180207, 2.49822578, 2.46462421, 2.51695563, 2.5760543 ,\n        2.49071897, 2.53671443, 2.52137859, 2.49279577, 2.56306052,\n        2.51822702, 2.48126451, 2.53924926, 2.52052455, 2.55733151,\n        2.5555121 , 2.49973208, 2.4258734 , 2.52271382, 2.56863276,\n        2.50688268, 2.5276936 , 2.55782198, 2.54776021, 2.52234439,\n        2.51268823, 2.49676087, 2.45900705, 2.56125384, 2.58503657,\n        2.51138746, 2.5207078 , 2.50366697, 2.60019579, 2.52316622,\n...\n        2.49535999, 2.49353004, 2.51535143, 2.50877841, 2.53171605,\n        2.5040428 , 2.49726655, 2.58736434, 2.50082657, 2.51649268,\n        2.51391987, 2.54554567, 2.47728797, 2.55767966, 2.52847382,\n        2.5275088 , 2.48759759, 2.57934197, 2.5390261 , 2.52405736,\n        2.5083195 , 2.52828045, 2.48662765, 2.54373354, 2.47919176,\n        2.55184322, 2.54845278, 2.5253251 , 2.43603071, 2.50998873,\n        2.55662323, 2.51234307, 2.54215468, 2.50436186, 2.55131122,\n        2.50785135, 2.52159214, 2.51483463, 2.48000642, 2.53509105,\n        2.59640706, 2.57055426, 2.5199336 , 2.53558887, 2.54747615,\n        2.54918386, 2.4937412 , 2.55104483, 2.56927646, 2.47999365,\n        2.54176743, 2.50020969, 2.51363282, 2.46564002, 2.52016968,\n        2.51775231, 2.45376469, 2.5102963 , 2.52861009, 2.57914797,\n        2.51853096, 2.55568962, 2.48797913, 2.55683255, 2.49639209,\n        2.50202337, 2.48911369, 2.53217756, 2.49591661, 2.53238383,\n        2.5269081 , 2.50092083, 2.61131655, 2.54845163, 2.45762896,\n        2.49086413, 2.45861723, 2.53776596, 2.51014937, 2.5351602 ,\n        2.50708415, 2.56923419, 2.52557381, 2.52282752, 2.5321681 ,\n        2.52414574, 2.52465288, 2.51222939, 2.52335651, 2.47154948,\n        2.5429307 , 2.49086182, 2.5838882 , 2.53529082, 2.47341935,\n        2.56350285, 2.42615172, 2.50141865, 2.51118134, 2.50964645]])</pre></li><li>scaler_1|participant_id_offset(chain, draw, participant_id__factor_dim)float64-2.051 0.01014 ... 0.146 -0.4999<pre>array([[[-2.05056742,  0.01013797,  0.3096852 , ...,  0.61903501,\n         -0.98708426, -0.09978186],\n        [-0.47712975,  1.93102074,  0.41323559, ...,  0.5200364 ,\n         -0.02210596,  0.48633275],\n        [ 0.1018957 ,  1.21269371, -0.15358574, ..., -1.748301  ,\n          0.41975086, -0.0391415 ],\n        ...,\n        [-0.22379294,  0.92925403,  0.63543299, ..., -0.92959947,\n         -0.15722627, -1.64792145],\n        [ 0.26352159,  0.67244945, -1.16656067, ...,  1.70958957,\n          1.2246781 , -0.87401142],\n        [-0.82687921,  1.9860926 ,  0.484801  , ...,  1.08380526,\n          0.14598461, -0.49989306]]], shape=(1, 1000, 20))</pre></li><li>z_1|participant_id(chain, draw, participant_id__factor_dim)float64-0.0126 -0.0003094 ... -0.03725<pre>array([[[-0.01260287, -0.00030942, -0.01762753, ..., -0.00328704,\n         -0.00377739, -0.03525674],\n        [-0.02404727,  0.03184434,  0.00414282, ...,  0.01609594,\n         -0.00209229, -0.01666323],\n        [ 0.01299981,  0.01343778,  0.00012212, ...,  0.00091332,\n         -0.00366639, -0.02015528],\n        ...,\n        [-0.0153904 , -0.00830649, -0.02506885, ...,  0.00778813,\n         -0.01089155, -0.01698397],\n        [-0.02847758, -0.01112451,  0.00300393, ...,  0.00411667,\n          0.00824547, -0.03331102],\n        [-0.0338899 ,  0.00439246, -0.0042456 , ...,  0.01489634,\n          0.0054235 , -0.03724796]]], shape=(1, 1000, 20))</pre></li><li>z_1|participant_id_sigma(chain, draw)float640.01769 0.02196 ... 0.0189 0.0204<pre>array([[0.01768763, 0.02196054, 0.01282223, 0.01877913, 0.02173432,\n        0.01776114, 0.02100703, 0.01408495, 0.01865425, 0.02724921,\n        0.01996346, 0.01844374, 0.01753511, 0.02182888, 0.0165639 ,\n        0.02143928, 0.01226699, 0.01746595, 0.01729078, 0.01610156,\n        0.01642605, 0.02429944, 0.02289353, 0.01798493, 0.01360491,\n        0.0182082 , 0.01725314, 0.02058778, 0.02566167, 0.01834114,\n        0.01847777, 0.01607485, 0.01173945, 0.02032805, 0.02215054,\n        0.02756877, 0.0128184 , 0.02318176, 0.01750824, 0.01791518,\n        0.01382616, 0.01979906, 0.01237262, 0.01935702, 0.01683644,\n        0.02343752, 0.02302447, 0.01305799, 0.014105  , 0.01789662,\n        0.01726956, 0.02248663, 0.02165924, 0.02129203, 0.01756841,\n        0.02193921, 0.02144741, 0.01734652, 0.01836023, 0.02166007,\n        0.01791985, 0.01904091, 0.02232749, 0.01443726, 0.01229193,\n        0.01970281, 0.01779237, 0.02464397, 0.01922153, 0.01702077,\n        0.02322277, 0.01980192, 0.01530466, 0.02035919, 0.02040948,\n        0.01492015, 0.0106392 , 0.02703627, 0.01856139, 0.01836967,\n        0.01227869, 0.0197817 , 0.01754997, 0.01269564, 0.01845457,\n        0.01545485, 0.0242043 , 0.02406758, 0.01875679, 0.0135004 ,\n        0.01582279, 0.01979154, 0.02461121, 0.01505679, 0.01530473,\n        0.01855109, 0.02049959, 0.01715641, 0.01735586, 0.01516796,\n...\n        0.0182112 , 0.01615347, 0.01661822, 0.01680489, 0.02012121,\n        0.02152638, 0.01728438, 0.01803719, 0.01649261, 0.0297497 ,\n        0.0223555 , 0.019749  , 0.02323316, 0.01497067, 0.01819934,\n        0.01397841, 0.02257096, 0.01683453, 0.02080889, 0.01997412,\n        0.01635431, 0.01889003, 0.02218014, 0.01779321, 0.01903556,\n        0.02226202, 0.01808286, 0.01497717, 0.01749455, 0.01891815,\n        0.01657425, 0.02149785, 0.02800586, 0.02594862, 0.02131031,\n        0.02122464, 0.01720153, 0.01968611, 0.0177847 , 0.01587109,\n        0.01564725, 0.0170264 , 0.02016613, 0.01545879, 0.02018622,\n        0.01498049, 0.01562665, 0.01999499, 0.03273284, 0.01486175,\n        0.01446128, 0.02165726, 0.02183804, 0.02286723, 0.02273278,\n        0.01845768, 0.02356461, 0.02632817, 0.01740204, 0.01900267,\n        0.02464645, 0.01554929, 0.01802683, 0.01647705, 0.01598823,\n        0.02294864, 0.02373547, 0.0201266 , 0.01481242, 0.01636433,\n        0.02477661, 0.02006949, 0.0170959 , 0.02065566, 0.02132151,\n        0.01732133, 0.02124346, 0.01857334, 0.01378792, 0.01439492,\n        0.01605833, 0.01642529, 0.02419672, 0.01889636, 0.02870088,\n        0.02341484, 0.02374663, 0.01738831, 0.01984404, 0.0219833 ,\n        0.01269337, 0.01937981, 0.01541226, 0.01855929, 0.02503822,\n        0.02102043, 0.01359588, 0.01768511, 0.01889587, 0.02040123]])</pre></li><li>scaler_1|participant_id_mu(chain, draw)float640.1667 -0.4928 ... -0.2971 -0.02243<pre>array([[ 1.66718886e-01, -4.92810051e-01,  2.49604514e-01,\n         1.91712497e-01,  1.39879382e-01,  2.48930565e-01,\n         3.29527728e-01,  1.99374475e-01,  4.83336574e-01,\n         1.21821989e-01, -5.20798406e-01,  6.35162434e-01,\n         1.31910095e-01,  3.94712000e-02, -7.35005045e-02,\n         8.18829804e-02, -9.34455964e-02,  1.17359525e-02,\n         8.97539401e-01, -3.52472743e-01,  2.00567608e-01,\n        -1.46710083e-01,  4.53725809e-01, -2.65762531e-01,\n        -1.37972244e-01,  3.58946763e-01,  8.97710395e-02,\n         4.57382697e-01,  2.28229427e-01,  6.79447487e-02,\n        -6.06957358e-01, -1.02189278e-01, -3.40112189e-02,\n         2.40134621e-01, -2.80102045e-01, -1.59125901e-01,\n        -2.85835298e-02,  1.27305476e-01,  3.89032486e-01,\n         5.35608546e-01,  1.14245020e-01, -7.85346906e-02,\n         1.60466628e-02,  1.29803123e-01,  1.56841152e-01,\n        -3.02959711e-01,  5.01623309e-02, -3.99002170e-01,\n         5.40466079e-01, -2.17407225e-01, -2.90357887e-02,\n        -2.43405009e-01,  2.74388174e-01,  2.93874582e-01,\n        -1.85826217e-02, -1.50295298e-01, -7.08193530e-02,\n        -9.44385503e-02,  1.06985272e-01,  1.35132993e-01,\n...\n         2.36140794e-02,  4.66696513e-01,  4.04794764e-01,\n        -1.73327305e-01,  2.34884650e-01, -3.52197786e-01,\n        -1.14831892e-02,  1.30762454e-01, -3.13566005e-01,\n         2.81261753e-01, -2.68735606e-01, -7.73738136e-02,\n         2.95515715e-01, -3.55710310e-01,  2.40106950e-01,\n        -2.59535971e-01, -1.50867559e-01, -1.20474646e-01,\n        -3.60008484e-01,  1.59644633e-01, -1.99755938e-01,\n         1.39613970e-01, -4.61930869e-01,  1.44703201e-03,\n        -2.25879546e-01,  6.36498237e-01,  7.77676713e-02,\n         2.33594625e-01, -1.95298759e-01, -3.13602263e-01,\n         1.16790011e-01, -2.67732210e-01, -4.61336825e-02,\n        -1.85869630e-01, -2.08399651e-01, -5.27873617e-02,\n        -1.44372757e-01, -1.14865515e-01,  3.80625456e-01,\n        -6.66676521e-01, -3.24032623e-01, -5.96068501e-02,\n         5.50018804e-01, -1.15682370e-02,  1.92464418e-01,\n         3.40444970e-01,  1.90206412e-01, -1.08762586e-01,\n        -7.14491456e-02,  1.15562762e-01,  1.11346165e-01,\n        -1.83921045e-01, -2.39224403e-01,  5.05086112e-01,\n        -4.80386293e-01,  6.11689161e-01, -2.97146419e-01,\n        -2.24306460e-02]])</pre></li><li>theta_1|participant_id_offset(chain, draw, participant_id__factor_dim)float641.845 0.9489 ... -0.4994 1.634<pre>array([[[ 1.84487797,  0.94892696,  0.0813614 , ...,  0.14225082,\n         -0.46791861,  1.48452641],\n        [-0.13250166,  1.69448923,  0.28336177, ..., -2.58866164,\n          1.13377349,  1.41427727],\n        [ 1.07806164, -1.90850509,  0.28230436, ..., -0.65609434,\n          0.29587402,  0.84088359],\n        ...,\n        [-0.75852251,  1.27028419,  1.36607952, ..., -1.25939481,\n         -0.34827874,  0.36862489],\n        [-0.30708237, -0.15671568,  0.23978728, ..., -1.98572042,\n         -0.45036283,  0.70063784],\n        [ 0.78031109,  0.55647358,  1.50083522, ..., -0.97453689,\n         -0.49935322,  1.63387678]]], shape=(1, 1000, 20))</pre></li><li>z_Intercept(chain, draw)float640.3013 0.3018 ... 0.3037 0.299<pre>array([[0.30129849, 0.30179498, 0.30226611, 0.30216419, 0.30656041,\n        0.30727168, 0.29409031, 0.30863984, 0.30590928, 0.29824375,\n        0.30232733, 0.30315366, 0.29827418, 0.30321472, 0.30874191,\n        0.30727393, 0.29762781, 0.30784545, 0.29966194, 0.29451048,\n        0.29869251, 0.29321025, 0.30197285, 0.30800082, 0.30131632,\n        0.29852211, 0.30472186, 0.2941633 , 0.30043677, 0.30274337,\n        0.29846182, 0.31097606, 0.30017447, 0.29699566, 0.2917486 ,\n        0.30030023, 0.30234865, 0.30105044, 0.30656191, 0.3010334 ,\n        0.29067112, 0.30802342, 0.30416302, 0.30000657, 0.29631517,\n        0.31353814, 0.29840197, 0.30382821, 0.29543259, 0.30003498,\n        0.29904178, 0.30023086, 0.29980465, 0.30336365, 0.29781537,\n        0.30647996, 0.28961269, 0.30772068, 0.31056378, 0.30509174,\n        0.2914672 , 0.2981606 , 0.29773992, 0.29894338, 0.30847409,\n        0.30055403, 0.29687721, 0.30066423, 0.30485201, 0.30254926,\n        0.30871135, 0.29611152, 0.30182794, 0.30440997, 0.29866359,\n        0.29436491, 0.30272598, 0.30341402, 0.29894465, 0.30052749,\n        0.31468864, 0.3048524 , 0.30146792, 0.30067345, 0.30097367,\n        0.2938498 , 0.29444452, 0.30307036, 0.30628871, 0.3066151 ,\n        0.30382806, 0.29623993, 0.2999088 , 0.29321326, 0.29810928,\n        0.30034426, 0.30438946, 0.29481583, 0.30180433, 0.29540613,\n...\n        0.31515541, 0.29490199, 0.30795776, 0.3072951 , 0.29938111,\n        0.30068378, 0.2930105 , 0.28872691, 0.29856199, 0.29645598,\n        0.30137028, 0.30023199, 0.30677153, 0.31043244, 0.29935305,\n        0.29840373, 0.30415101, 0.3007316 , 0.30415592, 0.30417207,\n        0.30277438, 0.29627318, 0.30642382, 0.30047257, 0.29574355,\n        0.29988342, 0.29572835, 0.31334069, 0.29573381, 0.30419243,\n        0.29344086, 0.29924163, 0.30475067, 0.30544483, 0.29893747,\n        0.29136189, 0.29875869, 0.29296179, 0.30133838, 0.29802906,\n        0.30440269, 0.29894577, 0.30694578, 0.30711285, 0.2999953 ,\n        0.30085528, 0.29567024, 0.29489595, 0.30140971, 0.30236982,\n        0.3049399 , 0.29900761, 0.3024741 , 0.30268531, 0.30538464,\n        0.30346667, 0.29917091, 0.2921097 , 0.30893056, 0.30710954,\n        0.29964955, 0.29880497, 0.29829011, 0.30447616, 0.29882787,\n        0.30709237, 0.30489264, 0.30018866, 0.30216058, 0.30236456,\n        0.29745498, 0.29564095, 0.30280426, 0.2957514 , 0.29347347,\n        0.29941869, 0.2991031 , 0.30544947, 0.29938079, 0.30207851,\n        0.3044975 , 0.29553597, 0.30218731, 0.30588684, 0.29834572,\n        0.30225517, 0.30837383, 0.29619779, 0.30442347, 0.3020416 ,\n        0.29879677, 0.30238479, 0.30384573, 0.30097022, 0.29930646,\n        0.30225849, 0.30396428, 0.30418153, 0.30369798, 0.29898854]])</pre></li><li>scaler_1|participant_id(chain, draw, participant_id__factor_dim)float64-0.2022 0.0009996 ... -0.02664<pre>array([[[-0.20219405,  0.00099964,  0.03053618, ...,  0.0610393 ,\n         -0.09733041, -0.00983889],\n        [-0.01224127,  0.0495424 ,  0.010602  , ...,  0.01334209,\n         -0.00056715,  0.01247739],\n        [ 0.00237309,  0.02824296, -0.00357693, ..., -0.04071695,\n          0.00977576, -0.00091158],\n        ...,\n        [-0.01180394,  0.04901341,  0.03351585, ..., -0.04903163,\n         -0.00829288, -0.08691946],\n        [ 0.01504812,  0.03839951, -0.06661521, ...,  0.0976243 ,\n          0.06993395, -0.0499095 ],\n        [-0.04407144,  0.1058558 ,  0.02583918, ...,  0.05776522,\n          0.00778076, -0.02664356]]], shape=(1, 1000, 20))</pre></li><li>rl.alpha_1|participant_id(chain, draw, rl.alpha_1|participant_id__factor_dim)float64-0.006131 0.03817 ... 0.00839<pre>array([[[-0.0061312 ,  0.03816955, -0.03235644, ...,  0.01489558,\n         -0.05823227, -0.04366052],\n        [ 0.00365908,  0.00765562,  0.01615831, ..., -0.13814291,\n          0.08323388,  0.1360755 ],\n        [ 0.10021313,  0.02154091,  0.14432139, ...,  0.02253133,\n         -0.07208155,  0.05551009],\n        ...,\n        [ 0.08828435,  0.10736953,  0.02513265, ...,  0.0489666 ,\n          0.05746759, -0.03453032],\n        [ 0.02460212, -0.03212226,  0.01973213, ...,  0.01632776,\n          0.02573875,  0.04429214],\n        [ 0.08947452,  0.01389968, -0.01390827, ..., -0.2007269 ,\n          0.04449785,  0.00839037]]], shape=(1, 1000, 20))</pre></li><li>rl.alpha_1|participant_id_offset(chain, draw, rl.alpha_1|participant_id__factor_dim)float64-0.09323 0.5804 ... 0.4811 0.09072<pre>array([[[-0.09323467,  0.58042862, -0.49203109, ...,  0.22651093,\n         -0.88551416, -0.6639275 ],\n        [ 0.05061309,  0.10589403,  0.22350506, ..., -1.91082098,\n          1.15130795,  1.88222406],\n        [ 0.8668462 ,  0.18632943,  1.24838373, ...,  0.19489663,\n         -0.62350726,  0.48016373],\n        ...,\n        [ 0.92288464,  1.12239238,  0.26272534, ...,  0.5118747 ,\n          0.60074013, -0.3609643 ],\n        [ 0.76905838, -1.00413652,  0.61682307, ...,  0.51040324,\n          0.80458925,  1.38456498],\n        [ 0.96742395,  0.15028726, -0.15038019, ..., -2.17031647,\n          0.48112348,  0.09071906]]], shape=(1, 1000, 20))</pre></li><li>a_1|participant_id_offset(chain, draw, participant_id__factor_dim)float640.4979 -0.6492 ... 0.6767 -1.017<pre>array([[[ 0.49793346, -0.64923463,  0.63595348, ..., -0.15792781,\n          0.4412815 , -1.33386915],\n        [ 0.35311837, -1.00606907,  1.121254  , ..., -0.55686372,\n         -0.69122081,  1.79440006],\n        [-0.74112832,  2.32037434, -0.59579721, ...,  1.20580685,\n          0.22937725, -1.52610663],\n        ...,\n        [ 0.12676712,  0.61145238,  1.98658183, ..., -1.02755002,\n         -0.37777339,  1.28257637],\n        [-1.39345274, -1.14450429,  0.78478061, ...,  0.64563633,\n         -1.07669193, -0.81846998],\n        [-0.56775549,  0.427719  , -0.57878966, ...,  1.28090943,\n          0.67665741, -1.01672582]]], shape=(1, 1000, 20))</pre></li><li>rl.alpha_Intercept(chain, draw)float640.6966 0.7095 ... 0.6486 0.6639<pre>array([[0.69656236, 0.70950016, 0.7041785 , 0.68202067, 0.71736767,\n        0.72820482, 0.62996029, 0.71293299, 0.66747196, 0.6713378 ,\n        0.68430917, 0.69336348, 0.60283138, 0.70029098, 0.67160885,\n        0.67488855, 0.66186448, 0.67253679, 0.71649669, 0.70139909,\n        0.71982294, 0.71226042, 0.72925556, 0.70387084, 0.69834712,\n        0.6753794 , 0.6512712 , 0.63186683, 0.71332502, 0.74807691,\n        0.70781593, 0.60056342, 0.75813116, 0.60732139, 0.73291206,\n        0.66552347, 0.73184437, 0.72814675, 0.67918843, 0.75897085,\n        0.60318849, 0.70871182, 0.66201816, 0.68621404, 0.71825692,\n        0.67853581, 0.6308171 , 0.66582852, 0.73436581, 0.66671885,\n        0.64928012, 0.71376745, 0.61524224, 0.7294023 , 0.75860013,\n        0.65926683, 0.62022812, 0.73922365, 0.58819484, 0.63209691,\n        0.77088814, 0.7090144 , 0.63718318, 0.63737419, 0.6809707 ,\n        0.68597235, 0.64472133, 0.73044742, 0.66810825, 0.74643378,\n        0.70915346, 0.67153304, 0.67552488, 0.69275787, 0.74867884,\n        0.74053615, 0.69524427, 0.6964758 , 0.75356763, 0.75328045,\n        0.70741327, 0.62784173, 0.6937562 , 0.68664578, 0.71981969,\n        0.65723457, 0.75092893, 0.65581241, 0.64666824, 0.747062  ,\n        0.70925891, 0.72499631, 0.66300756, 0.61872968, 0.72489265,\n        0.75309964, 0.66782663, 0.75028868, 0.70122628, 0.66595617,\n...\n        0.66953038, 0.76089519, 0.6512626 , 0.70177295, 0.68242726,\n        0.62549602, 0.61914797, 0.69984273, 0.73068047, 0.72070622,\n        0.72852574, 0.69621855, 0.60425344, 0.7028299 , 0.63493827,\n        0.66521269, 0.7033093 , 0.59017531, 0.66755958, 0.71555627,\n        0.65974652, 0.7384698 , 0.74592423, 0.68970658, 0.74923994,\n        0.73793856, 0.72991471, 0.67050606, 0.69871305, 0.65452172,\n        0.74612326, 0.72206591, 0.69408332, 0.70932309, 0.73584613,\n        0.73320752, 0.6936993 , 0.649561  , 0.6425511 , 0.66940475,\n        0.60195727, 0.69081641, 0.67942165, 0.7043301 , 0.73204124,\n        0.67089851, 0.6333243 , 0.75012334, 0.72682469, 0.73573753,\n        0.75171098, 0.7219843 , 0.73410519, 0.67319896, 0.6568723 ,\n        0.64973287, 0.69619428, 0.72295364, 0.66296616, 0.72140572,\n        0.6763805 , 0.68527214, 0.64263284, 0.7243291 , 0.7341849 ,\n        0.70244055, 0.69562355, 0.67398453, 0.64152875, 0.62585189,\n        0.60347137, 0.73441947, 0.69176779, 0.67401384, 0.75215358,\n        0.6749933 , 0.6704734 , 0.65883788, 0.72582116, 0.67039346,\n        0.70586621, 0.74843446, 0.67328784, 0.65095211, 0.68237184,\n        0.67673843, 0.67334336, 0.69049154, 0.76914728, 0.66046717,\n        0.69625855, 0.73775738, 0.75707865, 0.7006405 , 0.68250645,\n        0.72175232, 0.74678894, 0.74113823, 0.64861258, 0.66389938]])</pre></li><li>scaler_1|participant_id_sigma(chain, draw)float640.0986 0.02566 ... 0.0571 0.0533<pre>array([[0.09860395, 0.02565607, 0.02328944, 0.05535236, 0.07489105,\n        0.06332913, 0.07597952, 0.03932944, 0.0213802 , 0.04143448,\n        0.01591469, 0.03386436, 0.01801156, 0.0353483 , 0.03389369,\n        0.03644858, 0.09746747, 0.02800114, 0.03656685, 0.03422127,\n        0.04668939, 0.04734561, 0.0907456 , 0.04094818, 0.02544198,\n        0.0686132 , 0.02871821, 0.03166688, 0.02976104, 0.02312399,\n        0.01420305, 0.01785162, 0.08037538, 0.03432839, 0.03824388,\n        0.03454513, 0.05604209, 0.0311869 , 0.0340811 , 0.04780879,\n        0.07521897, 0.02556033, 0.03111504, 0.11609882, 0.05085826,\n        0.05414817, 0.06244355, 0.03746626, 0.06092513, 0.10210547,\n        0.04278319, 0.06202804, 0.0431117 , 0.12616709, 0.04334207,\n        0.02914776, 0.05232177, 0.04295055, 0.05154362, 0.02606208,\n        0.06548093, 0.05361188, 0.02409416, 0.01967526, 0.06239504,\n        0.02949842, 0.08907175, 0.07023681, 0.03267476, 0.08867723,\n        0.02958336, 0.04434786, 0.04306083, 0.01656396, 0.02007368,\n        0.02438738, 0.04796732, 0.06588991, 0.07469728, 0.06183431,\n        0.03081377, 0.04474334, 0.04259325, 0.0974335 , 0.07097379,\n        0.04434592, 0.063473  , 0.08803056, 0.02157161, 0.01755154,\n        0.03537871, 0.02181389, 0.06252321, 0.0304789 , 0.04737881,\n        0.01961793, 0.03617766, 0.03553471, 0.02281117, 0.01444805,\n...\n        0.0176206 , 0.0806823 , 0.0324693 , 0.03084861, 0.04586211,\n        0.06364084, 0.03265593, 0.01955048, 0.03277215, 0.03732342,\n        0.04464892, 0.04825705, 0.02518735, 0.03190897, 0.01946785,\n        0.02014979, 0.01912564, 0.01362125, 0.02469797, 0.01697174,\n        0.03318283, 0.06429148, 0.03695297, 0.02098965, 0.02958973,\n        0.02006474, 0.01300518, 0.02590328, 0.0417772 , 0.05547632,\n        0.0214614 , 0.05004483, 0.02324025, 0.03616162, 0.0453901 ,\n        0.05838599, 0.02596468, 0.03628535, 0.03242744, 0.02161985,\n        0.05819417, 0.03418385, 0.04706836, 0.02184101, 0.03421539,\n        0.03123505, 0.03604972, 0.03681069, 0.03958109, 0.03725017,\n        0.09690573, 0.0353586 , 0.01774431, 0.02409022, 0.03167161,\n        0.03364508, 0.08898815, 0.09721761, 0.02976243, 0.04225022,\n        0.02757773, 0.06674016, 0.0739911 , 0.07643362, 0.02523396,\n        0.02898635, 0.03234627, 0.04038739, 0.06481389, 0.08012804,\n        0.0399431 , 0.0419734 , 0.02432246, 0.11342157, 0.01373103,\n        0.06643172, 0.0196145 , 0.02540816, 0.07124306, 0.04548615,\n        0.10463589, 0.02344641, 0.02755371, 0.14784854, 0.02836598,\n        0.06462302, 0.04765932, 0.06597554, 0.02245508, 0.03000929,\n        0.03939776, 0.03463864, 0.09545147, 0.03101542, 0.02527651,\n        0.02567224, 0.04770788, 0.0527449 , 0.05710394, 0.05329852]])</pre></li><li>rl.alpha_1|participant_id_mu(chain, draw)float64-0.3837 -0.008436 ... 0.3733<pre>array([[-3.83695294e-01, -8.43591579e-03,  2.81956030e-01,\n         2.17689008e-01,  1.51273675e-01, -8.37519023e-02,\n         3.89012811e-01, -5.26940636e-01,  3.12776619e-01,\n         4.55250823e-02, -1.38718022e-01,  3.05587387e-01,\n        -1.98655322e-01, -1.44994954e-01,  1.85051730e-01,\n         7.63054060e-02,  3.86820252e-02, -2.85014636e-02,\n         2.96104917e-01, -1.31919347e-01,  2.64381949e-01,\n         2.28212779e-01, -3.90126665e-01,  3.01663451e-01,\n         1.25860389e-01, -1.11884254e-01,  1.47610031e-01,\n         1.38723954e-01, -1.69332327e-01, -2.40070800e-01,\n         5.39525207e-02, -9.00036247e-01,  2.10000085e-02,\n         2.01644147e-01, -2.34945150e-01,  7.19129927e-02,\n         1.25641646e-01, -1.29063951e-01, -2.12686664e-01,\n        -1.72653455e-01, -1.18578303e-01,  1.95460152e-01,\n         2.63706051e-01,  3.07242882e-01,  2.92359737e-01,\n         6.71715270e-03,  4.80528360e-01,  4.50564147e-02,\n         1.43098086e-01,  4.89628202e-01, -5.87818451e-01,\n        -5.73143937e-01,  2.01736813e-01,  1.07720149e-02,\n        -1.77215607e-01,  1.35446674e-01, -4.19778029e-01,\n        -7.82408920e-02, -2.40829936e-01,  5.61244270e-01,\n...\n         7.47098639e-02,  6.57383275e-02,  4.86517391e-01,\n         3.29918613e-01, -5.75855054e-02,  1.96327692e-01,\n         1.12883459e-01,  8.66900469e-02, -2.43419119e-01,\n        -9.32835532e-02, -1.63951909e-02, -2.84126473e-01,\n         4.49936250e-01, -2.96446625e-01,  3.35752580e-01,\n         3.53834092e-01, -1.73662895e-01,  3.07490580e-01,\n        -1.36043350e-01,  5.75779126e-01, -8.88059488e-02,\n        -1.88456034e-01,  1.00156094e-01, -1.39809536e-01,\n         1.29009247e-01, -4.27394447e-01, -2.23088442e-01,\n        -6.14729081e-02, -2.50058458e-01, -7.75010108e-02,\n        -2.10517417e-01, -1.80583664e-01,  3.18993961e-01,\n        -8.86578686e-02, -2.93969742e-01,  7.87521216e-02,\n         3.90571957e-01, -2.86157454e-01,  3.82011022e-01,\n         1.08263554e-01,  3.22357679e-01, -2.95411746e-01,\n         1.64292825e-01, -2.62797557e-01, -6.21446226e-01,\n        -2.93028838e-01, -6.40272904e-02, -2.71087255e-01,\n        -3.79505355e-01,  3.52294609e-01,  1.00961118e-01,\n         2.57681044e-01,  2.04062172e-01, -2.85722141e-01,\n        -1.34387582e-01,  3.20996077e-01, -3.19829955e-01,\n         3.73261735e-01]])</pre></li><li>theta_Intercept(chain, draw)float640.3869 0.3924 ... 0.39 0.3727<pre>array([[0.38688797, 0.39243417, 0.37734943, 0.38410525, 0.38166973,\n        0.37601042, 0.37898243, 0.37829703, 0.38267968, 0.3783027 ,\n        0.38255602, 0.38080874, 0.3590777 , 0.38101894, 0.39384831,\n        0.38979097, 0.37927663, 0.37695061, 0.3873369 , 0.3850934 ,\n        0.35724576, 0.3782855 , 0.37049828, 0.38275185, 0.38849427,\n        0.3759771 , 0.37810201, 0.38326665, 0.3644495 , 0.38829047,\n        0.37791825, 0.37561394, 0.38438325, 0.37311399, 0.37565101,\n        0.36536909, 0.36894072, 0.38229517, 0.37886566, 0.37748961,\n        0.38244162, 0.38442853, 0.38257498, 0.36253549, 0.36778734,\n        0.36919466, 0.38771838, 0.3794254 , 0.384383  , 0.38426575,\n        0.3774416 , 0.36752836, 0.37148648, 0.38468006, 0.36056616,\n        0.36613597, 0.37637482, 0.38924541, 0.38072271, 0.38732147,\n        0.38171683, 0.35959077, 0.37614804, 0.36486527, 0.35919656,\n        0.37468279, 0.38643743, 0.37422777, 0.37311877, 0.37245014,\n        0.37328231, 0.36427975, 0.3765555 , 0.3803809 , 0.37625989,\n        0.380233  , 0.37449443, 0.375771  , 0.35906312, 0.37375426,\n        0.36518791, 0.36556442, 0.38480826, 0.37292061, 0.38377618,\n        0.36816202, 0.37713827, 0.36926091, 0.37311079, 0.39174106,\n        0.3770767 , 0.36781402, 0.35974875, 0.37618545, 0.37807728,\n        0.36193709, 0.37341463, 0.3721925 , 0.39153028, 0.37007617,\n...\n        0.3759211 , 0.39187613, 0.37630174, 0.38216531, 0.37430406,\n        0.37798141, 0.37622971, 0.38419603, 0.3668016 , 0.37385978,\n        0.3784226 , 0.37603515, 0.38334915, 0.38634939, 0.38178368,\n        0.39088256, 0.3756697 , 0.37606124, 0.37939228, 0.37828865,\n        0.3706529 , 0.36446931, 0.37761057, 0.38480666, 0.39008707,\n        0.38039775, 0.37050008, 0.37496663, 0.365913  , 0.37749569,\n        0.37769211, 0.39092896, 0.35728003, 0.37933403, 0.39534786,\n        0.38576491, 0.37632416, 0.37849997, 0.38987788, 0.37372465,\n        0.3800481 , 0.36178645, 0.3665302 , 0.37076445, 0.36774507,\n        0.38215601, 0.39182939, 0.36689953, 0.38002396, 0.3806185 ,\n        0.36633059, 0.37729591, 0.38208733, 0.38397911, 0.38332122,\n        0.36205242, 0.37204755, 0.38034012, 0.37430724, 0.39229054,\n        0.37320837, 0.37239083, 0.38335403, 0.39305205, 0.37158802,\n        0.37277066, 0.36628055, 0.39402174, 0.37781685, 0.37094161,\n        0.38609721, 0.37554288, 0.36151752, 0.37396252, 0.37142637,\n        0.35557135, 0.36888311, 0.3772456 , 0.37706679, 0.37998802,\n        0.36909315, 0.37359536, 0.37223497, 0.39161646, 0.37659379,\n        0.38105748, 0.39293374, 0.38909572, 0.38531145, 0.3876271 ,\n        0.39754055, 0.36998188, 0.38649941, 0.36538308, 0.37993174,\n        0.38564476, 0.37428877, 0.37813434, 0.39001641, 0.3726705 ]])</pre></li><li>theta_1|participant_id(chain, draw, participant_id__factor_dim)float640.05197 0.02673 ... 0.04656<pre>array([[[ 0.05197214,  0.02673226,  0.00229204, ...,  0.00400735,\n         -0.01318176,  0.04182066],\n        [-0.00450722,  0.05764031,  0.00963893, ..., -0.08805678,\n          0.03856682,  0.04810853],\n        [ 0.02626197, -0.04649187,  0.00687704, ..., -0.01598269,\n          0.0072076 ,  0.02048423],\n        ...,\n        [-0.03492132,  0.05848212,  0.0628924 , ..., -0.05798079,\n         -0.01603427,  0.01697098],\n        [-0.00784125, -0.00400168,  0.00612289, ..., -0.05070473,\n         -0.01149987,  0.01789056],\n        [ 0.0222352 ,  0.01585688,  0.04276675, ..., -0.02776972,\n         -0.01422922,  0.04655781]]], shape=(1, 1000, 20))</pre></li><li>t_1|participant_id_sigma(chain, draw)float640.11 0.1089 ... 0.1042 0.09832<pre>array([[0.10999327, 0.10887101, 0.10575763, 0.1037619 , 0.09851535,\n        0.11963192, 0.11352882, 0.11405207, 0.11437131, 0.12360804,\n        0.10524053, 0.10269127, 0.10572498, 0.10662561, 0.11316525,\n        0.11671194, 0.10303137, 0.11330405, 0.10204166, 0.10336148,\n        0.11400807, 0.10925281, 0.11390189, 0.10476804, 0.10731786,\n        0.10624328, 0.10414639, 0.11503053, 0.1037078 , 0.11071256,\n        0.10849472, 0.10415003, 0.11062745, 0.10207226, 0.11126017,\n        0.11308954, 0.11211361, 0.10047096, 0.09975096, 0.10904464,\n        0.10455422, 0.10923236, 0.10157763, 0.11438193, 0.11522362,\n        0.10335892, 0.11475404, 0.10453332, 0.10722398, 0.10015319,\n        0.1085326 , 0.10426669, 0.10046828, 0.10052681, 0.11401728,\n        0.10052577, 0.10425401, 0.10911836, 0.11417242, 0.09796648,\n        0.10870524, 0.11027513, 0.11967806, 0.10680891, 0.0982495 ,\n        0.09973366, 0.10921695, 0.09939613, 0.10201108, 0.11466453,\n        0.11046064, 0.10494502, 0.11233321, 0.10915871, 0.10422149,\n        0.11735017, 0.11267198, 0.10901152, 0.1184422 , 0.12109878,\n        0.11606878, 0.10403785, 0.1032851 , 0.10618989, 0.10455772,\n        0.09940223, 0.10709592, 0.11932015, 0.11019302, 0.10880329,\n        0.11220944, 0.10696451, 0.1060649 , 0.10674659, 0.10763093,\n        0.10002378, 0.11912841, 0.1063785 , 0.10896074, 0.10839219,\n...\n        0.1030321 , 0.10963317, 0.10657803, 0.09389273, 0.11094485,\n        0.10173183, 0.10597159, 0.11503907, 0.10383127, 0.09675858,\n        0.10847858, 0.11355391, 0.09912534, 0.10409168, 0.11567848,\n        0.09537742, 0.10172354, 0.10456523, 0.09278208, 0.11001971,\n        0.11483724, 0.10625101, 0.11360051, 0.11497201, 0.11633177,\n        0.10872403, 0.10408178, 0.11264339, 0.10848717, 0.11032798,\n        0.10651973, 0.10114099, 0.10853074, 0.10839745, 0.10982833,\n        0.11619384, 0.10019105, 0.1079324 , 0.128767  , 0.10165943,\n        0.10718463, 0.1106125 , 0.11014717, 0.11200811, 0.10626824,\n        0.10131508, 0.10492802, 0.10602575, 0.10533   , 0.11758065,\n        0.09989594, 0.09989163, 0.1135149 , 0.10048286, 0.11075851,\n        0.1074281 , 0.10796406, 0.10997803, 0.10959003, 0.10958065,\n        0.11555516, 0.10825019, 0.10694261, 0.11343225, 0.11430979,\n        0.10593162, 0.10572353, 0.1128533 , 0.1034322 , 0.11933508,\n        0.09976875, 0.09571484, 0.11346234, 0.10828377, 0.10308423,\n        0.11061869, 0.10507477, 0.11028767, 0.11165658, 0.1143481 ,\n        0.111147  , 0.1140077 , 0.1088887 , 0.11300503, 0.11322507,\n        0.1080373 , 0.10933887, 0.10781137, 0.11187647, 0.10529946,\n        0.11331412, 0.09868607, 0.11489663, 0.11726386, 0.10703899,\n        0.10137536, 0.11145467, 0.1143102 , 0.10417098, 0.09832119]])</pre></li><li>a_Intercept(chain, draw)float641.519 1.514 1.501 ... 1.526 1.53<pre>array([[1.5187665 , 1.5137958 , 1.50095816, 1.53435739, 1.54251613,\n        1.4856667 , 1.51793233, 1.51669966, 1.51863316, 1.50786764,\n        1.50381834, 1.50303841, 1.52309705, 1.50520411, 1.51348244,\n        1.50385895, 1.54176911, 1.52256086, 1.50535532, 1.51926793,\n        1.517166  , 1.48837187, 1.50536412, 1.50618434, 1.50384858,\n        1.52937119, 1.51819938, 1.51564659, 1.49275508, 1.47915392,\n        1.53327813, 1.51622842, 1.52684895, 1.52536211, 1.53132017,\n        1.49952142, 1.54272646, 1.52085747, 1.50754032, 1.51643916,\n        1.52842709, 1.50379841, 1.53443968, 1.52859198, 1.53013309,\n        1.5027268 , 1.51094194, 1.49726552, 1.51712462, 1.51377869,\n        1.52762481, 1.52104013, 1.52204418, 1.52973966, 1.50318093,\n        1.51467575, 1.52672421, 1.53827489, 1.50341583, 1.52773342,\n        1.51634343, 1.52810845, 1.50949102, 1.5315618 , 1.51358523,\n        1.51725347, 1.53455716, 1.51760293, 1.51295781, 1.51017121,\n        1.53365938, 1.53215935, 1.52567582, 1.51193042, 1.50609876,\n        1.5227853 , 1.49940903, 1.52403832, 1.53911117, 1.51180318,\n        1.51858394, 1.53438446, 1.52035802, 1.51493317, 1.49530942,\n        1.52927907, 1.50316734, 1.51383527, 1.50967721, 1.51721167,\n        1.50184963, 1.49706839, 1.50436995, 1.53271864, 1.50605138,\n        1.51796486, 1.4990855 , 1.52304428, 1.52094377, 1.52009623,\n...\n        1.51398192, 1.51094115, 1.51096434, 1.52496994, 1.50342856,\n        1.52394132, 1.51766756, 1.50606596, 1.51783675, 1.52311055,\n        1.50812373, 1.50523756, 1.52406006, 1.51896876, 1.51621338,\n        1.49747373, 1.50394336, 1.49818981, 1.51894013, 1.52009865,\n        1.51499281, 1.50319009, 1.49445362, 1.5094117 , 1.5354427 ,\n        1.52735885, 1.5100856 , 1.53541372, 1.52918377, 1.52524304,\n        1.51549746, 1.52109208, 1.50565469, 1.50631875, 1.50738457,\n        1.52063015, 1.51512245, 1.52758875, 1.52303609, 1.50130787,\n        1.49711563, 1.5148591 , 1.51759762, 1.51307114, 1.50541552,\n        1.52571405, 1.50268251, 1.51247522, 1.51116922, 1.52198637,\n        1.50637688, 1.49792864, 1.53038577, 1.51430975, 1.51444954,\n        1.5272314 , 1.51549554, 1.50897203, 1.53578746, 1.51701737,\n        1.54937229, 1.53054826, 1.52329134, 1.52856692, 1.4945465 ,\n        1.50734807, 1.52203581, 1.50789668, 1.52098866, 1.52469054,\n        1.51182427, 1.51674372, 1.52373788, 1.51708935, 1.4902064 ,\n        1.51048904, 1.50407654, 1.51743012, 1.49762524, 1.51669319,\n        1.53196347, 1.50761892, 1.51238419, 1.53298654, 1.50733992,\n        1.50360545, 1.54398677, 1.53511277, 1.54186964, 1.50311472,\n        1.51055518, 1.51003353, 1.53124842, 1.52443165, 1.48436502,\n        1.53008199, 1.52140657, 1.51540645, 1.52584376, 1.53036458]])</pre></li><li>a_1|participant_id_sigma(chain, draw)float640.0197 0.01917 ... 0.01271 0.01156<pre>array([[0.01970394, 0.01916514, 0.02229545, 0.00491832, 0.01728962,\n        0.0138818 , 0.01267394, 0.01578884, 0.01586484, 0.00810654,\n        0.01336855, 0.01184566, 0.01475522, 0.0144244 , 0.01721365,\n        0.02441013, 0.01505257, 0.03040907, 0.01586522, 0.03679973,\n        0.01027876, 0.02335966, 0.02622377, 0.01193627, 0.00614822,\n        0.01707639, 0.02073417, 0.01846336, 0.00501338, 0.01219096,\n        0.01778885, 0.00741419, 0.01692305, 0.03727388, 0.0142547 ,\n        0.00831938, 0.00974066, 0.01010955, 0.01119031, 0.0440399 ,\n        0.03676156, 0.00905406, 0.00759662, 0.0127522 , 0.01159168,\n        0.01326213, 0.00612737, 0.01435492, 0.00987634, 0.01715162,\n        0.02463077, 0.0201539 , 0.01199657, 0.02910184, 0.01484409,\n        0.01180213, 0.02184471, 0.01162216, 0.01100965, 0.0113498 ,\n        0.01125842, 0.01021704, 0.01455456, 0.01369533, 0.01366706,\n        0.01629827, 0.01461451, 0.01119982, 0.02628395, 0.01606757,\n        0.01438608, 0.02563096, 0.01151644, 0.02009396, 0.01975094,\n        0.02026135, 0.03094204, 0.00886312, 0.01086536, 0.01661988,\n        0.02321717, 0.01269577, 0.01795736, 0.01831085, 0.01600529,\n        0.02754241, 0.01746665, 0.0085413 , 0.01532191, 0.01716768,\n        0.01769163, 0.01079627, 0.05084844, 0.01853145, 0.01138559,\n        0.01075955, 0.0183789 , 0.00779392, 0.00654432, 0.02360923,\n...\n        0.01420381, 0.00985135, 0.01222525, 0.01869893, 0.01972692,\n        0.03132657, 0.02195932, 0.01241266, 0.0129702 , 0.00813915,\n        0.03031027, 0.01120718, 0.01557993, 0.01821261, 0.01356684,\n        0.03356826, 0.02303412, 0.01790544, 0.01390114, 0.01967389,\n        0.00721192, 0.03357301, 0.01625302, 0.00932346, 0.02723185,\n        0.0128861 , 0.00943899, 0.0089071 , 0.0142637 , 0.00976943,\n        0.01796662, 0.01660497, 0.01481182, 0.02257534, 0.01183902,\n        0.0066801 , 0.01951651, 0.01332895, 0.01742189, 0.02368654,\n        0.02069824, 0.00552525, 0.01805332, 0.01291016, 0.01004047,\n        0.00977718, 0.01110431, 0.01146342, 0.03261009, 0.01511894,\n        0.01108055, 0.0125491 , 0.01452288, 0.02275241, 0.02798717,\n        0.01029305, 0.01764158, 0.01843611, 0.01175666, 0.00999036,\n        0.0142401 , 0.01498173, 0.02065661, 0.0060794 , 0.00859598,\n        0.01271615, 0.00805719, 0.01495512, 0.02400589, 0.00909473,\n        0.01453216, 0.02238897, 0.0066262 , 0.01591485, 0.01082429,\n        0.01797783, 0.03588701, 0.01832347, 0.00863037, 0.00550417,\n        0.01605688, 0.0349393 , 0.01706901, 0.01036951, 0.01324607,\n        0.00991442, 0.01296853, 0.01031407, 0.01656416, 0.03251199,\n        0.01705471, 0.01007348, 0.01688415, 0.0169155 , 0.01681853,\n        0.00925899, 0.01248152, 0.01421213, 0.01271354, 0.01155609]])</pre></li><li>z_1|participant_id_offset(chain, draw, participant_id__factor_dim)float64-0.7125 -0.01749 ... 0.2658 -1.826<pre>array([[[-0.71252482, -0.01749361, -0.99660237, ..., -0.18583817,\n         -0.21356126, -1.99329993],\n        [-1.09502154,  1.45007051,  0.18864846, ...,  0.73294813,\n         -0.09527506, -0.75878022],\n        [ 1.01384938,  1.04800652,  0.00952384, ...,  0.07122964,\n         -0.28593986, -1.57190124],\n        ...,\n        [-0.87024631, -0.46968835, -1.41751175, ...,  0.44037777,\n         -0.61586003, -0.9603546 ],\n        [-1.50708001, -0.58872707,  0.15897286, ...,  0.21786111,\n          0.43636363, -1.76287356],\n        [-1.6611697 ,  0.21530391, -0.20810496, ...,  0.73016883,\n          0.26584173, -1.82577073]]], shape=(1, 1000, 20))</pre></li><li>rl.alpha_1|participant_id_sigma(chain, draw)float640.06576 0.0723 ... 0.03199 0.09249<pre>array([[0.06576097, 0.07229506, 0.11560659, 0.09155735, 0.0787078 ,\n        0.04997225, 0.09402864, 0.09629966, 0.06367193, 0.03373508,\n        0.06452841, 0.04521244, 0.07128743, 0.05918712, 0.08156221,\n        0.12696523, 0.07455821, 0.05949126, 0.11904294, 0.04926705,\n        0.09525562, 0.13306735, 0.05190329, 0.07413437, 0.05741343,\n        0.08475095, 0.06074291, 0.08758192, 0.04617699, 0.05983498,\n        0.06086314, 0.04625114, 0.12067927, 0.06215166, 0.0815916 ,\n        0.06464535, 0.06691737, 0.06363273, 0.06565011, 0.08808628,\n        0.05177942, 0.05825685, 0.08645182, 0.13700509, 0.05810417,\n        0.05409311, 0.03473024, 0.03519432, 0.11872941, 0.1353541 ,\n        0.06268524, 0.09604875, 0.08298633, 0.09394592, 0.0835287 ,\n        0.06207931, 0.17101122, 0.06644356, 0.07120106, 0.08158989,\n        0.06457404, 0.0960653 , 0.08158738, 0.08295814, 0.06694003,\n        0.06035212, 0.1571742 , 0.11081907, 0.05961999, 0.09033887,\n        0.04641463, 0.10006995, 0.06454805, 0.09536663, 0.03945277,\n        0.07661688, 0.09318548, 0.10152935, 0.05438429, 0.06234939,\n        0.08170487, 0.11351483, 0.05478522, 0.02617228, 0.08486353,\n        0.05361093, 0.06212743, 0.12342282, 0.076719  , 0.13949918,\n        0.03006535, 0.10913213, 0.11591785, 0.04602654, 0.04583218,\n        0.05941498, 0.05941715, 0.06803101, 0.06412576, 0.06429069,\n...\n        0.05067679, 0.05721998, 0.03568669, 0.07442855, 0.07529961,\n        0.04997658, 0.19276674, 0.05983451, 0.05519353, 0.10375671,\n        0.08946591, 0.05435168, 0.08949008, 0.05669273, 0.07349884,\n        0.08944092, 0.08002895, 0.05603698, 0.07136297, 0.06475565,\n        0.13997912, 0.15415336, 0.12861898, 0.07397953, 0.05240982,\n        0.1391466 , 0.08500625, 0.06047996, 0.04956919, 0.11563925,\n        0.10307916, 0.09909699, 0.045194  , 0.06298156, 0.13276606,\n        0.10958182, 0.13024063, 0.03954988, 0.06009769, 0.06203855,\n        0.08285382, 0.12100153, 0.08330057, 0.11474399, 0.04908104,\n        0.17801246, 0.10975875, 0.11578488, 0.10769529, 0.07923204,\n        0.06488761, 0.06143104, 0.08864346, 0.03684017, 0.03578813,\n        0.09175318, 0.04204363, 0.17149245, 0.04794318, 0.17325817,\n        0.04306263, 0.19921966, 0.09206165, 0.07830652, 0.05901009,\n        0.05904598, 0.12454677, 0.06988988, 0.0435092 , 0.10042074,\n        0.08663003, 0.04742992, 0.13337464, 0.07195189, 0.06358902,\n        0.082749  , 0.06489047, 0.07757048, 0.05147483, 0.07868014,\n        0.07047847, 0.1270102 , 0.07338406, 0.06451146, 0.04562845,\n        0.07984102, 0.05268173, 0.06257425, 0.0554933 , 0.05669173,\n        0.06124474, 0.12718449, 0.19479748, 0.07726375, 0.04739261,\n        0.0471375 , 0.14626697, 0.09566131, 0.03198993, 0.09248739]])</pre></li><li>t_Intercept(chain, draw)float640.2921 0.3215 ... 0.2871 0.3217<pre>array([[0.29212148, 0.32149672, 0.30953982, 0.30441674, 0.31832156,\n        0.29887475, 0.29786092, 0.28931012, 0.30614265, 0.30311878,\n        0.31426456, 0.29668774, 0.32060163, 0.3014976 , 0.28529138,\n        0.30006687, 0.30606297, 0.29155522, 0.31838673, 0.29222267,\n        0.29395846, 0.30956449, 0.30085538, 0.30804285, 0.2985324 ,\n        0.30074981, 0.29307606, 0.31634405, 0.29813705, 0.30896571,\n        0.31148629, 0.28249797, 0.31572511, 0.30952227, 0.2980148 ,\n        0.30514743, 0.30786325, 0.3086075 , 0.31703439, 0.2892329 ,\n        0.2967016 , 0.30894934, 0.31985524, 0.31424807, 0.30016343,\n        0.30162429, 0.31660801, 0.29968565, 0.32559311, 0.30705137,\n        0.29491034, 0.31918897, 0.30882135, 0.29475231, 0.29120529,\n        0.28579238, 0.30558644, 0.29930659, 0.32088637, 0.31433696,\n        0.31799035, 0.29770484, 0.30497227, 0.31450205, 0.29846617,\n        0.31361402, 0.30514848, 0.29741959, 0.31276904, 0.2899198 ,\n        0.29241576, 0.30997213, 0.30772645, 0.29845368, 0.31581728,\n        0.30710201, 0.30319299, 0.30071327, 0.30388902, 0.3038364 ,\n        0.30620942, 0.32396097, 0.3015548 , 0.30545807, 0.29836647,\n        0.30925172, 0.30484799, 0.31317547, 0.29286924, 0.30789311,\n        0.30095773, 0.31583984, 0.30497422, 0.3224812 , 0.3058365 ,\n        0.30055584, 0.30132751, 0.28947294, 0.3148789 , 0.30793806,\n...\n        0.33092022, 0.30615127, 0.29712821, 0.30890443, 0.31107866,\n        0.31532151, 0.30543842, 0.31360394, 0.31495876, 0.31697389,\n        0.29523059, 0.3089995 , 0.30611485, 0.32442728, 0.31474034,\n        0.29608768, 0.31405012, 0.31331901, 0.29663898, 0.32385614,\n        0.29974616, 0.29217086, 0.318198  , 0.30055476, 0.31168052,\n        0.31911205, 0.29418828, 0.30875162, 0.30524383, 0.28607706,\n        0.30885945, 0.3103482 , 0.28729043, 0.28471024, 0.29233701,\n        0.30773254, 0.3084213 , 0.29149531, 0.28626094, 0.31895704,\n        0.30712341, 0.31311258, 0.32572649, 0.28684291, 0.30022147,\n        0.2958595 , 0.29881718, 0.33200549, 0.30914902, 0.32194138,\n        0.29016358, 0.30621215, 0.29871183, 0.29396563, 0.29734432,\n        0.30680745, 0.3153062 , 0.30688658, 0.31421602, 0.29465579,\n        0.30539276, 0.29574882, 0.30582661, 0.30632248, 0.2986816 ,\n        0.30980509, 0.29661198, 0.30129537, 0.29533752, 0.2978607 ,\n        0.29359927, 0.32797348, 0.30885577, 0.30999201, 0.3111922 ,\n        0.29843744, 0.31818789, 0.29978578, 0.30516506, 0.30695588,\n        0.29197644, 0.30767992, 0.30205864, 0.31706606, 0.3177318 ,\n        0.30882761, 0.32048876, 0.29064513, 0.28849422, 0.31613275,\n        0.29994048, 0.33001137, 0.30400968, 0.3212954 , 0.29729644,\n        0.31532987, 0.3049634 , 0.29260697, 0.28713798, 0.3216938 ]])</pre></li><li>theta_1|participant_id_sigma(chain, draw)float640.02817 0.03402 ... 0.02553 0.0285<pre>array([[0.02817104, 0.03401633, 0.02436036, 0.0215695 , 0.02696583,\n        0.0381325 , 0.0374725 , 0.01592176, 0.01985499, 0.04349535,\n        0.03363814, 0.02085719, 0.02191547, 0.03143101, 0.03159363,\n        0.01967509, 0.01998271, 0.01895678, 0.02614873, 0.02952824,\n        0.03060016, 0.0262747 , 0.02881946, 0.02578435, 0.02232248,\n        0.01537436, 0.03147593, 0.02638779, 0.02497023, 0.03462102,\n        0.03030676, 0.02770469, 0.03254418, 0.02387572, 0.01813904,\n        0.02128096, 0.02521327, 0.02029714, 0.02665018, 0.02398885,\n        0.04454156, 0.03241286, 0.030505  , 0.02852578, 0.03516522,\n        0.0288318 , 0.02025329, 0.01266546, 0.03565771, 0.03848797,\n        0.0291637 , 0.02728729, 0.03752326, 0.02238222, 0.02907475,\n        0.03160383, 0.03208691, 0.02160939, 0.03269271, 0.03266742,\n        0.021275  , 0.03528334, 0.03426899, 0.02554783, 0.03052769,\n        0.01585129, 0.03490905, 0.02599271, 0.02025974, 0.02703126,\n        0.02721161, 0.02581248, 0.02196708, 0.02492389, 0.01562791,\n        0.03099216, 0.02657481, 0.03424557, 0.02465162, 0.02315596,\n        0.03619514, 0.02807466, 0.02201397, 0.01947818, 0.02311263,\n        0.03856172, 0.02172363, 0.02380636, 0.02819118, 0.03438583,\n        0.016767  , 0.02271522, 0.0255366 , 0.01110159, 0.01921306,\n        0.02669771, 0.02331803, 0.02457994, 0.01844808, 0.02730417,\n...\n        0.02855198, 0.02075224, 0.02442478, 0.0234831 , 0.02762399,\n        0.02381344, 0.02239428, 0.01743662, 0.03104082, 0.03579353,\n        0.02306354, 0.02944171, 0.02100408, 0.02313724, 0.02004466,\n        0.02585504, 0.03676733, 0.02376912, 0.02382849, 0.03010617,\n        0.02368195, 0.03025272, 0.02365915, 0.03810547, 0.01767287,\n        0.02010578, 0.02431028, 0.02021824, 0.01884322, 0.02020299,\n        0.02734903, 0.02599302, 0.01982411, 0.04039439, 0.01909697,\n        0.02785291, 0.0293063 , 0.03046919, 0.0224701 , 0.02215192,\n        0.01996772, 0.0319622 , 0.02780115, 0.01790852, 0.02456638,\n        0.0329228 , 0.03360052, 0.03102755, 0.03191841, 0.03436615,\n        0.02409237, 0.03820335, 0.0181169 , 0.01716565, 0.03011514,\n        0.0264347 , 0.02376767, 0.02481967, 0.02893989, 0.02979245,\n        0.01851005, 0.03438542, 0.02191909, 0.01765421, 0.02575372,\n        0.02524264, 0.02924655, 0.03971804, 0.02734541, 0.01760644,\n        0.01684399, 0.02258915, 0.0252862 , 0.02055502, 0.03289204,\n        0.02658384, 0.02684344, 0.02759092, 0.02133262, 0.0340563 ,\n        0.01591091, 0.02296061, 0.03052898, 0.02488362, 0.03464798,\n        0.02735152, 0.03169573, 0.02624685, 0.0252861 , 0.03282481,\n        0.02775988, 0.02681927, 0.01420853, 0.02483175, 0.02529136,\n        0.02111472, 0.02615539, 0.04603861, 0.02553468, 0.0284953 ]])</pre></li><li>z_1|participant_id_mu(chain, draw)float64-0.2323 -0.06951 ... 0.02451 0.1987<pre>array([[-2.32337646e-01, -6.95129143e-02, -9.80274328e-02,\n        -5.91744090e-02, -5.15048489e-01,  1.78008483e-02,\n        -5.70761247e-02, -1.08905219e-01, -7.24138325e-02,\n        -3.24644057e-01,  9.34218109e-02,  4.66878906e-01,\n         1.31040058e-01,  2.41730536e-01,  1.40456286e-01,\n         3.70562274e-01, -2.46191287e-01,  4.20122063e-02,\n        -4.76645603e-01, -1.22600211e-01,  1.53037169e-01,\n         3.57957312e-01,  2.99375504e-01,  1.87521435e-02,\n         4.18931039e-02,  2.92572619e-01, -1.57962779e-01,\n        -7.57735105e-01,  2.07985740e-01, -2.07629082e-01,\n         4.51282137e-01, -2.04616207e-02,  7.69694049e-02,\n         5.32070837e-01, -3.69966523e-02,  2.24651216e-01,\n         7.56277707e-03, -7.17059225e-02, -2.76980510e-03,\n        -5.28603022e-02,  1.82159262e-01, -2.34061449e-01,\n        -1.84023865e-01, -3.95323507e-01, -8.48010500e-02,\n         9.31378913e-02, -1.06949439e-01, -4.21510802e-01,\n         3.90054424e-01, -4.97738073e-01,  1.01826351e-01,\n        -1.02347973e-01,  2.39970243e-01,  7.10453764e-03,\n         1.33305706e-01, -1.28519285e-01,  2.29154040e-01,\n        -1.09805775e-01,  4.42472889e-01, -3.68549555e-01,\n...\n         2.34737955e-01,  2.89882879e-01,  8.81122631e-02,\n        -3.71329570e-01, -3.65927184e-01,  5.80854801e-01,\n         2.94116739e-01,  1.74215767e-01, -1.27350854e-01,\n        -4.10383376e-01,  1.54916590e-01, -7.75254313e-02,\n        -7.63220689e-02,  2.83611347e-01,  1.69603700e-01,\n        -2.92378550e-01,  1.54411076e-01,  1.99730596e-01,\n         3.24596023e-02, -6.78285789e-04,  1.12441013e-01,\n         2.74919262e-01, -4.13364077e-01,  2.63820041e-01,\n         1.59311151e-01, -1.11882330e-01, -1.09368942e-01,\n        -1.79558468e-01, -5.23214122e-01, -1.41014686e-01,\n        -1.03954084e-01, -8.84771452e-02, -2.73145843e-01,\n         5.39250391e-01, -3.83255532e-02, -4.00919118e-01,\n        -6.68818264e-01,  3.46240165e-01,  3.15948541e-01,\n         1.12192178e-01, -9.23334940e-02,  3.08397906e-01,\n        -4.72613818e-02, -4.77888822e-01,  3.22088810e-02,\n         1.41462059e-01,  4.82703157e-02, -1.57179983e-03,\n         3.09768215e-01,  3.91774130e-02, -1.56684569e-01,\n        -2.33922383e-02, -8.38638780e-02,  5.10795134e-01,\n        -9.88962082e-02,  8.32733198e-02,  2.45141470e-02,\n         1.98724229e-01]])</pre></li><li>theta_1|participant_id_mu(chain, draw)float64-0.5682 -0.2231 ... -0.1285 0.2844<pre>array([[-5.68180879e-01, -2.23069107e-01, -1.96702028e-01,\n         4.14167063e-01,  2.06865840e-01, -1.52452083e-01,\n        -1.17564606e-01,  4.68416384e-01, -1.01950104e-01,\n         5.99188469e-02, -3.13937808e-01,  4.16750302e-01,\n        -1.15486586e-01, -5.00508110e-01,  4.19003865e-01,\n         6.40987446e-02, -9.06304979e-02,  2.13869158e-02,\n        -1.98995687e-02, -3.77127663e-01, -1.97576454e-01,\n         9.47392401e-02,  8.34730971e-02, -8.01779334e-02,\n        -2.03506896e-01, -4.46590164e-01,  1.05522394e-02,\n         5.61930658e-02, -3.58227354e-01, -2.52495499e-01,\n        -1.80708542e-01,  1.16999642e-01,  4.44964840e-03,\n        -3.24768323e-01, -5.48211784e-04,  1.79394232e-02,\n         1.80244755e-01,  1.73774567e-01, -6.25717875e-01,\n         1.40423062e-01,  2.12709791e-01, -6.15367215e-02,\n        -1.74964523e-01, -3.20758485e-01, -4.03960919e-01,\n         5.86446839e-01,  2.67144160e-01, -2.62666551e-01,\n         2.97594187e-02, -5.33394861e-02, -6.79785276e-02,\n         3.10454333e-01,  4.35353804e-01,  3.42805331e-01,\n         3.15091048e-02, -1.84832302e-01, -3.30507785e-01,\n        -5.51702766e-02,  4.52930931e-02,  3.27652872e-01,\n...\n         4.85186437e-01, -2.26040538e-01, -1.36722884e-01,\n         1.74786059e-01, -7.70933758e-02, -3.13608686e-01,\n        -3.60732116e-01, -1.24569922e-02,  1.04200798e-01,\n         5.16040594e-02, -2.34701431e-01, -3.00792942e-01,\n         1.13236010e-01, -6.91902256e-02, -2.34570331e-01,\n        -2.97893144e-02,  3.41798870e-01, -4.69320057e-01,\n         8.52199005e-02, -4.90525208e-02, -3.20691519e-01,\n         1.94137029e-01,  1.18291484e-02,  1.06405978e-01,\n         1.50046980e-01,  2.25485163e-01,  2.13533767e-01,\n         1.30113139e-01, -2.81535383e-01,  2.37003065e-01,\n         7.26998957e-02, -7.96148170e-02, -3.09937919e-01,\n         3.75350826e-02,  2.68014282e-01,  2.27548577e-01,\n         4.92538398e-01, -3.85902226e-01,  1.00813475e-01,\n        -1.58666986e-01,  2.61132496e-01,  3.21338118e-01,\n         1.24626743e-01,  4.04954769e-01,  3.42828033e-01,\n        -4.52344708e-02,  2.25564008e-01,  1.76209618e-01,\n        -4.70780857e-02,  5.16030921e-01, -4.24431231e-01,\n        -3.21610126e-01, -5.90022394e-02, -3.15655762e-01,\n        -2.71502779e-01,  2.35180657e-01, -1.28507505e-01,\n         2.84443114e-01]])</pre></li><li>a_1|participant_id_mu(chain, draw)float64-0.4297 0.3779 ... 0.693 -0.2092<pre>array([[-4.29707040e-01,  3.77879652e-01,  1.06654660e-01,\n        -3.00652761e-01, -2.26593503e-01, -5.97173725e-02,\n         4.70713097e-01,  1.89684187e-01, -3.68422252e-02,\n        -1.11058401e-01, -2.85707759e-01,  1.97048949e-01,\n        -2.16785584e-02, -4.12726172e-01, -3.51292446e-01,\n         1.74144725e-01, -2.46057087e-01,  1.50946287e-02,\n        -4.47895970e-02, -2.12894022e-01,  5.95362875e-02,\n         5.58997464e-02, -2.88073492e-01, -9.53227387e-02,\n        -3.29889187e-01, -4.89197085e-03,  7.55095151e-02,\n         3.86486156e-01, -7.49429286e-02, -2.63577621e-01,\n        -5.83419034e-02,  1.49963407e-01,  1.45657939e-01,\n         3.29323657e-01,  2.50543606e-01, -3.90583190e-01,\n        -1.45499282e-02, -1.64275206e-01,  6.13639172e-03,\n         8.38138654e-02,  1.68739091e-01, -2.45688723e-01,\n         3.00699592e-01,  8.10782515e-02,  1.94109109e-01,\n         1.33474944e-01,  9.28295264e-02, -3.15705112e-01,\n         5.85972498e-01,  5.56009654e-01, -4.91930941e-03,\n         2.79383523e-01,  3.73090303e-02, -6.31489476e-01,\n         1.72448302e-01, -2.69115949e-01, -3.24088996e-01,\n         2.37043440e-01,  3.74988260e-01, -2.44223390e-01,\n...\n        -1.71944645e-02,  3.03563348e-03,  4.62789589e-02,\n         1.19575431e-02, -8.55422122e-02, -1.83449772e-02,\n        -4.09018177e-01,  8.93813173e-02,  2.04568501e-01,\n        -1.98655404e-01, -4.13403904e-01, -3.93740830e-01,\n        -4.85580056e-01, -1.21281397e-01, -7.01704440e-02,\n         2.59040816e-01,  1.14169086e-02,  3.07840722e-01,\n        -2.35639933e-01, -2.51251826e-01,  1.85829295e-01,\n         3.68335905e-01, -2.10100176e-01,  5.99940918e-01,\n         3.04871074e-01,  3.28743483e-01, -2.24492433e-01,\n         1.13316914e-01, -8.02843669e-02,  5.82563990e-02,\n        -5.09652318e-02,  5.47150154e-02, -1.86369016e-01,\n         1.23205676e-01, -2.74606033e-01,  3.84634577e-01,\n        -8.53985806e-02, -2.75286434e-01, -4.85382894e-01,\n        -4.35190429e-02, -2.14636411e-01,  1.15686263e-01,\n        -3.59578713e-01,  2.22030815e-01, -1.10244793e-01,\n        -2.73771684e-01,  8.99835362e-02, -2.92510731e-01,\n         2.66104833e-01, -2.96390681e-03, -2.42262404e-01,\n         1.59249743e-01, -3.42077754e-01, -2.94777830e-01,\n         9.66929682e-03, -3.39989875e-01,  6.93001404e-01,\n        -2.09219940e-01]])</pre></li><li>t_1|participant_id_offset(chain, draw, participant_id__factor_dim)float640.609 1.089 0.8881 ... 1.103 0.6972<pre>array([[[0.60902984, 1.08859727, 0.88812892, ..., 1.69859409,\n         1.28832412, 0.6053564 ],\n        [0.99051081, 1.05335228, 0.88877348, ..., 1.61284381,\n         1.17382081, 1.02384479],\n        [0.9473577 , 0.94155007, 0.94678847, ..., 1.45180203,\n         1.24718422, 0.66799568],\n        ...,\n        [1.07506161, 1.03482653, 0.86929737, ..., 1.7699519 ,\n         1.3706353 , 0.93354787],\n        [0.88384904, 0.642959  , 1.16148166, ..., 1.39924806,\n         1.45644537, 0.94237343],\n        [0.96263956, 1.2228548 , 1.39732449, ..., 1.46743955,\n         1.10314494, 0.6971816 ]]], shape=(1, 1000, 20))</pre></li></ul></li><li>Indexes: (4)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li><li>rl.alpha_1|participant_id__factor_dimPandasIndex<pre>PandasIndex(Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n       '13', '14', '15', '16', '17', '18', '19'],\n      dtype='object', name='rl.alpha_1|participant_id__factor_dim'))</pre></li><li>participant_id__factor_dimPandasIndex<pre>PandasIndex(Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n       '13', '14', '15', '16', '17', '18', '19'],\n      dtype='object', name='participant_id__factor_dim'))</pre></li></ul></li><li>Attributes: (4)created_at :2025-07-20T18:52:02.063675+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.23.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 96kB\nDimensions:                  (__obs__: 4000, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 32kB 0 1 2 3 ... 3997 3998 3999\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 64kB ...\nAttributes:\n    created_at:                 2025-07-20T18:52:02.072293+00:00\n    arviz_version:              0.22.0\n    inference_library:          pymc\n    inference_library_version:  5.23.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 4000</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 ... 3996 3997 3998 3999<pre>array([   0,    1,    2, ..., 3997, 3998, 3999], shape=(4000,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float640.9356 0.0 1.114 ... 1.0 1.197 1.0<pre>array([[0.93560183, 0.        ],\n       [1.11437929, 0.        ],\n       [0.56431085, 0.        ],\n       ...,\n       [1.32044899, 1.        ],\n       [1.39262187, 1.        ],\n       [1.19681287, 1.        ]], shape=(4000, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999],\n      dtype='int64', name='__obs__', length=4000))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (4)created_at :2025-07-20T18:52:02.072293+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.23.0</li></ul> </ul> </li> </ul> In\u00a0[23]: Copied! <pre># Extract VI recovered parameters\nrecov_param_list_vi = extract_subject_parameters(idata_vi, model_config.list_params)\n</pre> # Extract VI recovered parameters recov_param_list_vi = extract_subject_parameters(idata_vi, model_config.list_params) In\u00a0[24]: Copied! <pre># Plot comparison between MCMC and VI recovery\nplot_parameter_recovery(\n    sim_param_list, \n    recov_param_list, \n    plot_param_names, \n    plot_param_ranges,\n    additional_data=recov_param_list_vi,\n    additional_label='VI',\n    show_correlation=False\n)\nplt.show()\n</pre> # Plot comparison between MCMC and VI recovery plot_parameter_recovery(     sim_param_list,      recov_param_list,      plot_param_names,      plot_param_ranges,     additional_data=recov_param_list_vi,     additional_label='VI',     show_correlation=False ) plt.show() In\u00a0[25]: Copied! <pre>import arviz as az\nimport matplotlib.pyplot as plt\n\n# Define constant for group mean parameters\nlist_group_mean_params = [\n    \"rl.alpha_Intercept\",\n    \"scaler_Intercept\",\n    \"a_Intercept\",\n    \"z_Intercept\",\n    \"t_Intercept\",\n    \"theta_Intercept\",\n]\n\ndef plot_posterior_comparison(idata_list, labels, var_names, title_suffix=\"\"):\n    \"\"\"Plot posterior density comparisons between different inference methods.\"\"\"\n    axes = az.plot_density(\n        idata_list,\n        data_labels=labels,\n        var_names=var_names,\n        hdi_prob=1.0,\n        shade=0.2,\n    )\n\n    fig = axes.flatten()[0].get_figure()\n\n    if title_suffix:\n        fig.suptitle(f\"Posterior Comparison - {title_suffix}\", y=1.02)\n\n    plt.tight_layout()\n    \n    return fig\n\n# Compare group-level means\nplot_posterior_comparison(\n    [idata_mcmc, idata_vi], \n    [\"NUTS\", \"VI\"], \n    list_group_mean_params,\n    \"Group Means\"\n)\nplt.show()\n</pre> import arviz as az import matplotlib.pyplot as plt  # Define constant for group mean parameters list_group_mean_params = [     \"rl.alpha_Intercept\",     \"scaler_Intercept\",     \"a_Intercept\",     \"z_Intercept\",     \"t_Intercept\",     \"theta_Intercept\", ]  def plot_posterior_comparison(idata_list, labels, var_names, title_suffix=\"\"):     \"\"\"Plot posterior density comparisons between different inference methods.\"\"\"     axes = az.plot_density(         idata_list,         data_labels=labels,         var_names=var_names,         hdi_prob=1.0,         shade=0.2,     )      fig = axes.flatten()[0].get_figure()      if title_suffix:         fig.suptitle(f\"Posterior Comparison - {title_suffix}\", y=1.02)      plt.tight_layout()          return fig  # Compare group-level means plot_posterior_comparison(     [idata_mcmc, idata_vi],      [\"NUTS\", \"VI\"],      list_group_mean_params,     \"Group Means\" ) plt.show() In\u00a0[26]: Copied! <pre># Compare group-level standard deviations  \nplot_posterior_comparison(\n    [idata_mcmc, idata_vi],\n    [\"NUTS\", \"VI\"],\n    list_group_sd_params, \n    \"Group Standard Deviations\"\n)\nplt.show()\n</pre> # Compare group-level standard deviations   plot_posterior_comparison(     [idata_mcmc, idata_vi],     [\"NUTS\", \"VI\"],     list_group_sd_params,      \"Group Standard Deviations\" ) plt.show() <p>While the VI and MCMC posteriors largely agree with each other, there are major discrepancies in parameters such as 't'. This is partly because of the fact that we are using mean field approximation for VI which relaxes the structure of covariance present in the model (eg. we also see that the VI posteriors are more peaked). While this tutorial is meant to serve as a proof-of-concept for RLSSM paradigm, we encourage users to leverage the HSSM functionality by tweaking and fine-tuning various settings including the choice of Bayesian inference method (say, VI vs MCMC) for robust results.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/rlssm_tutorial/#tutorial-for-hierarchical-bayesian-inference-for-reinforcement-learning-sequential-sampling-models","title":"Tutorial for hierarchical Bayesian inference for Reinforcement Learning - Sequential Sampling Models.\u00b6","text":"<p>This is a (preview) tutorial for using the HSSM Python package to simultaneously estimate reinforcement learning parameters and decision parameters within a fully hierarchical bayesian estimation framework, including steps for constructing HSSM-compatible likelihoods/distributions and sampling from the posterior. Further, the plots to assess the recovery of model parameters are also shown.</p> <p>The module uses the reinforcement learning sequential sampling model (RLSSM), a reinforcement learning model that replaces the standard \u201csoftmax\u201d choice function with a drift diffusion process with collapsing bounds (referred to as the 'angle' decision model hereon). The softmax and sequential sampling process is equivalent for capturing choice proportions, but the angle model also takes RT distributions into account. The RLSSM estimates trial-by-trial drift rate as a scaled difference in expected rewards (expected reward for upper bound alternative minus expected reward for lower bound alternative). Expected rewards are updated with a delta learning rule using a single learning rate. The model also includes the standard angle parameters such as the rate of collapse of the decision boundaries. The broader RLSSM framework is described in detail in Pedersen, Frank &amp; Biele (2017) and Fengler, Bera, Pedersen &amp; Frank (2022).</p>"},{"location":"tutorials/rlssm_tutorial/#load-and-prepare-the-demo-dataset","title":"Load and prepare the demo dataset\u00b6","text":"<p>This data file contains (synthetic) data from a simulated 2-armed bandit task. We examine the dataset -- it contains the typical columns that are expected from a canonical instrumental learning task. <code>participant_id</code> identifies the subject id, <code>trial</code> identifies the sequence of trials within the subject data, <code>response</code> and <code>rt</code> are the data columns recorded for each trial, <code>feedback</code> column shows the reward obtained on a given trial and <code>correct</code> records whether the response was correct.</p>"},{"location":"tutorials/rlssm_tutorial/#construct-hssm-compatible-pymc-distribution-from-a-simulator-and-jax-likelihood-callable","title":"Construct HSSM-compatible PyMC distribution from a simulator and JAX likelihood callable\u00b6","text":"<p>We now construct a custom model that is compatible with HSSM and PyMC. Note that HSSM internally constructs a PyMC object (which is used for sampling) based on the user-specified HSSM model. In other words, we are peeling the abstration layers conveniently afforded by HSSM to directly use the core machinery of HSSM. This advanced HSSM tutorial explains how to use HSSM when starting from the very basics of a model -- a simulator and a JAX likelihood callable.</p> <p>The simulator function is used for generating samples from the model (for posterior predictives, etc.) and the likelihood callable is employed for sampling/inference. This preview tutorial exposes the key flexibility of the HSSM for use in fitting RLSSM models. Therefore, the subsequent tutorial will focus only on the sampling/inference aspect. We create a dummy simulator function to bypass the need for defining the actual simulator.</p>"},{"location":"tutorials/rlssm_tutorial/#step-1-define-a-pytensor-randomvariable","title":"Step 1: Define a pytensor RandomVariable\u00b6","text":""},{"location":"tutorials/rlssm_tutorial/#step-2-define-a-likelihood-function","title":"Step 2: Define a likelihood function\u00b6","text":""},{"location":"tutorials/rlssm_tutorial/#step-3-define-a-model-config-and-hssm-model","title":"Step 3: Define a model config and HSSM model\u00b6","text":""},{"location":"tutorials/rlssm_tutorial/#sample-using-nuts-mcmc","title":"Sample using NUTS MCMC\u00b6","text":""},{"location":"tutorials/rlssm_tutorial/#assess-the-model-fits","title":"Assess the model fits\u00b6","text":"<p>We examine the quality of fits by comparing the recovered parameters with the ground-truth data generating parameters of the simulated dataset. We examine the quality of fits both at group-level as well as subject-level.</p>"},{"location":"tutorials/rlssm_tutorial/#examining-group-level-posteriors","title":"Examining group-level posteriors\u00b6","text":""},{"location":"tutorials/rlssm_tutorial/#examining-participant-level-posteriors","title":"Examining participant-level posteriors\u00b6","text":""},{"location":"tutorials/rlssm_tutorial/#estimating-the-posterior-using-variation-inference-vi","title":"Estimating the posterior using Variation Inference (VI)\u00b6","text":""},{"location":"tutorials/rlssm_tutorial/#compare-mcmc-and-vi-parameter-recoveries","title":"Compare MCMC and VI parameter recoveries\u00b6","text":""},{"location":"tutorials/save_load_tutorial/","title":"Saving and loading models","text":"In\u00a0[1]: Copied! <pre>import hssm\n\ncav_data = hssm.load_data(\"cavanagh_theta\")\n\nbasic_hssm_model = hssm.HSSM(\n    data=cav_data,\n    process_initvals=True,\n    link_settings=\"log_logit\",\n    model=\"angle\",\n    include=[\n        {\n            \"name\": \"v\",\n            \"formula\": \"v ~ 1 + C(stim)\",\n        }\n    ],\n)\n</pre> import hssm  cav_data = hssm.load_data(\"cavanagh_theta\")  basic_hssm_model = hssm.HSSM(     data=cav_data,     process_initvals=True,     link_settings=\"log_logit\",     model=\"angle\",     include=[         {             \"name\": \"v\",             \"formula\": \"v ~ 1 + C(stim)\",         }     ], ) <pre>Model initialized successfully.\n</pre> In\u00a0[2]: Copied! <pre>basic_hssm_model.sample(sampler=\"nuts_numpyro\",\n                        tune=100,\n                        draws=100,\n                        chains=2)\n</pre> basic_hssm_model.sample(sampler=\"nuts_numpyro\",                         tune=100,                         draws=100,                         chains=2) <pre>Using default initvals. \n\n</pre> <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/pymc/sampling/jax.py:475: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  pmap_numpyro = MCMC(\nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [01:06&lt;00:00,  3.01it/s, 127 steps of size 3.16e-02. acc. prob=0.96]\nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:57&lt;00:00,  3.47it/s, 127 steps of size 2.55e-02. acc. prob=0.92]\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:00&lt;00:00, 306.78it/s]\n</pre> Out[2]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 12kB\nDimensions:        (chain: 2, draw: 100, v_C(stim)_dim: 2)\nCoordinates:\n  * chain          (chain) int64 16B 0 1\n  * draw           (draw) int64 800B 0 1 2 3 4 5 6 7 ... 92 93 94 95 96 97 98 99\n  * v_C(stim)_dim  (v_C(stim)_dim) &lt;U2 16B 'WL' 'WW'\nData variables:\n    v_C(stim)      (chain, draw, v_C(stim)_dim) float64 3kB 0.2478 ... -0.01796\n    z              (chain, draw) float64 2kB 0.4999 0.5086 ... 0.5136 0.5006\n    t              (chain, draw) float64 2kB 0.2896 0.2853 ... 0.2931 0.2688\n    a              (chain, draw) float64 2kB 1.289 1.317 1.356 ... 1.309 1.351\n    v_Intercept    (chain, draw) float64 2kB 0.1255 0.1369 ... 0.1131 0.1124\n    theta          (chain, draw) float64 2kB 0.2111 0.2201 ... 0.2339 0.2392\nAttributes:\n    created_at:                  2025-09-27T00:19:51.139489+00:00\n    arviz_version:               0.22.0\n    inference_library:           numpyro\n    inference_library_version:   0.19.0\n    sampling_time:               128.046042\n    tuning_steps:                100\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 100</li><li>v_C(stim)_dim: 2</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99<pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])</pre></li><li>v_C(stim)_dim(v_C(stim)_dim)&lt;U2'WL' 'WW'<pre>array(['WL', 'WW'], dtype='&lt;U2')</pre></li></ul></li><li>Data variables: (6)<ul><li>v_C(stim)(chain, draw, v_C(stim)_dim)float640.2478 -0.03257 ... 0.2943 -0.01796<pre>array([[[ 2.47786397e-01, -3.25715146e-02],\n        [ 2.56693660e-01, -2.69431529e-02],\n        [ 2.59687385e-01, -3.39634816e-02],\n        [ 2.60295894e-01, -3.67342539e-02],\n        [ 2.51885116e-01, -1.25108729e-02],\n        [ 2.35854315e-01, -5.88194984e-02],\n        [ 2.24356783e-01, -3.37645728e-02],\n        [ 2.61045679e-01, -6.41845895e-02],\n        [ 2.55550354e-01, -4.79380732e-02],\n        [ 2.64563835e-01, -5.59813057e-02],\n        [ 2.48989408e-01, -3.63335705e-02],\n        [ 3.10525793e-01,  1.89059386e-02],\n        [ 3.25664681e-01, -1.82850250e-02],\n        [ 3.00186354e-01, -6.41723856e-02],\n        [ 2.58288009e-01, -2.27014139e-02],\n        [ 2.63368569e-01, -3.43231633e-03],\n        [ 2.75750777e-01, -1.41977560e-02],\n        [ 2.49790504e-01, -3.59149110e-02],\n        [ 2.55304939e-01, -3.97826192e-02],\n        [ 2.54607058e-01, -3.97992322e-02],\n...\n        [ 2.48046694e-01, -4.32265484e-02],\n        [ 2.66196794e-01, -2.05225666e-02],\n        [ 2.65413315e-01, -2.19709747e-02],\n        [ 2.65755146e-01, -2.23310651e-02],\n        [ 2.73872386e-01, -2.48187855e-02],\n        [ 2.94937525e-01,  1.59932909e-02],\n        [ 3.07170241e-01, -2.82341600e-02],\n        [ 2.90722518e-01, -2.50460711e-02],\n        [ 2.88983521e-01,  1.66251098e-02],\n        [ 2.81119902e-01,  3.45963736e-02],\n        [ 2.97145397e-01,  7.98233904e-03],\n        [ 2.65018183e-01, -1.75052328e-02],\n        [ 2.81604289e-01, -6.33374060e-02],\n        [ 2.98165869e-01, -4.65229487e-02],\n        [ 2.38191805e-01, -2.13071643e-02],\n        [ 2.59235957e-01, -3.97518546e-02],\n        [ 3.11137873e-01,  4.23346848e-03],\n        [ 2.88988203e-01, -7.82774698e-04],\n        [ 2.35936457e-01, -2.19483071e-02],\n        [ 2.94288725e-01, -1.79611109e-02]]])</pre></li><li>z(chain, draw)float640.4999 0.5086 ... 0.5136 0.5006<pre>array([[0.49987897, 0.50864212, 0.50486877, 0.50439937, 0.50035211,\n        0.50419151, 0.50163676, 0.50954881, 0.50919876, 0.50767582,\n        0.50985733, 0.49907023, 0.50569153, 0.50512311, 0.50741872,\n        0.50667579, 0.50679041, 0.49753402, 0.50012725, 0.50631517,\n        0.50800309, 0.49852165, 0.50505441, 0.50936129, 0.51279923,\n        0.508523  , 0.50390044, 0.50314733, 0.49636304, 0.49595851,\n        0.50002543, 0.50649313, 0.50669288, 0.5056028 , 0.49656862,\n        0.5004973 , 0.49838909, 0.49524968, 0.50970748, 0.50486335,\n        0.50140159, 0.50962184, 0.49319775, 0.49565749, 0.49772701,\n        0.50424324, 0.50262021, 0.50160242, 0.50356627, 0.50598604,\n        0.50209002, 0.51096186, 0.49623914, 0.49876425, 0.49706241,\n        0.50333354, 0.51000057, 0.50721928, 0.50334656, 0.50201245,\n        0.50263777, 0.50345197, 0.50205708, 0.50305103, 0.50647556,\n        0.50313319, 0.5009946 , 0.49832923, 0.50391856, 0.49750607,\n        0.49345856, 0.49749728, 0.50661136, 0.5089347 , 0.50778886,\n        0.5043959 , 0.50533657, 0.50632776, 0.5144475 , 0.50255337,\n        0.50547244, 0.49963754, 0.51071926, 0.51093493, 0.50061203,\n        0.49757915, 0.497835  , 0.49938637, 0.51141981, 0.49621997,\n        0.4993319 , 0.48513939, 0.48634545, 0.48984919, 0.49611231,\n        0.49864561, 0.50192445, 0.49917998, 0.50383047, 0.49787141],\n       [0.49275022, 0.49438932, 0.51044599, 0.51192074, 0.49662996,\n        0.50272076, 0.49401273, 0.51204548, 0.51136586, 0.49952566,\n        0.49874168, 0.50520286, 0.5072275 , 0.49908783, 0.49813179,\n        0.49997596, 0.51208147, 0.50695305, 0.491071  , 0.49391721,\n        0.51640351, 0.50776004, 0.49337082, 0.50071399, 0.50388455,\n        0.4939315 , 0.5010106 , 0.49492338, 0.49488219, 0.49908431,\n        0.50445372, 0.50947157, 0.51178001, 0.49892264, 0.50052349,\n        0.50225144, 0.49445985, 0.51203663, 0.4963787 , 0.49436705,\n        0.50189235, 0.48983583, 0.51172884, 0.50669688, 0.5030135 ,\n        0.4960494 , 0.51053906, 0.50399825, 0.50616482, 0.4951815 ,\n        0.50651274, 0.50705338, 0.50200623, 0.50001245, 0.49433587,\n        0.49650799, 0.4959487 , 0.50557574, 0.50133556, 0.50203973,\n        0.50010553, 0.51277838, 0.51238216, 0.51403323, 0.50944704,\n        0.50662585, 0.50396585, 0.49996445, 0.51031119, 0.50242508,\n        0.49928372, 0.49604433, 0.49865952, 0.50244585, 0.50242966,\n        0.49998693, 0.49244259, 0.49588852, 0.50165136, 0.49812548,\n        0.49826355, 0.51466189, 0.5125328 , 0.51276168, 0.49470395,\n        0.51130178, 0.50071674, 0.50808986, 0.49575782, 0.50153076,\n        0.49892678, 0.51091969, 0.50467395, 0.50606879, 0.50220695,\n        0.50610355, 0.50428869, 0.50868882, 0.51356968, 0.50063898]])</pre></li><li>t(chain, draw)float640.2896 0.2853 ... 0.2931 0.2688<pre>array([[0.28963607, 0.285315  , 0.2736115 , 0.26994168, 0.28430052,\n        0.28541659, 0.27592309, 0.28728086, 0.25857503, 0.2937224 ,\n        0.28999817, 0.27689075, 0.29799353, 0.27434272, 0.27792795,\n        0.28070835, 0.28832945, 0.28953326, 0.26801804, 0.29006838,\n        0.27241378, 0.27927289, 0.27078568, 0.28925495, 0.27024896,\n        0.27944618, 0.28681414, 0.27012443, 0.26998618, 0.29654812,\n        0.29011835, 0.30290026, 0.25803568, 0.29466113, 0.26121377,\n        0.29002059, 0.27977438, 0.28232143, 0.28324351, 0.28884263,\n        0.28601703, 0.27979717, 0.27853096, 0.27709331, 0.27793205,\n        0.2617821 , 0.29707024, 0.27857381, 0.27842543, 0.29313895,\n        0.26318458, 0.26651661, 0.27246122, 0.26546666, 0.28912726,\n        0.28547095, 0.27566313, 0.26383397, 0.27743201, 0.2680533 ,\n        0.27450817, 0.28864969, 0.27669303, 0.26965758, 0.27523143,\n        0.2857726 , 0.28166244, 0.26505422, 0.29009058, 0.29010648,\n        0.2644507 , 0.27982659, 0.2814815 , 0.27921969, 0.27820489,\n        0.27509269, 0.28516133, 0.28318534, 0.29163191, 0.26862345,\n        0.28661517, 0.26815252, 0.26537447, 0.27881485, 0.29116317,\n        0.28561608, 0.27112915, 0.27270036, 0.29581579, 0.26035608,\n        0.2621768 , 0.25596013, 0.28307914, 0.29145946, 0.273154  ,\n        0.27457924, 0.27779551, 0.26825518, 0.28056329, 0.27793946],\n       [0.2846891 , 0.28809034, 0.28625758, 0.28353427, 0.27945021,\n        0.27667708, 0.272093  , 0.28441914, 0.28372686, 0.27455992,\n        0.2797845 , 0.28061989, 0.26817127, 0.28310216, 0.29212801,\n        0.29001276, 0.29062318, 0.28231683, 0.28002674, 0.28037404,\n        0.28526242, 0.27294422, 0.27018489, 0.27060322, 0.2820452 ,\n        0.28464241, 0.26768214, 0.2852928 , 0.29129031, 0.30526469,\n        0.2858296 , 0.28165341, 0.29297101, 0.2527173 , 0.29487649,\n        0.27477029, 0.26313324, 0.27967722, 0.28066344, 0.2626408 ,\n        0.27694421, 0.27844604, 0.28345868, 0.2795014 , 0.27922753,\n        0.28404631, 0.26897903, 0.26136306, 0.27665899, 0.26829896,\n        0.28188104, 0.28508936, 0.27956543, 0.29319844, 0.2591669 ,\n        0.28784746, 0.27692849, 0.26965567, 0.2836482 , 0.29044245,\n        0.26766057, 0.29996986, 0.2651463 , 0.2602039 , 0.29751384,\n        0.26867796, 0.29161297, 0.27400571, 0.26101827, 0.30049124,\n        0.26154535, 0.26968607, 0.29684627, 0.24849784, 0.25689429,\n        0.29327291, 0.25182863, 0.28287137, 0.27178957, 0.26901953,\n        0.27511072, 0.29839017, 0.27802233, 0.26620283, 0.26437793,\n        0.27223741, 0.28270515, 0.28633502, 0.27876629, 0.27518814,\n        0.27414101, 0.28845286, 0.26443314, 0.28767833, 0.28528195,\n        0.26451806, 0.27154601, 0.2783306 , 0.29306828, 0.26881886]])</pre></li><li>a(chain, draw)float641.289 1.317 1.356 ... 1.309 1.351<pre>array([[1.2894851 , 1.31681411, 1.35633937, 1.34537966, 1.31351723,\n        1.30999434, 1.3359536 , 1.30783188, 1.3714005 , 1.27568453,\n        1.33044471, 1.32657614, 1.30672583, 1.32539202, 1.32569828,\n        1.32434799, 1.29788646, 1.29103436, 1.35112864, 1.29131635,\n        1.34781518, 1.33407884, 1.31360807, 1.31695623, 1.36958783,\n        1.3239242 , 1.32092951, 1.383088  , 1.34613901, 1.29089198,\n        1.29060806, 1.27169141, 1.35591633, 1.28544603, 1.35832444,\n        1.33319104, 1.33685107, 1.30165082, 1.34279933, 1.29959442,\n        1.30562347, 1.31082039, 1.31279481, 1.32696244, 1.31579921,\n        1.39153196, 1.27601463, 1.33984538, 1.32520489, 1.30601755,\n        1.36444956, 1.37344622, 1.33263134, 1.34254451, 1.29306683,\n        1.29762302, 1.35910692, 1.35837799, 1.31500024, 1.34271829,\n        1.33745442, 1.31503958, 1.32381187, 1.33192717, 1.32712043,\n        1.31078267, 1.30549677, 1.34509446, 1.30236828, 1.31203813,\n        1.3407952 , 1.31850818, 1.3219183 , 1.3151066 , 1.33728342,\n        1.3229139 , 1.31231589, 1.31788601, 1.30935122, 1.34843088,\n        1.31070351, 1.36518746, 1.35052744, 1.34078141, 1.29287894,\n        1.30949194, 1.34064286, 1.3460516 , 1.29286228, 1.35608704,\n        1.35858141, 1.36044547, 1.30265028, 1.27127702, 1.32758024,\n        1.32553703, 1.32171453, 1.33863218, 1.33379707, 1.33092837],\n       [1.3142627 , 1.28407636, 1.32238917, 1.32462989, 1.31945859,\n        1.32720308, 1.32402042, 1.32401073, 1.31106137, 1.33626808,\n        1.32327124, 1.31601349, 1.3502655 , 1.29227285, 1.28719159,\n        1.2892943 , 1.30191969, 1.32953307, 1.31585168, 1.32463106,\n        1.30368496, 1.31212284, 1.31550096, 1.31397896, 1.32710492,\n        1.32078475, 1.34158691, 1.29998391, 1.29373214, 1.26497551,\n        1.31597014, 1.34733056, 1.32719488, 1.37013539, 1.29217907,\n        1.3345999 , 1.3561066 , 1.3391144 , 1.31820189, 1.33907956,\n        1.34040908, 1.30927374, 1.31462813, 1.31038681, 1.32616592,\n        1.30971243, 1.3575855 , 1.36219998, 1.30575963, 1.33289118,\n        1.30758131, 1.32242155, 1.32145216, 1.29778306, 1.3688642 ,\n        1.32590202, 1.32044991, 1.33387263, 1.2873583 , 1.27791435,\n        1.36849422, 1.26658346, 1.37205292, 1.38365416, 1.2742191 ,\n        1.36327691, 1.28822031, 1.3563078 , 1.38824538, 1.26398476,\n        1.37109953, 1.35802228, 1.27192181, 1.38348042, 1.36798635,\n        1.27930299, 1.38371939, 1.31796242, 1.36449193, 1.33709583,\n        1.32250815, 1.31257819, 1.35459608, 1.34907703, 1.33762822,\n        1.33433432, 1.32152661, 1.31058389, 1.31559553, 1.32334143,\n        1.34737017, 1.29105575, 1.36416247, 1.29619124, 1.29507039,\n        1.37767393, 1.35549789, 1.33084245, 1.30938633, 1.35119837]])</pre></li><li>v_Intercept(chain, draw)float640.1255 0.1369 ... 0.1131 0.1124<pre>array([[0.12551431, 0.13687268, 0.15076777, 0.15171453, 0.10584615,\n        0.17020109, 0.15408652, 0.12248873, 0.1253672 , 0.1239427 ,\n        0.12087076, 0.10182302, 0.09856646, 0.10847696, 0.12395042,\n        0.11970177, 0.10052638, 0.15355198, 0.13812561, 0.13776066,\n        0.12481354, 0.12597939, 0.15511535, 0.06343301, 0.08406924,\n        0.07320825, 0.08297128, 0.0915462 , 0.09291118, 0.08826296,\n        0.11544485, 0.13549539, 0.14710472, 0.10353716, 0.1291779 ,\n        0.13220591, 0.14111874, 0.15078896, 0.13140871, 0.11668281,\n        0.11762003, 0.09947938, 0.1327243 , 0.13508678, 0.13749125,\n        0.11841631, 0.14209081, 0.10222955, 0.11063127, 0.11358776,\n        0.08037391, 0.09680547, 0.13482346, 0.13889293, 0.12277367,\n        0.10501525, 0.10332867, 0.12074176, 0.11667711, 0.10833071,\n        0.11131725, 0.14880373, 0.1403917 , 0.1381435 , 0.12928663,\n        0.13428289, 0.10992001, 0.10454291, 0.13804878, 0.12764382,\n        0.14891441, 0.13899826, 0.1249693 , 0.07209583, 0.09469547,\n        0.10796289, 0.10916894, 0.12150473, 0.12625764, 0.11154106,\n        0.14797987, 0.10252593, 0.10689557, 0.10961645, 0.13324053,\n        0.10710643, 0.09618896, 0.10360439, 0.07988574, 0.15469174,\n        0.14298571, 0.17468325, 0.13243769, 0.13467793, 0.1174089 ,\n        0.11977142, 0.11332507, 0.12435531, 0.11169624, 0.11049975],\n       [0.15572018, 0.1516449 , 0.10378379, 0.09962121, 0.09542749,\n        0.13438272, 0.15408194, 0.0901176 , 0.09231489, 0.15207494,\n        0.15375224, 0.12767389, 0.15430101, 0.15302362, 0.15456648,\n        0.10338465, 0.12310776, 0.11588872, 0.1253718 , 0.12635735,\n        0.11790048, 0.11756963, 0.12137361, 0.13471445, 0.13164852,\n        0.14652473, 0.1471656 , 0.14800297, 0.14706665, 0.14669134,\n        0.10778513, 0.08966818, 0.08784914, 0.10651384, 0.11897809,\n        0.11084264, 0.11737089, 0.09603257, 0.13378297, 0.13506529,\n        0.14536047, 0.1419732 , 0.11558216, 0.10580983, 0.11765651,\n        0.12387512, 0.10428217, 0.09416964, 0.0961578 , 0.10701236,\n        0.10168476, 0.10194319, 0.09264325, 0.11005882, 0.11767884,\n        0.11922986, 0.11919745, 0.14335461, 0.09536215, 0.09370839,\n        0.10233336, 0.10059294, 0.08507356, 0.08744344, 0.09185536,\n        0.11166569, 0.11716949, 0.11232148, 0.12154257, 0.10573141,\n        0.17387027, 0.13006036, 0.12650722, 0.10317428, 0.11224981,\n        0.12496255, 0.11067021, 0.12478647, 0.11642788, 0.13795147,\n        0.13846859, 0.10676974, 0.09946726, 0.09893274, 0.1380099 ,\n        0.09489482, 0.09435707, 0.11371824, 0.11350666, 0.11164049,\n        0.1069427 , 0.09993337, 0.11166614, 0.11747547, 0.13201177,\n        0.11759177, 0.09774354, 0.08325727, 0.11310072, 0.11242786]])</pre></li><li>theta(chain, draw)float640.2111 0.2201 ... 0.2339 0.2392<pre>array([[0.21114948, 0.2201244 , 0.2448452 , 0.24691044, 0.22758451,\n        0.2118614 , 0.23246388, 0.22968552, 0.25444706, 0.21329025,\n        0.24844903, 0.24666276, 0.2271454 , 0.23994209, 0.23011954,\n        0.23863695, 0.21764499, 0.22452051, 0.25103805, 0.21390344,\n        0.25100379, 0.24579388, 0.2214081 , 0.23037612, 0.25227905,\n        0.23637984, 0.22911368, 0.2701962 , 0.23000971, 0.23085182,\n        0.22736598, 0.21174045, 0.24706564, 0.21699666, 0.24576758,\n        0.24562126, 0.25202304, 0.21007883, 0.25705562, 0.21375955,\n        0.21649536, 0.22375832, 0.22090423, 0.23999757, 0.22257645,\n        0.26203964, 0.21273778, 0.24667948, 0.23592001, 0.21203254,\n        0.2645852 , 0.26085639, 0.24094288, 0.24609222, 0.21500687,\n        0.20795803, 0.26177958, 0.24672844, 0.22437138, 0.24164072,\n        0.23347463, 0.22996326, 0.24057015, 0.23264819, 0.23060054,\n        0.22914786, 0.22446532, 0.24593976, 0.21274081, 0.22934207,\n        0.23314985, 0.21757919, 0.2450553 , 0.22397807, 0.24452975,\n        0.22106545, 0.2376378 , 0.24158817, 0.23555152, 0.2455844 ,\n        0.21623313, 0.24642318, 0.24874743, 0.23925834, 0.22185878,\n        0.21992453, 0.25100009, 0.23368056, 0.22527859, 0.24000268,\n        0.24162579, 0.24236436, 0.21474767, 0.19574973, 0.24149988,\n        0.23895464, 0.24154286, 0.24070787, 0.24330913, 0.23417799],\n       [0.22238888, 0.1937118 , 0.24594808, 0.23272888, 0.23406551,\n        0.23222285, 0.23135612, 0.23274164, 0.22837162, 0.23666489,\n        0.2338659 , 0.23011911, 0.24679877, 0.21495533, 0.21563331,\n        0.21429269, 0.21557807, 0.24327253, 0.22907574, 0.23510812,\n        0.21979598, 0.22242069, 0.22757132, 0.2229551 , 0.2252867 ,\n        0.23747255, 0.23493019, 0.23657041, 0.23687174, 0.21906379,\n        0.22498572, 0.24403192, 0.24166096, 0.25534539, 0.21193444,\n        0.23723614, 0.25253841, 0.24248678, 0.23159883, 0.23812078,\n        0.23398997, 0.23411327, 0.2326248 , 0.23272557, 0.23310171,\n        0.23058342, 0.24191749, 0.24552518, 0.22421558, 0.2367256 ,\n        0.22678743, 0.23298324, 0.23885096, 0.2227665 , 0.24584816,\n        0.24882255, 0.21525089, 0.2278934 , 0.20327291, 0.21695946,\n        0.25212305, 0.2035756 , 0.26887494, 0.26032186, 0.21131426,\n        0.26193088, 0.20638741, 0.25263054, 0.26242497, 0.19446962,\n        0.2621171 , 0.25747483, 0.20449082, 0.2599197 , 0.25136769,\n        0.21531786, 0.2559474 , 0.24177639, 0.26474963, 0.22852897,\n        0.22829843, 0.22302725, 0.25150173, 0.2547041 , 0.23964829,\n        0.2448488 , 0.23643258, 0.23114336, 0.22998879, 0.23984873,\n        0.24838746, 0.21079205, 0.25142107, 0.21221087, 0.21655302,\n        0.26143794, 0.25817777, 0.22872483, 0.23393712, 0.23921622]])</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n      dtype='int64', name='draw'))</pre></li><li>v_C(stim)_dimPandasIndex<pre>PandasIndex(Index(['WL', 'WW'], dtype='object', name='v_C(stim)_dim'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T00:19:51.139489+00:00arviz_version :0.22.0inference_library :numpyroinference_library_version :0.19.0sampling_time :128.046042tuning_steps :100modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 6MB\nDimensions:      (chain: 2, draw: 100, __obs__: 3988)\nCoordinates:\n  * chain        (chain) int64 16B 0 1\n  * draw         (draw) int64 800B 0 1 2 3 4 5 6 7 8 ... 92 93 94 95 96 97 98 99\n  * __obs__      (__obs__) int64 32kB 0 1 2 3 4 5 ... 3983 3984 3985 3986 3987\nData variables:\n    rt,response  (chain, draw, __obs__) float64 6MB -1.045 -1.203 ... -1.117\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 100</li><li>__obs__: 3988</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99<pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])</pre></li><li>__obs__(__obs__)int640 1 2 3 4 ... 3984 3985 3986 3987<pre>array([   0,    1,    2, ..., 3985, 3986, 3987], shape=(3988,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-1.045 -1.203 ... -2.424 -1.117<pre>array([[[-1.045234  , -1.20260171, -0.98392839, ..., -0.50332234,\n         -2.51073826, -1.12220718],\n        [-1.02281899, -1.18552861, -0.95270161, ..., -0.47266778,\n         -2.49858209, -1.09028677],\n        [-0.99844618, -1.15565219, -0.95135517, ..., -0.47626903,\n         -2.48878473, -1.07312009],\n        ...,\n        [-1.04347013, -1.17101183, -0.98784982, ..., -0.47955839,\n         -2.4723303 , -1.11108982],\n        [-1.04536539, -1.17732141, -0.94527962, ..., -0.50223964,\n         -2.45025552, -1.07417393],\n        [-1.06248382, -1.16184561, -0.99742853, ..., -0.47004792,\n         -2.44185478, -1.11856994]],\n\n       [[-1.00636084, -1.16784841, -1.00898285, ..., -0.50155331,\n         -2.532939  , -1.12948429],\n        [-1.02972018, -1.19128674, -1.00430849, ..., -0.49102554,\n         -2.5509271 , -1.13842714],\n        [-1.04237882, -1.18520978, -0.91806349, ..., -0.45368527,\n         -2.46160259, -1.05863272],\n        ...,\n        [-1.09786955, -1.18892457, -0.99737739, ..., -0.50541176,\n         -2.39259424, -1.12808538],\n        [-1.03169664, -1.20645532, -0.94950857, ..., -0.49758115,\n         -2.48080504, -1.09333382],\n        [-1.06316187, -1.1609575 , -0.99880186, ..., -0.49165392,\n         -2.42400698, -1.11653095]]], shape=(2, 100, 3988))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n      dtype='int64', name='draw'))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987],\n      dtype='int64', name='__obs__', length=3988))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 11kB\nDimensions:          (chain: 2, draw: 100)\nCoordinates:\n  * chain            (chain) int64 16B 0 1\n  * draw             (draw) int64 800B 0 1 2 3 4 5 6 7 ... 93 94 95 96 97 98 99\nData variables:\n    acceptance_rate  (chain, draw) float64 2kB 0.9995 0.943 ... 0.9951 0.9995\n    step_size        (chain, draw) float64 2kB 0.03157 0.03157 ... 0.02553\n    diverging        (chain, draw) bool 200B False False False ... False False\n    energy           (chain, draw) float64 2kB 5.939e+03 5.939e+03 ... 5.937e+03\n    n_steps          (chain, draw) int64 2kB 127 63 63 127 ... 191 127 127 127\n    tree_depth       (chain, draw) int64 2kB 7 6 6 7 6 7 6 7 ... 7 7 7 7 8 7 7 7\n    lp               (chain, draw) float64 2kB 5.934e+03 5.934e+03 ... 5.933e+03\nAttributes:\n    created_at:                  2025-09-27T00:19:51.160945+00:00\n    arviz_version:               0.22.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 100</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99<pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])</pre></li></ul></li><li>Data variables: (7)<ul><li>acceptance_rate(chain, draw)float640.9995 0.943 ... 0.9951 0.9995<pre>array([[0.99947497, 0.9429698 , 0.98925675, 0.99883321, 0.99889819,\n        0.9688784 , 0.98350911, 0.99983244, 0.97942672, 0.97480833,\n        0.74408891, 0.92013557, 0.94148222, 0.98322539, 0.98966278,\n        0.98540343, 0.99296979, 0.8858107 , 0.99952864, 0.99990335,\n        0.99327737, 0.99408646, 0.82034221, 0.97345291, 0.98888809,\n        0.98719762, 0.95952252, 0.97957949, 0.97659523, 0.88473691,\n        0.99342896, 0.97263215, 0.74729087, 0.97569598, 0.99206781,\n        0.8311919 , 0.99604827, 0.99954324, 0.96984887, 0.99970808,\n        0.97070855, 0.89698271, 0.99941124, 0.99150021, 0.99274071,\n        0.81314312, 0.99351608, 0.97071725, 0.99677219, 0.87891558,\n        0.81521009, 0.99987448, 0.98978742, 0.89402778, 0.97512302,\n        0.98202798, 0.93845963, 0.96291293, 0.95025252, 0.99321649,\n        0.93493635, 0.99180102, 0.93127254, 0.99885811, 0.98103877,\n        0.99942001, 0.84260706, 0.91496282, 0.99574799, 0.96901403,\n        0.99344699, 0.98939992, 0.99283544, 0.99986906, 0.969984  ,\n        0.95988449, 0.86313465, 0.98296481, 0.99274972, 0.99596546,\n        0.95922537, 0.93124269, 0.99299245, 0.99635664, 0.82056538,\n        0.95375251, 0.97929917, 0.97290371, 0.96949573, 0.9935982 ,\n        0.99645212, 0.97697855, 0.99267968, 0.99825161, 0.72491002,\n        0.97089882, 0.98773107, 0.9991141 , 0.99187353, 0.96152245],\n       [0.99991215, 0.98119677, 0.80484589, 0.85196493, 0.98406825,\n        0.91521857, 0.82720944, 0.97059569, 0.69900613, 0.94931316,\n        0.99998192, 0.82163929, 0.99195037, 0.95208581, 0.9961346 ,\n        0.99525843, 0.78328794, 0.90350675, 0.99269845, 0.94680416,\n        0.89966419, 0.83899836, 0.99680872, 0.94611516, 0.99043772,\n        0.93356873, 0.99943069, 0.90153026, 0.82736535, 0.99546061,\n        0.98621436, 0.51598635, 0.98623837, 0.80788338, 0.97985892,\n        0.99560539, 0.93460866, 0.91997701, 0.95819106, 0.82693428,\n        0.99942086, 0.96897523, 0.99890308, 0.89907686, 0.99913166,\n        0.97528741, 0.65415515, 0.9184215 , 0.76737571, 0.99668816,\n        0.84137148, 0.98113275, 0.98874927, 0.98340267, 0.94993719,\n        0.68672686, 0.99688169, 0.86550937, 0.76284807, 0.91093007,\n        1.        , 0.98359196, 0.98948003, 0.98478719, 0.97592858,\n        0.97728732, 1.        , 0.92593637, 0.9727777 , 0.99088193,\n        0.93350625, 0.98535827, 0.99911009, 0.84405013, 0.91134071,\n        0.99964958, 0.99086548, 0.96937831, 0.99514809, 0.70149758,\n        0.9622942 , 0.72431493, 0.97659466, 0.90109571, 0.99448445,\n        0.98199757, 0.99380639, 0.97966603, 0.99717112, 0.87734089,\n        0.99402502, 0.99956112, 0.573611  , 0.91070027, 0.99247126,\n        0.96969765, 0.79846298, 0.82896922, 0.99513796, 0.9994776 ]])</pre></li><li>step_size(chain, draw)float640.03157 0.03157 ... 0.02553 0.02553<pre>array([[0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607,\n        0.03156607, 0.03156607, 0.03156607, 0.03156607, 0.03156607],\n       [0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304,\n        0.02553304, 0.02553304, 0.02553304, 0.02553304, 0.02553304]])</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False],\n       [False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False]])</pre></li><li>energy(chain, draw)float645.939e+03 5.939e+03 ... 5.937e+03<pre>array([[5939.04128822, 5938.99197283, 5939.71455307, 5936.37055317,\n        5936.0505918 , 5940.05285575, 5940.23796909, 5935.84107998,\n        5936.35644043, 5937.65113175, 5940.26369167, 5938.57051596,\n        5942.34749752, 5942.33648167, 5938.63205867, 5932.81182533,\n        5934.42395711, 5938.97262372, 5936.15848595, 5933.21929303,\n        5933.63390903, 5933.77048012, 5937.83147054, 5940.18942077,\n        5941.69662081, 5941.38460349, 5936.60771138, 5940.96575012,\n        5942.87665014, 5945.01682926, 5941.93011402, 5942.5420749 ,\n        5942.69519384, 5938.95932386, 5936.51793119, 5941.56810379,\n        5939.93942754, 5938.40620701, 5939.3022376 , 5938.5611248 ,\n        5941.64195556, 5941.40383823, 5937.52427851, 5935.54327071,\n        5934.55147447, 5939.89478243, 5938.57452609, 5939.57072522,\n        5936.03905868, 5938.89557315, 5942.75279474, 5938.22509154,\n        5936.87057057, 5937.49968958, 5937.222237  , 5936.28970984,\n        5936.78087793, 5941.47300903, 5938.66200331, 5934.92107519,\n        5937.69122356, 5936.71829784, 5936.14064143, 5934.19307129,\n        5934.23494946, 5932.60118777, 5935.61496456, 5935.53485443,\n        5936.60990748, 5935.0703478 , 5936.45817165, 5935.74314418,\n        5936.00724322, 5937.99270531, 5937.44764006, 5936.79695893,\n        5943.70268756, 5938.67899425, 5939.44613468, 5938.48303096,\n...\n        5939.06930411, 5939.69656345, 5938.29053124, 5936.77434929,\n        5941.54908339, 5938.78938403, 5939.96345046, 5941.70988844,\n        5942.48375048, 5944.84526234, 5945.73885036, 5938.34756913,\n        5936.40320938, 5940.08728454, 5940.876724  , 5938.38899262,\n        5941.20106735, 5939.53988924, 5939.30015487, 5939.14652495,\n        5937.24719579, 5939.06177952, 5939.10342447, 5936.26733575,\n        5934.32324517, 5934.58267425, 5938.76383488, 5937.45296171,\n        5942.99916038, 5936.39428899, 5937.35426013, 5933.73747824,\n        5936.05136791, 5935.36344855, 5938.77161143, 5942.05084529,\n        5940.62301437, 5941.28500912, 5942.73609515, 5940.27044552,\n        5937.72314382, 5945.116606  , 5941.04087688, 5940.9980393 ,\n        5938.59871025, 5938.25091245, 5934.64709164, 5934.44335348,\n        5937.49116069, 5941.12147165, 5943.51341566, 5939.81221307,\n        5936.52174052, 5939.28921634, 5939.45869329, 5937.77553372,\n        5941.10913834, 5944.14732387, 5939.85823265, 5941.90539549,\n        5934.89266714, 5937.11428635, 5936.73386426, 5935.7166906 ,\n        5937.42033409, 5936.4286354 , 5937.84731474, 5935.92228104,\n        5935.9089063 , 5936.30771747, 5935.52891018, 5935.25002285,\n        5942.62466805, 5938.22177679, 5936.02084563, 5937.04617541,\n        5940.44181053, 5939.79955883, 5938.30124323, 5936.81371486]])</pre></li><li>n_steps(chain, draw)int64127 63 63 127 ... 191 127 127 127<pre>array([[127,  63,  63, 127,  63, 127,  63, 127,  63,  63,  63, 127,  63,\n        127, 127, 127, 127,  95,  39,  63,  95, 127, 127, 255,  63,  63,\n         63,  31,  31,   7,  63, 127,  63, 127, 127, 127,  63,  63, 127,\n        127,  63, 127, 127,  31,  31,  63, 127, 127, 127,  31, 127, 127,\n        127,  63,  63, 127,  63, 191,  63,  31,  31,  63,  63,  23,  63,\n         63, 191,  63, 191, 191, 127,  63,  87, 127,  63,  95,  31,  79,\n         63, 127,  63, 127, 191,  63, 127, 127,  95,  31, 127, 127, 127,\n        127, 191,  63, 127,  63, 127, 127,  95, 127],\n       [ 63,  63, 143,  63,  63, 159, 127, 127,  11, 159,  63, 127,  63,\n        127,  31, 191, 127,  63, 127,  63, 127,  63, 127,  63, 127,  63,\n        127,  47,  11,  39, 119,  83,  31,  63,  63,  63,  63, 127,  95,\n        191,  63,  63, 135,  63,  63,  95,  95, 127, 127,  63, 127, 127,\n         63,  63,  63,  23,  79, 191, 159,   7,  63, 127,  63,  63,  63,\n         63,  95,  31, 127,  95, 127, 127,  95,  79,  47,  63,  31, 127,\n         95, 127,  15, 127,  35,   7, 127, 127, 127,  63,  63,  63,  63,\n        255, 127, 127, 127, 127, 191, 127, 127, 127]])</pre></li><li>tree_depth(chain, draw)int647 6 6 7 6 7 6 7 ... 7 7 7 7 8 7 7 7<pre>array([[7, 6, 6, 7, 6, 7, 6, 7, 6, 6, 6, 7, 6, 7, 7, 7, 7, 7, 6, 6, 7, 7,\n        7, 8, 6, 6, 6, 5, 5, 3, 6, 7, 6, 7, 7, 7, 6, 6, 7, 7, 6, 7, 7, 5,\n        5, 6, 7, 7, 7, 5, 7, 7, 7, 6, 6, 7, 6, 8, 6, 5, 5, 6, 6, 5, 6, 6,\n        8, 6, 8, 8, 7, 6, 7, 7, 6, 7, 5, 7, 6, 7, 6, 7, 8, 6, 7, 7, 7, 5,\n        7, 7, 7, 7, 8, 6, 7, 6, 7, 7, 7, 7],\n       [6, 6, 8, 6, 6, 8, 7, 7, 4, 8, 6, 7, 6, 7, 5, 8, 7, 6, 7, 6, 7, 6,\n        7, 6, 7, 6, 7, 6, 4, 6, 7, 7, 5, 6, 6, 6, 6, 7, 7, 8, 6, 6, 8, 6,\n        6, 7, 7, 7, 7, 6, 7, 7, 6, 6, 6, 5, 7, 8, 8, 3, 6, 7, 6, 6, 6, 6,\n        7, 5, 7, 7, 7, 7, 7, 7, 6, 6, 5, 7, 7, 7, 4, 7, 6, 3, 7, 7, 7, 6,\n        6, 6, 6, 8, 7, 7, 7, 7, 8, 7, 7, 7]])</pre></li><li>lp(chain, draw)float645.934e+03 5.934e+03 ... 5.933e+03<pre>array([[5933.51074635, 5933.83648201, 5935.16061384, 5934.7743146 ,\n        5934.78289381, 5937.18236739, 5933.84371683, 5933.30817267,\n        5934.64125328, 5935.22334464, 5935.03100783, 5935.04581742,\n        5937.04287881, 5936.7406066 , 5931.57413703, 5931.97550556,\n        5932.3132358 , 5934.45378106, 5932.32302563, 5932.74184011,\n        5932.40417755, 5932.23200138, 5935.96770582, 5935.28452587,\n        5939.25844204, 5935.7211319 , 5934.3387867 , 5938.81676818,\n        5937.15784604, 5939.42922631, 5936.65962912, 5936.68728516,\n        5936.079414  , 5933.61607106, 5933.51126681, 5935.34929273,\n        5934.92987268, 5935.3942487 , 5935.83313248, 5934.30317071,\n        5935.19906013, 5935.14313698, 5933.25954959, 5932.19189949,\n        5931.91930286, 5936.38926082, 5934.06098713, 5933.34354171,\n        5934.26511804, 5938.05618475, 5936.79420841, 5934.94545204,\n        5932.06208984, 5932.82363907, 5933.42240277, 5934.92118183,\n        5934.7287502 , 5935.22489915, 5932.73198765, 5934.25158298,\n        5934.22874701, 5933.98785781, 5933.33631359, 5933.06789678,\n        5932.13936385, 5931.44010348, 5931.7886643 , 5933.61598217,\n        5933.05397569, 5933.0169204 , 5934.13874303, 5933.47306919,\n        5933.86259301, 5934.8165827 , 5932.78867738, 5934.52731597,\n        5935.01132346, 5933.05555575, 5935.71270298, 5935.52816939,\n...\n        5934.6969838 , 5936.75325912, 5933.90500486, 5934.50280133,\n        5933.095684  , 5936.95233237, 5934.75029547, 5937.91456646,\n        5939.73088469, 5940.00913105, 5932.31447515, 5933.99268735,\n        5935.76490751, 5935.42589213, 5937.33363027, 5933.4162378 ,\n        5937.50926001, 5935.44029351, 5933.64122964, 5935.9663555 ,\n        5935.64438411, 5936.30755234, 5933.58593837, 5933.42242542,\n        5931.34491724, 5932.23996258, 5933.99504019, 5935.60311784,\n        5935.58051706, 5933.25738974, 5932.07559791, 5931.70701014,\n        5933.75717928, 5932.89056531, 5935.70191811, 5935.9952277 ,\n        5935.35417793, 5934.15776293, 5936.31752273, 5936.77963199,\n        5935.88943124, 5939.13474892, 5937.77201613, 5937.25312636,\n        5935.82660691, 5933.66128976, 5933.39108903, 5933.26378543,\n        5936.07427125, 5937.21805836, 5936.97426159, 5934.49741141,\n        5934.75177893, 5936.57000528, 5933.75497585, 5933.81483313,\n        5938.96207195, 5935.61925159, 5936.90315782, 5933.92529342,\n        5932.10213319, 5936.10792259, 5934.02844846, 5935.29598992,\n        5933.41338426, 5934.90889245, 5934.24092177, 5932.80441068,\n        5932.52300358, 5934.57130174, 5932.58253087, 5933.9134258 ,\n        5935.51330316, 5934.74858702, 5932.93843691, 5934.28495943,\n        5934.33696235, 5933.96314185, 5935.05015665, 5933.02797709]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n      dtype='int64', name='draw'))</pre></li></ul></li><li>Attributes: (4)created_at :2025-09-27T00:19:51.160945+00:00arviz_version :0.22.0modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 96kB\nDimensions:                  (__obs__: 3988, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 32kB 0 1 2 3 ... 3985 3986 3987\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 64kB ...\nAttributes:\n    created_at:                  2025-09-27T00:19:51.162170+00:00\n    arviz_version:               0.22.0\n    inference_library:           numpyro\n    inference_library_version:   0.19.0\n    sampling_time:               128.046042\n    tuning_steps:                100\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 3988</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 ... 3984 3985 3986 3987<pre>array([   0,    1,    2, ..., 3985, 3986, 3987], shape=(3988,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float641.21 1.0 1.63 1.0 ... -1.0 1.25 1.0<pre>array([[ 1.21 ,  1.   ],\n       [ 1.63 ,  1.   ],\n       [ 1.03 ,  1.   ],\n       ...,\n       [ 0.784,  1.   ],\n       [ 2.35 , -1.   ],\n       [ 1.25 ,  1.   ]], shape=(3988, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987],\n      dtype='int64', name='__obs__', length=3988))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T00:19:51.162170+00:00arviz_version :0.22.0inference_library :numpyroinference_library_version :0.19.0sampling_time :128.046042tuning_steps :100modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[3]: Copied! <pre>basic_hssm_model.vi(method=\"advi\",\n                    niter=5000)\n</pre> basic_hssm_model.vi(method=\"advi\",                     niter=5000) <pre>Using MCMC starting point defaults.\n</pre> <pre>Output()</pre> <pre></pre> <pre>Finished [100%]: Average Loss = 7,614.1\n</pre> Out[3]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 64kB\nDimensions:        (chain: 1, draw: 1000, v_C(stim)_dim: 2)\nCoordinates:\n  * chain          (chain) int64 8B 0\n  * draw           (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\n  * v_C(stim)_dim  (v_C(stim)_dim) &lt;U2 16B 'WL' 'WW'\nData variables:\n    v_C(stim)      (chain, draw, v_C(stim)_dim) float64 16kB 0.03516 ... -0.2433\n    z              (chain, draw) float64 8kB 0.5211 0.4881 ... 0.5238 0.5404\n    t              (chain, draw) float64 8kB 0.02918 0.07872 ... 0.01165\n    a              (chain, draw) float64 8kB 1.53 1.772 1.796 ... 1.631 1.963\n    v_Intercept    (chain, draw) float64 8kB 0.7442 0.1945 ... 0.09171 0.211\n    theta          (chain, draw) float64 8kB 0.4323 0.2049 ... 0.1619 0.6074\nAttributes:\n    created_at:                 2025-09-27T00:20:42.669194+00:00\n    arviz_version:              0.22.0\n    inference_library:          pymc\n    inference_library_version:  5.25.1</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 1000</li><li>v_C(stim)_dim: 2</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li><li>v_C(stim)_dim(v_C(stim)_dim)&lt;U2'WL' 'WW'<pre>array(['WL', 'WW'], dtype='&lt;U2')</pre></li></ul></li><li>Data variables: (6)<ul><li>v_C(stim)(chain, draw, v_C(stim)_dim)float640.03516 0.03483 ... -0.2611 -0.2433<pre>array([[[ 0.0351573 ,  0.03482643],\n        [-0.10498079, -0.49311349],\n        [ 0.31646085, -0.28599154],\n        ...,\n        [ 0.13455778, -0.67012498],\n        [ 0.70694155, -0.29200021],\n        [-0.26109388, -0.24333296]]], shape=(1, 1000, 2))</pre></li><li>z(chain, draw)float640.5211 0.4881 ... 0.5238 0.5404<pre>array([[0.52105564, 0.48806249, 0.48025291, 0.4376045 , 0.53150058,\n        0.47644584, 0.56332634, 0.57454157, 0.51203588, 0.40433304,\n        0.53422067, 0.41195431, 0.47712224, 0.57277498, 0.5611194 ,\n        0.50235664, 0.60392562, 0.40956596, 0.55834963, 0.26562285,\n        0.47006161, 0.55120267, 0.61327951, 0.41821391, 0.61157551,\n        0.62353163, 0.44659322, 0.51219663, 0.65651803, 0.36216349,\n        0.50182866, 0.40367008, 0.59426455, 0.57685829, 0.49957707,\n        0.43857432, 0.59108579, 0.58439588, 0.53765794, 0.58019386,\n        0.66561404, 0.60373316, 0.57963695, 0.53724679, 0.55776282,\n        0.53978727, 0.60454866, 0.55343615, 0.600553  , 0.58595937,\n        0.46608762, 0.41943577, 0.6541234 , 0.47311643, 0.42832003,\n        0.53410856, 0.50411261, 0.61269769, 0.6571442 , 0.49065784,\n        0.64649433, 0.4987515 , 0.52181646, 0.49626345, 0.42802465,\n        0.47898096, 0.34483236, 0.47163799, 0.3971054 , 0.41753257,\n        0.58526461, 0.5284113 , 0.55731872, 0.52042926, 0.44166166,\n        0.44493602, 0.43506246, 0.52200832, 0.48904443, 0.48904226,\n        0.48933848, 0.55337861, 0.58146328, 0.44217293, 0.4452618 ,\n        0.4837709 , 0.58872834, 0.63640777, 0.56236899, 0.59616088,\n        0.49703244, 0.39251645, 0.52342581, 0.45879067, 0.5038696 ,\n        0.46349426, 0.51286713, 0.45941203, 0.57237759, 0.56114678,\n...\n        0.47169298, 0.58228066, 0.59504853, 0.61236259, 0.47524806,\n        0.62965574, 0.60979914, 0.39775265, 0.61178095, 0.59204529,\n        0.54330583, 0.57706158, 0.55930824, 0.61156614, 0.52713907,\n        0.53992433, 0.50255205, 0.47232262, 0.49829988, 0.38392336,\n        0.47042187, 0.58155681, 0.48913856, 0.5873757 , 0.41704619,\n        0.47945459, 0.53185803, 0.37894828, 0.47219185, 0.52549278,\n        0.61188698, 0.4752253 , 0.61133842, 0.528273  , 0.55777149,\n        0.52988071, 0.39276694, 0.52369815, 0.5857492 , 0.4786373 ,\n        0.65521049, 0.53488919, 0.54480738, 0.42391472, 0.48212834,\n        0.59789514, 0.56546197, 0.38128159, 0.4375431 , 0.49543913,\n        0.41774699, 0.35267412, 0.51486305, 0.49546548, 0.48063302,\n        0.4908424 , 0.62795004, 0.42827523, 0.38199583, 0.63286763,\n        0.42308037, 0.44769608, 0.50294775, 0.53373281, 0.53724886,\n        0.54855235, 0.53347333, 0.46052964, 0.44373898, 0.4102363 ,\n        0.50542859, 0.60194811, 0.51785326, 0.44308584, 0.40846214,\n        0.54822157, 0.36618052, 0.46580449, 0.43138086, 0.46969585,\n        0.52178537, 0.39926154, 0.4030424 , 0.5611175 , 0.53008781,\n        0.52767196, 0.59167958, 0.51397421, 0.62716154, 0.37313515,\n        0.49532082, 0.44811372, 0.49840554, 0.51485959, 0.56195529,\n        0.63732891, 0.4768904 , 0.66027895, 0.52383351, 0.54040986]])</pre></li><li>t(chain, draw)float640.02918 0.07872 ... 0.01165<pre>array([[0.02917527, 0.07871673, 0.0398964 , 0.06408464, 0.14782773,\n        0.03373419, 0.01759448, 0.09744873, 0.08420841, 0.02582151,\n        0.07146255, 0.0440035 , 0.03424162, 0.09243095, 0.24625358,\n        0.01411368, 0.08111578, 0.02165341, 0.02304807, 0.0150082 ,\n        0.0449476 , 0.15107989, 0.02748441, 0.01741841, 0.03003218,\n        0.09169519, 0.06752698, 0.0863311 , 0.05028233, 0.09333588,\n        0.04369395, 0.03823191, 0.02055152, 0.00953968, 0.02393282,\n        0.01635671, 0.03100871, 0.06512581, 0.06111599, 0.01772153,\n        0.09322158, 0.01983541, 0.1740735 , 0.03765948, 0.04163559,\n        0.11017629, 0.02488095, 0.18049969, 0.05375409, 0.17244787,\n        0.04640787, 0.03811666, 0.0457581 , 0.13694706, 0.02958096,\n        0.00571469, 0.05769456, 0.09315819, 0.04859733, 0.0329939 ,\n        0.04235649, 0.04206018, 0.01663433, 0.02730189, 0.04820602,\n        0.13557054, 0.21305307, 0.06030422, 0.01902772, 0.03492103,\n        0.16442766, 0.02180558, 0.05201848, 0.02438717, 0.07714277,\n        0.17260448, 0.04943506, 0.07001153, 0.05939016, 0.09007699,\n        0.00740126, 0.02933474, 0.01247643, 0.01768783, 0.04529   ,\n        0.01943708, 0.01819848, 0.16844045, 0.04641913, 0.06301706,\n        0.16420108, 0.14763031, 0.07769594, 0.03762604, 0.06998849,\n        0.16432851, 0.0350502 , 0.05938598, 0.06280382, 0.08298662,\n...\n        0.09800727, 0.02736155, 0.05784504, 0.13992108, 0.07481232,\n        0.0748989 , 0.03662656, 0.05593962, 0.02700207, 0.01685986,\n        0.01962835, 0.03676522, 0.05060251, 0.34961566, 0.07433266,\n        0.03167247, 0.05371553, 0.01043032, 0.06541987, 0.03360284,\n        0.04636137, 0.09639636, 0.0497123 , 0.02741229, 0.04518636,\n        0.02015583, 0.02839364, 0.01705565, 0.03849809, 0.07216813,\n        0.03741778, 0.31730639, 0.01809332, 0.07234512, 0.04077453,\n        0.09240467, 0.14801124, 0.03881337, 0.02527784, 0.01284095,\n        0.01741057, 0.03097176, 0.0688549 , 0.063854  , 0.08995025,\n        0.0123808 , 0.04185652, 0.03416321, 0.01517579, 0.07234594,\n        0.06036173, 0.24348043, 0.02467086, 0.04137172, 0.04393298,\n        0.05103176, 0.19933732, 0.05455843, 0.03872963, 0.02707317,\n        0.09490275, 0.07627218, 0.0405565 , 0.04826343, 0.05081071,\n        0.01224762, 0.02864522, 0.0191991 , 0.02990314, 0.01190907,\n        0.03033318, 0.0640768 , 0.04975282, 0.07595735, 0.07849464,\n        0.06992625, 0.01976236, 0.0276571 , 0.03936563, 0.02071115,\n        0.10370988, 0.03724958, 0.04534109, 0.16155922, 0.02642192,\n        0.04286177, 0.06498153, 0.06159001, 0.02005566, 0.06132203,\n        0.08008861, 0.02429973, 0.02038979, 0.07238425, 0.0617651 ,\n        0.01984922, 0.0303466 , 0.09191849, 0.00980294, 0.01164529]])</pre></li><li>a(chain, draw)float641.53 1.772 1.796 ... 1.631 1.963<pre>array([[1.53033203, 1.77193098, 1.79566845, 2.07828231, 1.35338832,\n        1.73356017, 1.79603301, 2.16582235, 2.08469184, 1.58535607,\n        2.08082986, 1.43118592, 1.48320438, 1.92671991, 1.66080962,\n        1.88169474, 1.94517308, 1.60049418, 2.13676417, 1.66949854,\n        2.20236637, 1.82919585, 1.65704547, 1.53862851, 1.95626156,\n        1.21264447, 2.11082685, 2.04455251, 1.62913787, 1.79198868,\n        2.03101823, 2.06468236, 1.41070042, 1.85524888, 1.81935382,\n        2.31384233, 1.89261725, 1.75390633, 1.70027026, 1.72013496,\n        1.93581799, 1.42698864, 1.73623046, 1.8132452 , 1.5961057 ,\n        2.13077644, 1.99063371, 1.87166167, 1.55489259, 2.0625434 ,\n        2.12667978, 2.06696003, 1.92944845, 1.58445442, 2.18489385,\n        1.74549485, 1.30303512, 1.58796592, 1.7185404 , 1.62207626,\n        1.5674154 , 2.11564285, 1.91920361, 1.6615457 , 1.90701219,\n        2.05459842, 1.59641255, 1.55253825, 1.81847821, 2.28344522,\n        2.00148163, 2.30627528, 1.42324633, 1.53368165, 2.04703557,\n        1.88658107, 1.53206185, 1.9968407 , 1.78801349, 1.65635135,\n        2.10863051, 1.57989758, 1.78111278, 1.68617532, 1.76172269,\n        1.63822625, 1.70612789, 1.74279899, 1.85949795, 1.92878341,\n        1.89973006, 1.70272004, 1.99025194, 1.87030347, 1.93579039,\n        1.76947767, 1.94095758, 1.73332778, 1.42054033, 2.05453718,\n...\n        1.68642394, 1.95362274, 2.00454622, 2.07896312, 1.99102673,\n        1.51247293, 2.15569621, 2.0761767 , 1.76964063, 1.70496064,\n        1.82700615, 2.16852697, 1.91153542, 1.76767967, 2.13037675,\n        1.97426118, 2.0514776 , 2.21255522, 1.76868001, 2.02853367,\n        2.14964097, 1.83372309, 2.12534998, 2.1586351 , 1.61159793,\n        2.23239113, 2.18165712, 1.80300701, 1.99238514, 2.1018443 ,\n        1.89924958, 1.82232049, 1.53983039, 1.68632812, 1.69720858,\n        1.98453209, 2.01148495, 1.96455332, 2.00606393, 2.05058909,\n        1.74824451, 1.36886044, 1.88989394, 1.79094399, 1.38517563,\n        1.7165713 , 1.72317883, 1.70786586, 1.17174049, 1.72498805,\n        2.02690027, 1.96774392, 1.88343521, 1.57896522, 1.93539661,\n        1.76542798, 2.0348824 , 1.91935596, 1.92146722, 1.82394757,\n        2.2849887 , 1.4913699 , 1.46362119, 1.66870891, 1.74669301,\n        1.64400795, 1.96305445, 1.76643124, 1.84256105, 2.15664664,\n        1.74824445, 1.75056939, 1.63826579, 1.6914172 , 2.26100597,\n        1.74221948, 1.95731605, 1.73433153, 2.01060466, 1.63586307,\n        1.89419581, 1.60851105, 1.5322422 , 1.43493744, 1.67449765,\n        1.94265683, 1.86509785, 2.34336442, 2.05909912, 2.07493204,\n        2.23885396, 1.57759016, 2.371581  , 1.75442766, 1.59250793,\n        2.05396074, 1.62351595, 2.07308541, 1.63146988, 1.9625768 ]])</pre></li><li>v_Intercept(chain, draw)float640.7442 0.1945 ... 0.09171 0.211<pre>array([[ 7.44226429e-01,  1.94514477e-01, -3.83583482e-01,\n         6.91778912e-01, -2.61801804e-01, -7.84085677e-03,\n        -5.81610522e-01,  4.65501632e-01,  4.46441099e-01,\n        -9.62561189e-02,  4.18406274e-01,  3.80099435e-01,\n         2.02525704e-01,  3.10953111e-02,  4.73373446e-01,\n         3.36137656e-02,  5.73889368e-01,  7.52300137e-01,\n         3.13205247e-01,  1.42984982e-02, -3.67199174e-01,\n         1.37704127e-01,  3.36986642e-01,  1.84635998e-01,\n         2.16800689e-01, -1.08814316e-01,  7.53557030e-02,\n        -2.72146730e-01,  4.00191682e-01,  4.99283899e-01,\n         2.74702314e-01,  2.98186395e-01,  1.75561315e-01,\n        -1.11014526e-02,  4.43158405e-03,  4.43996762e-01,\n        -6.00753216e-02,  5.14121838e-01,  9.92490327e-01,\n         6.26081513e-01, -2.16946477e-01,  7.27252917e-01,\n         9.49037098e-01, -3.36027242e-01,  1.76880172e-01,\n         1.66254021e-01,  1.58869803e-01,  4.27772221e-01,\n         3.20816703e-01,  2.71073797e-01,  6.49370170e-01,\n         3.00608720e-01,  6.11388759e-01,  2.40801914e-01,\n         4.34933883e-01,  1.98649541e-02, -7.08295880e-02,\n         1.25026879e+00,  2.93676362e-01,  1.28853415e-02,\n...\n         5.68529820e-01, -5.08787937e-03,  5.68429134e-01,\n        -9.67304291e-02, -4.12022839e-01,  7.64767348e-01,\n         1.23580116e-02, -1.21774134e-03,  7.03534809e-02,\n         9.02372433e-02,  3.31985812e-01,  7.77978021e-01,\n        -4.16058217e-01, -1.11785847e-01,  8.25858562e-01,\n        -1.14602550e-01,  6.43825686e-02,  3.60385840e-01,\n        -9.66900089e-02,  4.42090693e-01,  4.97481364e-01,\n         1.84495550e-01,  5.44442313e-01,  1.00542987e-01,\n         4.55420198e-01,  4.03310875e-01,  8.10477497e-01,\n         6.13439547e-01,  4.67470264e-01,  1.11688320e-01,\n         3.33341616e-01,  7.02650835e-01,  3.77843073e-01,\n         1.60390993e-01,  8.54265008e-02,  4.62321582e-03,\n         2.77623621e-01,  4.22050708e-01,  3.64320837e-01,\n         4.42259644e-01,  2.56653075e-01,  2.79669073e-01,\n         1.71665069e-01,  5.52192784e-01,  1.24851968e-01,\n         4.56525512e-01,  4.79509307e-01,  2.33167725e-01,\n         3.26494297e-01, -7.53854973e-03,  7.81687582e-02,\n        -1.70708936e-01,  6.49823035e-01,  2.62737046e-01,\n        -2.68233316e-02,  2.98535702e-01,  9.17099759e-02,\n         2.11004763e-01]])</pre></li><li>theta(chain, draw)float640.4323 0.2049 ... 0.1619 0.6074<pre>array([[0.43227222, 0.20487902, 0.40827252, 0.42873871, 0.39435178,\n        0.38464971, 0.32657534, 0.44979895, 0.36477571, 0.35336487,\n        0.34536276, 0.26615391, 0.47356444, 0.30126095, 0.42608886,\n        0.30213365, 0.23589918, 0.49637516, 0.36276256, 0.37874334,\n        0.46881557, 0.79566147, 0.32362046, 0.56270353, 0.46343687,\n        0.31972472, 0.36961018, 0.36454278, 0.60813456, 0.46050181,\n        0.47081239, 0.40078399, 0.45096785, 0.47930629, 0.4410819 ,\n        0.26690121, 0.36380314, 0.25366065, 0.69206587, 0.30961248,\n        0.20399502, 0.35289861, 0.44216077, 0.45911166, 0.69142418,\n        0.28130537, 0.45485106, 0.41353608, 0.26567835, 0.31526569,\n        0.58172345, 0.52006795, 0.22014676, 0.34245124, 0.64584157,\n        0.4752088 , 0.44410016, 0.32225134, 0.54624837, 0.33772741,\n        0.29945343, 0.46165904, 0.37231247, 0.51920568, 0.39971834,\n        0.4515597 , 0.33374355, 0.25513596, 0.5947926 , 0.39102942,\n        0.45380391, 0.36739245, 0.45747661, 0.37482721, 0.60102719,\n        0.46765718, 0.39708128, 0.35151445, 0.53737103, 0.57370304,\n        0.25849827, 0.37608316, 0.3713673 , 0.62927086, 0.32928161,\n        0.4075674 , 0.39970159, 0.39554659, 0.58991907, 0.41994401,\n        0.28650494, 0.32270388, 0.28185071, 0.40915706, 0.22694813,\n        0.62496397, 0.21736565, 0.31661029, 0.34796651, 0.4320792 ,\n...\n        0.48890708, 0.45858137, 0.46534661, 0.20111512, 0.40246976,\n        0.41192875, 0.32925859, 0.53257824, 0.35691396, 0.317809  ,\n        0.17151132, 0.40220298, 0.56255113, 0.56266774, 0.45742498,\n        0.49901469, 0.3847519 , 0.27857488, 0.40551526, 0.56877037,\n        0.40177279, 0.32929082, 0.27318279, 0.28192529, 0.48658388,\n        0.31648067, 0.3720933 , 0.56165496, 0.28922127, 0.42659043,\n        0.46390518, 0.43205542, 0.3048933 , 0.45227013, 0.63997984,\n        0.54617053, 0.39850673, 0.2013786 , 0.45181574, 0.39292936,\n        0.3333209 , 0.36297979, 0.40784211, 0.53967361, 0.56473003,\n        0.44946901, 0.36156644, 0.59746536, 0.25300604, 0.20544422,\n        0.363134  , 0.34113689, 0.54286174, 0.43900991, 0.59147518,\n        0.49462186, 0.2228743 , 0.43081822, 0.48770595, 0.30068582,\n        0.42172687, 0.38645919, 0.62152289, 0.29957271, 0.5206848 ,\n        0.20087856, 0.24460995, 0.40000986, 0.52536924, 0.56753528,\n        0.32956277, 0.3973506 , 0.27281499, 0.42925351, 0.36381884,\n        0.44708389, 0.36875002, 0.44259756, 0.58167862, 0.39940099,\n        0.49203812, 0.43226315, 0.56134771, 0.33660122, 0.39004158,\n        0.48626566, 0.47642925, 0.27371733, 0.2587128 , 0.50822816,\n        0.26747579, 0.53477826, 0.54030876, 0.41646148, 0.52168027,\n        0.45500614, 0.3780295 , 0.60771559, 0.16193765, 0.6073592 ]])</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li><li>v_C(stim)_dimPandasIndex<pre>PandasIndex(Index(['WL', 'WW'], dtype='object', name='v_C(stim)_dim'))</pre></li></ul></li><li>Attributes: (4)created_at :2025-09-27T00:20:42.669194+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 96kB\nDimensions:                  (__obs__: 3988, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 32kB 0 1 2 3 ... 3985 3986 3987\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 64kB ...\nAttributes:\n    created_at:                 2025-09-27T00:20:42.675659+00:00\n    arviz_version:              0.22.0\n    inference_library:          pymc\n    inference_library_version:  5.25.1</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 3988</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 ... 3984 3985 3986 3987<pre>array([   0,    1,    2, ..., 3985, 3986, 3987], shape=(3988,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float641.21 1.0 1.63 1.0 ... -1.0 1.25 1.0<pre>array([[ 1.21 ,  1.   ],\n       [ 1.63 ,  1.   ],\n       [ 1.03 ,  1.   ],\n       ...,\n       [ 0.784,  1.   ],\n       [ 2.35 , -1.   ],\n       [ 1.25 ,  1.   ]], shape=(3988, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987],\n      dtype='int64', name='__obs__', length=3988))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (4)created_at :2025-09-27T00:20:42.675659+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1</li></ul> </ul> </li> </ul> In\u00a0[4]: Copied! <pre>basic_hssm_model.save_model(model_name=\"test_model\")\n</pre> basic_hssm_model.save_model(model_name=\"test_model\") <p>We are using the defaults here, which save the model and its inference results to the <code>hssm_models/test_model/</code> directory inside your curerent working directory.</p> <p>Up to three files are saved in the model directory:</p> <ul> <li><code>model.pkl</code>: The model instance.</li> <li><code>traces.nc</code>: The MCMC traces.</li> <li><code>vi_traces.nc</code>: The VI traces.</li> </ul> <p>We can now load the model from the directory we just created, using the <code>HSSM</code> classmethod <code>load_model</code>.</p> In\u00a0[5]: Copied! <pre>loaded_model = hssm.HSSM.load_model(path=\"hssm_models/test_model\")\n</pre> loaded_model = hssm.HSSM.load_model(path=\"hssm_models/test_model\") <pre>Model initialized successfully.\n</pre> <p>With this simple workflow your models are portable across sessions and machines.</p>"},{"location":"tutorials/save_load_tutorial/#saving-and-loading-models","title":"Saving and loading models\u00b6","text":"<p>In this short how-to, tutorial, we show how to save a HSSM model instance and its inference results to disk and then re-instantiate the model from the saved files.</p>"},{"location":"tutorials/save_load_tutorial/#load-data-and-instantiate-hssm-model","title":"Load data and instantiate HSSM model\u00b6","text":""},{"location":"tutorials/save_load_tutorial/#generate-inference-results","title":"Generate inference results\u00b6","text":""},{"location":"tutorials/save_load_tutorial/#mcmc","title":"MCMC\u00b6","text":""},{"location":"tutorials/save_load_tutorial/#vi","title":"VI\u00b6","text":""},{"location":"tutorials/save_load_tutorial/#saving-and-loading-the-model","title":"Saving and Loading the model\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/","title":"Scientific Workflow with HSSM","text":"<p>Welcome to the scientific workflow tutorial. This tutorial starts with a basic experimental dataset and we will inch from a very simple HSSM model iteratively toward a model that captures many of the main patterns we can identify in our dataset.</p> <p>Along the way we try to achieve the following balance:</p> <ol> <li>Illustrate how HSSM can be used for real scientific workflows. HSSM helps us with model building, running the stats, and reporting results.</li> <li>Allow this tutorial to be used as a first look into HSSM, shirking conceptually advanced features that are discused in the many dedicated tutorials you can find on the documentation</li> </ol> In\u00a0[1]: Copied! <pre># If running this on Colab, please uncomment the next line\n# !pip install hssm\n</pre> # If running this on Colab, please uncomment the next line # !pip install hssm In\u00a0[2]: Copied! <pre># # Data Files\n# !wget -P  data/carney_workshop_2025_data/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/data/carney_workshop_2025_full.parquet\n# !wget -P  data/carney_workshop_2025_data/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/data/carney_workshop_2025_modeling.parquet\n# !wget -P  data/carney_workshop_2025_data/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/data/carney_workshop_2025_parameters.pkl\n\n# # Presampled traces\n# !wget -P  idata/basic_ddm/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/idata/basic_ddm/traces.nc\n# !wget -P  idata/ddm_hier/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/idata/ddm_hier/traces.nc\n# !wget -P  idata/angle_hier/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/idata/angle_hier/traces.nc\n# !wget -P  idata/angle_hier_v2/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/idata/angle_hier_v2/traces.nc\n# !wget -P  idata/angle_hier_v3/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/idata/angle_hier_v3/traces.nc\n# !wget -P  idata/angle_hier_v4/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/idata/angle_hier_v4/traces.nc\n# !wget -P  idata/angle_v5/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/idata/angle_v5/traces.nc\n</pre> # # Data Files # !wget -P  data/carney_workshop_2025_data/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/data/carney_workshop_2025_full.parquet # !wget -P  data/carney_workshop_2025_data/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/data/carney_workshop_2025_modeling.parquet # !wget -P  data/carney_workshop_2025_data/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/data/carney_workshop_2025_parameters.pkl  # # Presampled traces # !wget -P  idata/basic_ddm/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/idata/basic_ddm/traces.nc # !wget -P  idata/ddm_hier/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/idata/ddm_hier/traces.nc # !wget -P  idata/angle_hier/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/idata/angle_hier/traces.nc # !wget -P  idata/angle_hier_v2/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/idata/angle_hier_v2/traces.nc # !wget -P  idata/angle_hier_v3/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/idata/angle_hier_v3/traces.nc # !wget -P  idata/angle_hier_v4/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/idata/angle_hier_v4/traces.nc # !wget -P  idata/angle_v5/ https://raw.githubusercontent.com/lnccbrown/HSSM/main/scientific_workflow_hssm/idata/angle_v5/traces.nc In\u00a0[3]: Copied! <pre>import hssm\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport arviz as az\nfrom matplotlib import pyplot as plt\n</pre> import hssm import pandas as pd import pickle import numpy as np import arviz as az from matplotlib import pyplot as plt In\u00a0[4]: Copied! <pre>def load_data(filename_base: str,\n              folder: str = \"data\") -&gt; tuple[pd.DataFrame, pd.DataFrame, dict]:\n    \"\"\"Load saved simulation data and parameters from files.\n\n    Parameters\n    ----------\n    filename_base : str\n        Base filename used when saving files\n    folder : str, optional\n        Folder containing saved files, by default \"data\"\n\n    Returns\n    -------\n    tuple[pd.DataFrame, pd.DataFrame, dict]\n        Contains:\n        - DataFrame with modeling data\n        - DataFrame with full data  \n        - Dict containing group and subject parameters\n    \"\"\"\n    df_modeling = pd.read_parquet(f\"{folder}/{filename_base}_modeling.parquet\")\n    return df_modeling\n</pre> def load_data(filename_base: str,               folder: str = \"data\") -&gt; tuple[pd.DataFrame, pd.DataFrame, dict]:     \"\"\"Load saved simulation data and parameters from files.      Parameters     ----------     filename_base : str         Base filename used when saving files     folder : str, optional         Folder containing saved files, by default \"data\"      Returns     -------     tuple[pd.DataFrame, pd.DataFrame, dict]         Contains:         - DataFrame with modeling data         - DataFrame with full data           - Dict containing group and subject parameters     \"\"\"     df_modeling = pd.read_parquet(f\"{folder}/{filename_base}_modeling.parquet\")     return df_modeling In\u00a0[5]: Copied! <pre>workshop_data  = load_data(filename_base = \"carney_workshop_2025\",\n                           folder = \"scientific_workflow_hssm/data/\")\n</pre> workshop_data  = load_data(filename_base = \"carney_workshop_2025\",                            folder = \"scientific_workflow_hssm/data/\") In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\ndef plot_rt_by_choice(df: pd.DataFrame,\n                      categorical_column: str | None = None,\n                      colors: dict[str, str] | dict[int, str] | None = None,\n                      ax: plt.Axes | None = None):\n    if categorical_column is None:\n        ax.hist(df['rt'] * df['response'],\n                bins = np.linspace(-5,5, 50), \n                # label=f'Condition {cond}', \n                histtype='step',\n                density = True,\n                color='tab:blue')\n    else:\n        for cond in df[categorical_column].unique():\n            df_cond = df[df[categorical_column] == cond]\n            ax.hist(df_cond['rt'] * df_cond['response'],\n                    bins = np.linspace(-5,5, 50), \n                    label=f'Condition {cond}', \n                    histtype='step',\n                    density = True,\n                    color=colors[cond])\n        ax.set_xlabel('RT * Choice')\n        ax.set_ylabel('Density')\n    return ax\n        \ndef inset_bar_plot(df: pd.DataFrame, \n                   categorical_column: str,\n                   response_options: list[int],\n                   colors: dict[str, str] | dict[int, str] | None = None,\n                   ax: plt.Axes | None = None):\n    \n    axins = inset_axes(ax, \n                       width=\"35%\",\n                       height=\"35%\",\n                       loc='upper left',\n                       borderpad=2.75)\n    bar_width = 0.55\n    for j, resp in enumerate(response_options):\n        for k, cond in enumerate(df[categorical_column].unique()):\n            k_displace = -1 if k == 0 else 1\n            df_cond = df[df[categorical_column] == cond]\n            prop = (df_cond[df_cond.response == resp].shape[0] / len(df_cond))\n            axins.bar((resp + ((bar_width / 2) * k_displace)), \n                        prop,\n                        width=bar_width,\n                        fill = False,\n                        edgecolor=colors[cond],\n                        label=f'Response {resp}')\n    axins.set_xticks(response_options)\n    axins.set_ylim(0, 1)\n    axins.set_yticks([0.0, 0.5, 1])\n    axins.set_title('choice proportion / option', fontsize=8)\n    axins.tick_params(axis='both', which='major', labelsize=7)\n    axins.set_xlabel('')\n    axins.set_ylabel('')\n    return ax\n\ndef inset_bar_plot_vertical(df: pd.DataFrame,\n                            categorical_column: str,\n                            response_options: list[int],\n                            colors: dict[str, str] | dict[int, str] | None = None,\n                            ax: plt.Axes | None = None):\n    \n    axins = inset_axes(ax,\n                       width=\"35%\",\n                       height=\"35%\",\n                       loc='upper left',\n                       borderpad=2.25)\n    bar_width = 0.55\n    for j, resp in enumerate(response_options):\n        # k_displace_dict = {0:}\n        for k, cond in enumerate(df[categorical_column].unique()):\n            k_displace = -1 if k == 0 else 1\n            df_cond = df[df[categorical_column] == cond]\n            rt_mean = (df_cond[df_cond.response == resp]).rt.mean()\n            axins.barh((resp + ((bar_width / 2) * k_displace)), \n                       rt_mean,\n                       height=bar_width,\n                       fill = False,\n                       edgecolor=colors[cond],\n                       label=f'Response {resp}')\n\n    axins.set_yticks(response_options)\n    axins.set_xticks([0.0, 1., 2.])\n    axins.set_title('rt-mean by choice option', fontsize=8)\n    axins.tick_params(axis='both', which='major', labelsize=7)\n    axins.set_xlabel('')\n    axins.set_ylabel('')\n    return ax\n\ndef plot_rt_hists(df: pd.DataFrame,\n                  by_participant: bool = True,\n                  split_by_column: str | None = None,\n                  inset_plot: str | None = \"choice proportion\",\n                  cols: int = 5):\n    if split_by_column is not None:\n        colors = {cond: color for cond, color in zip(df[split_by_column].unique(), \n                            ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple'])}\n    else:\n        colors = None\n\n    if by_participant:\n        # Get unique participant IDs and costly_fail_conditions\n        participants = df['participant_id'].unique()\n\n        # Set up subplot grid (adjust cols as needed)\n        rows = (len(participants) + cols - 1) // cols\n        fig, axes = plt.subplots(rows, cols, \n                                 figsize=(cols*4, rows*3), \n                                 sharey=True, sharex=True)\n        axes = axes.flatten()\n        for i, pid in enumerate(participants):\n            ax = axes[i]\n            df_part = df[df['participant_id'] == pid]\n            ax = plot_rt_by_choice(df_part,\n                                   split_by_column,\n                                   colors,\n                                   ax)\n            \n            # Take care of inset plots\n            if inset_plot == \"choice_proportion\":\n                ax = inset_bar_plot(df_part, \n                                    split_by_column,\n                                    df['response'].unique(),\n                                    colors,\n                                    ax)\n            elif inset_plot == \"rt_mean\":\n                ax = inset_bar_plot_vertical(df_part, \n                                             split_by_column,\n                                             df['response'].unique(),\n                                             colors,\n                                             ax)\n            if i == 0:\n                ax.legend(title=split_by_column, loc='best', fontsize='small')\n\n        # Hide unused axes\n        for j in range(i+1, len(axes)):\n            axes[j].set_visible(False)\n\n        plt.tight_layout()\n        plt.suptitle('RT, Split by Costly Fail Condition and Participant', y=1.02)\n        plt.show()\n    else:\n        fig, ax = plt.subplots(1, 1, figsize=(4, 3))\n        ax = plot_rt_by_choice(df,\n                               split_by_column,\n                               colors,\n                               ax)\n\n        # Take care of inset plots\n        if inset_plot == \"choice_proportion\":\n            ax = inset_bar_plot(df,\n                                split_by_column,\n                                df['response'].unique(),\n                                colors,\n                                ax)\n        elif inset_plot == \"rt_mean\":\n            ax = inset_bar_plot_vertical(df,\n                                         split_by_column,\n                                         df['response'].unique(),\n                                         colors,\n                                         ax)\n        \n        ax.legend(title=split_by_column, loc='best', fontsize='small')\n        plt.tight_layout()\n        plt.suptitle('RT by Trial, Split by Costly Fail Condition', y=1.02)\n        plt.show()\n</pre> import matplotlib.pyplot as plt from mpl_toolkits.axes_grid1.inset_locator import inset_axes  def plot_rt_by_choice(df: pd.DataFrame,                       categorical_column: str | None = None,                       colors: dict[str, str] | dict[int, str] | None = None,                       ax: plt.Axes | None = None):     if categorical_column is None:         ax.hist(df['rt'] * df['response'],                 bins = np.linspace(-5,5, 50),                  # label=f'Condition {cond}',                  histtype='step',                 density = True,                 color='tab:blue')     else:         for cond in df[categorical_column].unique():             df_cond = df[df[categorical_column] == cond]             ax.hist(df_cond['rt'] * df_cond['response'],                     bins = np.linspace(-5,5, 50),                      label=f'Condition {cond}',                      histtype='step',                     density = True,                     color=colors[cond])         ax.set_xlabel('RT * Choice')         ax.set_ylabel('Density')     return ax          def inset_bar_plot(df: pd.DataFrame,                     categorical_column: str,                    response_options: list[int],                    colors: dict[str, str] | dict[int, str] | None = None,                    ax: plt.Axes | None = None):          axins = inset_axes(ax,                         width=\"35%\",                        height=\"35%\",                        loc='upper left',                        borderpad=2.75)     bar_width = 0.55     for j, resp in enumerate(response_options):         for k, cond in enumerate(df[categorical_column].unique()):             k_displace = -1 if k == 0 else 1             df_cond = df[df[categorical_column] == cond]             prop = (df_cond[df_cond.response == resp].shape[0] / len(df_cond))             axins.bar((resp + ((bar_width / 2) * k_displace)),                          prop,                         width=bar_width,                         fill = False,                         edgecolor=colors[cond],                         label=f'Response {resp}')     axins.set_xticks(response_options)     axins.set_ylim(0, 1)     axins.set_yticks([0.0, 0.5, 1])     axins.set_title('choice proportion / option', fontsize=8)     axins.tick_params(axis='both', which='major', labelsize=7)     axins.set_xlabel('')     axins.set_ylabel('')     return ax  def inset_bar_plot_vertical(df: pd.DataFrame,                             categorical_column: str,                             response_options: list[int],                             colors: dict[str, str] | dict[int, str] | None = None,                             ax: plt.Axes | None = None):          axins = inset_axes(ax,                        width=\"35%\",                        height=\"35%\",                        loc='upper left',                        borderpad=2.25)     bar_width = 0.55     for j, resp in enumerate(response_options):         # k_displace_dict = {0:}         for k, cond in enumerate(df[categorical_column].unique()):             k_displace = -1 if k == 0 else 1             df_cond = df[df[categorical_column] == cond]             rt_mean = (df_cond[df_cond.response == resp]).rt.mean()             axins.barh((resp + ((bar_width / 2) * k_displace)),                         rt_mean,                        height=bar_width,                        fill = False,                        edgecolor=colors[cond],                        label=f'Response {resp}')      axins.set_yticks(response_options)     axins.set_xticks([0.0, 1., 2.])     axins.set_title('rt-mean by choice option', fontsize=8)     axins.tick_params(axis='both', which='major', labelsize=7)     axins.set_xlabel('')     axins.set_ylabel('')     return ax  def plot_rt_hists(df: pd.DataFrame,                   by_participant: bool = True,                   split_by_column: str | None = None,                   inset_plot: str | None = \"choice proportion\",                   cols: int = 5):     if split_by_column is not None:         colors = {cond: color for cond, color in zip(df[split_by_column].unique(),                              ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple'])}     else:         colors = None      if by_participant:         # Get unique participant IDs and costly_fail_conditions         participants = df['participant_id'].unique()          # Set up subplot grid (adjust cols as needed)         rows = (len(participants) + cols - 1) // cols         fig, axes = plt.subplots(rows, cols,                                   figsize=(cols*4, rows*3),                                   sharey=True, sharex=True)         axes = axes.flatten()         for i, pid in enumerate(participants):             ax = axes[i]             df_part = df[df['participant_id'] == pid]             ax = plot_rt_by_choice(df_part,                                    split_by_column,                                    colors,                                    ax)                          # Take care of inset plots             if inset_plot == \"choice_proportion\":                 ax = inset_bar_plot(df_part,                                      split_by_column,                                     df['response'].unique(),                                     colors,                                     ax)             elif inset_plot == \"rt_mean\":                 ax = inset_bar_plot_vertical(df_part,                                               split_by_column,                                              df['response'].unique(),                                              colors,                                              ax)             if i == 0:                 ax.legend(title=split_by_column, loc='best', fontsize='small')          # Hide unused axes         for j in range(i+1, len(axes)):             axes[j].set_visible(False)          plt.tight_layout()         plt.suptitle('RT, Split by Costly Fail Condition and Participant', y=1.02)         plt.show()     else:         fig, ax = plt.subplots(1, 1, figsize=(4, 3))         ax = plot_rt_by_choice(df,                                split_by_column,                                colors,                                ax)          # Take care of inset plots         if inset_plot == \"choice_proportion\":             ax = inset_bar_plot(df,                                 split_by_column,                                 df['response'].unique(),                                 colors,                                 ax)         elif inset_plot == \"rt_mean\":             ax = inset_bar_plot_vertical(df,                                          split_by_column,                                          df['response'].unique(),                                          colors,                                          ax)                  ax.legend(title=split_by_column, loc='best', fontsize='small')         plt.tight_layout()         plt.suptitle('RT by Trial, Split by Costly Fail Condition', y=1.02)         plt.show() In\u00a0[7]: Copied! <pre>workshop_data\n</pre> workshop_data Out[7]: response rt participant_id trial costly_fail_condition continuous_difficulty response_l1 0 1 0.556439 0 1 1 -0.277337 0 1 1 0.741682 0 2 0 -0.810919 1 2 1 0.461832 0 3 0 -0.673330 1 3 1 0.626154 0 4 0 0.755445 1 4 1 0.651677 0 5 1 0.136755 1 ... ... ... ... ... ... ... ... 4995 1 1.039342 19 246 0 -0.612223 -1 4996 1 1.587827 19 247 0 0.732396 1 4997 1 0.668594 19 248 1 -0.175321 1 4998 1 1.616471 19 249 0 -0.630447 1 4999 1 1.051329 19 250 1 0.511197 1 <p>5000 rows \u00d7 7 columns</p> In\u00a0[8]: Copied! <pre># Binary version of difficulty\nworkshop_data['bin_difficulty'] = workshop_data['continuous_difficulty'].apply(lambda x: 'high' if x &gt; 0 else 'low')\n\n# I want a a ordinal variable that is composed of 5 quantile levels of difficulty\nworkshop_data['quantile_difficulty'] = pd.qcut(workshop_data['continuous_difficulty'],\n                                                             3, labels = ['-1', '0', '1'])\n\nworkshop_data['quantile_difficulty_binary'] = pd.qcut(workshop_data['continuous_difficulty'],\n                                                             2, labels = ['-1', '1'])\n\n# Slightly\nworkshop_data['response_l1_plotting'] = workshop_data['response_l1'].apply(lambda x: str(-1) if x == -1 else str(1))\n</pre> # Binary version of difficulty workshop_data['bin_difficulty'] = workshop_data['continuous_difficulty'].apply(lambda x: 'high' if x &gt; 0 else 'low')  # I want a a ordinal variable that is composed of 5 quantile levels of difficulty workshop_data['quantile_difficulty'] = pd.qcut(workshop_data['continuous_difficulty'],                                                              3, labels = ['-1', '0', '1'])  workshop_data['quantile_difficulty_binary'] = pd.qcut(workshop_data['continuous_difficulty'],                                                              2, labels = ['-1', '1'])  # Slightly workshop_data['response_l1_plotting'] = workshop_data['response_l1'].apply(lambda x: str(-1) if x == -1 else str(1))  In\u00a0[9]: Copied! <pre>plot_rt_hists(workshop_data, \n              by_participant = False, \n              split_by_column = None,\n              inset_plot = None)\n</pre> plot_rt_hists(workshop_data,                by_participant = False,                split_by_column = None,               inset_plot = None) <pre>/var/folders/gx/s43vynx550qbypcxm83fv56dzq4hgg/T/ipykernel_64548/900136031.py:166: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  ax.legend(title=split_by_column, loc='best', fontsize='small')\n</pre> <p>So far so good. Looking at the global reaction time pattern, it does seem commensurate with what we might expect out of basic Sequential Sampling Model (SSM). The basic DDM might be a good start here.</p> <p>The picture above illustrates the basic Drift Diffusion Model. Note the parameters,</p> <ol> <li><code>v</code> the drift rate (how much evidence do I collect on average per unit of time)</li> <li><code>a</code> the boundary separation (how much evidence do I need to commit to a choice)</li> <li><code>z</code> how biased am I toward a particular choice a priori</li> <li><code>ndt</code> (we will simply call it <code>t</code> below), the delay between being exposed to a stimulus and starting the actual evidence accumulation process</li> </ol> In\u00a0[10]: Copied! <pre>BasicDDMModel = hssm.HSSM(data = workshop_data,\n                          model = \"ddm\",\n                          loglik_kind = \"approx_differentiable\",\n                          global_formula = \"y ~ 1\",\n                          noncentered = False,\n                          )\n</pre> BasicDDMModel = hssm.HSSM(data = workshop_data,                           model = \"ddm\",                           loglik_kind = \"approx_differentiable\",                           global_formula = \"y ~ 1\",                           noncentered = False,                           ) <pre>Model initialized successfully.\n</pre> In\u00a0[11]: Copied! <pre>BasicDDMModel\n</pre> BasicDDMModel Out[11]: <pre>Hierarchical Sequential Sampling Model\nModel: ddm\n\nResponse variable: rt,response\nLikelihood: approx_differentiable\nObservations: 5000\n\nParameters:\n\nv:\n    Formula: v ~ 1\n    Priors:\n        v_Intercept ~ Normal(mu: 0.0, sigma: 0.25)\n    Link: identity\n    Explicit bounds: (-3.0, 3.0)\n\na:\n    Formula: a ~ 1\n    Priors:\n        a_Intercept ~ Normal(mu: 1.4, sigma: 0.25)\n    Link: identity\n    Explicit bounds: (0.3, 2.5)\n\nz:\n    Formula: z ~ 1\n    Priors:\n        z_Intercept ~ Normal(mu: 0.5, sigma: 0.25)\n    Link: identity\n    Explicit bounds: (0.0, 1.0)\n\nt:\n    Formula: t ~ 1\n    Priors:\n        t_Intercept ~ Normal(mu: 1.0, sigma: 0.25)\n    Link: identity\n    Explicit bounds: (0.0, 2.0)\n\n\nLapse probability: 0.05\nLapse distribution: Uniform(lower: 0.0, upper: 20.0)</pre> In\u00a0[12]: Copied! <pre>BasicDDMModel.graph()\n</pre> BasicDDMModel.graph() Out[12]: In\u00a0[13]: Copied! <pre>try:\n    # Load pre-computed traces\n    BasicDDMModel.restore_traces(traces = \"scientific_workflow_hssm/idata/basic_ddm/traces.nc\")\nexcept:\n    # Sample posterior\n    basic_ddm_idata = BasicDDMModel.sample(chains = 2,\n                                            sampler = \"nuts_numpyro\",\n                                            tune = 500,\n                                            draws = 500,\n                                        )\n\n    # Sample posterior predictive\n    BasicDDMModel.sample_posterior_predictive(draws = 200,\n                                              safe_mode = True)\n\n    # Save Model\n    BasicDDMModel.save_model(model_name = \"basic_ddm\",\n                             allow_absolute_base_path = True,\n                             base_path = \"scientific_workflow_hssm/idata/\",\n                             save_idata_only = True)\n</pre> try:     # Load pre-computed traces     BasicDDMModel.restore_traces(traces = \"scientific_workflow_hssm/idata/basic_ddm/traces.nc\") except:     # Sample posterior     basic_ddm_idata = BasicDDMModel.sample(chains = 2,                                             sampler = \"nuts_numpyro\",                                             tune = 500,                                             draws = 500,                                         )      # Sample posterior predictive     BasicDDMModel.sample_posterior_predictive(draws = 200,                                               safe_mode = True)      # Save Model     BasicDDMModel.save_model(model_name = \"basic_ddm\",                              allow_absolute_base_path = True,                              base_path = \"scientific_workflow_hssm/idata/\",                              save_idata_only = True) In\u00a0[14]: Copied! <pre>BasicDDMModel.traces\n</pre> BasicDDMModel.traces Out[14]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 36kB\nDimensions:      (chain: 2, draw: 500)\nCoordinates:\n  * chain        (chain) int64 16B 0 1\n  * draw         (draw) int64 4kB 0 1 2 3 4 5 6 ... 493 494 495 496 497 498 499\nData variables:\n    t_Intercept  (chain, draw) float64 8kB ...\n    z_Intercept  (chain, draw) float64 8kB ...\n    a_Intercept  (chain, draw) float64 8kB ...\n    v_Intercept  (chain, draw) float64 8kB ...\nAttributes:\n    created_at:                  2025-07-11T17:29:22.668745+00:00\n    arviz_version:               0.21.0\n    inference_library:           numpyro\n    inference_library_version:   0.17.0\n    sampling_time:               95.06823\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (4)<ul><li>t_Intercept(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>z_Intercept(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>a_Intercept(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>v_Intercept(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-07-11T17:29:22.668745+00:00arviz_version :0.21.0inference_library :numpyroinference_library_version :0.17.0sampling_time :95.06823tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> posterior_predictive <ul> <pre>&lt;xarray.Dataset&gt; Size: 32MB\nDimensions:          (chain: 2, draw: 200, __obs__: 5000, rt,response_dim: 2)\nCoordinates:\n  * chain            (chain) int64 16B 0 1\n  * draw             (draw) int64 2kB 0 1 2 3 4 5 6 ... 194 195 196 197 198 199\n  * __obs__          (__obs__) int64 40kB 0 1 2 3 4 ... 4995 4996 4997 4998 4999\n  * rt,response_dim  (rt,response_dim) int64 16B 0 1\nData variables:\n    rt,response      (chain, draw, __obs__, rt,response_dim) float64 32MB ...\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 200</li><li>__obs__: 5000</li><li>rt,response_dim: 2</li></ul></li><li>Coordinates: (4)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 195 196 197 198 199<pre>array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n       154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n       168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n       196, 197, 198, 199])</pre></li><li>__obs__(__obs__)int640 1 2 3 4 ... 4996 4997 4998 4999<pre>array([   0,    1,    2, ..., 4997, 4998, 4999], shape=(5000,))</pre></li><li>rt,response_dim(rt,response_dim)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__, rt,response_dim)float64...<pre>[4000000 values with dtype=float64]</pre></li></ul></li><li>Indexes: (4)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       190, 191, 192, 193, 194, 195, 196, 197, 198, 199],\n      dtype='int64', name='draw', length=200))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999],\n      dtype='int64', name='__obs__', length=5000))</pre></li><li>rt,response_dimPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_dim'))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 40MB\nDimensions:      (chain: 2, draw: 500, __obs__: 5000)\nCoordinates:\n  * chain        (chain) int64 16B 0 1\n  * draw         (draw) int64 4kB 0 1 2 3 4 5 6 ... 493 494 495 496 497 498 499\n  * __obs__      (__obs__) int64 40kB 0 1 2 3 4 5 ... 4995 4996 4997 4998 4999\nData variables:\n    rt,response  (chain, draw, __obs__) float64 40MB ...\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li><li>__obs__: 5000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 ... 4996 4997 4998 4999<pre>array([   0,    1,    2, ..., 4997, 4998, 4999], shape=(5000,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64...<pre>[5000000 values with dtype=float64]</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999],\n      dtype='int64', name='__obs__', length=5000))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 53kB\nDimensions:          (chain: 2, draw: 500)\nCoordinates:\n  * chain            (chain) int64 16B 0 1\n  * draw             (draw) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\nData variables:\n    acceptance_rate  (chain, draw) float64 8kB ...\n    step_size        (chain, draw) float64 8kB ...\n    diverging        (chain, draw) bool 1kB ...\n    energy           (chain, draw) float64 8kB ...\n    n_steps          (chain, draw) int64 8kB ...\n    tree_depth       (chain, draw) int64 8kB ...\n    lp               (chain, draw) float64 8kB ...\nAttributes:\n    created_at:                  2025-07-11T17:29:22.675898+00:00\n    arviz_version:               0.21.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (7)<ul><li>acceptance_rate(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>step_size(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>diverging(chain, draw)bool...<pre>[1000 values with dtype=bool]</pre></li><li>energy(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li><li>n_steps(chain, draw)int64...<pre>[1000 values with dtype=int64]</pre></li><li>tree_depth(chain, draw)int64...<pre>[1000 values with dtype=int64]</pre></li><li>lp(chain, draw)float64...<pre>[1000 values with dtype=float64]</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (4)created_at :2025-07-11T17:29:22.675898+00:00arviz_version :0.21.0modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 120kB\nDimensions:                  (__obs__: 5000, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 40kB 0 1 2 3 ... 4997 4998 4999\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 80kB ...\nAttributes:\n    created_at:                  2025-07-11T17:29:22.676800+00:00\n    arviz_version:               0.21.0\n    inference_library:           numpyro\n    inference_library_version:   0.17.0\n    sampling_time:               95.06823\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 5000</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 ... 4996 4997 4998 4999<pre>array([   0,    1,    2, ..., 4997, 4998, 4999], shape=(5000,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float64...<pre>[10000 values with dtype=float64]</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999],\n      dtype='int64', name='__obs__', length=5000))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-07-11T17:29:22.676800+00:00arviz_version :0.21.0inference_library :numpyroinference_library_version :0.17.0sampling_time :95.06823tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[15]: Copied! <pre>az.summary(BasicDDMModel.traces)\n</pre> az.summary(BasicDDMModel.traces) Out[15]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat t_Intercept 0.328 0.005 0.317 0.338 0.000 0.000 576.0 573.0 1.0 z_Intercept 0.466 0.007 0.453 0.478 0.000 0.000 529.0 496.0 1.0 a_Intercept 1.017 0.009 1.000 1.033 0.000 0.000 583.0 610.0 1.0 v_Intercept 0.943 0.023 0.900 0.985 0.001 0.001 536.0 526.0 1.0 In\u00a0[16]: Copied! <pre>az.plot_trace(BasicDDMModel.traces)\nplt.tight_layout()\n</pre> az.plot_trace(BasicDDMModel.traces) plt.tight_layout() In\u00a0[17]: Copied! <pre>az.plot_forest(BasicDDMModel.traces)\nplt.tight_layout()\n</pre> az.plot_forest(BasicDDMModel.traces) plt.tight_layout() In\u00a0[18]: Copied! <pre>az.plot_pair(BasicDDMModel.traces,\n             kind=\"kde\",\n             marginals=True)\n</pre> az.plot_pair(BasicDDMModel.traces,              kind=\"kde\",              marginals=True) Out[18]: <pre>array([[&lt;Axes: ylabel='t_Intercept'&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;],\n       [&lt;Axes: ylabel='z_Intercept'&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;],\n       [&lt;Axes: ylabel='a_Intercept'&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;],\n       [&lt;Axes: xlabel='t_Intercept', ylabel='v_Intercept'&gt;,\n        &lt;Axes: xlabel='z_Intercept'&gt;, &lt;Axes: xlabel='a_Intercept'&gt;,\n        &lt;Axes: xlabel='v_Intercept'&gt;]], dtype=object)</pre> In\u00a0[19]: Copied! <pre>ax = hssm.plotting.plot_model_cartoon(\n    BasicDDMModel,\n    n_samples=10,\n    bins=20,\n    plot_pp_mean=True,\n    plot_pp_samples=False,\n    n_trajectories=2,  # extra arguments for the underlying plot_model_cartoon() function\n);\n</pre> ax = hssm.plotting.plot_model_cartoon(     BasicDDMModel,     n_samples=10,     bins=20,     plot_pp_mean=True,     plot_pp_samples=False,     n_trajectories=2,  # extra arguments for the underlying plot_model_cartoon() function ); <pre>No posterior predictive samples found. Generating posterior predictive samples using the provided InferenceData object and the original data. This will modify the provided InferenceData object, or if not provided, the traces object stored inside the model.\n</pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> In\u00a0[20]: Copied! <pre>ax = hssm.plotting.plot_quantile_probability(BasicDDMModel, \n                                             cond=\"quantile_difficulty\",\n                                             )\nax.set_ylim(0, 3);\n# ax.set_xlim(-0.1, 1.1);\n</pre> ax = hssm.plotting.plot_quantile_probability(BasicDDMModel,                                               cond=\"quantile_difficulty\",                                              ) ax.set_ylim(0, 3); # ax.set_xlim(-0.1, 1.1); <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/src/hssm/plotting/utils.py:327: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  df.groupby([\"observed\", \"chain\", \"draw\", cond, \"is_correct\"])[\"rt\"]\n/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/src/hssm/plotting/utils.py:334: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  df.groupby([\"observed\", \"chain\", \"draw\", cond])[\"is_correct\"]\n</pre> In\u00a0[21]: Copied! <pre>ax = hssm.plotting.plot_quantile_probability(BasicDDMModel, \n                                             cond=\"costly_fail_condition\",\n                                             )\nax.set_ylim(0, 3);\n</pre> ax = hssm.plotting.plot_quantile_probability(BasicDDMModel,                                               cond=\"costly_fail_condition\",                                              ) ax.set_ylim(0, 3); In\u00a0[22]: Copied! <pre>ax = hssm.plotting.plot_quantile_probability(BasicDDMModel, \n                                             cond=\"response_l1_plotting\",\n                                             )\nax.set_ylim(0, 3);\n</pre> ax = hssm.plotting.plot_quantile_probability(BasicDDMModel,                                               cond=\"response_l1_plotting\",                                              ) ax.set_ylim(0, 3); In\u00a0[23]: Copied! <pre># Posterior predictive\nBasicDDMModel.plot_predictive(step = True, \n                                        col = 'participant_id',\n                                        col_wrap = 5,\n                                        bins = np.linspace(-5,5, 50))\n</pre> # Posterior predictive BasicDDMModel.plot_predictive(step = True,                                          col = 'participant_id',                                         col_wrap = 5,                                         bins = np.linspace(-5,5, 50)) Out[23]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x2ba7a3110&gt;</pre> In\u00a0[24]: Copied! <pre>DDMHierModel = hssm.HSSM(data = workshop_data,\n                         model = \"ddm\",\n                         loglik_kind = \"approx_differentiable\",\n                         global_formula = \"y ~ (1|participant_id)\", # New\n                         noncentered = False,\n                        )\n</pre> DDMHierModel = hssm.HSSM(data = workshop_data,                          model = \"ddm\",                          loglik_kind = \"approx_differentiable\",                          global_formula = \"y ~ (1|participant_id)\", # New                          noncentered = False,                         ) <pre>Model initialized successfully.\n</pre> In\u00a0[25]: Copied! <pre>try:\n    # Load pre-computed traces\n    DDMHierModel.restore_traces(traces = \"scientific_workflow_hssm/idata/ddm_hier/traces.nc\")\nexcept:\n    # Sample posterior\n    ddm_hier_idata = DDMHierModel.sample(chains = 2,\n                                             sampler = \"nuts_numpyro\",\n                                             tune = 500,\n                                             draws = 500,\n                                            )\n\n    # Sample posterior predictive\n    DDMHierModel.sample_posterior_predictive(draws = 200,\n                                             safe_mode = True)\n\n    # Save Model\n    DDMHierModel.save_model(model_name = \"ddm_hier\",\n                            allow_absolute_base_path = True,\n                            base_path = \"scientific_workflow_hssm/idata/\",\n                            save_idata_only = True)\n</pre> try:     # Load pre-computed traces     DDMHierModel.restore_traces(traces = \"scientific_workflow_hssm/idata/ddm_hier/traces.nc\") except:     # Sample posterior     ddm_hier_idata = DDMHierModel.sample(chains = 2,                                              sampler = \"nuts_numpyro\",                                              tune = 500,                                              draws = 500,                                             )      # Sample posterior predictive     DDMHierModel.sample_posterior_predictive(draws = 200,                                              safe_mode = True)      # Save Model     DDMHierModel.save_model(model_name = \"ddm_hier\",                             allow_absolute_base_path = True,                             base_path = \"scientific_workflow_hssm/idata/\",                             save_idata_only = True) In\u00a0[26]: Copied! <pre>DDMHierModel.graph()\n</pre> DDMHierModel.graph() Out[26]: In\u00a0[27]: Copied! <pre>az.plot_trace(DDMHierModel.traces)\nplt.tight_layout()\n</pre> az.plot_trace(DDMHierModel.traces) plt.tight_layout() In\u00a0[28]: Copied! <pre>ax = hssm.plotting.plot_model_cartoon(\n    DDMHierModel,\n    col = \"participant_id\",\n    col_wrap = 5,\n    n_samples=100,\n    bin_size=0.2,\n    plot_pp_mean=True,\n    # color_pp_mean = \"red\",\n    # color_pp = \"black\",\n    plot_pp_samples=False,\n    n_trajectories=2,  # extra arguments for the underlying plot_model_cartoon() function\n);\n</pre> ax = hssm.plotting.plot_model_cartoon(     DDMHierModel,     col = \"participant_id\",     col_wrap = 5,     n_samples=100,     bin_size=0.2,     plot_pp_mean=True,     # color_pp_mean = \"red\",     # color_pp = \"black\",     plot_pp_samples=False,     n_trajectories=2,  # extra arguments for the underlying plot_model_cartoon() function ); <pre>No posterior predictive samples found. Generating posterior predictive samples using the provided InferenceData object and the original data. This will modify the provided InferenceData object, or if not provided, the traces object stored inside the model.\n</pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> In\u00a0[29]: Copied! <pre>az.summary(DDMHierModel.traces,\n           filter_vars = \"like\",\n           var_names = [\"~participant_id\"]).sort_index()\n</pre> az.summary(DDMHierModel.traces,            filter_vars = \"like\",            var_names = [\"~participant_id\"]).sort_index() Out[29]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat a_Intercept 1.041 0.019 1.005 1.078 0.001 0.001 311.0 398.0 1.01 t_Intercept 0.375 0.021 0.337 0.414 0.002 0.002 89.0 121.0 1.01 v_Intercept 0.948 0.110 0.734 1.146 0.010 0.006 121.0 205.0 1.01 z_Intercept 0.455 0.016 0.424 0.485 0.001 0.001 190.0 259.0 1.01 In\u00a0[30]: Copied! <pre>az.summary(BasicDDMModel.traces).sort_index()\n</pre> az.summary(BasicDDMModel.traces).sort_index() Out[30]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat a_Intercept 1.017 0.009 1.000 1.033 0.000 0.000 583.0 610.0 1.0 t_Intercept 0.328 0.005 0.317 0.338 0.000 0.000 576.0 573.0 1.0 v_Intercept 0.943 0.023 0.900 0.985 0.001 0.001 536.0 526.0 1.0 z_Intercept 0.466 0.007 0.453 0.478 0.000 0.000 529.0 496.0 1.0 <p>The mean parameters of our models are de facto quite similar. Allowing subject wise variation however dramatically improved our fit to the data!</p> In\u00a0[31]: Copied! <pre>az.compare(\n    {\"DDM\": BasicDDMModel.traces, \n     \"DDM Hierarchical\": DDMHierModel.traces}\n)\n</pre> az.compare(     {\"DDM\": BasicDDMModel.traces,       \"DDM Hierarchical\": DDMHierModel.traces} ) <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/arviz/stats/stats.py:797: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.67 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n</pre> Out[31]: rank elpd_loo p_loo elpd_diff weight se dse warning scale DDM Hierarchical 0 -4954.407753 74.496783 0.000000 0.999617 76.438410 0.000000 True log DDM 1 -5645.531361 4.771162 691.123609 0.000383 74.728599 32.604471 False log In\u00a0[32]: Copied! <pre># Posterior predictive\nDDMHierModel.plot_predictive(step = True, \n                                       col_wrap = 5,\n                                       bins = np.linspace(-5,5, 50));\n</pre> # Posterior predictive DDMHierModel.plot_predictive(step = True,                                         col_wrap = 5,                                        bins = np.linspace(-5,5, 50)); In\u00a0[33]: Copied! <pre># Posterior predictive\nBasicDDMModel.plot_predictive(step = True, \n                                        col_wrap = 5,\n                                        bins = np.linspace(-5,5, 50));\n</pre> # Posterior predictive BasicDDMModel.plot_predictive(step = True,                                          col_wrap = 5,                                         bins = np.linspace(-5,5, 50)); In\u00a0[34]: Copied! <pre># Posterior predictive\nDDMHierModel.plot_predictive(step = True, \n                                       col = 'participant_id',\n                                       col_wrap = 5,\n                                       bins = np.linspace(-5,5, 50))\n</pre> # Posterior predictive DDMHierModel.plot_predictive(step = True,                                         col = 'participant_id',                                        col_wrap = 5,                                        bins = np.linspace(-5,5, 50)) Out[34]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x2d129f850&gt;</pre> In\u00a0[35]: Copied! <pre>AngleHierModel = hssm.HSSM(data = workshop_data,\n                           model = \"angle\",\n                           loglik_kind = \"approx_differentiable\",\n                           global_formula = \"y ~ (1|participant_id)\",\n                           noncentered = False,\n                          )\n</pre> AngleHierModel = hssm.HSSM(data = workshop_data,                            model = \"angle\",                            loglik_kind = \"approx_differentiable\",                            global_formula = \"y ~ (1|participant_id)\",                            noncentered = False,                           ) <pre>Model initialized successfully.\n</pre> In\u00a0[36]: Copied! <pre>AngleHierModel.graph()\n</pre> AngleHierModel.graph() Out[36]: In\u00a0[37]: Copied! <pre>try:\n    # Load pre-computed traces\n    AngleHierModel.restore_traces(traces = \"scientific_workflow_hssm/idata/angle_hier/traces.nc\")\nexcept:\n    # Sample posterior\n    angle_hier_idata = AngleHierModel.sample(chains = 2,\n                                             sampler = \"nuts_numpyro\",\n                                             tune = 500,\n                                             draws = 500,\n                                            )\n\n    # Sample posterior predictive\n    AngleHierModel.sample_posterior_predictive(draws = 200,\n                                               safe_mode = True)\n\n    # Save Model\n    AngleHierModel.save_model(model_name = \"angle_hier\",\n                              allow_absolute_base_path = True,\n                              base_path = \"scientific_workflow_hssm/idata/\",\n                              save_idata_only = True)\n</pre> try:     # Load pre-computed traces     AngleHierModel.restore_traces(traces = \"scientific_workflow_hssm/idata/angle_hier/traces.nc\") except:     # Sample posterior     angle_hier_idata = AngleHierModel.sample(chains = 2,                                              sampler = \"nuts_numpyro\",                                              tune = 500,                                              draws = 500,                                             )      # Sample posterior predictive     AngleHierModel.sample_posterior_predictive(draws = 200,                                                safe_mode = True)      # Save Model     AngleHierModel.save_model(model_name = \"angle_hier\",                               allow_absolute_base_path = True,                               base_path = \"scientific_workflow_hssm/idata/\",                               save_idata_only = True) In\u00a0[38]: Copied! <pre>ax = hssm.plotting.plot_model_cartoon(\n    AngleHierModel,\n    col = 'participant_id',\n    col_wrap = 5,\n    n_samples=10,\n    bin_size=0.2,\n    plot_pp_mean=True,\n    plot_pp_samples=False,\n    n_trajectories=2,  # extra arguments for the underlying plot_model_cartoon() function\n);\n</pre> ax = hssm.plotting.plot_model_cartoon(     AngleHierModel,     col = 'participant_id',     col_wrap = 5,     n_samples=10,     bin_size=0.2,     plot_pp_mean=True,     plot_pp_samples=False,     n_trajectories=2,  # extra arguments for the underlying plot_model_cartoon() function ); <pre>No posterior predictive samples found. Generating posterior predictive samples using the provided InferenceData object and the original data. This will modify the provided InferenceData object, or if not provided, the traces object stored inside the model.\n</pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>We can up it one notch and include the parameter uncertainty in the <code>model_cartoon_plot()</code>. This helps us assess how certain we are about the setting of the boundary collapse here. Let's see what that looks like!</p> In\u00a0[39]: Copied! <pre>ax = hssm.plotting.plot_model_cartoon(\n    AngleHierModel,\n    col = 'participant_id',\n    col_wrap = 5,\n    n_samples=50,\n    bin_size=0.2,\n    plot_pp_mean=True,\n    plot_pp_samples=True,\n    n_trajectories=2,  # extra arguments for the underlying plot_model_cartoon() function\n);\n</pre> ax = hssm.plotting.plot_model_cartoon(     AngleHierModel,     col = 'participant_id',     col_wrap = 5,     n_samples=50,     bin_size=0.2,     plot_pp_mean=True,     plot_pp_samples=True,     n_trajectories=2,  # extra arguments for the underlying plot_model_cartoon() function ); <pre>No posterior predictive samples found. Generating posterior predictive samples using the provided InferenceData object and the original data. This will modify the provided InferenceData object, or if not provided, the traces object stored inside the model.\n</pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> <pre>Output()</pre> <pre></pre> In\u00a0[40]: Copied! <pre>az.plot_posterior(AngleHierModel.traces,\n                  var_names = [\"theta_Intercept\"],\n                  ref_val = 0,\n                  kind = \"hist\",\n                  ref_val_color = \"red\",\n                  histtype = \"step\")\n</pre> az.plot_posterior(AngleHierModel.traces,                   var_names = [\"theta_Intercept\"],                   ref_val = 0,                   kind = \"hist\",                   ref_val_color = \"red\",                   histtype = \"step\") Out[40]: <pre>&lt;Axes: title={'center': 'theta_Intercept'}&gt;</pre> In\u00a0[41]: Copied! <pre># Posterior predictive\nAngleHierModel.plot_predictive(step = True, \n                                         col_wrap = 5,\n                                         bins = np.linspace(-5,5, 50))\n</pre> # Posterior predictive AngleHierModel.plot_predictive(step = True,                                           col_wrap = 5,                                          bins = np.linspace(-5,5, 50)) Out[41]: <pre>&lt;Axes: title={'center': 'Posterior Predictive Distribution'}, xlabel='Response Time', ylabel='Density'&gt;</pre> In\u00a0[42]: Copied! <pre># Posterior predictive\nAngleHierModel.plot_predictive(step = True, \n                                        col = 'participant_id',\n                                        col_wrap = 5,\n                                        bins = np.linspace(-5,5, 50))\n</pre> # Posterior predictive AngleHierModel.plot_predictive(step = True,                                          col = 'participant_id',                                         col_wrap = 5,                                         bins = np.linspace(-5,5, 50)) Out[42]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x3124ee5d0&gt;</pre> <p>Visually it seems like we did improve the fit (even though the difference in visual improvement is much less than what we had witnessed introducing the hierarchy in the first place).</p> <p>Let us corroborate the visual intuition via formal model comparison.</p> In\u00a0[43]: Copied! <pre>az.compare(\n    {\"DDM\": BasicDDMModel.traces,\n     \"DDM Hierarchical\": DDMHierModel.traces,\n     \"Angle Hierarchical\": AngleHierModel.traces}\n)\n</pre> az.compare(     {\"DDM\": BasicDDMModel.traces,      \"DDM Hierarchical\": DDMHierModel.traces,      \"Angle Hierarchical\": AngleHierModel.traces} ) <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/arviz/stats/stats.py:797: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.67 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n</pre> Out[43]: rank elpd_loo p_loo elpd_diff weight se dse warning scale Angle Hierarchical 0 -4736.150740 78.295641 0.000000 1.000000e+00 72.823473 0.000000 False log DDM Hierarchical 1 -4954.407753 74.496783 218.257012 1.547572e-07 76.438410 16.444052 True log DDM 2 -5645.531361 4.771162 909.380621 0.000000e+00 74.728599 36.362620 False log <p>Good, introducing the <code>angle</code> model seemed to have helped us quite a bit, even though, as intuited by the simple visual inspection, the improvement in <code>elpd_loo</code> is not as the improvement in going from a simple model toward a hierarchical model (even though the actual SSM was misspecified).</p> <p>So what next? On the surface, it looks like we have a model that fits our data quite well.</p> <p>Let's take another look at our data to identify more patterns that we may not capture with out current efforts.</p> In\u00a0[44]: Copied! <pre># Posterior predictive\nAngleHierModel.plot_predictive(step = True, \n                                         col = 'costly_fail_condition',\n                                         bins = np.linspace(-5,5, 50))\nplt.tight_layout()\n</pre> # Posterior predictive AngleHierModel.plot_predictive(step = True,                                           col = 'costly_fail_condition',                                          bins = np.linspace(-5,5, 50)) plt.tight_layout() In\u00a0[45]: Copied! <pre>ax = hssm.plotting.plot_quantile_probability(AngleHierModel, \n                                             cond=\"costly_fail_condition\",\n                                             )\nax.set_ylim(0, 3);\n</pre> ax = hssm.plotting.plot_quantile_probability(AngleHierModel,                                               cond=\"costly_fail_condition\",                                              ) ax.set_ylim(0, 3); In\u00a0[46]: Copied! <pre>plot_rt_hists(workshop_data,\n              by_participant = True,\n              split_by_column = \"costly_fail_condition\",\n              inset_plot =  \"choice_proportion\")\n</pre> plot_rt_hists(workshop_data,               by_participant = True,               split_by_column = \"costly_fail_condition\",               inset_plot =  \"choice_proportion\") <pre>/var/folders/gx/s43vynx550qbypcxm83fv56dzq4hgg/T/ipykernel_64548/900136031.py:142: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n</pre> In\u00a0[47]: Copied! <pre>plot_rt_hists(workshop_data,\n              by_participant = True,\n              split_by_column = \"costly_fail_condition\",\n              inset_plot =  \"rt_mean\")\n</pre> plot_rt_hists(workshop_data,               by_participant = True,               split_by_column = \"costly_fail_condition\",               inset_plot =  \"rt_mean\") <pre>/var/folders/gx/s43vynx550qbypcxm83fv56dzq4hgg/T/ipykernel_64548/900136031.py:142: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n</pre> <p>We can identify two patterns.</p> <ol> <li>On average in the <code>costly_fail_condition</code> participants seem to make slightly fewer mistakes</li> <li>On average in the <code>costly_fail_condition</code> participants seem to take a little longer for their choices!</li> </ol> <p>This meshes with how we expect the incentives to act. Participants should be slightly more cautious to get it right, if mistakes are costly!</p> <p>In the contect of SSMs, this is usually mapped on to the <code>decision threshold</code> (parameter <code>a</code>), so maybe we should try to incorporate the <code>costly_fail_condition</code> in the regression function for that parameter in our model.</p> In\u00a0[48]: Copied! <pre>AngleHierModelV2 = hssm.HSSM(data = workshop_data,\n                             model = \"angle\",\n                             loglik_kind = \"approx_differentiable\",\n                             global_formula = \"y ~ (1|participant_id)\",\n                             include = [{\"name\": \"a\",\n                                         \"formula\": \"a ~ (1 + C(costly_fail_condition)|participant_id)\"}],\n                             noncentered = False,\n                            )\n</pre> AngleHierModelV2 = hssm.HSSM(data = workshop_data,                              model = \"angle\",                              loglik_kind = \"approx_differentiable\",                              global_formula = \"y ~ (1|participant_id)\",                              include = [{\"name\": \"a\",                                          \"formula\": \"a ~ (1 + C(costly_fail_condition)|participant_id)\"}],                              noncentered = False,                             ) <pre>Model initialized successfully.\n</pre> In\u00a0[49]: Copied! <pre>AngleHierModelV2.graph()\n</pre> AngleHierModelV2.graph() Out[49]: In\u00a0[50]: Copied! <pre>try:\n    # Load pre-computed traces\n    AngleHierModelV2.restore_traces(traces = \"scientific_workflow_hssm/idata/angle_hier_v2/traces.nc\")\nexcept:\n    # Sample posterior\n    angle_hier_idata = AngleHierModelV2.sample(chains = 2,\n                                             sampler = \"nuts_numpyro\",\n                                             tune = 500,\n                                             draws = 500,\n                                            )\n\n    # Sample posterior predictive\n    AngleHierModelV2.sample_posterior_predictive(draws = 200,\n                                                 safe_mode = True)\n\n    # Save Model\n    AngleHierModelV2.save_model(model_name = \"angle_hier_v2\",\n                                allow_absolute_base_path = True,\n                                base_path = \"scientific_workflow_hssm/idata/\",\n                                save_idata_only = True)\n</pre> try:     # Load pre-computed traces     AngleHierModelV2.restore_traces(traces = \"scientific_workflow_hssm/idata/angle_hier_v2/traces.nc\") except:     # Sample posterior     angle_hier_idata = AngleHierModelV2.sample(chains = 2,                                              sampler = \"nuts_numpyro\",                                              tune = 500,                                              draws = 500,                                             )      # Sample posterior predictive     AngleHierModelV2.sample_posterior_predictive(draws = 200,                                                  safe_mode = True)      # Save Model     AngleHierModelV2.save_model(model_name = \"angle_hier_v2\",                                 allow_absolute_base_path = True,                                 base_path = \"scientific_workflow_hssm/idata/\",                                 save_idata_only = True) In\u00a0[51]: Copied! <pre>az.plot_trace(AngleHierModelV2.traces)\nplt.tight_layout()\n</pre> az.plot_trace(AngleHierModelV2.traces) plt.tight_layout() In\u00a0[52]: Copied! <pre>az.plot_posterior(AngleHierModelV2.traces,\n                  var_names = [\"a_C(costly_fail_condition)|participant_id_mu\"],\n                  ref_val = 0,\n                  kind = \"hist\",\n                  ref_val_color = \"red\",\n                  histtype = \"step\")\n</pre> az.plot_posterior(AngleHierModelV2.traces,                   var_names = [\"a_C(costly_fail_condition)|participant_id_mu\"],                   ref_val = 0,                   kind = \"hist\",                   ref_val_color = \"red\",                   histtype = \"step\") Out[52]: <pre>&lt;Axes: title={'center': 'a_C(costly_fail_condition)|participant_id_mu\\n1'}&gt;</pre> In\u00a0[53]: Copied! <pre># Posterior predictive\nAngleHierModelV2.plot_predictive(step = True, \n                                           # row = 'participant_id',\n                                           col = 'costly_fail_condition',\n                                           bins = np.linspace(-5, 5, 50),\n                                           )\nplt.tight_layout()\nplt.show()\n</pre> # Posterior predictive AngleHierModelV2.plot_predictive(step = True,                                             # row = 'participant_id',                                            col = 'costly_fail_condition',                                            bins = np.linspace(-5, 5, 50),                                            ) plt.tight_layout() plt.show() In\u00a0[54]: Copied! <pre>ax = hssm.plotting.plot_quantile_probability(AngleHierModelV2, \n                                             cond=\"costly_fail_condition\",\n                                             )\nax.set_ylim(0, 3);\n</pre> ax = hssm.plotting.plot_quantile_probability(AngleHierModelV2,                                               cond=\"costly_fail_condition\",                                              ) ax.set_ylim(0, 3); <p>Quite an improvement! Let's see what our quantitative model comparison metrics say.</p> In\u00a0[55]: Copied! <pre>az.compare(\n    {\"DDM\": BasicDDMModel.traces,\n     \"DDM Hierarchical\": DDMHierModel.traces,\n     \"Angle Hierarchical\": AngleHierModel.traces,\n     \"Angle Hierarchical Cost\": AngleHierModelV2.traces}\n)\n</pre> az.compare(     {\"DDM\": BasicDDMModel.traces,      \"DDM Hierarchical\": DDMHierModel.traces,      \"Angle Hierarchical\": AngleHierModel.traces,      \"Angle Hierarchical Cost\": AngleHierModelV2.traces} ) <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/arviz/stats/stats.py:797: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.67 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n</pre> Out[55]: rank elpd_loo p_loo elpd_diff weight se dse warning scale Angle Hierarchical Cost 0 -4470.826272 87.516854 0.000000 1.000000e+00 70.898643 0.000000 False log Angle Hierarchical 1 -4736.150740 78.295641 265.324468 2.313717e-07 72.823473 20.493430 False log DDM Hierarchical 2 -4954.407753 74.496783 483.581480 1.523141e-07 76.438410 25.173534 True log DDM 3 -5645.531361 4.771162 1174.705089 0.000000e+00 74.728599 40.610196 False log <p>Good, we now incorporated the <code>costly_fail_condition</code> in a conceptually coherent manner.</p> <p>Let's take a look at <code>difficulty</code> next.</p> In\u00a0[56]: Copied! <pre>ax = hssm.plotting.plot_quantile_probability(AngleHierModelV2, \n                                             cond=\"quantile_difficulty_binary\",\n                                             )\nax.set_ylim(0, 3);\n</pre> ax = hssm.plotting.plot_quantile_probability(AngleHierModelV2,                                               cond=\"quantile_difficulty_binary\",                                              ) ax.set_ylim(0, 3); <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/src/hssm/plotting/utils.py:327: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  df.groupby([\"observed\", \"chain\", \"draw\", cond, \"is_correct\"])[\"rt\"]\n/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/src/hssm/plotting/utils.py:334: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  df.groupby([\"observed\", \"chain\", \"draw\", cond])[\"is_correct\"]\n</pre> In\u00a0[57]: Copied! <pre>plot_rt_hists(workshop_data,\n              by_participant = True,\n              split_by_column = \"quantile_difficulty_binary\",\n              inset_plot =  \"rt_mean\")\n</pre> plot_rt_hists(workshop_data,               by_participant = True,               split_by_column = \"quantile_difficulty_binary\",               inset_plot =  \"rt_mean\") <pre>/var/folders/gx/s43vynx550qbypcxm83fv56dzq4hgg/T/ipykernel_64548/900136031.py:142: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n</pre> In\u00a0[58]: Copied! <pre>plot_rt_hists(workshop_data,\n              by_participant = True,\n              split_by_column = \"quantile_difficulty_binary\",\n              inset_plot =  \"choice_proportion\")\n</pre> plot_rt_hists(workshop_data,               by_participant = True,               split_by_column = \"quantile_difficulty_binary\",               inset_plot =  \"choice_proportion\") <pre>/var/folders/gx/s43vynx550qbypcxm83fv56dzq4hgg/T/ipykernel_64548/900136031.py:142: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n</pre> <p>We see a similar pattern. Difficulty affects choice probability, however the effect on RT is less clear.</p> <p>What parameter should difficulty map onto? Usually it maps onto the rate of evidence accumulation, which is the drift (<code>v</code>) parameter most SSMs.</p> <p>We will move ahead and try this. To add specialized regression for <code>v</code> we can add another parameter dictionary to the list we pass to <code>include</code>.</p> In\u00a0[59]: Copied! <pre>AngleHierModelV3 = hssm.HSSM(data = workshop_data,\n                             model = \"angle\",\n                             loglik_kind = \"approx_differentiable\",\n                             global_formula = \"y ~ (1|participant_id)\",\n                             include = [{\"name\": \"a\",\n                                         \"formula\": \"a ~ (1 + C(costly_fail_condition)|participant_id)\"},\n                                         {\"name\": \"v\",\n                                         \"formula\": \"v ~ (1 + continuous_difficulty|participant_id)\"},\n                                        ],\n                             noncentered = False,\n                            )\n</pre> AngleHierModelV3 = hssm.HSSM(data = workshop_data,                              model = \"angle\",                              loglik_kind = \"approx_differentiable\",                              global_formula = \"y ~ (1|participant_id)\",                              include = [{\"name\": \"a\",                                          \"formula\": \"a ~ (1 + C(costly_fail_condition)|participant_id)\"},                                          {\"name\": \"v\",                                          \"formula\": \"v ~ (1 + continuous_difficulty|participant_id)\"},                                         ],                              noncentered = False,                             ) <pre>Model initialized successfully.\n</pre> In\u00a0[60]: Copied! <pre>AngleHierModelV3.graph()\n</pre> AngleHierModelV3.graph() Out[60]: In\u00a0[61]: Copied! <pre>try:\n    # Load pre-computed traces\n    AngleHierModelV3.restore_traces(traces = \"scientific_workflow_hssm/idata/angle_hier_v3/traces.nc\")\nexcept:\n    # Sample posterior\n    angle_hier_idata = AngleHierModelV3.sample(chains = 2,\n                                             sampler = \"nuts_numpyro\",\n                                             tune = 500,\n                                             draws = 500,\n                                            )\n\n    # Sample posterior predictive\n    AngleHierModelV3.sample_posterior_predictive(draws = 200,\n                                                 safe_mode = True)\n\n    # Save Model\n    AngleHierModelV3.save_model(model_name = \"angle_hier_v3\",\n                                allow_absolute_base_path = True,\n                                base_path = \"scientific_workflow_hssm/idata/\",\n                                save_idata_only = True)\n</pre> try:     # Load pre-computed traces     AngleHierModelV3.restore_traces(traces = \"scientific_workflow_hssm/idata/angle_hier_v3/traces.nc\") except:     # Sample posterior     angle_hier_idata = AngleHierModelV3.sample(chains = 2,                                              sampler = \"nuts_numpyro\",                                              tune = 500,                                              draws = 500,                                             )      # Sample posterior predictive     AngleHierModelV3.sample_posterior_predictive(draws = 200,                                                  safe_mode = True)      # Save Model     AngleHierModelV3.save_model(model_name = \"angle_hier_v3\",                                 allow_absolute_base_path = True,                                 base_path = \"scientific_workflow_hssm/idata/\",                                 save_idata_only = True) In\u00a0[62]: Copied! <pre>az.plot_posterior(AngleHierModelV3.traces,\n                  var_names = [\"v_continuous_difficulty|participant_id_mu\"],\n                  ref_val = 0,\n                  kind = \"hist\",\n                  ref_val_color = \"red\",\n                  histtype = \"step\")\n</pre> az.plot_posterior(AngleHierModelV3.traces,                   var_names = [\"v_continuous_difficulty|participant_id_mu\"],                   ref_val = 0,                   kind = \"hist\",                   ref_val_color = \"red\",                   histtype = \"step\") Out[62]: <pre>&lt;Axes: title={'center': 'v_continuous_difficulty|participant_id_mu'}&gt;</pre> <p>Looks like the effect on <code>v</code> is small (to the trained eye :)), but it is significant! Let's check if we can account for the data pattern we missed previously.</p> In\u00a0[63]: Copied! <pre>ax = hssm.plotting.plot_quantile_probability(AngleHierModelV3, \n                                             cond=\"quantile_difficulty_binary\",\n                                             )\nax.set_ylim(0, 3);\n</pre> ax = hssm.plotting.plot_quantile_probability(AngleHierModelV3,                                               cond=\"quantile_difficulty_binary\",                                              ) ax.set_ylim(0, 3); <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/src/hssm/plotting/utils.py:327: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  df.groupby([\"observed\", \"chain\", \"draw\", cond, \"is_correct\"])[\"rt\"]\n/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/src/hssm/plotting/utils.py:334: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  df.groupby([\"observed\", \"chain\", \"draw\", cond])[\"is_correct\"]\n</pre> In\u00a0[64]: Copied! <pre># Posterior predictive\nAngleHierModelV3.plot_predictive(step = True, \n                                           col = \"quantile_difficulty_binary\",\n                                           bins = np.linspace(-5,5, 50))\nplt.tight_layout()\n</pre> # Posterior predictive AngleHierModelV3.plot_predictive(step = True,                                             col = \"quantile_difficulty_binary\",                                            bins = np.linspace(-5,5, 50)) plt.tight_layout() <p>Success! This looks much better.</p> In\u00a0[65]: Copied! <pre>az.compare(\n    {\n     \"DDM\": BasicDDMModel.traces,\n     \"DDM Hierarchical\": DDMHierModel.traces,\n     \"Angle Hierarchical\": AngleHierModel.traces,\n     \"Angle Hierarchical Cost\": AngleHierModelV2.traces,\n     \"Angle Hierarchical Cost/Diff\": AngleHierModelV3.traces\n     }\n)\n</pre> az.compare(     {      \"DDM\": BasicDDMModel.traces,      \"DDM Hierarchical\": DDMHierModel.traces,      \"Angle Hierarchical\": AngleHierModel.traces,      \"Angle Hierarchical Cost\": AngleHierModelV2.traces,      \"Angle Hierarchical Cost/Diff\": AngleHierModelV3.traces      } ) <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/arviz/stats/stats.py:797: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.67 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/arviz/stats/stats.py:797: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.67 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n</pre> Out[65]: rank elpd_loo p_loo elpd_diff weight se dse warning scale Angle Hierarchical Cost/Diff 0 -4425.940827 69.185385 0.000000 9.876277e-01 70.554577 0.000000 True log Angle Hierarchical Cost 1 -4470.826272 87.516854 44.885445 1.237269e-02 70.898643 9.559271 False log Angle Hierarchical 2 -4736.150740 78.295641 310.209913 2.258851e-07 72.823473 22.959566 False log DDM Hierarchical 3 -4954.407753 74.496783 528.466926 1.496612e-07 76.438410 27.593021 True log DDM 4 -5645.531361 4.771162 1219.590534 0.000000e+00 74.728599 41.671983 False log <p>At this point, we have a model that fits the data quite well.</p> <p>We figured that a hierarchy significantly improves our fit, that the <code>angle</code> model dominates the basic <code>ddm</code> model for our data, and we incorporated effects based on our experiment manipulations.</p> <p>A natural next step is to check for patterns based on more generic properties of human choice data that we may be able to reason about.</p> <p>Anything that comes to mind? Let's take another look at our dataset for some inspiration.</p> In\u00a0[66]: Copied! <pre>workshop_data\n</pre> workshop_data Out[66]: response rt participant_id trial costly_fail_condition continuous_difficulty response_l1 bin_difficulty quantile_difficulty quantile_difficulty_binary response_l1_plotting 0 1 0.556439 0 1 1 -0.277337 0 low 0 -1 1 1 1 0.741682 0 2 0 -0.810919 1 low -1 -1 1 2 1 0.461832 0 3 0 -0.673330 1 low -1 -1 1 3 1 0.626154 0 4 0 0.755445 1 high 1 1 1 4 1 0.651677 0 5 1 0.136755 1 high 0 1 1 ... ... ... ... ... ... ... ... ... ... ... ... 4995 1 1.039342 19 246 0 -0.612223 -1 low -1 -1 -1 4996 1 1.587827 19 247 0 0.732396 1 high 1 1 1 4997 1 0.668594 19 248 1 -0.175321 1 low 0 -1 1 4998 1 1.616471 19 249 0 -0.630447 1 low -1 -1 1 4999 1 1.051329 19 250 1 0.511197 1 high 1 1 1 <p>5000 rows \u00d7 11 columns</p> <p>At risk of stating the obvious,we have a column that went unused thus far: <code>response_l1</code>, the lagged response.</p> <p>Maybe this hints at some level of stickiness in the choice behavior? How could we incorporate this?</p> <p>Let us first investigate if there is indeed such a pattern in the data!</p> In\u00a0[67]: Copied! <pre># Posterior predictive\nAngleHierModelV3.plot_predictive(step = True, \n                                           col = 'response_l1_plotting',\n                                           bins = np.linspace(-5, 5, 50))\nplt.tight_layout()\n</pre> # Posterior predictive AngleHierModelV3.plot_predictive(step = True,                                             col = 'response_l1_plotting',                                            bins = np.linspace(-5, 5, 50)) plt.tight_layout() <p>Indeed, it does seem like there is a bit of a pattern here, that we miss so far!</p> <p>To incoporate choice <code>stickiness</code>, a reasonable candidate parameter is <code>z</code>, the a priori choice bias. Maybe this parameter is affected by the last choice taken?</p> <p>Let's try to incoporate this. We will</p> In\u00a0[68]: Copied! <pre>AngleHierModelV4 = hssm.HSSM(data = workshop_data,\n                             model = \"angle\",\n                             loglik_kind = \"approx_differentiable\",\n                             global_formula = \"y ~ (1|participant_id)\",\n                             include = [{\"name\": \"a\",\n                                         \"formula\": \"a ~ (1 + C(costly_fail_condition)|participant_id)\"},\n                                         {\"name\": \"v\",\n                                          \"formula\": \"v ~ (1 + continuous_difficulty|participant_id)\"},\n                                         {\"name\": \"z\",\n                                          \"formula\": \"z ~ (1 + response_l1|participant_id)\"},\n                                        ],\n                             noncentered = False,\n                            )\n</pre> AngleHierModelV4 = hssm.HSSM(data = workshop_data,                              model = \"angle\",                              loglik_kind = \"approx_differentiable\",                              global_formula = \"y ~ (1|participant_id)\",                              include = [{\"name\": \"a\",                                          \"formula\": \"a ~ (1 + C(costly_fail_condition)|participant_id)\"},                                          {\"name\": \"v\",                                           \"formula\": \"v ~ (1 + continuous_difficulty|participant_id)\"},                                          {\"name\": \"z\",                                           \"formula\": \"z ~ (1 + response_l1|participant_id)\"},                                         ],                              noncentered = False,                             ) <pre>Model initialized successfully.\n</pre> In\u00a0[69]: Copied! <pre>AngleHierModelV4.graph()\n</pre> AngleHierModelV4.graph() Out[69]: In\u00a0[70]: Copied! <pre>try:\n    # Load pre-computed traces\n    AngleHierModelV4.restore_traces(traces = \"scientific_workflow_hssm/idata/angle_hier_v4/traces.nc\")\nexcept:\n    # Sample posterior\n    angle_hier_idata = AngleHierModelV4.sample(chains = 2,\n                                             sampler = \"nuts_numpyro\",\n                                             tune = 500,\n                                             draws = 500,\n                                            )\n\n    # Sample posterior predictive\n    AngleHierModelV4.sample_posterior_predictive(draws = 200,\n                                                 safe_mode = True)\n\n    # Save Model\n    AngleHierModelV4.save_model(model_name = \"angle_hier_v4\",\n                                allow_absolute_base_path = True,\n                                base_path = \"scientific_workflow_hssm/idata/\",\n                                save_idata_only = True)\n</pre> try:     # Load pre-computed traces     AngleHierModelV4.restore_traces(traces = \"scientific_workflow_hssm/idata/angle_hier_v4/traces.nc\") except:     # Sample posterior     angle_hier_idata = AngleHierModelV4.sample(chains = 2,                                              sampler = \"nuts_numpyro\",                                              tune = 500,                                              draws = 500,                                             )      # Sample posterior predictive     AngleHierModelV4.sample_posterior_predictive(draws = 200,                                                  safe_mode = True)      # Save Model     AngleHierModelV4.save_model(model_name = \"angle_hier_v4\",                                 allow_absolute_base_path = True,                                 base_path = \"scientific_workflow_hssm/idata/\",                                 save_idata_only = True) In\u00a0[71]: Copied! <pre>az.plot_trace(AngleHierModelV4.traces,\n              divergences = None);\nplt.tight_layout()\n</pre> az.plot_trace(AngleHierModelV4.traces,               divergences = None); plt.tight_layout() <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/arviz/plots/traceplot.py:223: UserWarning: rcParams['plot.max_subplots'] (20) is smaller than the number of variables to plot (24), generating only 20 plots\n  warnings.warn(\n</pre> <p>** Note **:</p> <p>We can see some rather interesting artifacts in the chains above. Around samples <code>300-375</code> it looks like our solid-blue chain got quite stuck. This indicates some problems with the posterior geometry for this model. One diagnostic that can be helpful here whether or not we observe a lot of <code>divergences</code> during sampling.</p> <p>Let's take a look below (notice, we change the <code>diveregences</code> argument from <code>None</code> to it's default)</p> In\u00a0[72]: Copied! <pre>az.plot_trace(AngleHierModelV4.traces,\n              divergences = 'auto');\nplt.tight_layout()\n</pre> az.plot_trace(AngleHierModelV4.traces,               divergences = 'auto'); plt.tight_layout() <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/arviz/plots/traceplot.py:223: UserWarning: rcParams['plot.max_subplots'] (20) is smaller than the number of variables to plot (24), generating only 20 plots\n  warnings.warn(\n</pre> <p>Indeed, we observe a fee divergences here... as rigorous scientists, we should now try to get to the bottom of this phenomenon (it happens often if one tries hierarchical models naively on real experimental data). In the context of this tutorial, we will let it slide however. It would warrant a longer detour.</p> <p>Let's move on and focus on whether or not we actually identify a significant choice stickyness effect with our analysis:</p> In\u00a0[73]: Copied! <pre>az.plot_posterior(AngleHierModelV4.traces,\n                  var_names = [\"z_response_l1|participant_id_mu\"],\n                  ref_val = 0,\n                  kind = \"hist\",\n                  ref_val_color = \"red\",\n                  histtype = \"step\")\n</pre> az.plot_posterior(AngleHierModelV4.traces,                   var_names = [\"z_response_l1|participant_id_mu\"],                   ref_val = 0,                   kind = \"hist\",                   ref_val_color = \"red\",                   histtype = \"step\") Out[73]: <pre>&lt;Axes: title={'center': 'z_response_l1|participant_id_mu'}&gt;</pre> <p>We observe a significant effect on the <code>z</code> parameter, in fact a mean effect of <code>0.073</code> insinuate a fairly big effect of choice stickyness. In direct comparison, we might expect this effect to overall have a larger impact on our model fit than the effect of difficulty on <code>v</code>, which we investigated in the previous section.</p> In\u00a0[74]: Copied! <pre># Posterior predictive\nAngleHierModelV4.plot_predictive(step = True, \n                                           col = 'response_l1_plotting',\n                                           bins = np.linspace(-5, 5, 50))\nplt.tight_layout()\n</pre> # Posterior predictive AngleHierModelV4.plot_predictive(step = True,                                             col = 'response_l1_plotting',                                            bins = np.linspace(-5, 5, 50)) plt.tight_layout() In\u00a0[75]: Copied! <pre>az.compare(\n    {\"DDM\": BasicDDMModel.traces,\n     \"DDM Hierarchical\": DDMHierModel.traces,\n     \"Angle Hierarchical\": AngleHierModel.traces,\n     \"Angle Hierarchical Cost\": AngleHierModelV2.traces,\n     \"Angle Hierarchical Cost/Diff\": AngleHierModelV3.traces,\n     \"Angle Hierarchical Cost/Diff/Sticky\": AngleHierModelV4.traces}\n)\n</pre> az.compare(     {\"DDM\": BasicDDMModel.traces,      \"DDM Hierarchical\": DDMHierModel.traces,      \"Angle Hierarchical\": AngleHierModel.traces,      \"Angle Hierarchical Cost\": AngleHierModelV2.traces,      \"Angle Hierarchical Cost/Diff\": AngleHierModelV3.traces,      \"Angle Hierarchical Cost/Diff/Sticky\": AngleHierModelV4.traces} ) <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/arviz/stats/stats.py:797: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.67 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/arviz/stats/stats.py:797: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.67 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n</pre> Out[75]: rank elpd_loo p_loo elpd_diff weight se dse warning scale Angle Hierarchical Cost/Diff/Sticky 0 -4327.769237 103.455670 0.000000 0.918195 71.004980 0.000000 False log Angle Hierarchical Cost/Diff 1 -4425.940827 69.185385 98.171590 0.081805 70.554577 15.685768 True log Angle Hierarchical Cost 2 -4470.826272 87.516854 143.057036 0.000000 70.898643 17.113407 False log Angle Hierarchical 3 -4736.150740 78.295641 408.381504 0.000000 72.823473 26.073614 False log DDM Hierarchical 4 -4954.407753 74.496783 626.638516 0.000000 76.438410 29.304281 True log DDM 5 -5645.531361 4.771162 1317.762125 0.000000 74.728599 43.174373 False log <p>And indeed, the drop in<code>elpd_loo</code> is even more substantial, than the improvement generated by incorporating the <code>difficulty</code> effect.</p> In\u00a0[76]: Copied! <pre>ax = hssm.plotting.plot_quantile_probability(AngleHierModelV4, \n                                             cond=\"response_l1_plotting\",\n                                             )\nax.set_ylim(0, 3);\n</pre> ax = hssm.plotting.plot_quantile_probability(AngleHierModelV4,                                               cond=\"response_l1_plotting\",                                              ) ax.set_ylim(0, 3); In\u00a0[77]: Copied! <pre>ax = hssm.plotting.plot_quantile_probability(AngleHierModelV4, \n                                             cond=\"costly_fail_condition\",\n                                             )\nax.set_ylim(0, 3);\n</pre> ax = hssm.plotting.plot_quantile_probability(AngleHierModelV4,                                               cond=\"costly_fail_condition\",                                              ) ax.set_ylim(0, 3); In\u00a0[78]: Copied! <pre>ax = hssm.plotting.plot_quantile_probability(AngleHierModelV4, \n                                             cond=\"quantile_difficulty_binary\",\n                                             )\nax.set_ylim(0, 3);\n</pre> ax = hssm.plotting.plot_quantile_probability(AngleHierModelV4,                                               cond=\"quantile_difficulty_binary\",                                              ) ax.set_ylim(0, 3); <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/src/hssm/plotting/utils.py:327: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  df.groupby([\"observed\", \"chain\", \"draw\", cond, \"is_correct\"])[\"rt\"]\n/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/src/hssm/plotting/utils.py:334: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  df.groupby([\"observed\", \"chain\", \"draw\", cond])[\"is_correct\"]\n</pre> In\u00a0[79]: Copied! <pre>AngleModelV5 = hssm.HSSM(data = workshop_data,\n                         model = \"angle\",\n                         loglik_kind = \"approx_differentiable\",\n                         global_formula = \"y ~ 1\",\n                         include = [{\"name\": \"a\",\n                                        \"formula\": \"a ~ 1 + C(costly_fail_condition)\"},\n                                        {\"name\": \"v\",\n                                        \"formula\": \"v ~ 1 + continuous_difficulty\"},\n                                        {\"name\": \"z\",\n                                        \"formula\": \"z ~ 1 + response_l1\"},\n                                    ],\n                         noncentered = False,\n                         )\n</pre> AngleModelV5 = hssm.HSSM(data = workshop_data,                          model = \"angle\",                          loglik_kind = \"approx_differentiable\",                          global_formula = \"y ~ 1\",                          include = [{\"name\": \"a\",                                         \"formula\": \"a ~ 1 + C(costly_fail_condition)\"},                                         {\"name\": \"v\",                                         \"formula\": \"v ~ 1 + continuous_difficulty\"},                                         {\"name\": \"z\",                                         \"formula\": \"z ~ 1 + response_l1\"},                                     ],                          noncentered = False,                          ) <pre>Model initialized successfully.\n</pre> In\u00a0[80]: Copied! <pre>try:\n    # Load pre-computed traces\n    AngleModelV5.restore_traces(traces = \"scientific_workflow_hssm/idata/angle_v5/traces.nc\")\nexcept:\n    # Sample posterior\n    angle_hier_idata = AngleModelV5.sample(chains = 2,\n                                           sampler = \"nuts_numpyro\",\n                                           tune = 500,\n                                           draws = 500,\n                                           )\n\n    # Sample posterior predictive\n    AngleModelV5.sample_posterior_predictive(draws = 200,\n                                                 safe_mode = True)\n\n    # Save Model\n    AngleModelV5.save_model(model_name = \"angle_v5\",\n                            allow_absolute_base_path = True,\n                            base_path = \"scientific_workflow_hssm/idata/\",\n                            save_idata_only = True)\n</pre> try:     # Load pre-computed traces     AngleModelV5.restore_traces(traces = \"scientific_workflow_hssm/idata/angle_v5/traces.nc\") except:     # Sample posterior     angle_hier_idata = AngleModelV5.sample(chains = 2,                                            sampler = \"nuts_numpyro\",                                            tune = 500,                                            draws = 500,                                            )      # Sample posterior predictive     AngleModelV5.sample_posterior_predictive(draws = 200,                                                  safe_mode = True)      # Save Model     AngleModelV5.save_model(model_name = \"angle_v5\",                             allow_absolute_base_path = True,                             base_path = \"scientific_workflow_hssm/idata/\",                             save_idata_only = True) In\u00a0[81]: Copied! <pre>az.compare(\n    {\n     \"DDM\": BasicDDMModel.traces,\n     \"DDM Hierarchical\": DDMHierModel.traces,\n     \"Angle Hierarchical\": AngleHierModel.traces,\n     \"Angle Hierarchical Cost\": AngleHierModelV2.traces,\n     \"Angle Hierarchical Cost/Diff\": AngleHierModelV3.traces,\n     \"Angle Hierarchical Cost/Diff/Sticky\": AngleHierModelV4.traces,\n     \"Angle Cost/Diff/Sticky\": AngleModelV5.traces\n     }\n)\n</pre> az.compare(     {      \"DDM\": BasicDDMModel.traces,      \"DDM Hierarchical\": DDMHierModel.traces,      \"Angle Hierarchical\": AngleHierModel.traces,      \"Angle Hierarchical Cost\": AngleHierModelV2.traces,      \"Angle Hierarchical Cost/Diff\": AngleHierModelV3.traces,      \"Angle Hierarchical Cost/Diff/Sticky\": AngleHierModelV4.traces,      \"Angle Cost/Diff/Sticky\": AngleModelV5.traces      } ) <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/arviz/stats/stats.py:797: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.67 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/arviz/stats/stats.py:797: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.67 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n</pre> Out[81]: rank elpd_loo p_loo elpd_diff weight se dse warning scale Angle Hierarchical Cost/Diff/Sticky 0 -4327.769237 103.455670 0.000000 0.918195 71.004980 0.000000 False log Angle Hierarchical Cost/Diff 1 -4425.940827 69.185385 98.171590 0.081805 70.554577 15.685768 True log Angle Hierarchical Cost 2 -4470.826272 87.516854 143.057036 0.000000 70.898643 17.113407 False log Angle Hierarchical 3 -4736.150740 78.295641 408.381504 0.000000 72.823473 26.073614 False log DDM Hierarchical 4 -4954.407753 74.496783 626.638516 0.000000 76.438410 29.304281 True log Angle Cost/Diff/Sticky 5 -5001.726336 8.676713 673.957099 0.000000 70.471189 33.148987 False log DDM 6 -5645.531361 4.771162 1317.762125 0.000000 74.728599 43.174373 False log <p>So far so good, we completed a rather comprehensive model exploration and we generated quite a few insights! We could obviously go on and try more and more complex models and maybe there is more to find out here... we leave this up to you and hope that HSSM will continue to help you along the way :).</p>"},{"location":"tutorials/scientific_workflow_hssm/#scientific-workflow-with-hssm","title":"Scientific Workflow with HSSM\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#colab-instructions","title":"Colab Instructions\u00b6","text":"<p>If you would like to run this tutorial on Google colab, please click this link.</p> <p>Once you are in the colab:</p> <ol> <li>Follow the installation instructions below  (uncomment the respective code)</li> <li>restart your runtime.</li> </ol> <p>NOTE:</p> <p>You may want to switch your runtime to have a GPU or TPU. To do so, go to Runtime &gt; Change runtime type and select the desired hardware accelerator. Note that if you switch your runtime you have to follow the installation instructions again.</p>"},{"location":"tutorials/scientific_workflow_hssm/#install-hssm","title":"Install hssm\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#download-tutorial-data","title":"Download tutorial data\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#start-of-tutorial","title":"Start of Tutorial\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#load-modules","title":"Load modules\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#load-workshop-data","title":"Load workshop data\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#load-plotting-utilities","title":"Load Plotting Utilities\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#exploratory-data-analysis","title":"Exploratory Data Analysis\u00b6","text":"<p>Now that we are done preparing the setup, let's get to the meat of it! The picture above gives us a bit of an idea, where the dataset that we are going to work with below comes from (alert: the backstory may or may not be real).</p> <p>20 subjects, performed 250 trials each of a basic Random dot motion task. The task seemingly had two important manipulations.</p> <ol> <li>A costly fail condition, in which subjects get punished for mistakes.</li> <li>A trial by trail manipulation of difficulty (in the Random dot motion task, this refers to degree of coherence with which the dots move in a particular direction)</li> </ol> <p>Let's take a look at the actual dataframe.</p>"},{"location":"tutorials/scientific_workflow_hssm/#adding-a-few-columns","title":"Adding a few columns\u00b6","text":"<p>As part of prep work for plotting etc. we will add a few columns here. These will be motivated later (close your eyes :)).</p>"},{"location":"tutorials/scientific_workflow_hssm/#most-basic-reaction-time-plot","title":"Most basic reaction time plot\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#basic-model-ddm","title":"Basic Model: DDM\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#taking-stock","title":"Taking stock\u00b6","text":"<p>We can observe a few patterns here.</p> <ul> <li>First, cleary the reaction time distributions are not the same for every subject, we need to account for that.</li> <li>Second, I does seem like the tail of the reaction time distribution is more graceful in for our predictions than it is in the original subject data. (This was less clear when looking only at the global pattern...)</li> </ul> <p>We will now adjust our model to tackle these patterns one by one. Let's begin by specializing our parameters by subject.</p> <p>In Bayesian Inference we approach this by introducing a Hierarchy, we assume that subject level parameters derive from a common group distribution.</p> <p>Inference then proceeds over the parameters of this group distribution, as well as the subject wise parameters.</p> <p>Hierarchies serve as a form of regularization of our parameter estimates, the group distribution allows us to share information between the single subject parameters estimates.</p> <p>You don't have to use a hierarchy, we could introduce a subject wise parameterization e.g. by simply treating <code>participant_id</code> as a categorical variable / collection of dummy variables without using any notion of a group distribution (and you are welcome to try this).</p>"},{"location":"tutorials/scientific_workflow_hssm/#ddm-hierarchical","title":"DDM Hierarchical\u00b6","text":"<p>Moving on to our first hierarchical model. As a first step, we will use our <code>global_formula</code> argument to <code>(1|participant_id)</code>, which is equivalent to <code>1 + (1|participant_id)</code>, (use <code>0 + (1|participant_id)</code> is you explicitly don't want to create an intercept).</p> <p>This will make all parameters of our model hierarchical.</p>"},{"location":"tutorials/scientific_workflow_hssm/#comparing-parameter-loadings","title":"Comparing Parameter Loadings\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#quantitative-model-comparison","title":"Quantitative Model Comparison\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#comparing-predictions","title":"Comparing predictions\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#taking-stock","title":"Taking Stock\u00b6","text":"<p>Let's take stock again of any obvious pontential for improving our model here. We are now capturing the data much better subject by subject, however looking closely,</p> <p>it seems like the tail behavior of the observed and the predicted data is somewhat different, for a few subjects.</p> <p>The particularly suspicious subjest, are:</p> <ul> <li><code>participant_id = 1</code></li> <li><code>participant_id = 14</code></li> <li><code>participant_id = 15</code></li> <li><code>participant_id = 17</code></li> </ul> <p>It seems that for these (and there are others) participants, the model predicted data has a wider tail than what we actually observe in our dataset.</p> <p>This will motivate a change in the Sequential Sampling Model that we apply.</p>"},{"location":"tutorials/scientific_workflow_hssm/#angle-model-hierarchical","title":"Angle Model Hierarchical\u00b6","text":"<p>Given what we concluded about the tail behavior of the observed RTs, we will adjust our SSM, to allow for linear collapsing bounds. HSSM ships with a such a model, and we can apply it to our data simple by changing the <code>model</code> argument. The corresponding model is called <code>angle</code> model in our lingo, and is illustrated below conceptually.</p>"},{"location":"tutorials/scientific_workflow_hssm/#angle-theta-parameter-bayesian-t-test","title":"Angle (theta) parameter Bayesian t-test\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#quantitative-model-comparison","title":"Quantitative Model Comparison\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#further-eda","title":"Further EDA\u00b6","text":"<p>Maybe it is time to look more directly at the effects of our experiment manipulations.</p> <p>Below are a few graphs to understand what might be happening.</p>"},{"location":"tutorials/scientific_workflow_hssm/#addressing-costly-fail-condition","title":"Addressing costly fail condition\u00b6","text":"<p>To include parameter specific regressions, we can rely on the <code>include</code> argument in HSSM. Let's illustrate this.</p>"},{"location":"tutorials/scientific_workflow_hssm/#quantitative-model-comparison","title":"Quantitative Model Comparison\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#addressing-difficulty","title":"Addressing difficulty\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#quantitative-model-comparison","title":"Quantitative Model Comparison\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#anything-else","title":"Anything else?\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#addressing-stickyness","title":"Addressing Stickyness\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#quantitative-model-comparison","title":"Quantitative Model Comparison\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#taking-stock","title":"Taking Stock\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#sanity-check-was-the-hierarchy-really-necessary","title":"Sanity Check, was the hierarchy really necessary\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#the-end","title":"The End:\u00b6","text":""},{"location":"tutorials/scientific_workflow_hssm/#pointers-to-advanced-topics","title":"Pointers to advanced Topics\u00b6","text":"<p>We are scratching only the surface of what cann be done with HSSM, let alone the broader eco-system supporting simulation based inference (SBI).</p> <p>Check out our simulator package, ssm-simulators as well as our our little neural network library for training LANs, lanfactory.</p> <p>Exciting work is being done (more on this in the next tutorial) on connecting to other packages in the wider eco-system, such as BayesFlow as well as the sbi package.</p> <p>Here is a taste of advanced topics with links to corresponding tutorials:</p> <ul> <li>Variational Inference with HSSM</li> <li>Build PyMC models with HSSM random variables</li> <li>Connect compiled models to third party MCMC libraries</li> <li>Construct custom models from simulators and contributed likelihoods</li> <li>Using link functions to transform parameters</li> </ul> <p>you will find this and a lot more information in the official documentation</p>"},{"location":"tutorials/tutorial_bayesian_t_test/","title":"Bayesian t-tests","text":"In\u00a0[1]: Copied! <pre>import arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport hssm\n</pre> import arviz as az import matplotlib.pyplot as plt import numpy as np import pandas as pd import hssm In\u00a0[2]: Copied! <pre># Condition 1\ncondition_1 = hssm.simulate_data(\n    model=\"ddm\", theta=dict(v=0.5, a=1.5, z=0.5, t=0.1), size=500\n)\n\n# Condition 2\ncondition_2 = hssm.simulate_data(\n    model=\"ddm\", theta=dict(v=1.0, a=1.5, z=0.5, t=0.1), size=500\n)\n</pre> # Condition 1 condition_1 = hssm.simulate_data(     model=\"ddm\", theta=dict(v=0.5, a=1.5, z=0.5, t=0.1), size=500 )  # Condition 2 condition_2 = hssm.simulate_data(     model=\"ddm\", theta=dict(v=1.0, a=1.5, z=0.5, t=0.1), size=500 ) In\u00a0[3]: Copied! <pre># Model 1\nm1 = hssm.HSSM(model=\"ddm\", data=condition_1)\n\nm1.sample(sampler=\"mcmc\", tune=500, draws=500)\n\n# Model 2\nm2 = hssm.HSSM(model=\"ddm\", data=condition_2)\n\nm2.sample(sampler=\"mcmc\", tune=500, draws=500)\n</pre> # Model 1 m1 = hssm.HSSM(model=\"ddm\", data=condition_1)  m1.sample(sampler=\"mcmc\", tune=500, draws=500)  # Model 2 m2 = hssm.HSSM(model=\"ddm\", data=condition_2)  m2.sample(sampler=\"mcmc\", tune=500, draws=500) <pre>Model initialized successfully.\nUsing default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [a, z, t, v]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 500 tune and 500 draw iterations (2_000 + 2_000 draws total) took 7 seconds.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:00&lt;00:00, 5561.68it/s]\n</pre> <pre>Model initialized successfully.\nUsing default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [a, z, t, v]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 500 tune and 500 draw iterations (2_000 + 2_000 draws total) took 7 seconds.\nThere were 6 divergences after tuning. Increase `target_accept` or reparameterize.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:00&lt;00:00, 4822.23it/s]\n</pre> Out[3]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 68kB\nDimensions:  (chain: 4, draw: 500)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    z        (chain, draw) float64 16kB 0.4935 0.4286 0.4434 ... 0.3994 0.3966\n    t        (chain, draw) float64 16kB 0.0703 0.05121 ... 0.02736 0.02714\n    v        (chain, draw) float64 16kB 1.192 1.146 1.111 ... 1.1 1.328 1.3\n    a        (chain, draw) float64 16kB 1.608 1.533 1.528 ... 1.458 1.618 1.633\nAttributes:\n    created_at:                  2025-09-27T16:57:14.317159+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               7.405345916748047\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (4)<ul><li>z(chain, draw)float640.4935 0.4286 ... 0.3994 0.3966<pre>array([[0.49349484, 0.4285789 , 0.44338056, ..., 0.44958598, 0.47417138,\n        0.46608932],\n       [0.49555337, 0.48977805, 0.48977805, ..., 0.46310757, 0.44866579,\n        0.45188143],\n       [0.45115097, 0.47786316, 0.45178687, ..., 0.46070196, 0.48570391,\n        0.4135025 ],\n       [0.45864816, 0.44344328, 0.5001549 , ..., 0.45412267, 0.39942697,\n        0.39656997]], shape=(4, 500))</pre></li><li>t(chain, draw)float640.0703 0.05121 ... 0.02736 0.02714<pre>array([[0.07029848, 0.05120521, 0.01374302, ..., 0.05611041, 0.05478065,\n        0.06981251],\n       [0.07481271, 0.09982246, 0.09982246, ..., 0.02680043, 0.052913  ,\n        0.06470444],\n       [0.09338207, 0.06987941, 0.10834648, ..., 0.04056476, 0.08210329,\n        0.02691114],\n       [0.07425231, 0.05761962, 0.13903685, ..., 0.09441841, 0.02736141,\n        0.02713746]], shape=(4, 500))</pre></li><li>v(chain, draw)float641.192 1.146 1.111 ... 1.1 1.328 1.3<pre>array([[1.19229554, 1.1456787 , 1.11106986, ..., 1.17933684, 1.21891382,\n        1.17406094],\n       [1.20319461, 1.19643717, 1.19643717, ..., 1.17887587, 1.17899763,\n        1.15095217],\n       [1.18917615, 1.0967298 , 1.22084858, ..., 1.26882204, 1.1869137 ,\n        1.27591611],\n       [1.19170276, 1.19405077, 1.08948539, ..., 1.09962185, 1.32805952,\n        1.2999831 ]], shape=(4, 500))</pre></li><li>a(chain, draw)float641.608 1.533 1.528 ... 1.618 1.633<pre>array([[1.60843755, 1.53289017, 1.528146  , ..., 1.49806703, 1.56228224,\n        1.5380734 ],\n       [1.62463658, 1.55557719, 1.55557719, ..., 1.59447834, 1.54556065,\n        1.48926266],\n       [1.51752276, 1.54414997, 1.51979343, ..., 1.66754768, 1.63204551,\n        1.5064056 ],\n       [1.52543275, 1.52096335, 1.45707896, ..., 1.45776508, 1.61823487,\n        1.63297684]], shape=(4, 500))</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T16:57:14.317159+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :7.405345916748047tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 8MB\nDimensions:      (chain: 4, draw: 500, __obs__: 500)\nCoordinates:\n  * chain        (chain) int64 32B 0 1 2 3\n  * draw         (draw) int64 4kB 0 1 2 3 4 5 6 ... 493 494 495 496 497 498 499\n  * __obs__      (__obs__) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\nData variables:\n    rt,response  (chain, draw, __obs__) float64 8MB -0.4059 -0.5374 ... -2.267\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 500</li><li>__obs__: 500</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-0.4059 -0.5374 ... -0.9927 -2.267<pre>array([[[-0.40592376, -0.53736735, -0.76395681, ..., -3.35995242,\n         -1.14502977, -2.39775923],\n        [-0.5593152 , -0.75246611, -0.75381781, ..., -3.23702347,\n         -1.08791972, -2.28313805],\n        [-0.5204751 , -0.63538984, -0.81358945, ..., -3.25380188,\n         -1.1538841 , -2.32761192],\n        ...,\n        [-0.42896763, -0.55551947, -0.77534208, ..., -3.44060204,\n         -1.15451388, -2.43949619],\n        [-0.38766923, -0.50259433, -0.76984697, ..., -3.46759305,\n         -1.1619064 , -2.4635692 ],\n        [-0.43419141, -0.57651016, -0.76403489, ..., -3.37157132,\n         -1.13772301, -2.39447906]],\n\n       [[-0.40686568, -0.55015807, -0.75270242, ..., -3.35759975,\n         -1.13440773, -2.39200355],\n        [-0.37765766, -0.52396488, -0.75142912, ..., -3.42731659,\n         -1.14599188, -2.43750527],\n        [-0.37765766, -0.52396488, -0.75142912, ..., -3.42731659,\n         -1.14599188, -2.43750527],\n...\n        [-0.4745288 , -0.66462392, -0.70910374, ..., -3.3539401 ,\n         -1.07340639, -2.34928934],\n        [-0.46557972, -0.65148295, -0.73344077, ..., -3.26430601,\n         -1.0953382 , -2.31636981],\n        [-0.42822181, -0.57606884, -0.73814004, ..., -3.58750223,\n         -1.12976811, -2.5063959 ]],\n\n       [[-0.42880182, -0.58170565, -0.75209467, ..., -3.41219481,\n         -1.13084661, -2.41347464],\n        [-0.4561105 , -0.6120429 , -0.75233761, ..., -3.40305224,\n         -1.122924  , -2.400696  ],\n        [-0.36539429, -0.49036155, -0.79401602, ..., -3.38870993,\n         -1.18721594, -2.43730247],\n        ...,\n        [-0.46813253, -0.62235858, -0.78169655, ..., -3.33166675,\n         -1.14523454, -2.37346115],\n        [-0.5558593 , -0.81414152, -0.66159211, ..., -3.42056595,\n         -1.0127397 , -2.34296716],\n        [-0.61863392, -0.89999282, -0.66409742, ..., -3.31119689,\n         -0.9927193 , -2.26745175]]], shape=(4, 500, 500))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='__obs__', length=500))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 264kB\nDimensions:                (chain: 4, draw: 500)\nCoordinates:\n  * chain                  (chain) int64 32B 0 1 2 3\n  * draw                   (draw) int64 4kB 0 1 2 3 4 5 ... 495 496 497 498 499\nData variables: (12/18)\n    process_time_diff      (chain, draw) float64 16kB 0.006708 ... 0.001682\n    lp                     (chain, draw) float64 16kB -649.6 -650.4 ... -656.1\n    energy                 (chain, draw) float64 16kB 651.0 653.7 ... 656.9\n    energy_error           (chain, draw) float64 16kB -0.05129 0.5167 ... 0.9327\n    diverging              (chain, draw) bool 2kB False False ... False False\n    index_in_trajectory    (chain, draw) int64 16kB -5 -8 5 -3 -3 ... -4 9 12 1\n    ...                     ...\n    perf_counter_diff      (chain, draw) float64 16kB 0.006709 ... 0.001681\n    n_steps                (chain, draw) float64 16kB 15.0 15.0 ... 15.0 3.0\n    tree_depth             (chain, draw) int64 16kB 4 4 5 3 4 3 ... 4 3 4 4 4 2\n    largest_eigval         (chain, draw) float64 16kB nan nan nan ... nan nan\n    step_size_bar          (chain, draw) float64 16kB 0.3382 0.3382 ... 0.3092\n    reached_max_treedepth  (chain, draw) bool 2kB False False ... False False\nAttributes:\n    created_at:                  2025-09-27T16:57:14.329471+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               7.405345916748047\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (18)<ul><li>process_time_diff(chain, draw)float640.006708 0.006715 ... 0.001682<pre>array([[0.006708, 0.006715, 0.013255, ..., 0.001682, 0.006819, 0.005067],\n       [0.003546, 0.003303, 0.003415, ..., 0.00705 , 0.003496, 0.007448],\n       [0.003826, 0.006991, 0.007208, ..., 0.0033  , 0.006923, 0.006844],\n       [0.003476, 0.003295, 0.006931, ..., 0.006972, 0.006853, 0.001682]],\n      shape=(4, 500))</pre></li><li>lp(chain, draw)float64-649.6 -650.4 ... -653.2 -656.1<pre>array([[-649.56579948, -650.44461969, -651.93541241, ..., -648.11655429,\n        -649.42467964, -647.30940432],\n       [-650.05887221, -648.95743839, -648.95743839, ..., -649.21231519,\n        -647.59843519, -647.81789203],\n       [-648.10536386, -648.24553808, -649.32980993, ..., -650.33653807,\n        -650.19986066, -650.77290999],\n       [-647.245969  , -647.52724133, -649.02283407, ..., -648.56597999,\n        -653.22173042, -656.1088406 ]], shape=(4, 500))</pre></li><li>energy(chain, draw)float64651.0 653.7 655.0 ... 656.2 656.9<pre>array([[651.04352095, 653.74056695, 655.01077372, ..., 651.43511378,\n        652.14536676, 652.14191344],\n       [652.26432746, 651.28773605, 652.72841377, ..., 649.89583464,\n        649.58277665, 648.63560739],\n       [651.58183797, 649.69110834, 650.77106473, ..., 651.85596054,\n        653.58094846, 651.79467987],\n       [648.25309989, 647.9085036 , 649.58747204, ..., 650.46949593,\n        656.19708617, 656.91354839]], shape=(4, 500))</pre></li><li>energy_error(chain, draw)float64-0.05129 0.5167 ... 1.278 0.9327<pre>array([[-0.05129176,  0.51670625, -0.08806568, ..., -0.84359169,\n         0.28735413, -0.46744602],\n       [-0.16915465, -0.02880408,  0.        , ...,  0.02177398,\n        -0.04787177,  0.0615682 ],\n       [-0.69254858, -0.27029756,  0.84059073, ..., -0.00245343,\n        -0.35079649,  0.20822562],\n       [-0.01794214,  0.01890071, -0.04728531, ...,  0.01327317,\n         1.27766225,  0.93268955]], shape=(4, 500))</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]], shape=(4, 500))</pre></li><li>index_in_trajectory(chain, draw)int64-5 -8 5 -3 -3 3 ... 12 5 -4 9 12 1<pre>array([[ -5,  -8,   5, ...,   1,  -5,   1],\n       [  4,   3,   0, ...,  -5,  -4, -14],\n       [ -2,  -3,  -4, ...,   7,   4,  -8],\n       [ -4,   6,  -6, ...,   9,  12,   1]], shape=(4, 500))</pre></li><li>smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], shape=(4, 500))</pre></li><li>max_energy_error(chain, draw)float640.2509 0.8696 ... 1.589 0.9327<pre>array([[ 2.50944841e-01,  8.69645539e-01,  8.94122763e+01, ...,\n        -8.43591690e-01,  9.80735455e-01,  1.59208617e+00],\n       [ 1.87898835e-01, -1.20046213e-01,  3.83776284e+00, ...,\n        -1.13750769e-01, -4.78717727e-02,  1.79747963e-01],\n       [ 3.53320734e+01, -5.23491690e-01,  3.59953754e+01, ...,\n         5.87183153e-02,  6.29081072e-01, -7.09803423e-01],\n       [-2.45761179e-02,  5.70218910e-02,  9.10313280e-02, ...,\n         4.06002318e-01,  1.58903432e+00,  9.32689550e-01]],\n      shape=(4, 500))</pre></li><li>step_size(chain, draw)float640.5558 0.5558 ... 0.3135 0.3135<pre>array([[0.55575146, 0.55575146, 0.55575146, ..., 0.55575146, 0.55575146,\n        0.55575146],\n       [0.27164608, 0.27164608, 0.27164608, ..., 0.27164608, 0.27164608,\n        0.27164608],\n       [0.34125636, 0.34125636, 0.34125636, ..., 0.34125636, 0.34125636,\n        0.34125636],\n       [0.31347322, 0.31347322, 0.31347322, ..., 0.31347322, 0.31347322,\n        0.31347322]], shape=(4, 500))</pre></li><li>divergences(chain, draw)int640 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0<pre>array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 6, 6, 6],\n       [0, 0, 0, ..., 0, 0, 0]], shape=(4, 500))</pre></li><li>perf_counter_start(chain, draw)float641.494e+06 1.494e+06 ... 1.494e+06<pre>array([[1493531.97098275, 1493531.97774667, 1493531.98451887, ...,\n        1493535.12296808, 1493535.12468208, 1493535.13157792],\n       [1493532.29375725, 1493532.29737921, 1493532.30072942, ...,\n        1493535.51525363, 1493535.52240858, 1493535.52599196],\n       [1493532.41513146, 1493532.41912821, 1493532.42621833, ...,\n        1493535.19923733, 1493535.20258267, 1493535.20957129],\n       [1493532.09981963, 1493532.10336108, 1493532.106691  , ...,\n        1493535.01076175, 1493535.01781504, 1493535.02472933]],\n      shape=(4, 500))</pre></li><li>acceptance_rate(chain, draw)float640.9343 0.6789 ... 0.3661 0.7978<pre>array([[0.93433305, 0.67891287, 0.8033988 , ..., 0.9618739 , 0.68538013,\n        0.84781928],\n       [0.96259616, 0.99028492, 0.15522813, ..., 0.98613776, 0.9877558 ,\n        0.91613372],\n       [0.75495499, 0.97481226, 0.46777235, ..., 0.97389271, 0.73222203,\n        0.93352543],\n       [0.99916592, 0.97203281, 0.95662912, ..., 0.85653472, 0.36605566,\n        0.79783132]], shape=(4, 500))</pre></li><li>perf_counter_diff(chain, draw)float640.006709 0.006723 ... 0.001681<pre>array([[0.00670929, 0.00672275, 0.013255  , ..., 0.0016825 , 0.00683242,\n        0.00506675],\n       [0.00355554, 0.00330371, 0.00341779, ..., 0.0070645 , 0.00350058,\n        0.00749887],\n       [0.00394325, 0.00701075, 0.00761025, ..., 0.00330017, 0.00693729,\n        0.00685138],\n       [0.00348725, 0.00329467, 0.00695025, ..., 0.00699096, 0.006857  ,\n        0.00168088]], shape=(4, 500))</pre></li><li>n_steps(chain, draw)float6415.0 15.0 31.0 ... 15.0 15.0 3.0<pre>array([[15., 15., 31., ...,  3., 15., 11.],\n       [ 7.,  7.,  7., ..., 15.,  7., 15.],\n       [ 7., 15., 15., ...,  7., 15., 15.],\n       [ 7.,  7., 15., ..., 15., 15.,  3.]], shape=(4, 500))</pre></li><li>tree_depth(chain, draw)int644 4 5 3 4 3 4 4 ... 3 3 4 3 4 4 4 2<pre>array([[4, 4, 5, ..., 2, 4, 4],\n       [3, 3, 3, ..., 4, 3, 4],\n       [3, 4, 4, ..., 3, 4, 4],\n       [3, 3, 4, ..., 4, 4, 2]], shape=(4, 500))</pre></li><li>largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], shape=(4, 500))</pre></li><li>step_size_bar(chain, draw)float640.3382 0.3382 ... 0.3092 0.3092<pre>array([[0.33815079, 0.33815079, 0.33815079, ..., 0.33815079, 0.33815079,\n        0.33815079],\n       [0.26673794, 0.26673794, 0.26673794, ..., 0.26673794, 0.26673794,\n        0.26673794],\n       [0.30488575, 0.30488575, 0.30488575, ..., 0.30488575, 0.30488575,\n        0.30488575],\n       [0.30922323, 0.30922323, 0.30922323, ..., 0.30922323, 0.30922323,\n        0.30922323]], shape=(4, 500))</pre></li><li>reached_max_treedepth(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]], shape=(4, 500))</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T16:57:14.329471+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :7.405345916748047tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 12kB\nDimensions:                  (__obs__: 500, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 4kB 0 1 2 3 4 ... 496 497 498 499\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 8kB 0...\nAttributes:\n    created_at:                  2025-09-27T16:57:14.333068+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 500</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float640.6753 1.0 0.5521 ... 1.0 2.7 1.0<pre>array([[ 0.67525798,  1.        ],\n       [ 0.55206305,  1.        ],\n       [ 1.26721382,  1.        ],\n       [ 2.31447196,  1.        ],\n       [ 0.73786056,  1.        ],\n       [ 0.94333988,  1.        ],\n       [ 4.37634468,  1.        ],\n       [ 0.49889386,  1.        ],\n       [ 1.23885679,  1.        ],\n       [ 1.91536915,  1.        ],\n       [ 0.81648427,  1.        ],\n       [ 1.30327666,  1.        ],\n       [ 1.14394295,  1.        ],\n       [ 1.93799412, -1.        ],\n       [ 1.38684154,  1.        ],\n       [ 0.41942188, -1.        ],\n       [ 0.68075711,  1.        ],\n       [ 1.00469995,  1.        ],\n       [ 1.43768978,  1.        ],\n       [ 1.63838327,  1.        ],\n...\n       [ 0.49419194,  1.        ],\n       [ 1.31887293,  1.        ],\n       [ 1.2696749 ,  1.        ],\n       [ 1.37153053,  1.        ],\n       [ 1.78645837,  1.        ],\n       [ 1.00328541,  1.        ],\n       [ 2.57950211,  1.        ],\n       [ 1.21593606,  1.        ],\n       [ 3.59342504,  1.        ],\n       [ 0.84310931,  1.        ],\n       [ 1.04517829,  1.        ],\n       [ 0.42847055,  1.        ],\n       [ 0.94007903,  1.        ],\n       [ 0.86228836,  1.        ],\n       [ 1.01907361,  1.        ],\n       [ 1.28607798,  1.        ],\n       [ 1.21491778,  1.        ],\n       [ 3.55040216,  1.        ],\n       [ 1.61505103,  1.        ],\n       [ 2.69959259,  1.        ]])</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='__obs__', length=500))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (6)created_at :2025-09-27T16:57:14.333068+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[4]: Copied! <pre># Percent of posterior samples\nprint(\n    f\"p(v_1 &gt; v_2) = {np.mean(m1.traces.posterior['v'].values &gt; m2.traces.posterior['v'].values)}\"\n)\n</pre> # Percent of posterior samples print(     f\"p(v_1 &gt; v_2) = {np.mean(m1.traces.posterior['v'].values &gt; m2.traces.posterior['v'].values)}\" ) <pre>p(v_1 &gt; v_2) = 0.0\n</pre> <p>Looks like our inference indicates that there is a $0\\%$ chance for <code>v</code> parameter of condition 1 to be higher than the <code>v</code> parameter of condition 2.</p> In\u00a0[5]: Copied! <pre># Specify a plot with one row and two columns\nfig, ax = plt.subplots(1, 1, figsize=(10, 5))\n\n# Plot the posterior samples for condition 1\naz.plot_posterior(\n    m1.traces.posterior, var_names=\"v\", ax=ax, color=\"blue\", hdi_prob=0.95\n)\n\n# Plot the posterior samples for condition 2\naz.plot_posterior(m2.traces.posterior, var_names=\"v\", ax=ax, color=\"red\", hdi_prob=0.95)\n\n# Set x-axis limit\nax.set_xlim(0.25, 1.25)\n\n# Create proxy artists for the legend\nfrom matplotlib.lines import Line2D\n\nlegend_elements = [\n    Line2D([0], [0], color=\"blue\", label=\"Condition 1\"),\n    Line2D([0], [0], color=\"red\", label=\"Condition 2\"),\n]\n\n# Add a legend\nax.legend(handles=legend_elements)\n\n# Add a title\nax.set_title(\"Posterior samples for v\")\n\n# Add a x-axis label\nax.set_xlabel(\"v\")\n</pre> # Specify a plot with one row and two columns fig, ax = plt.subplots(1, 1, figsize=(10, 5))  # Plot the posterior samples for condition 1 az.plot_posterior(     m1.traces.posterior, var_names=\"v\", ax=ax, color=\"blue\", hdi_prob=0.95 )  # Plot the posterior samples for condition 2 az.plot_posterior(m2.traces.posterior, var_names=\"v\", ax=ax, color=\"red\", hdi_prob=0.95)  # Set x-axis limit ax.set_xlim(0.25, 1.25)  # Create proxy artists for the legend from matplotlib.lines import Line2D  legend_elements = [     Line2D([0], [0], color=\"blue\", label=\"Condition 1\"),     Line2D([0], [0], color=\"red\", label=\"Condition 2\"), ]  # Add a legend ax.legend(handles=legend_elements)  # Add a title ax.set_title(\"Posterior samples for v\")  # Add a x-axis label ax.set_xlabel(\"v\") Out[5]: <pre>Text(0.5, 0, 'v')</pre> <p>A glance a these posteriors visually corroborates our simple counting analysis. The two posteriors for <code>v</code> are clearly separated.</p> <p>To appreciate the difference, let us also plot the posteriors of the respective <code>a</code> parameters.</p> In\u00a0[6]: Copied! <pre># Specify a plot with one row and two columns\nfig, ax = plt.subplots(1, 1, figsize=(10, 5))\n\n# Plot the posterior samples for condition 1\naz.plot_posterior(\n    m1.traces.posterior, var_names=\"a\", ax=ax, color=\"blue\", hdi_prob=0.95\n)\n\n# Plot the posterior samples for condition 2\naz.plot_posterior(m2.traces.posterior, var_names=\"a\", ax=ax, color=\"red\", hdi_prob=0.95)\n\n# Set x-axis limit\nax.set_xlim(1.3, 1.7)\n\n# Create proxy artists for the legend\nfrom matplotlib.lines import Line2D\n\nlegend_elements = [\n    Line2D([0], [0], color=\"blue\", label=\"Condition 1\"),\n    Line2D([0], [0], color=\"red\", label=\"Condition 2\"),\n]\n\n# Add a legend\nax.legend(handles=legend_elements)\n\n# Add a title\nax.set_title(\"Posterior samples for a\")\n\n# Add a x-axis label\nax.set_xlabel(\"v\")\n</pre> # Specify a plot with one row and two columns fig, ax = plt.subplots(1, 1, figsize=(10, 5))  # Plot the posterior samples for condition 1 az.plot_posterior(     m1.traces.posterior, var_names=\"a\", ax=ax, color=\"blue\", hdi_prob=0.95 )  # Plot the posterior samples for condition 2 az.plot_posterior(m2.traces.posterior, var_names=\"a\", ax=ax, color=\"red\", hdi_prob=0.95)  # Set x-axis limit ax.set_xlim(1.3, 1.7)  # Create proxy artists for the legend from matplotlib.lines import Line2D  legend_elements = [     Line2D([0], [0], color=\"blue\", label=\"Condition 1\"),     Line2D([0], [0], color=\"red\", label=\"Condition 2\"), ]  # Add a legend ax.legend(handles=legend_elements)  # Add a title ax.set_title(\"Posterior samples for a\")  # Add a x-axis label ax.set_xlabel(\"v\") Out[6]: <pre>Text(0.5, 0, 'v')</pre> <p>and correspondingly,</p> In\u00a0[7]: Copied! <pre>print(\n    f\"p(a_1 &gt; a_2) = {np.mean(m1.traces.posterior['a'].values &gt; m2.traces.posterior['a'].values)}\"\n)\n</pre> print(     f\"p(a_1 &gt; a_2) = {np.mean(m1.traces.posterior['a'].values &gt; m2.traces.posterior['a'].values)}\" ) <pre>p(a_1 &gt; a_2) = 0.456\n</pre> In\u00a0[8]: Copied! <pre>condition_1[\"condition\"] = \"C1\"\ncondition_2[\"condition\"] = \"C2\"\n\ndata = pd.concat([condition_1, condition_2]).reset_index(drop=True)\n</pre> condition_1[\"condition\"] = \"C1\" condition_2[\"condition\"] = \"C2\"  data = pd.concat([condition_1, condition_2]).reset_index(drop=True) In\u00a0[9]: Copied! <pre>m_combined = hssm.HSSM(\n    model=\"ddm\",\n    data=data,\n    include=[\n        {\n            \"name\": \"v\",\n            \"formula\": \"v ~ 1 + condition\",\n        }\n    ],\n)\n\nidata_combined = m_combined.sample(sampler=\"mcmc\", tune=500, draws=500)\n</pre> m_combined = hssm.HSSM(     model=\"ddm\",     data=data,     include=[         {             \"name\": \"v\",             \"formula\": \"v ~ 1 + condition\",         }     ], )  idata_combined = m_combined.sample(sampler=\"mcmc\", tune=500, draws=500) <pre>Model initialized successfully.\nUsing default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [a, z, t, v_Intercept, v_condition]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 500 tune and 500 draw iterations (2_000 + 2_000 draws total) took 12 seconds.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:00&lt;00:00, 2782.40it/s]\n</pre> <p>Note, now we don't have two distinct <code>idata</code> objects, but instead we have a single <code>idata</code> object with a posterior for <code>v_condition</code>. Let's take a closer look.</p> In\u00a0[10]: Copied! <pre>m_combined.traces.posterior\n</pre> m_combined.traces.posterior Out[10]: <pre>&lt;xarray.Dataset&gt; Size: 84kB\nDimensions:          (chain: 4, draw: 500, v_condition_dim: 1)\nCoordinates:\n  * chain            (chain) int64 32B 0 1 2 3\n  * draw             (draw) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\n  * v_condition_dim  (v_condition_dim) &lt;U2 8B 'C2'\nData variables:\n    v_Intercept      (chain, draw) float64 16kB 0.623 0.6149 ... 0.6416 0.5532\n    t                (chain, draw) float64 16kB 0.08535 0.0944 ... 0.05899\n    z                (chain, draw) float64 16kB 0.4658 0.4764 ... 0.4529 0.4515\n    v_condition      (chain, draw, v_condition_dim) float64 16kB 0.5639 ... 0...\n    a                (chain, draw) float64 16kB 1.53 1.536 1.524 ... 1.606 1.508\nAttributes:\n    created_at:                  2025-09-27T16:57:33.821207+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               12.113749980926514\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 500</li><li>v_condition_dim: 1</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>v_condition_dim(v_condition_dim)&lt;U2'C2'<pre>array(['C2'], dtype='&lt;U2')</pre></li></ul></li><li>Data variables: (5)<ul><li>v_Intercept(chain, draw)float640.623 0.6149 ... 0.6416 0.5532<pre>array([[0.623013  , 0.61494938, 0.59650528, ..., 0.62081669, 0.66065298,\n        0.62240188],\n       [0.58729999, 0.62163724, 0.52160495, ..., 0.48956916, 0.51858519,\n        0.61284364],\n       [0.64319655, 0.55787039, 0.6129524 , ..., 0.62934694, 0.62934694,\n        0.62934694],\n       [0.57969651, 0.57969651, 0.56181137, ..., 0.57010414, 0.64162017,\n        0.55320176]], shape=(4, 500))</pre></li><li>t(chain, draw)float640.08535 0.0944 ... 0.05436 0.05899<pre>array([[0.08535261, 0.09440185, 0.05408372, ..., 0.01799404, 0.03584252,\n        0.03993884],\n       [0.0888627 , 0.08993176, 0.06381757, ..., 0.11228194, 0.09627461,\n        0.10564842],\n       [0.0838552 , 0.05603812, 0.07467639, ..., 0.13029014, 0.13029014,\n        0.13029014],\n       [0.05403128, 0.05403128, 0.08748213, ..., 0.12099541, 0.05436461,\n        0.05898506]], shape=(4, 500))</pre></li><li>z(chain, draw)float640.4658 0.4764 ... 0.4529 0.4515<pre>array([[0.46575492, 0.47635574, 0.44776477, ..., 0.44728932, 0.4502872 ,\n        0.45076447],\n       [0.46698583, 0.45440546, 0.47617648, ..., 0.48615039, 0.47559632,\n        0.4858296 ],\n       [0.45227791, 0.47421572, 0.4493505 , ..., 0.45707147, 0.45707147,\n        0.45707147],\n       [0.44848043, 0.44848043, 0.4602001 , ..., 0.48310048, 0.45292514,\n        0.45153486]], shape=(4, 500))</pre></li><li>v_condition(chain, draw, v_condition_dim)float640.5639 0.5173 ... 0.5732 0.661<pre>array([[[0.5638735 ],\n        [0.51734469],\n        [0.61076465],\n        ...,\n        [0.56962897],\n        [0.53763525],\n        [0.6208607 ]],\n\n       [[0.59599902],\n        [0.5621224 ],\n        [0.53592887],\n        ...,\n        [0.65443866],\n        [0.64976763],\n        [0.44258538]],\n\n       [[0.62614225],\n        [0.59755504],\n        [0.53727413],\n        ...,\n        [0.59056503],\n        [0.59056503],\n        [0.59056503]],\n\n       [[0.59271633],\n        [0.59271633],\n        [0.65665227],\n        ...,\n        [0.52045409],\n        [0.57317091],\n        [0.66096534]]], shape=(4, 500, 1))</pre></li><li>a(chain, draw)float641.53 1.536 1.524 ... 1.606 1.508<pre>array([[1.53034234, 1.53618775, 1.52427828, ..., 1.57737424, 1.57899222,\n        1.56939787],\n       [1.53841941, 1.52247031, 1.51042738, ..., 1.51703145, 1.4960821 ,\n        1.51385704],\n       [1.54080547, 1.55763775, 1.48888413, ..., 1.48152418, 1.48152418,\n        1.48152418],\n       [1.55611539, 1.55611539, 1.50028379, ..., 1.5252191 , 1.6064601 ,\n        1.50767574]], shape=(4, 500))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>v_condition_dimPandasIndex<pre>PandasIndex(Index(['C2'], dtype='object', name='v_condition_dim'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T16:57:33.821207+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :12.113749980926514tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> <p>Under the hood, Bambi created a model with a dummy variable for the <code>C2</code> condition (the <code>C1</code> condition represents the Intercept).</p> <p>So what do we need to test here...</p> <p>We still ask the same question: Is the <code>v_condition[C2]</code> posterior higher than the <code>v_condition[C1]</code> posterior?</p> <p>But now we only need to check if our <code>v_condition[C2]</code> variable is above <code>0</code>, since it represents the offset from the Intercept directly.</p> <p>Let's check:</p> In\u00a0[11]: Copied! <pre>print(\n    f\"p(v_condition[C2] &gt; v_condition[C1]) = {np.mean(idata_combined.posterior['v_condition'].values &gt; 0)}\"\n)\n</pre> print(     f\"p(v_condition[C2] &gt; v_condition[C1]) = {np.mean(idata_combined.posterior['v_condition'].values &gt; 0)}\" ) <pre>p(v_condition[C2] &gt; v_condition[C1]) = 1.0\n</pre> <p>or visually,</p> In\u00a0[12]: Copied! <pre>az.plot_posterior(idata_combined.posterior, var_names=\"v_condition\", hdi_prob=0.95)\n</pre> az.plot_posterior(idata_combined.posterior, var_names=\"v_condition\", hdi_prob=0.95) Out[12]: <pre>&lt;Axes: title={'center': 'v_condition\\nC2'}&gt;</pre> <p>You can use this approach to test any number of complex statements about your parameters. There will essentially always be a way to turn your question into a simple comparison of the posterior samples, a simple counting problem.</p>"},{"location":"tutorials/tutorial_bayesian_t_test/#bayesian-t-tests","title":"Bayesian t-tests\u00b6","text":"<p>In this quick tutorial we illustrate how to use posterior samples to perform Bayesian hypothesis testing.</p>"},{"location":"tutorials/tutorial_bayesian_t_test/#example-1-separate-models","title":"Example 1: Separate models\u00b6","text":""},{"location":"tutorials/tutorial_bayesian_t_test/#simulate-data","title":"Simulate Data\u00b6","text":"<p>We will simulate a simple dataset that contains two conditions.</p>"},{"location":"tutorials/tutorial_bayesian_t_test/#specify-models","title":"Specify Models\u00b6","text":"<p>We will fit two separate models to the data.</p>"},{"location":"tutorials/tutorial_bayesian_t_test/#bayesian-t-test","title":"Bayesian t-test\u00b6","text":"<p>Now, let's ask the (informed) question:</p> <p>Is the <code>v</code> parameter higher in condition 1 than condition 2?</p> <p>The beauty of the Bayesian approach is that this questions boils down to simple counting.</p> <p>Reformulating the question: Are the <code>v</code> samples of my posterior for condition 1 higher than the samples of my posterior for condition 2?</p> <p>Let's check for that, we have everything we need in our posterior samples!</p>"},{"location":"tutorials/tutorial_bayesian_t_test/#plotting","title":"Plotting\u00b6","text":"<p>We can also plot the posterior samples to get a sense of the uncertainty.</p>"},{"location":"tutorials/tutorial_bayesian_t_test/#example-2-combined-model","title":"Example 2: Combined Model\u00b6","text":""},{"location":"tutorials/tutorial_bayeux/","title":"Sampling with Bayeux","text":"In\u00a0[\u00a0]: Copied! <pre># Un-comment and run this only on Google Colab\n# !pip install hssm bayeux-ml flowMC==0.3.4 jax[cuda12]==0.4.38 jaxlib==0.4.38\n</pre> # Un-comment and run this only on Google Colab # !pip install hssm bayeux-ml flowMC==0.3.4 jax[cuda12]==0.4.38 jaxlib==0.4.38 In\u00a0[\u00a0]: Copied! <pre>import arviz as az\nimport bayeux as bx\nimport jax\nimport hssm\n</pre> import arviz as az import bayeux as bx import jax import hssm In\u00a0[\u00a0]: Copied! <pre># Load a package-supplied dataset\ncav_data = hssm.load_data(\"cavanagh_theta\")\n\n# Define a basic hierarchical model with trial-level covariates\nmodel = hssm.HSSM(\n    model=\"ddm\",\n    data=cav_data,\n    include=[\n        {\n            \"name\": \"v\",\n            \"prior\": {\n                \"Intercept\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 0.1},\n                \"theta\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 0.1},\n            },\n            \"formula\": \"v ~ theta + (1|participant_id)\",\n            \"link\": \"identity\",\n        },\n    ],\n)\n</pre> # Load a package-supplied dataset cav_data = hssm.load_data(\"cavanagh_theta\")  # Define a basic hierarchical model with trial-level covariates model = hssm.HSSM(     model=\"ddm\",     data=cav_data,     include=[         {             \"name\": \"v\",             \"prior\": {                 \"Intercept\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 0.1},                 \"theta\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 0.1},             },             \"formula\": \"v ~ theta + (1|participant_id)\",             \"link\": \"identity\",         },     ], ) <pre>No common intercept. Bounds for parameter v is not applied due to a current limitation of Bambi. This will change in the future.\n</pre> <pre>WARNING:hssm:No common intercept. Bounds for parameter v is not applied due to a current limitation of Bambi. This will change in the future.\n</pre> <pre>Model initialized successfully.\n</pre> <pre>INFO:hssm:Model initialized successfully.\n</pre> In\u00a0[\u00a0]: Copied! <pre>bx_model = bx.Model.from_pymc(model.pymc_model)\nidata = bx_model.mcmc.numpyro_nuts(seed=jax.random.key(0))\n</pre> bx_model = bx.Model.from_pymc(model.pymc_model) idata = bx_model.mcmc.numpyro_nuts(seed=jax.random.key(0)) <pre>sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1500/1500 [05:59&lt;00:00,  4.18it/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre>az.plot_trace(idata)\n</pre> az.plot_trace(idata) Out[\u00a0]: <pre>array([[&lt;Axes: title={'center': 'a'}&gt;, &lt;Axes: title={'center': 'a'}&gt;],\n       [&lt;Axes: title={'center': 't'}&gt;, &lt;Axes: title={'center': 't'}&gt;],\n       [&lt;Axes: title={'center': 'v_1|participant_id_mu'}&gt;,\n        &lt;Axes: title={'center': 'v_1|participant_id_mu'}&gt;],\n       [&lt;Axes: title={'center': 'v_1|participant_id_offset'}&gt;,\n        &lt;Axes: title={'center': 'v_1|participant_id_offset'}&gt;],\n       [&lt;Axes: title={'center': 'v_1|participant_id_sigma'}&gt;,\n        &lt;Axes: title={'center': 'v_1|participant_id_sigma'}&gt;],\n       [&lt;Axes: title={'center': 'v_Intercept'}&gt;,\n        &lt;Axes: title={'center': 'v_Intercept'}&gt;],\n       [&lt;Axes: title={'center': 'v_theta'}&gt;,\n        &lt;Axes: title={'center': 'v_theta'}&gt;],\n       [&lt;Axes: title={'center': 'z'}&gt;, &lt;Axes: title={'center': 'z'}&gt;]],\n      dtype=object)</pre> <p>You can also use any other sampler provided via <code>bayeux</code>:</p> In\u00a0[\u00a0]: Copied! <pre>idata = bx_model.mcmc.tfp_nuts(seed=jax.random.key(0))\n</pre> idata = bx_model.mcmc.tfp_nuts(seed=jax.random.key(0)) In\u00a0[\u00a0]: Copied! <pre>az.plot_trace(idata)\n</pre> az.plot_trace(idata) Out[\u00a0]: <pre>array([[&lt;Axes: title={'center': 'a'}&gt;, &lt;Axes: title={'center': 'a'}&gt;],\n       [&lt;Axes: title={'center': 't'}&gt;, &lt;Axes: title={'center': 't'}&gt;],\n       [&lt;Axes: title={'center': 'v_1|participant_id_mu'}&gt;,\n        &lt;Axes: title={'center': 'v_1|participant_id_mu'}&gt;],\n       [&lt;Axes: title={'center': 'v_1|participant_id_offset'}&gt;,\n        &lt;Axes: title={'center': 'v_1|participant_id_offset'}&gt;],\n       [&lt;Axes: title={'center': 'v_1|participant_id_sigma'}&gt;,\n        &lt;Axes: title={'center': 'v_1|participant_id_sigma'}&gt;],\n       [&lt;Axes: title={'center': 'v_Intercept'}&gt;,\n        &lt;Axes: title={'center': 'v_Intercept'}&gt;],\n       [&lt;Axes: title={'center': 'v_theta'}&gt;,\n        &lt;Axes: title={'center': 'v_theta'}&gt;],\n       [&lt;Axes: title={'center': 'z'}&gt;, &lt;Axes: title={'center': 'z'}&gt;]],\n      dtype=object)</pre>"},{"location":"tutorials/tutorial_bayeux/#sampling-with-bayeux","title":"Sampling with Bayeux\u00b6","text":"<p>This tutorial demonstrates how to use the <code>bayeux-ml</code> package with models defined with HSSM.</p>"},{"location":"tutorials/tutorial_bayeux/#install-bayeux-ml-with-pip","title":"Install <code>bayeux-ml</code> with pip\u00b6","text":"<p>We assume that you already have HSSM installed in a virtual environment. If you have not, please follow this tutorial to install HSSM.</p> <p>In the virtual environment where HSSM is installed, you need to install <code>bayeux-ml</code> exactly as follows:</p> <pre>pip install hssm bayeux-ml flowMC==0.3.4 jax[cuda12]==0.4.38 jaxlib==0.4.38\n</pre> <p>Bayeux has some conflicts with some dependencies because it has not been updated for a while. You might also need to install other packages that it depends on for samplers, such as <code>blackjax</code>.</p>"},{"location":"tutorials/tutorial_bayeux/#step-1-build-the-model-with-hssm","title":"Step 1. Build the model with HSSM\u00b6","text":"<p>You can build a model with HSSM as usual. No special steps required. Below is an example:</p>"},{"location":"tutorials/tutorial_bayeux/#step-2-sample-with-bayeux","title":"Step 2. Sample with Bayeux\u00b6","text":"<p><code>Bayeux</code> is compatible with any PyMC model through <code>bx.Model.from_pymc()</code>, which you can access via <code>model.pymc_model</code> from the HSSM object you have constructed.</p> <p>The example below samples with the <code>numpyro_nuts</code> sampler that <code>bayeux</code> provides.</p>"},{"location":"tutorials/tutorial_bayeux/#step-3-plot-trace-with-inference-data","title":"Step 3. Plot trace with inference data\u00b6","text":"<p>The object returned from the sampling process is an <code>InferenceData</code> object, just as HSSM does. You can use <code>ArviZ</code> to inspect it:</p>"},{"location":"tutorials/tutorial_p_outlier_regression/","title":"Regression on \"p_outlier\"","text":"In\u00a0[1]: Copied! <pre>import hssm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport arviz as az\n</pre> import hssm import matplotlib.pyplot as plt import numpy as np import pandas as pd import arviz as az In\u00a0[2]: Copied! <pre># synth data\nn_participants = 10\nconditions = ['switch', 'noswitch','someswitch']\nn_samples_per_condition = 250\n\nv_by_participant = np.random.normal(loc=0.0, scale=0.5, size=n_participants)\nv_displacement_by_condition = {'switch': 1.0, 'noswitch': 0., 'someswitch': -1.0}\na_true = 1.0\nz_true = 0.5\nt_true = 0.5\ndfs = []\n\nfor participant in range(n_participants):\n    for condition in conditions:\n        tmp_df = hssm.simulate_data(model = 'ddm', theta = dict(v = v_by_participant[participant] + v_displacement_by_condition[condition],\n                                                                a = a_true,\n                                                                z = z_true,\n                                                                t = t_true), size = n_samples_per_condition)\n        tmp_df['true_v'] = v_by_participant[participant] + v_displacement_by_condition[condition]\n        tmp_df['true_a'] = a_true\n        tmp_df['true_z'] = z_true\n        tmp_df['true_t'] = t_true\n        tmp_df['participant_id'] = str(participant)\n        tmp_df['trialtype'] = condition\n        dfs.append(tmp_df)\n\n\ndata_df = pd.concat(dfs).reset_index(drop=True)\n</pre> # synth data n_participants = 10 conditions = ['switch', 'noswitch','someswitch'] n_samples_per_condition = 250  v_by_participant = np.random.normal(loc=0.0, scale=0.5, size=n_participants) v_displacement_by_condition = {'switch': 1.0, 'noswitch': 0., 'someswitch': -1.0} a_true = 1.0 z_true = 0.5 t_true = 0.5 dfs = []  for participant in range(n_participants):     for condition in conditions:         tmp_df = hssm.simulate_data(model = 'ddm', theta = dict(v = v_by_participant[participant] + v_displacement_by_condition[condition],                                                                 a = a_true,                                                                 z = z_true,                                                                 t = t_true), size = n_samples_per_condition)         tmp_df['true_v'] = v_by_participant[participant] + v_displacement_by_condition[condition]         tmp_df['true_a'] = a_true         tmp_df['true_z'] = z_true         tmp_df['true_t'] = t_true         tmp_df['participant_id'] = str(participant)         tmp_df['trialtype'] = condition         dfs.append(tmp_df)   data_df = pd.concat(dfs).reset_index(drop=True) In\u00a0[3]: Copied! <pre># Inject noise\np_outlier_noise = np.random.uniform(0, 0.15, size = n_participants)\nfor i in range(n_participants):\n    # Get indices of trials to inject noise\n    p_outlier_indices = np.random.choice(data_df[data_df['participant_id'] == str(i)].index,\n                                         size = int(p_outlier_noise[i] * \\\n                                                    len(data_df[data_df['participant_id'] == str(i)])),\n                                         replace = False)\n    # Inject noise\n    data_df.loc[p_outlier_indices, \"rt\"] = np.random.uniform(low = 0., high = 20., \n                                                             size = data_df.loc[p_outlier_indices, \"rt\"].shape)\n</pre> # Inject noise p_outlier_noise = np.random.uniform(0, 0.15, size = n_participants) for i in range(n_participants):     # Get indices of trials to inject noise     p_outlier_indices = np.random.choice(data_df[data_df['participant_id'] == str(i)].index,                                          size = int(p_outlier_noise[i] * \\                                                     len(data_df[data_df['participant_id'] == str(i)])),                                          replace = False)     # Inject noise     data_df.loc[p_outlier_indices, \"rt\"] = np.random.uniform(low = 0., high = 20.,                                                               size = data_df.loc[p_outlier_indices, \"rt\"].shape) In\u00a0[4]: Copied! <pre>test_model_1_C = hssm.HSSM(data_df,\n          model = \"ddm\",\n          loglik_kind = \"approx_differentiable\",\n          include = [{\"name\": \"v\", \"formula\": \"v ~ 0 + (1 + C(trialtype) |participant_id)\"}],\n          p_outlier = {\"formula\": \"p_outlier ~ 1 + (1|C(participant_id))\",\n                       \"prior\": {\"Intercept\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1}},\n                       \"link\": \"logit\"\n                       },\n         )\n</pre> test_model_1_C = hssm.HSSM(data_df,           model = \"ddm\",           loglik_kind = \"approx_differentiable\",           include = [{\"name\": \"v\", \"formula\": \"v ~ 0 + (1 + C(trialtype) |participant_id)\"}],           p_outlier = {\"formula\": \"p_outlier ~ 1 + (1|C(participant_id))\",                        \"prior\": {\"Intercept\": {\"name\": \"Normal\", \"mu\": 0, \"sigma\": 1}},                        \"link\": \"logit\"                        },          ) <pre>No common intercept. Bounds for parameter v is not applied due to a current limitation of Bambi. This will change in the future.\nModel initialized successfully.\n</pre> In\u00a0[5]: Copied! <pre>test_model_1_C.sample(\n    sampler=\"nuts_numpyro\",  # type of sampler to choose, 'nuts_numpyro', 'nuts_blackjax' or default pymc nuts sampler\n    cores=2,  # how many cores to use\n    chains=2,  # how many chains to run\n    draws=500,  # number of draws from the markov chain\n    tune=500,  # number of burn-in samples\n    idata_kwargs=dict(log_likelihood=True),  # return log likelihood\n)\n</pre> test_model_1_C.sample(     sampler=\"nuts_numpyro\",  # type of sampler to choose, 'nuts_numpyro', 'nuts_blackjax' or default pymc nuts sampler     cores=2,  # how many cores to use     chains=2,  # how many chains to run     draws=500,  # number of draws from the markov chain     tune=500,  # number of burn-in samples     idata_kwargs=dict(log_likelihood=True),  # return log likelihood ) <pre>Using default initvals. \n\n</pre> <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/pymc/sampling/jax.py:475: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  pmap_numpyro = MCMC(\nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [13:29&lt;00:00,  1.24it/s, 31 steps of size 1.06e-01. acc. prob=0.91]  \nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [11:19&lt;00:00,  1.47it/s, 63 steps of size 1.12e-01. acc. prob=0.90]  \nWe recommend running at least 4 chains for robust computation of convergence diagnostics\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:06&lt;00:00, 146.11it/s]\n</pre> Out[5]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 732kB\nDimensions:                                    (chain: 2, draw: 500,\n                                                v_1|participant_id__factor_dim: 10,\n                                                v_C(trialtype)|participant_id__expr_dim: 2,\n                                                C(participant_id)__factor_dim: 10,\n                                                v_C(trialtype)|participant_id__factor_dim: 10)\nCoordinates:\n  * chain                                      (chain) int64 16B 0 1\n  * draw                                       (draw) int64 4kB 0 1 ... 498 499\n  * v_1|participant_id__factor_dim             (v_1|participant_id__factor_dim) &lt;U1 40B ...\n  * v_C(trialtype)|participant_id__expr_dim    (v_C(trialtype)|participant_id__expr_dim) &lt;U10 80B ...\n  * v_C(trialtype)|participant_id__factor_dim  (v_C(trialtype)|participant_id__factor_dim) &lt;U1 40B ...\n  * C(participant_id)__factor_dim              (C(participant_id)__factor_dim) &lt;U1 40B ...\nData variables: (12/15)\n    v_1|participant_id_mu                      (chain, draw) float64 8kB 0.06...\n    v_1|participant_id                         (chain, draw, v_1|participant_id__factor_dim) float64 80kB ...\n    t                                          (chain, draw) float64 8kB 0.50...\n    v_C(trialtype)|participant_id_sigma        (chain, draw, v_C(trialtype)|participant_id__expr_dim) float64 16kB ...\n    p_outlier_1|C(participant_id)_sigma        (chain, draw) float64 8kB 0.60...\n    a                                          (chain, draw) float64 8kB 1.00...\n    ...                                         ...\n    v_1|participant_id_offset                  (chain, draw, v_1|participant_id__factor_dim) float64 80kB ...\n    v_C(trialtype)|participant_id_mu           (chain, draw, v_C(trialtype)|participant_id__expr_dim) float64 16kB ...\n    v_C(trialtype)|participant_id_offset       (chain, draw, v_C(trialtype)|participant_id__expr_dim, v_C(trialtype)|participant_id__factor_dim) float64 160kB ...\n    p_outlier_Intercept                        (chain, draw) float64 8kB -2.1...\n    v_C(trialtype)|participant_id              (chain, draw, v_C(trialtype)|participant_id__expr_dim, v_C(trialtype)|participant_id__factor_dim) float64 160kB ...\n    v_1|participant_id_sigma                   (chain, draw) float64 8kB 0.39...\nAttributes:\n    created_at:                  2025-09-27T18:57:44.208796+00:00\n    arviz_version:               0.22.0\n    inference_library:           numpyro\n    inference_library_version:   0.19.0\n    sampling_time:               1494.128823\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li><li>v_1|participant_id__factor_dim: 10</li><li>v_C(trialtype)|participant_id__expr_dim: 2</li><li>C(participant_id)__factor_dim: 10</li><li>v_C(trialtype)|participant_id__factor_dim: 10</li></ul></li><li>Coordinates: (6)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>v_1|participant_id__factor_dim(v_1|participant_id__factor_dim)&lt;U1'0' '1' '2' '3' ... '6' '7' '8' '9'<pre>array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='&lt;U1')</pre></li><li>v_C(trialtype)|participant_id__expr_dim(v_C(trialtype)|participant_id__expr_dim)&lt;U10'someswitch' 'switch'<pre>array(['someswitch', 'switch'], dtype='&lt;U10')</pre></li><li>v_C(trialtype)|participant_id__factor_dim(v_C(trialtype)|participant_id__factor_dim)&lt;U1'0' '1' '2' '3' ... '6' '7' '8' '9'<pre>array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='&lt;U1')</pre></li><li>C(participant_id)__factor_dim(C(participant_id)__factor_dim)&lt;U1'0' '1' '2' '3' ... '6' '7' '8' '9'<pre>array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='&lt;U1')</pre></li></ul></li><li>Data variables: (15)<ul><li>v_1|participant_id_mu(chain, draw)float640.06558 -0.199 ... -0.2883 -0.3764<pre>array([[ 6.55784082e-02, -1.99030260e-01, -1.68565054e-02,\n         1.72697639e-02, -2.21228331e-01, -1.86584550e-01,\n         1.78533579e-01,  5.60247492e-03,  2.28745092e-03,\n        -2.48191935e-01, -1.39172746e-01,  8.35486794e-02,\n         1.16125904e-01,  1.65374589e-01,  2.69841298e-01,\n         4.92429734e-01,  1.77543240e-01, -1.55580388e-01,\n         1.42904502e-01,  1.25477829e-01,  1.64463123e-01,\n        -3.35402405e-01,  1.63402149e-01,  4.14611275e-01,\n         1.65667202e-01, -7.92554530e-02, -4.29386581e-01,\n        -4.89906344e-01,  1.54197762e-01,  3.92154607e-02,\n         4.06450359e-01,  2.43711071e-01,  2.33680663e-01,\n         2.16814935e-01, -2.26412773e-01, -1.63822067e-02,\n         8.86349636e-02, -1.28302493e-01, -1.49410659e-01,\n        -2.08805172e-01, -1.41226925e-01,  2.86120157e-01,\n        -2.06626585e-01,  7.20010766e-02,  4.42417083e-01,\n        -2.09092065e-01, -1.29784640e-01,  2.17617621e-01,\n         1.09724109e-01,  3.55419855e-02, -4.60694975e-02,\n         6.54209739e-02,  1.57440511e-01,  9.32499659e-03,\n        -2.21603045e-02, -3.14658892e-01, -2.93388890e-01,\n         4.16791235e-02,  7.62605724e-02,  2.42896499e-01,\n...\n        -1.02490191e-01,  2.22554079e-02, -4.27783034e-02,\n        -1.13811801e-01, -9.77262115e-02,  1.56959470e-01,\n         2.65073660e-01, -6.11950497e-02, -2.51636089e-01,\n        -5.39073533e-01,  5.68830143e-01,  5.50974236e-02,\n         2.93636235e-01, -2.56399823e-01,  3.41974684e-01,\n        -9.96655026e-02, -2.88615515e-01,  2.13416151e-02,\n         8.04487085e-02, -7.22582378e-02,  1.00382861e-01,\n        -1.49622837e-02,  6.36173890e-02,  4.36329217e-01,\n         3.38131858e-01, -3.03404727e-01, -1.29983297e-01,\n        -3.82741918e-02, -2.48688312e-02, -1.09832225e-01,\n        -9.56744620e-02, -4.16365490e-01, -3.28403659e-01,\n         1.52910884e-01, -1.90089433e-01, -1.61785455e-01,\n        -3.24106005e-01, -8.91384580e-02, -7.84074218e-02,\n        -5.16543837e-02,  2.74304799e-02,  2.98410804e-01,\n         1.14407652e-01, -4.42392564e-01, -5.65564931e-01,\n         4.20000626e-01, -3.62169137e-01, -1.03997485e-01,\n         9.39491141e-02,  1.34595540e-01,  3.22078287e-01,\n         3.55127548e-01,  1.52247539e-01, -1.94319893e-01,\n         1.92517117e-01, -2.55463714e-01,  1.33069456e-01,\n        -2.88311309e-01, -3.76410401e-01]])</pre></li><li>v_1|participant_id(chain, draw, v_1|participant_id__factor_dim)float64-0.4468 -0.5708 ... 0.1264 0.09778<pre>array([[[-0.44683439, -0.57083488,  0.1446001 , ...,  0.28418131,\n          0.20163916,  0.07537987],\n        [-0.50041868, -0.5275415 ,  0.12705525, ...,  0.13361673,\n          0.11227681,  0.00305158],\n        [-0.32445369, -0.65388511,  0.08203591, ...,  0.26201331,\n          0.32518152,  0.21220081],\n        ...,\n        [-0.47317081, -0.52162647,  0.15598418, ...,  0.22540557,\n          0.12339786, -0.05596364],\n        [-0.28931   , -0.61231685,  0.14006568, ...,  0.28448317,\n          0.32193491,  0.31005244],\n        [-0.38691842, -0.52432882,  0.14666315, ...,  0.16884874,\n          0.14371246,  0.07978017]],\n\n       [[-0.40319298, -0.56826678,  0.11732421, ...,  0.2590544 ,\n          0.07146863,  0.16457154],\n        [-0.40062772, -0.61260959,  0.08855053, ...,  0.27476259,\n          0.12704059,  0.06356891],\n        [-0.45334737, -0.51199759,  0.12100992, ...,  0.09152681,\n          0.21314386,  0.17166266],\n        ...,\n        [-0.49701574, -0.53279198,  0.26386136, ...,  0.22899595,\n          0.16864552,  0.07290829],\n        [-0.3491537 , -0.54139657,  0.22965866, ...,  0.18341219,\n          0.2950146 ,  0.25144245],\n        [-0.50522406, -0.57882999,  0.10913765, ...,  0.18807718,\n          0.12644678,  0.09777709]]], shape=(2, 500, 10))</pre></li><li>t(chain, draw)float640.5032 0.5033 ... 0.5035 0.5036<pre>array([[0.50321387, 0.50327104, 0.50288065, 0.50810087, 0.50176372,\n        0.50480002, 0.51363263, 0.50683614, 0.49944515, 0.50926112,\n        0.50809259, 0.50063276, 0.50659954, 0.50802615, 0.50727894,\n        0.50673415, 0.50046378, 0.51517185, 0.50626673, 0.50996391,\n        0.50605799, 0.50633569, 0.50648109, 0.50339287, 0.5061993 ,\n        0.51263574, 0.50099101, 0.50815887, 0.50329619, 0.50262441,\n        0.50846642, 0.50811592, 0.5082643 , 0.50804953, 0.50352528,\n        0.5041455 , 0.50394286, 0.50075057, 0.51000702, 0.50994774,\n        0.50730573, 0.50593554, 0.50558422, 0.5018496 , 0.51171548,\n        0.50348262, 0.50082601, 0.51360683, 0.50529601, 0.50625895,\n        0.50009432, 0.5017552 , 0.50788432, 0.50828595, 0.5062623 ,\n        0.51178981, 0.51197102, 0.50566293, 0.50856348, 0.50818338,\n        0.50592476, 0.51038325, 0.50329087, 0.51035025, 0.50346515,\n        0.50036023, 0.50662168, 0.50872473, 0.50830109, 0.50805301,\n        0.50200095, 0.50466232, 0.50202029, 0.50629279, 0.50862571,\n        0.50817799, 0.51006466, 0.50645853, 0.50489679, 0.50292326,\n        0.51165489, 0.50651749, 0.50403015, 0.50616011, 0.50678814,\n        0.5053887 , 0.50445502, 0.49493295, 0.51325913, 0.50383988,\n        0.51164042, 0.50382046, 0.50983549, 0.50364712, 0.50363022,\n        0.51062218, 0.50450508, 0.51196886, 0.50764629, 0.50581662,\n...\n        0.49890948, 0.50439412, 0.50595728, 0.50891015, 0.51042768,\n        0.51114155, 0.50313476, 0.51186743, 0.5092632 , 0.51093235,\n        0.50508396, 0.50980407, 0.5015563 , 0.51000145, 0.50740268,\n        0.50368688, 0.50697959, 0.49832263, 0.51067648, 0.51286337,\n        0.49796336, 0.51249316, 0.50816054, 0.50566984, 0.5071684 ,\n        0.50823109, 0.50806847, 0.50489419, 0.50501601, 0.50505194,\n        0.50998594, 0.51138557, 0.50377538, 0.5043528 , 0.50551457,\n        0.50512258, 0.51024639, 0.50740386, 0.50455257, 0.50762316,\n        0.50647602, 0.51144674, 0.50434678, 0.50737785, 0.50924311,\n        0.5028223 , 0.50819817, 0.50516058, 0.50467686, 0.50914579,\n        0.50549465, 0.50427065, 0.50241705, 0.50492627, 0.51336726,\n        0.51005971, 0.5062819 , 0.50552909, 0.50605446, 0.51004885,\n        0.50471216, 0.51218712, 0.49747177, 0.51198315, 0.50786543,\n        0.51215602, 0.50960887, 0.51020235, 0.50349873, 0.5100099 ,\n        0.50099648, 0.5106742 , 0.50202907, 0.50100852, 0.50338723,\n        0.50452714, 0.50822825, 0.50367025, 0.5054304 , 0.50421753,\n        0.51262549, 0.498036  , 0.51061147, 0.50118429, 0.5038447 ,\n        0.50536197, 0.5081946 , 0.50683473, 0.50960996, 0.50815918,\n        0.50540879, 0.51161644, 0.51243156, 0.50586508, 0.51308177,\n        0.50264949, 0.51144339, 0.50051511, 0.50354915, 0.50364851]])</pre></li><li>v_C(trialtype)|participant_id_sigma(chain, draw, v_C(trialtype)|participant_id__expr_dim)float641.045 1.067 0.8241 ... 0.676 0.9779<pre>array([[[1.0450334 , 1.06661608],\n        [0.82406838, 1.19493741],\n        [0.77185652, 0.91355771],\n        ...,\n        [0.91640751, 0.82854729],\n        [0.9394795 , 0.88142148],\n        [0.8135634 , 0.78928493]],\n\n       [[0.9731705 , 0.97610052],\n        [1.02112629, 0.9526402 ],\n        [0.89013679, 0.93364199],\n        ...,\n        [0.59997962, 0.88265727],\n        [0.6275665 , 0.89830425],\n        [0.67596294, 0.97788216]]], shape=(2, 500, 2))</pre></li><li>p_outlier_1|C(participant_id)_sigma(chain, draw)float640.6033 0.8842 ... 0.7842 0.8726<pre>array([[0.60328537, 0.88415882, 0.81950246, 0.80223234, 0.94866962,\n        1.00242785, 0.78493696, 0.83444933, 0.71098072, 0.91172849,\n        0.73042531, 0.98145518, 0.95332724, 1.02595039, 1.0790353 ,\n        0.88172119, 0.81429883, 0.81265644, 1.16816126, 0.94540583,\n        1.12072665, 0.88097405, 0.81629079, 0.85696296, 0.97891325,\n        0.86625466, 0.70360561, 0.76509909, 0.75628571, 0.71415749,\n        0.7255272 , 0.77689685, 0.51270902, 0.75691369, 0.78537758,\n        0.97644941, 0.77563001, 0.97731291, 0.85320794, 0.94240257,\n        0.90180528, 0.98168105, 0.88045856, 0.85705614, 0.79719193,\n        0.8009389 , 1.05852116, 0.78880243, 0.7108462 , 0.81314243,\n        0.79183187, 0.82146289, 0.93848787, 1.19755118, 0.84312726,\n        0.72762322, 0.78274358, 0.68609485, 0.94893613, 0.8710282 ,\n        1.34934359, 1.26602253, 0.78743635, 0.78250821, 0.80173069,\n        0.96345028, 1.03303754, 1.06730504, 0.84569216, 0.98070025,\n        0.94272854, 0.88628941, 1.21113208, 0.81380357, 0.75760717,\n        0.79932195, 0.65357092, 0.68032235, 0.94429508, 0.84277513,\n        0.69107116, 0.7179778 , 0.84190509, 0.93312017, 1.02848351,\n        0.85027524, 0.89404398, 0.95136391, 0.69513712, 0.78086927,\n        0.72819271, 0.64842008, 0.75323654, 0.73072341, 0.75175046,\n        0.86251802, 1.09315682, 0.80179527, 1.09873263, 0.81968029,\n...\n        1.01437036, 0.97123474, 0.81242571, 0.7467827 , 0.79189925,\n        0.70617498, 0.80525623, 0.80915854, 0.95185046, 1.10755023,\n        0.80854258, 0.87330032, 0.97308366, 0.93335381, 0.81494079,\n        0.99176782, 1.24821205, 0.91325513, 0.90840785, 0.98488424,\n        0.86561768, 0.88437644, 1.03446246, 0.79283434, 1.00034092,\n        1.12020919, 0.93150668, 1.028101  , 1.01355795, 1.01977114,\n        0.71362403, 1.02129998, 0.86721225, 0.89161328, 1.19995477,\n        0.88786355, 0.87478268, 1.2275888 , 0.93118791, 0.69041071,\n        0.74647597, 0.78560173, 0.64070706, 0.60057007, 0.77190412,\n        0.89339961, 1.02575398, 0.76677164, 0.89931751, 0.86166664,\n        0.85978041, 0.69679628, 0.74224745, 0.73523869, 0.78600718,\n        0.87399865, 0.86190632, 0.8824343 , 1.03041135, 0.70169031,\n        0.86887921, 0.70118892, 0.98117486, 0.76987873, 0.72015193,\n        0.86964709, 0.8073351 , 0.95214688, 1.24295031, 1.24019236,\n        1.06539971, 1.02460592, 1.00848546, 1.00326974, 0.76358679,\n        0.74298593, 0.99101934, 1.30322998, 1.06239591, 0.96025135,\n        1.08876717, 1.34598373, 1.10852278, 0.99637011, 1.07211701,\n        1.04843151, 0.81810536, 0.73534109, 0.68555452, 0.78198716,\n        0.76459174, 0.9284753 , 0.92795957, 0.89127238, 0.76349428,\n        0.91842568, 0.71264543, 0.83630064, 0.78424942, 0.87260813]])</pre></li><li>a(chain, draw)float641.008 0.9912 0.9964 ... 1.001 1.001<pre>array([[1.00827428, 0.99119199, 0.9964241 , 0.98814098, 0.99663601,\n        1.01052479, 0.9868873 , 0.99116146, 1.00718639, 1.00196763,\n        0.98918464, 1.00327662, 0.99696991, 0.98722142, 0.99415651,\n        0.99437571, 1.00930465, 0.98242336, 0.98988525, 0.99267055,\n        0.99804202, 0.99208769, 0.99417303, 1.00088552, 1.00054765,\n        0.9782566 , 1.00170003, 0.99789072, 0.99104793, 1.00395108,\n        0.99155441, 0.99361757, 0.98841774, 0.99655677, 1.00602785,\n        1.00402568, 0.9978994 , 1.00350649, 0.98460303, 0.9842837 ,\n        0.99951796, 0.98957889, 1.00543143, 0.99529355, 0.99620619,\n        0.99291075, 0.99289456, 0.99642845, 0.97934294, 0.98466953,\n        1.00132991, 1.00557558, 0.99781012, 0.9879857 , 1.0004192 ,\n        0.98408398, 0.98625685, 0.99333929, 0.99257357, 0.99037381,\n        0.99703   , 0.98819694, 0.99952047, 0.98442033, 1.00584051,\n        0.99966988, 0.9853759 , 0.99448642, 0.99329404, 0.98676325,\n        1.00898123, 1.00507912, 0.99395695, 0.99739749, 0.99583656,\n        0.98790021, 0.98763808, 1.00084228, 0.99115735, 0.99789543,\n        0.99401325, 0.98733975, 0.99676922, 0.99819554, 0.99549718,\n        0.99610906, 1.00361195, 1.02138902, 0.98916605, 0.9893648 ,\n        0.99616329, 0.99251125, 0.99856575, 0.99895161, 0.99797565,\n        0.99052109, 1.00328254, 0.98659062, 0.98916525, 0.99685526,\n...\n        1.00514426, 1.00244256, 0.99837911, 0.99744551, 0.98610604,\n        0.99000882, 1.00204673, 0.98709397, 0.9826581 , 0.98391138,\n        0.9888155 , 0.98432323, 1.00266756, 0.99508575, 0.99003534,\n        1.00191854, 0.99501472, 1.01154888, 0.98795694, 0.98515901,\n        1.0116787 , 0.98096339, 0.99618323, 0.99093324, 0.99699369,\n        0.98876929, 0.99425468, 0.9932754 , 0.99260528, 0.99665057,\n        0.9849775 , 0.98595919, 0.99818785, 1.00337287, 0.99732555,\n        0.99193013, 0.99570375, 0.98418224, 0.99216446, 0.99968046,\n        0.99317392, 0.98886888, 0.99674541, 0.98920277, 0.99475715,\n        1.00024938, 0.99433467, 0.99233931, 1.00124466, 0.98981393,\n        0.99103297, 1.00020573, 1.00112535, 1.00652707, 0.98453321,\n        0.98646226, 0.99666818, 0.99494946, 0.99350015, 0.99637391,\n        1.00759676, 0.97893936, 1.01189309, 0.9820839 , 0.99319971,\n        0.99424943, 0.980493  , 0.98952454, 0.99275744, 1.00045335,\n        0.9958843 , 0.99693745, 0.99187514, 0.9867891 , 1.01207862,\n        0.98922575, 1.00265909, 0.99337099, 1.00428052, 1.00710848,\n        0.98138896, 1.00723646, 0.99263102, 1.00526386, 1.00182591,\n        0.99500336, 0.99139127, 0.99849441, 0.98646374, 0.99627181,\n        0.99615445, 0.99160986, 0.99482607, 0.99065389, 0.98383782,\n        1.00734688, 0.97920829, 1.00846464, 1.00119476, 1.00109947]])</pre></li><li>p_outlier_1|C(participant_id)_offset(chain, draw, C(participant_id)__factor_dim)float64-1.529 -3.001 ... -0.658 0.507<pre>array([[[-1.52859431, -3.00081989,  0.70325016, ...,  1.08062995,\n         -1.87384877,  0.58114809],\n        [-1.77401161, -2.93869292,  0.20190234, ...,  0.18553215,\n         -1.19995749,  0.42434624],\n        [-1.59168242, -2.7350334 ,  0.63841226, ...,  0.68112651,\n         -1.0362473 ,  0.77435563],\n        ...,\n        [-1.06624563, -3.33843309,  0.87428162, ...,  0.61173962,\n         -0.28607807,  1.33989953],\n        [-0.70297839, -1.48869727,  0.79595795, ...,  0.86295319,\n         -1.00007392,  1.60084131],\n        [-1.62981869, -3.49803014,  0.47499008, ...,  0.46428441,\n         -0.62791088,  1.02757986]],\n\n       [[-0.85701367, -2.57603509,  0.64074554, ...,  0.63432375,\n         -0.72413192,  0.60974549],\n        [-0.99237403, -2.46574765,  0.40633584, ...,  0.71279967,\n         -0.84911977,  0.62782312],\n        [-1.07274098, -1.57550893,  0.15417928, ...,  0.68293071,\n         -0.81589448,  0.46879669],\n        ...,\n        [-1.40410904, -3.18928007,  0.57365769, ...,  0.63555135,\n         -1.06704398,  0.42669425],\n        [-1.60563612, -2.43186003,  0.57800998, ...,  0.78769743,\n         -1.52352201,  0.65738051],\n        [-1.12251326, -2.90411268,  0.6781353 , ...,  0.7293375 ,\n         -0.65800372,  0.507023  ]]], shape=(2, 500, 10))</pre></li><li>z(chain, draw)float640.5091 0.5052 ... 0.5038 0.5007<pre>array([[0.50907526, 0.50522722, 0.49843665, 0.50841066, 0.49784957,\n        0.49781986, 0.50310421, 0.49656011, 0.50219061, 0.49744417,\n        0.50386029, 0.50062671, 0.50273652, 0.51503662, 0.50329009,\n        0.49959442, 0.49897391, 0.4982927 , 0.5022283 , 0.50261572,\n        0.49403338, 0.49979802, 0.49746112, 0.50530158, 0.5007692 ,\n        0.49921897, 0.50267621, 0.49759364, 0.50610308, 0.50154174,\n        0.49893825, 0.49806402, 0.49891693, 0.49595798, 0.50643025,\n        0.50924681, 0.50821779, 0.509791  , 0.50050075, 0.50035244,\n        0.49945756, 0.50221782, 0.49973114, 0.49394707, 0.49897174,\n        0.50126007, 0.50379818, 0.50021377, 0.50382638, 0.49838452,\n        0.50615391, 0.50328426, 0.50192866, 0.50412023, 0.49646806,\n        0.50454773, 0.50478748, 0.50887255, 0.50010327, 0.50235451,\n        0.49831575, 0.50075653, 0.50177781, 0.50202368, 0.50187022,\n        0.49844503, 0.50328905, 0.50168768, 0.50674001, 0.50325552,\n        0.50187683, 0.49667116, 0.50604168, 0.51015734, 0.50363498,\n        0.50429179, 0.50887932, 0.49998776, 0.50610516, 0.49808017,\n        0.49191007, 0.496711  , 0.50148696, 0.4967017 , 0.50560829,\n        0.49798566, 0.49161596, 0.49743396, 0.51100367, 0.50293495,\n        0.5015905 , 0.50079727, 0.49945918, 0.50114836, 0.50263445,\n        0.50119808, 0.49569923, 0.50110428, 0.50342585, 0.50787965,\n...\n        0.50295392, 0.50284746, 0.50260425, 0.50425429, 0.50069327,\n        0.49342694, 0.51276839, 0.50373323, 0.49736068, 0.49599901,\n        0.50369913, 0.49341577, 0.50160524, 0.49111839, 0.51026016,\n        0.50500197, 0.50239204, 0.50529017, 0.49726665, 0.49448106,\n        0.50474329, 0.49841594, 0.50412002, 0.5080594 , 0.49767469,\n        0.50162658, 0.50285453, 0.50796939, 0.50805982, 0.49638362,\n        0.49985551, 0.49829408, 0.50393916, 0.5059218 , 0.5004735 ,\n        0.50284989, 0.50132884, 0.50201552, 0.50323524, 0.49739984,\n        0.50539512, 0.4989157 , 0.50078144, 0.50608571, 0.50961563,\n        0.50805756, 0.49897027, 0.50359845, 0.49902168, 0.50039856,\n        0.50145686, 0.49895601, 0.50278613, 0.4994125 , 0.50227443,\n        0.49685693, 0.50531528, 0.50536968, 0.49845245, 0.50088159,\n        0.50008184, 0.50083991, 0.50272711, 0.49841462, 0.50130171,\n        0.49913365, 0.49869388, 0.4984984 , 0.50077961, 0.49308891,\n        0.50523475, 0.4951056 , 0.50119276, 0.502516  , 0.50262765,\n        0.5040908 , 0.49525348, 0.50550588, 0.50206523, 0.50412873,\n        0.49792583, 0.5016798 , 0.49896366, 0.49937486, 0.50357401,\n        0.5015416 , 0.49753403, 0.50731068, 0.50403558, 0.49159652,\n        0.49543872, 0.4935486 , 0.49503655, 0.49698387, 0.49851264,\n        0.50295859, 0.50042883, 0.50341546, 0.50377805, 0.50071806]])</pre></li><li>p_outlier_1|C(participant_id)(chain, draw, C(participant_id)__factor_dim)float64-0.9222 -1.81 ... -0.5742 0.4424<pre>array([[[-0.92217858, -1.81035072,  0.42426053, ...,  0.65192824,\n         -1.13046554,  0.35059814],\n        [-1.56850801, -2.59827126,  0.17851373, ...,  0.16403989,\n         -1.060953  ,  0.37518947],\n        [-1.30438766, -2.24136661,  0.52318042, ...,  0.55818486,\n         -0.84920721,  0.63458635],\n        ...,\n        [-0.71634343, -2.24288338,  0.58737488, ...,  0.4109894 ,\n         -0.19219788,  0.90019429],\n        [-0.51337492, -1.08717403,  0.58127655, ...,  0.6302022 ,\n         -0.73033948,  1.16907119],\n        [-1.50077052, -3.22105799,  0.43738062, ...,  0.42752262,\n         -0.57819323,  0.94621664]],\n\n       [[-0.80795248, -2.42856565,  0.60406499, ...,  0.59801083,\n         -0.68267778,  0.57483959],\n        [-0.84916083, -2.10990641,  0.347696  , ...,  0.60993289,\n         -0.72658013,  0.53721962],\n        [-1.19261414, -1.75156377,  0.171408  , ...,  0.75924463,\n         -0.9070664 ,  0.52118225],\n        ...,\n        [-1.1742573 , -2.66719698,  0.4797503 , ...,  0.531512  ,\n         -0.89236957,  0.35684468],\n        [-1.25921919, -1.90718482,  0.45330399, ...,  0.61775125,\n         -1.19482125,  0.51555028],\n        [-0.9795142 , -2.53415234,  0.59174638, ...,  0.63642583,\n         -0.5741794 ,  0.44243239]]], shape=(2, 500, 10))</pre></li><li>v_1|participant_id_offset(chain, draw, v_1|participant_id__factor_dim)float64-1.123 -1.434 ... 0.2632 0.2035<pre>array([[[-1.12273672, -1.43430608,  0.36332888, ...,  0.71404708,\n          0.50664786,  0.18940295],\n        [-1.51947789, -1.60183398,  0.38579224, ...,  0.4057156 ,\n          0.34091879,  0.00926587],\n        [-0.62359633, -1.25675979,  0.1576721 , ...,  0.50358662,\n          0.62499521,  0.40784756],\n        ...,\n        [-0.78578795, -0.86625755,  0.25904068, ...,  0.37432778,\n          0.20492504, -0.09293801],\n        [-0.50457703, -1.06792376,  0.24428442, ...,  0.49615871,\n          0.56147718,  0.54075331],\n        [-0.9735064 , -1.31923795,  0.36901194, ...,  0.42483201,\n          0.36158784,  0.20073096]],\n\n       [[-0.85695968, -1.20781299,  0.24936475, ...,  0.55060278,\n          0.15190178,  0.34978578],\n        [-0.84674743, -1.2947821 ,  0.18715613, ...,  0.58072498,\n          0.26850687,  0.13435618],\n        [-0.94728901, -1.06984119,  0.25285548, ...,  0.19124924,\n          0.44537335,  0.35869658],\n        ...,\n        [-1.03337962, -1.10776446,  0.5486123 , ...,  0.47612124,\n          0.35064251,  0.15158864],\n        [-0.61044412, -0.94655263,  0.40152453, ...,  0.32066936,\n          0.51578984,  0.43961032],\n        [-1.05167853, -1.20489723,  0.22718183, ...,  0.39150299,\n          0.26321265,  0.20353359]]], shape=(2, 500, 10))</pre></li><li>v_C(trialtype)|participant_id_mu(chain, draw, v_C(trialtype)|participant_id__expr_dim)float64-0.1672 0.2117 ... 0.2513 -0.06343<pre>array([[[-0.16722805,  0.21171641],\n        [ 0.17712932, -0.16131414],\n        [ 0.0902443 ,  0.06914141],\n        ...,\n        [-0.08679806, -0.4356106 ],\n        [ 0.03069077,  0.44901746],\n        [ 0.08924784, -0.17810316]],\n\n       [[ 0.1553901 ,  0.72351926],\n        [ 0.14381198,  0.70442786],\n        [-0.54057402, -0.01462775],\n        ...,\n        [-0.43923828,  0.06372193],\n        [ 0.05009442,  0.13599765],\n        [ 0.25130221, -0.06343392]]], shape=(2, 500, 2))</pre></li><li>v_C(trialtype)|participant_id_offset(chain, draw, v_C(trialtype)|participant_id__expr_dim, v_C(trialtype)|participant_id__factor_dim)float64-0.9113 -1.057 ... 1.103 0.99<pre>array([[[[-0.91130033, -1.05717064, -1.04800779, ..., -1.07260431,\n          -0.97627255, -0.96432321],\n         [ 1.26954662,  1.03036908,  0.79226616, ...,  0.89148533,\n           1.04811205,  1.08893746]],\n\n        [[-0.87429229, -1.44494337, -1.23759654, ..., -1.1166454 ,\n          -1.28456444, -1.01805139],\n         [ 1.04716821,  0.86110089,  0.83139054, ...,  0.80765815,\n           0.95333882,  0.9163853 ]],\n\n        [[-1.11184983, -1.20539259, -1.30853152, ..., -1.52842958,\n          -1.70182013, -1.42416241],\n         [ 1.07634713,  1.31390959,  1.05579951, ...,  0.90429765,\n           1.06313547,  0.95457411]],\n\n        ...,\n\n        [[-0.9060125 , -1.29080099, -1.2241762 , ..., -1.07308668,\n          -1.08435427, -0.93016097],\n         [ 1.29254982,  1.10855004,  1.03161788, ...,  1.03197277,\n...\n          -1.33231937, -1.17768398],\n         [ 1.32641782,  0.91978447,  1.0301866 , ...,  0.90022125,\n           1.20846395,  1.0247765 ]],\n\n        ...,\n\n        [[-1.3055549 , -1.71867396, -2.19090753, ..., -1.56599903,\n          -1.71882003, -1.58108431],\n         [ 1.43049873,  1.09638624,  0.95211565, ...,  1.05964563,\n           1.25680421,  1.094131  ]],\n\n        [[-1.49395789, -2.15991557, -1.63703587, ..., -1.68185127,\n          -1.82508493, -1.78207404],\n         [ 1.18948307,  1.1629844 ,  0.91451451, ...,  1.01171704,\n           1.19720296,  1.17987297]],\n\n        [[-0.9631702 , -1.29852159, -1.70813096, ..., -1.49770218,\n          -1.48606837, -1.33603582],\n         [ 1.34595393,  1.05648807,  0.93349989, ...,  0.84482399,\n           1.103158  ,  0.9900352 ]]]], shape=(2, 500, 2, 10))</pre></li><li>p_outlier_Intercept(chain, draw)float64-2.156 -1.929 ... -2.248 -2.344<pre>array([[-2.15597522, -1.92914419, -2.27660139, -2.19662247, -2.24279709,\n        -2.12877494, -2.45530068, -2.52506045, -2.41290711, -2.24233487,\n        -2.27521835, -1.97996626, -2.13633484, -2.32405638, -2.07740467,\n        -2.2580938 , -2.23951473, -2.40933463, -2.38898678, -2.29265387,\n        -1.95498963, -1.91223503, -2.06001222, -2.49597739, -2.64858212,\n        -2.86590514, -2.668219  , -2.74689617, -2.82709948, -2.68087397,\n        -2.67859832, -2.75329325, -2.31803095, -2.7886679 , -2.61812833,\n        -2.88393768, -2.65295811, -2.85127901, -2.45409866, -2.44249389,\n        -2.25751605, -2.6620752 , -2.36444889, -2.35908773, -2.35430039,\n        -2.39001923, -2.49488049, -2.36437032, -2.5806005 , -2.33478408,\n        -2.37146552, -2.32603293, -2.6851422 , -2.56068287, -2.56870884,\n        -2.42031008, -2.35738892, -2.35441811, -2.32239013, -2.43290077,\n        -2.64582338, -2.858386  , -2.59150933, -2.60477232, -2.19811424,\n        -2.38920409, -2.30166236, -2.24695205, -1.98870814, -2.92282616,\n        -2.32255246, -2.13110492, -1.92285495, -2.59049615, -2.85248452,\n        -2.90888143, -2.88266268, -2.58977481, -2.56678965, -2.42580141,\n        -2.07874746, -2.29377307, -2.41123767, -2.28846214, -2.25254164,\n        -2.60380623, -2.71333525, -2.68452758, -2.36629424, -2.57903036,\n        -2.61623244, -2.38540406, -2.22753038, -2.11849326, -2.19115461,\n        -2.1172459 , -2.04056521, -2.62362171, -2.75022279, -2.166842  ,\n...\n        -1.99294245, -2.15548444, -2.69011758, -2.48180365, -2.70262431,\n        -2.81941594, -2.0185356 , -2.05592597, -1.99374305, -1.96265413,\n        -1.95517329, -1.9849407 , -1.98117569, -2.39340538, -1.93735318,\n        -2.04752569, -2.26141297, -2.12109816, -1.72498579, -1.67484   ,\n        -2.55834229, -2.17504813, -2.25774125, -2.22361153, -2.36595403,\n        -2.47449031, -2.21797011, -2.49478823, -2.48784212, -2.39205435,\n        -2.20273749, -2.01036236, -2.11520742, -2.59270231, -2.25773818,\n        -2.64980238, -2.74665431, -1.88998209, -2.37486431, -2.31785307,\n        -2.45779833, -2.53213496, -2.61224274, -2.61217028, -1.89047487,\n        -2.09681349, -2.21580982, -2.36771527, -2.82478157, -2.53043088,\n        -2.6229486 , -2.39447705, -2.34840609, -2.32896004, -2.56279283,\n        -2.44316963, -2.64345401, -2.49439355, -2.59146417, -2.69174521,\n        -2.64788383, -2.68278612, -2.27917723, -2.63199496, -2.50568803,\n        -2.51468235, -2.30285083, -2.33470613, -2.62515492, -2.56889359,\n        -2.40840443, -2.4985178 , -2.57481475, -2.55010583, -2.53540185,\n        -2.61118961, -2.80815908, -2.87661999, -2.8221422 , -2.81287861,\n        -2.28051385, -2.25485479, -2.25165836, -2.17534328, -1.88227509,\n        -1.92640634, -2.00569303, -2.14012969, -2.13899013, -2.12516613,\n        -2.00367711, -1.99055892, -2.04919197, -2.49586718, -2.47381423,\n        -2.68020116, -2.85436354, -2.09277636, -2.24801634, -2.34393571]])</pre></li><li>v_C(trialtype)|participant_id(chain, draw, v_C(trialtype)|participant_id__expr_dim, v_C(trialtype)|participant_id__factor_dim)float64-0.9523 -1.105 ... 1.079 0.9681<pre>array([[[[-0.95233928, -1.10477862, -1.09520314, ..., -1.12090733,\n          -1.02023742, -1.00774996],\n         [ 1.35411884,  1.09900822,  0.84504382, ...,  0.95087259,\n           1.11793317,  1.16147821]],\n\n        [[-0.72047663, -1.19073214, -1.01986417, ..., -0.92019217,\n          -1.05856894, -0.83894396],\n         [ 1.25130047,  1.02896167,  0.99345966, ...,  0.96510093,\n           1.13918022,  1.09502307]],\n\n        [[-0.85818854, -0.93039013, -1.00999859, ..., -1.17972834,\n          -1.31356097, -1.09924905],\n         [ 0.98330522,  1.20033224,  0.96453378, ...,  0.82612809,\n           0.97123561,  0.87205854]],\n\n        ...,\n\n        [[-0.83027665, -1.18289972, -1.12184426, ..., -0.98338469,\n          -0.99371039, -0.8524065 ],\n         [ 1.07093866,  0.91848613,  0.8547442 , ...,  0.85503824,\n...\n          -1.18594649, -1.04829985],\n         [ 1.23839937,  0.8587494 ,  0.96182547, ...,  0.84048436,\n           1.12827269,  0.95677437]],\n\n        ...,\n\n        [[-0.78330633, -1.03116935, -1.31449987, ..., -0.9395675 ,\n          -1.03125699, -0.94861837],\n         [ 1.2626401 ,  0.96773328,  0.84039179, ...,  0.93530392,\n           1.10932737,  0.96574268]],\n\n        [[-0.93755791, -1.35549064, -1.02734886, ..., -1.05547351,\n          -1.14536215, -1.11836996],\n         [ 1.06851769,  1.04471383,  0.82151226, ...,  0.90882972,\n           1.0754525 ,  1.0598849 ]],\n\n        [[-0.65106736, -0.87775247, -1.15463322, ..., -1.01239116,\n          -1.00452714, -0.9031107 ],\n         [ 1.31618433,  1.03312083,  0.91285288, ...,  0.8261383 ,\n           1.07875852,  0.96813776]]]], shape=(2, 500, 2, 10))</pre></li><li>v_1|participant_id_sigma(chain, draw)float640.398 0.3293 ... 0.572 0.4804<pre>array([[0.3979868 , 0.32933594, 0.52029442, 0.42819384, 0.42729182,\n        0.3643724 , 0.56182136, 0.45912385, 0.42153848, 0.32569417,\n        0.45317658, 0.49011546, 0.45061999, 0.45397927, 0.40939949,\n        0.40573562, 0.3314505 , 0.33098722, 0.32850394, 0.34852727,\n        0.4161696 , 0.39982608, 0.38096634, 0.30765596, 0.34467893,\n        0.46751969, 0.47415263, 0.42140784, 0.44638106, 0.42817908,\n        0.42933829, 0.45543281, 0.36800215, 0.39914948, 0.44904628,\n        0.37450812, 0.48934611, 0.50928904, 0.37922091, 0.40765049,\n        0.4104457 , 0.49704508, 0.61404095, 0.55840902, 0.56050524,\n        0.61176122, 0.48762216, 0.33518767, 0.36727743, 0.29637743,\n        0.54467728, 0.5231622 , 0.50729212, 0.53765312, 0.65558449,\n        0.66633581, 0.70448123, 0.92957223, 0.84029306, 0.82642217,\n        0.67489531, 0.80645983, 0.68944771, 0.5483914 , 0.53474121,\n        0.5811727 , 0.44334624, 0.47180089, 0.42971432, 0.39926776,\n        0.60347956, 0.37904013, 0.34034675, 0.45718583, 0.52852941,\n        0.578508  , 0.55155675, 0.6152597 , 0.50861047, 0.47834241,\n        0.59197849, 0.65315302, 0.56721444, 0.56630229, 0.51423798,\n        0.45695911, 0.40606294, 0.49694156, 0.39313365, 0.41985871,\n        0.49288797, 0.49214982, 0.36271614, 0.34570292, 0.37643242,\n        0.37114374, 0.50281711, 0.37930592, 0.41470644, 0.50707687,\n...\n        0.3939573 , 0.41249924, 0.3544989 , 0.33287899, 0.34449758,\n        0.32237592, 0.43571218, 0.47560047, 0.51540517, 0.51433227,\n        0.43359615, 0.40360078, 0.27445566, 0.62217104, 0.60695308,\n        0.61319197, 0.53275449, 0.5819923 , 0.35599382, 0.37060172,\n        0.3735807 , 0.36815088, 0.33880878, 0.3825467 , 0.34910413,\n        0.31096672, 0.32605089, 0.32249104, 0.32718263, 0.27615842,\n        0.61042364, 0.5465735 , 0.39608367, 0.32195674, 0.37881344,\n        0.37514877, 0.38913489, 0.52632365, 0.59402137, 0.50932707,\n        0.44906556, 0.49922105, 0.52096307, 0.52717668, 0.4303631 ,\n        0.46231282, 0.41819654, 0.41861819, 0.41691553, 0.43756577,\n        0.41458066, 0.43588102, 0.32943014, 0.39509044, 0.4938051 ,\n        0.5214652 , 0.59676008, 0.52158961, 0.52987845, 0.58868598,\n        0.51113836, 0.57071532, 0.41544596, 0.36168942, 0.30692826,\n        0.32128409, 0.35879239, 0.41043493, 0.42716313, 0.44615748,\n        0.45273156, 0.36427702, 0.45279313, 0.45994514, 0.62512567,\n        0.47415751, 0.449387  , 0.49261915, 0.49704331, 0.45989254,\n        0.41384225, 0.51186444, 0.49878239, 0.38938962, 0.3568318 ,\n        0.56227522, 0.55634914, 0.43510076, 0.43243072, 0.38228025,\n        0.37297985, 0.41456323, 0.42553742, 0.45424908, 0.40956618,\n        0.33426429, 0.4066109 , 0.48096143, 0.57196668, 0.48039781]])</pre></li></ul></li><li>Indexes: (6)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>v_1|participant_id__factor_dimPandasIndex<pre>PandasIndex(Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='object', name='v_1|participant_id__factor_dim'))</pre></li><li>v_C(trialtype)|participant_id__expr_dimPandasIndex<pre>PandasIndex(Index(['someswitch', 'switch'], dtype='object', name='v_C(trialtype)|participant_id__expr_dim'))</pre></li><li>v_C(trialtype)|participant_id__factor_dimPandasIndex<pre>PandasIndex(Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='object', name='v_C(trialtype)|participant_id__factor_dim'))</pre></li><li>C(participant_id)__factor_dimPandasIndex<pre>PandasIndex(Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='object', name='C(participant_id)__factor_dim'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T18:57:44.208796+00:00arviz_version :0.22.0inference_library :numpyroinference_library_version :0.19.0sampling_time :1494.128823tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 60MB\nDimensions:      (chain: 2, draw: 500, __obs__: 7500)\nCoordinates:\n  * chain        (chain) int64 16B 0 1\n  * draw         (draw) int64 4kB 0 1 2 3 4 5 6 ... 493 494 495 496 497 498 499\n  * __obs__      (__obs__) int64 60kB 0 1 2 3 4 5 ... 7495 7496 7497 7498 7499\nData variables:\n    rt,response  (chain, draw, __obs__) float64 60MB -1.594 -0.09011 ... -1.824\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li><li>__obs__: 7500</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 ... 7496 7497 7498 7499<pre>array([   0,    1,    2, ..., 7497, 7498, 7499], shape=(7500,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-1.594 -0.09011 ... -5.036 -1.824<pre>array([[[-1.59389903, -0.09010676, -3.94874062, ..., -0.56139782,\n         -4.95331381, -1.83645019],\n        [-1.58820515, -0.17442989, -3.85242505, ..., -0.64749003,\n         -4.7414627 , -1.88198676],\n        [-1.56863109, -0.25721716, -3.72632893, ..., -0.61476299,\n         -4.81470218, -1.88507726],\n        ...,\n        [-1.60705242, -0.28562487, -3.70318811, ..., -0.61102643,\n         -4.73505745, -1.89593436],\n        [-1.57638521, -0.19747739, -3.85031825, ..., -0.68431784,\n         -4.59973929, -1.93218779],\n        [-1.58934621, -0.08477123, -3.98375003, ..., -0.67696902,\n         -4.55939207, -1.88967752]],\n\n       [[-1.56904085, -0.21271813, -3.76521573, ..., -0.63202709,\n         -4.8492619 , -1.84789398],\n        [-1.57748303, -0.27089844, -3.69838915, ..., -0.57454288,\n         -4.81656297, -1.90921989],\n        [-1.57151823, -0.15645715, -3.85346298, ..., -0.58955141,\n         -4.94691952, -1.84862079],\n        ...,\n        [-1.56078339, -0.1921747 , -3.77283193, ..., -0.60202203,\n         -4.89397204, -1.83930294],\n        [-1.56424837, -0.20845067, -3.76442664, ..., -0.60433556,\n         -4.89102648, -1.84868478],\n        [-1.5688645 , -0.15459483, -3.85797104, ..., -0.61246445,\n         -5.03641346, -1.82357344]]], shape=(2, 500, 7500))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499],\n      dtype='int64', name='__obs__', length=7500))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 53kB\nDimensions:          (chain: 2, draw: 500)\nCoordinates:\n  * chain            (chain) int64 16B 0 1\n  * draw             (draw) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\nData variables:\n    acceptance_rate  (chain, draw) float64 8kB 0.8577 1.0 ... 0.7922 0.9614\n    step_size        (chain, draw) float64 8kB 0.1055 0.1055 ... 0.1125 0.1125\n    diverging        (chain, draw) bool 1kB False False False ... False False\n    energy           (chain, draw) float64 8kB 1.107e+04 1.107e+04 ... 1.107e+04\n    n_steps          (chain, draw) int64 8kB 31 63 63 95 31 ... 31 63 95 31 63\n    tree_depth       (chain, draw) int64 8kB 5 6 6 7 5 5 5 5 ... 5 6 5 5 6 7 5 6\n    lp               (chain, draw) float64 8kB 1.104e+04 1.104e+04 ... 1.104e+04\nAttributes:\n    created_at:                  2025-09-27T18:57:44.266359+00:00\n    arviz_version:               0.22.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (7)<ul><li>acceptance_rate(chain, draw)float640.8577 1.0 0.9794 ... 0.7922 0.9614<pre>array([[0.85766441, 1.        , 0.97941993, 0.87799001, 0.66057002,\n        0.96100212, 0.71212282, 0.91431139, 0.98439777, 0.96850974,\n        0.67003686, 0.99942566, 0.93397179, 0.83739976, 0.98292886,\n        0.95618858, 0.9737181 , 0.95568065, 0.96711451, 0.73367069,\n        0.81202877, 0.85139189, 0.85470858, 0.98129661, 0.99939167,\n        0.94122111, 0.98460493, 0.92795437, 0.97627153, 0.63145554,\n        0.99776376, 0.76188792, 0.83012651, 0.98870474, 0.94795379,\n        0.91120105, 0.99146732, 0.75723983, 0.96834659, 0.83428521,\n        0.98305923, 1.        , 0.91144521, 0.84150386, 0.94586234,\n        0.93480691, 0.91764447, 0.93576869, 0.98997731, 0.92650619,\n        0.98540715, 0.99402726, 1.        , 0.94417014, 0.96781696,\n        0.92167515, 0.84653678, 0.89304995, 0.94269115, 0.97811468,\n        0.97463711, 0.85695632, 0.97027902, 0.70439679, 0.9208118 ,\n        0.81854874, 0.99321429, 0.92096722, 0.8821967 , 0.94713224,\n        0.98948606, 0.90526953, 0.83147268, 0.81381233, 0.95526749,\n        1.        , 0.97370864, 0.99452445, 0.87707376, 0.78378871,\n        0.98229991, 0.95460942, 0.9399773 , 0.83738198, 0.9637077 ,\n        0.82187325, 0.92633183, 0.9689411 , 0.87407857, 0.8065382 ,\n        0.90297387, 0.88231709, 0.699969  , 0.789111  , 0.93002367,\n        0.98230589, 0.85305498, 0.9185434 , 0.8914941 , 0.980289  ,\n...\n        0.99958641, 0.72937337, 0.79931363, 0.71824665, 0.99251741,\n        0.66177884, 1.        , 0.94106299, 0.9826299 , 0.9369173 ,\n        0.76671347, 0.91891863, 0.98955086, 0.87154907, 0.9096578 ,\n        0.9656679 , 0.97739219, 0.64182075, 0.9398039 , 0.80741272,\n        0.62891792, 0.77051745, 0.95991532, 0.90597797, 0.88380578,\n        0.86426301, 0.82445844, 0.99990054, 0.98938658, 0.94807151,\n        0.9654385 , 0.96639257, 0.89736782, 0.77355535, 0.88490399,\n        0.99812299, 0.81904711, 0.75014537, 0.99519004, 0.709598  ,\n        0.98868663, 0.99667575, 0.84491292, 0.94330746, 0.94983765,\n        0.70238463, 0.85075742, 0.92651437, 0.68493659, 0.99967254,\n        0.7794337 , 0.94266607, 0.79431317, 0.92181297, 0.98966449,\n        0.97523705, 0.8372493 , 0.9857852 , 0.9959557 , 0.97991825,\n        0.9575515 , 0.84896225, 0.74527422, 0.79474866, 0.98998571,\n        0.98027199, 0.91580517, 0.9109444 , 0.98062794, 0.73282395,\n        0.94511211, 0.86209983, 0.91972691, 0.94575144, 0.99596229,\n        0.97525017, 0.65380117, 1.        , 0.88945114, 0.99024914,\n        0.9182207 , 0.86306543, 0.98239812, 0.96034461, 0.93171671,\n        0.7918796 , 0.7904338 , 0.44392622, 0.97218707, 0.96578567,\n        0.82476109, 0.94087043, 0.99449092, 0.94136052, 0.97550418,\n        0.96690207, 0.94125167, 0.82559156, 0.79220335, 0.96141244]])</pre></li><li>step_size(chain, draw)float640.1055 0.1055 ... 0.1125 0.1125<pre>array([[0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n...\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486]])</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False]])</pre></li><li>energy(chain, draw)float641.107e+04 1.107e+04 ... 1.107e+04<pre>array([[11069.72368448, 11069.99055974, 11060.17912799, 11065.87275656,\n        11065.17725383, 11073.00822958, 11072.56282181, 11065.90036668,\n        11074.01471057, 11074.02759712, 11074.60090829, 11061.77974583,\n        11061.74333982, 11069.71852431, 11061.18763172, 11056.57484786,\n        11068.17057838, 11060.81349472, 11062.03683338, 11067.95569775,\n        11069.43382097, 11067.19292502, 11067.5952454 , 11078.27721652,\n        11075.18363953, 11068.93254623, 11072.43964206, 11069.59574996,\n        11076.11063922, 11080.33578243, 11069.44015719, 11076.9984568 ,\n        11075.00459531, 11079.77475974, 11078.59462959, 11077.03081183,\n        11060.51829603, 11070.7707861 , 11063.36552368, 11062.71554979,\n        11065.6421888 , 11059.62561047, 11066.8219877 , 11061.89081748,\n        11064.22972911, 11069.40999675, 11080.59457726, 11069.57932765,\n        11073.31115437, 11098.81450071, 11080.08089277, 11076.95429519,\n        11062.81965197, 11057.53431948, 11054.41647894, 11054.24543696,\n        11068.66657791, 11069.58132486, 11060.13733109, 11061.14100781,\n        11060.96790864, 11060.24072386, 11064.41010506, 11060.87465802,\n        11061.58341607, 11062.46548002, 11057.0647316 , 11063.33697028,\n        11061.67229866, 11063.83034433, 11060.52964354, 11065.78624276,\n        11063.49087112, 11075.61255857, 11079.10743628, 11070.86635609,\n        11063.81262879, 11065.48736106, 11061.85819016, 11056.91322151,\n...\n        11080.16710126, 11076.41339419, 11064.64901733, 11059.68434001,\n        11072.32934028, 11067.85975949, 11064.83847131, 11066.56615989,\n        11072.87062282, 11073.87043322, 11084.9073876 , 11067.86534434,\n        11072.5685314 , 11077.86493268, 11067.1497906 , 11067.24853909,\n        11065.35417743, 11078.1156764 , 11069.26640797, 11065.36493917,\n        11065.54909509, 11058.20240321, 11062.06436604, 11066.07649883,\n        11077.28446721, 11076.222642  , 11065.04960116, 11069.44267398,\n        11076.24288043, 11070.94872914, 11071.02651576, 11070.1502479 ,\n        11071.60153729, 11064.22943501, 11069.82743172, 11058.17205561,\n        11067.33926351, 11067.38138979, 11055.66427831, 11056.19343645,\n        11059.26618897, 11056.59561578, 11065.02031205, 11075.78907549,\n        11069.98480353, 11079.92337667, 11060.86091256, 11060.24511823,\n        11064.88571298, 11063.90367491, 11061.7600543 , 11071.97545387,\n        11072.89976367, 11068.94900723, 11070.20047203, 11068.08580274,\n        11067.65932332, 11068.19882487, 11061.53361396, 11052.75080098,\n        11052.98455683, 11056.75994931, 11058.10025072, 11059.94778709,\n        11066.71300564, 11071.88587957, 11077.776286  , 11082.28370729,\n        11077.25876279, 11064.02236418, 11065.8127158 , 11064.44136202,\n        11057.02840522, 11061.86670645, 11071.21653971, 11072.31778009,\n        11071.70418289, 11081.12757073, 11075.57132848, 11071.83439814]])</pre></li><li>n_steps(chain, draw)int6431 63 63 95 31 ... 31 63 95 31 63<pre>array([[ 31,  63,  63,  95,  31,  31,  31,  31,  31, 111,  63,  95, 127,\n         63,  31,  31, 127,  31,  63,  31, 127,  63,  31,  63,  63,  63,\n         63,  31,  31,  31,  63,  15,  63,  31, 127,  31,  31,  31,  95,\n         63,  31,  31,  31,  31,  63,  47, 127, 127,  15, 111,  79,  63,\n         63,  31,  31,  31, 127,  31,  31,  15,  31,  31,  31,  31,  63,\n         31,  31,  63,  63, 127, 127,  95,  63,  95,  63,  31,  31,  63,\n         63,  95,  63,  31,  31,  31, 127,  63,  31,  31,  63,  63,  31,\n         63,  63,  31,  31, 127,  63, 127,  63,  95,  63,  95,  63,  15,\n         95,  31,  63,  31,  31,  63,  31,  63,  63, 127,  31,  31,  31,\n         31,  63,  63,  31,  95,  63,  63, 127,  31,  31,  31,  31,  63,\n         31,  63,  31,  31,  63,  31,  31,  63,  47,  63,  95,  31,  31,\n         63,  63, 127,  63,  63,  79,  63,  31,  79,  31,  63,  63,  31,\n         31,  31, 127,  31,  31,  63,  31,  31,  31,  95,  63,  31,  63,\n         31,  63,  31,  63,  63,  31,  15,  63,  63,  31,  63,  63,  95,\n         63,  63,  63,  63,  95,  63,  63,  63,  63,  63,  95,  63,  31,\n         31,  31,  31, 127,  95,  15,  95,  31,  63,  31,  31,  31,  31,\n         63, 127,  31,  63,  63,  63,  31,  63,  63,  63,  63,  63,  63,\n         63,  31,  31,  31,  31,  31,  31,  31, 127,  63,  63,  63,  31,\n         31,  31,  31,  63,  95,  31,  31, 127,  63,  31,  31, 127,  63,\n         63,  31,  31,  63,  63,  63,  63, 127,  63,  63,  63,  63,  63,\n...\n         63,  95,  31,  31,  31,  63,  31,  31, 127,  95,  63,  95,  31,\n         63,  63,  95,  63, 127,  31,  31,  31,  31,  31,  31,  63,  31,\n         31,  31,  31,  63,  63,  31,  31,  15,  63, 127,  95, 127,  63,\n         31,  47,  31,  31,  63,  31,  31,  31,  31,  31,  31,  31,  31,\n         31,  31,  63,  63,  31,  63,  63,  63,  63,  31,  63,  31,  63,\n         31,  31, 127,  63,  31,  63,  63,  31,  63,  47,  31,  15,  31,\n         31,  15, 127,  63,  63,  31,  63,  31,  63,  31,  63,  31,  31,\n         63,  95,  31,  63,  63,  63,  47,  95,  31,  95,  95,  63,  31,\n         63,  63,  31,  63,  31,  31,  31,  95,  31,  31,  31,  31,  31,\n         31, 127, 127,  31,  31,  31,  31,  31,  31,  31, 127,  47,  63,\n         63,  63,  63,  31,  95,  63,  31,  63,  63,  31,  31,  31,  63,\n         31,  31,  63,  31,  63,  31,  63,  31,  31,  63,  31, 127, 127,\n         15,  31,  15,  63,  63,  63,  63,  31,  31,  63,  63,  31,  31,\n         63,  95,  95,  31,  63,  63,  63,  63,  63,  63,  31,  31,  31,\n         63,  95,  31,  31, 127,  63,  95,  31, 127,  63,  31,  31,  63,\n         95,  31,  79,  63,  31,  63,  31,  31,  31,  31,  31,  31,  63,\n         63,  31,  31,  31,  31, 127,  31,  95,  79,  63,  95,  31,  95,\n         31,  31,  63,  31,  31,  63,  31,  31,  31,  31,  31,  15,  63,\n         31,  31,  31,  63,  63,  31, 111,  63,  31,  31,  63,  31,  63,\n         31,  31,  63,  95,  31,  63]])</pre></li><li>tree_depth(chain, draw)int645 6 6 7 5 5 5 5 ... 5 6 5 5 6 7 5 6<pre>array([[5, 6, 6, 7, 5, 5, 5, 5, 5, 7, 6, 7, 7, 6, 5, 5, 7, 5, 6, 5, 7, 6,\n        5, 6, 6, 6, 6, 5, 5, 5, 6, 4, 6, 5, 7, 5, 5, 5, 7, 6, 5, 5, 5, 5,\n        6, 6, 7, 7, 4, 7, 7, 6, 6, 5, 5, 5, 7, 5, 5, 4, 5, 5, 5, 5, 6, 5,\n        5, 6, 6, 7, 7, 7, 6, 7, 6, 5, 5, 6, 6, 7, 6, 5, 5, 5, 7, 6, 5, 5,\n        6, 6, 5, 6, 6, 5, 5, 7, 6, 7, 6, 7, 6, 7, 6, 4, 7, 5, 6, 5, 5, 6,\n        5, 6, 6, 7, 5, 5, 5, 5, 6, 6, 5, 7, 6, 6, 7, 5, 5, 5, 5, 6, 5, 6,\n        5, 5, 6, 5, 5, 6, 6, 6, 7, 5, 5, 6, 6, 7, 6, 6, 7, 6, 5, 7, 5, 6,\n        6, 5, 5, 5, 7, 5, 5, 6, 5, 5, 5, 7, 6, 5, 6, 5, 6, 5, 6, 6, 5, 4,\n        6, 6, 5, 6, 6, 7, 6, 6, 6, 6, 7, 6, 6, 6, 6, 6, 7, 6, 5, 5, 5, 5,\n        7, 7, 4, 7, 5, 6, 5, 5, 5, 5, 6, 7, 5, 6, 6, 6, 5, 6, 6, 6, 6, 6,\n        6, 6, 5, 5, 5, 5, 5, 5, 5, 7, 6, 6, 6, 5, 5, 5, 5, 6, 7, 5, 5, 7,\n        6, 5, 5, 7, 6, 6, 5, 5, 6, 6, 6, 6, 7, 6, 6, 6, 6, 6, 7, 7, 6, 5,\n        7, 5, 6, 5, 5, 6, 6, 6, 6, 5, 7, 5, 6, 6, 6, 6, 5, 5, 6, 6, 6, 6,\n        7, 6, 5, 4, 6, 5, 7, 5, 7, 6, 6, 6, 6, 6, 6, 6, 6, 5, 7, 7, 5, 5,\n        5, 6, 5, 5, 6, 7, 6, 7, 7, 7, 7, 5, 6, 5, 6, 5, 5, 5, 6, 4, 5, 6,\n        6, 5, 5, 6, 7, 6, 6, 7, 7, 6, 5, 6, 5, 7, 6, 6, 7, 7, 7, 6, 7, 4,\n        5, 7, 6, 5, 6, 6, 6, 6, 6, 7, 7, 8, 6, 6, 6, 6, 6, 6, 5, 7, 5, 6,\n        7, 6, 6, 6, 7, 7, 7, 5, 5, 5, 5, 6, 7, 6, 4, 5, 6, 5, 6, 6, 6, 6,\n        7, 5, 6, 6, 5, 7, 7, 6, 5, 6, 7, 7, 6, 5, 5, 6, 6, 5, 5, 6, 6, 5,\n        6, 6, 5, 6, 5, 7, 6, 5, 5, 6, 7, 5, 7, 5, 7, 5, 7, 5, 5, 5, 6, 6,\n...\n        6, 5, 6, 5, 5, 6, 6, 6, 6, 4, 6, 6, 5, 7, 6, 5, 5, 5, 7, 6, 5, 5,\n        5, 5, 5, 5, 6, 6, 7, 5, 6, 5, 6, 6, 6, 6, 5, 6, 6, 6, 7, 6, 6, 6,\n        5, 5, 7, 6, 6, 6, 6, 6, 5, 5, 6, 6, 4, 4, 6, 5, 7, 5, 5, 7, 5, 5,\n        5, 7, 6, 5, 5, 5, 5, 6, 6, 4, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 6, 6,\n        5, 6, 6, 6, 5, 7, 6, 5, 6, 6, 6, 6, 6, 7, 7, 5, 6, 6, 5, 4, 6, 5,\n        5, 6, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 6, 6,\n        7, 6, 6, 4, 5, 5, 6, 5, 5, 5, 5, 6, 5, 7, 7, 4, 5, 5, 5, 5, 5, 6,\n        5, 5, 5, 6, 5, 5, 6, 5, 7, 5, 5, 6, 4, 5, 6, 5, 7, 7, 6, 5, 6, 5,\n        5, 6, 5, 6, 6, 6, 7, 5, 5, 5, 6, 5, 5, 7, 7, 6, 7, 5, 6, 6, 7, 6,\n        7, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5, 5, 6, 6, 5, 5, 4, 6, 7, 7, 7, 6,\n        5, 6, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 5, 6, 6, 6, 6,\n        5, 6, 5, 6, 5, 5, 7, 6, 5, 6, 6, 5, 6, 6, 5, 4, 5, 5, 4, 7, 6, 6,\n        5, 6, 5, 6, 5, 6, 5, 5, 6, 7, 5, 6, 6, 6, 6, 7, 5, 7, 7, 6, 5, 6,\n        6, 5, 6, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5,\n        7, 6, 6, 6, 6, 6, 5, 7, 6, 5, 6, 6, 5, 5, 5, 6, 5, 5, 6, 5, 6, 5,\n        6, 5, 5, 6, 5, 7, 7, 4, 5, 4, 6, 6, 6, 6, 5, 5, 6, 6, 5, 5, 6, 7,\n        7, 5, 6, 6, 6, 6, 6, 6, 5, 5, 5, 6, 7, 5, 5, 7, 6, 7, 5, 7, 6, 5,\n        5, 6, 7, 5, 7, 6, 5, 6, 5, 5, 5, 5, 5, 5, 6, 6, 5, 5, 5, 5, 7, 5,\n        7, 7, 6, 7, 5, 7, 5, 5, 6, 5, 5, 6, 5, 5, 5, 5, 5, 4, 6, 5, 5, 5,\n        6, 6, 5, 7, 6, 5, 5, 6, 5, 6, 5, 5, 6, 7, 5, 6]])</pre></li><li>lp(chain, draw)float641.104e+04 1.104e+04 ... 1.104e+04<pre>array([[11043.75091703, 11038.04082754, 11033.9545578 , 11041.87939255,\n        11045.73461249, 11047.05410485, 11045.33990302, 11044.0073329 ,\n        11052.0290493 , 11044.58790774, 11041.37927347, 11037.67487282,\n        11038.37024009, 11040.9452714 , 11035.46721947, 11040.34168629,\n        11039.49952183, 11043.97739234, 11038.48970354, 11037.57968093,\n        11039.57874162, 11044.12946084, 11047.55710254, 11052.29509116,\n        11047.11758798, 11045.06191271, 11044.93805548, 11046.21379878,\n        11048.32511792, 11049.67519975, 11042.78512508, 11047.50531618,\n        11050.66513856, 11051.75784599, 11049.95385276, 11040.60814191,\n        11043.55596015, 11044.55956848, 11037.15923449, 11042.42620746,\n        11043.03259436, 11036.00737182, 11034.6100264 , 11038.37826116,\n        11042.97061366, 11049.71353191, 11046.47417525, 11046.58535652,\n        11057.32773653, 11059.88542381, 11048.57490189, 11044.18809254,\n        11035.96370249, 11032.75882679, 11033.84527444, 11040.18476306,\n        11042.68005616, 11041.957095  , 11031.34480915, 11039.17001701,\n        11032.63076101, 11036.87393015, 11035.99239817, 11035.78124637,\n        11037.72432663, 11037.39691693, 11039.57569287, 11037.51346614,\n        11041.08850546, 11035.57712867, 11041.54939298, 11035.10833101,\n        11041.83279464, 11049.55641267, 11052.52328901, 11044.69878946,\n        11044.83615907, 11039.48775983, 11033.04196834, 11038.49410108,\n...\n        11052.1611176 , 11040.0463718 , 11037.62798793, 11043.99045246,\n        11041.25460413, 11041.42762067, 11045.4072395 , 11048.19034023,\n        11047.78704742, 11052.70621449, 11048.87145931, 11041.39429215,\n        11045.46079871, 11041.70925607, 11043.34538809, 11040.33664939,\n        11045.739785  , 11046.70236078, 11031.92944578, 11044.83691761,\n        11041.00797205, 11041.5599584 , 11039.33606726, 11044.81927219,\n        11050.437636  , 11046.65759051, 11038.98324032, 11045.62882096,\n        11053.32803878, 11039.22951473, 11047.51155874, 11048.01638489,\n        11042.1078171 , 11043.35562874, 11038.14563341, 11039.40634209,\n        11046.0031235 , 11039.25993293, 11033.6511059 , 11038.79597086,\n        11037.52135572, 11037.1672036 , 11047.04304251, 11047.41793273,\n        11048.22076491, 11044.27789251, 11035.14105416, 11034.19328095,\n        11036.72275779, 11038.72400584, 11038.87347748, 11045.01948008,\n        11046.46222181, 11051.06214316, 11043.58232809, 11039.75466147,\n        11041.2316548 , 11040.70573729, 11035.43556711, 11033.47659632,\n        11031.83358736, 11036.96811062, 11039.25980109, 11035.58927897,\n        11044.8213847 , 11049.65612656, 11044.21458746, 11050.57690866,\n        11045.13082867, 11044.12002617, 11041.15588776, 11040.48977865,\n        11041.15494145, 11040.99462139, 11046.96504157, 11044.01060853,\n        11052.29145506, 11046.47183921, 11049.51719689, 11039.51064768]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (4)created_at :2025-09-27T18:57:44.266359+00:00arviz_version :0.22.0modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 180kB\nDimensions:                  (__obs__: 7500, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 60kB 0 1 2 3 ... 7497 7498 7499\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 120kB ...\nAttributes:\n    created_at:                  2025-09-27T18:57:44.267620+00:00\n    arviz_version:               0.22.0\n    inference_library:           numpyro\n    inference_library_version:   0.19.0\n    sampling_time:               1494.128823\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 7500</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 ... 7496 7497 7498 7499<pre>array([   0,    1,    2, ..., 7497, 7498, 7499], shape=(7500,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float641.887 1.0 0.8828 ... 1.0 1.997 -1.0<pre>array([[ 1.88742435,  1.        ],\n       [ 0.88283288,  1.        ],\n       [ 3.43690109,  1.        ],\n       ...,\n       [ 1.18231833, -1.        ],\n       [14.26356594,  1.        ],\n       [ 1.99704194, -1.        ]], shape=(7500, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499],\n      dtype='int64', name='__obs__', length=7500))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T18:57:44.267620+00:00arviz_version :0.22.0inference_library :numpyroinference_library_version :0.19.0sampling_time :1494.128823tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[6]: Copied! <pre># Add tiral-wise parameters to idata (since we define p_outlier as a regression)\ntest_model_1_C.add_likelihood_parameters_to_idata(inplace=True)\n</pre> # Add tiral-wise parameters to idata (since we define p_outlier as a regression) test_model_1_C.add_likelihood_parameters_to_idata(inplace=True) Out[6]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 121MB\nDimensions:                                    (chain: 2, draw: 500,\n                                                v_1|participant_id__factor_dim: 10,\n                                                v_C(trialtype)|participant_id__expr_dim: 2,\n                                                C(participant_id)__factor_dim: 10,\n                                                v_C(trialtype)|participant_id__factor_dim: 10,\n                                                __obs__: 7500)\nCoordinates:\n  * chain                                      (chain) int64 16B 0 1\n  * draw                                       (draw) int64 4kB 0 1 ... 498 499\n  * v_1|participant_id__factor_dim             (v_1|participant_id__factor_dim) &lt;U1 40B ...\n  * v_C(trialtype)|participant_id__expr_dim    (v_C(trialtype)|participant_id__expr_dim) &lt;U10 80B ...\n  * v_C(trialtype)|participant_id__factor_dim  (v_C(trialtype)|participant_id__factor_dim) &lt;U1 40B ...\n  * C(participant_id)__factor_dim              (C(participant_id)__factor_dim) &lt;U1 40B ...\n  * __obs__                                    (__obs__) int64 60kB 0 1 ... 7499\nData variables: (12/17)\n    v_1|participant_id_mu                      (chain, draw) float64 8kB 0.06...\n    v_1|participant_id                         (chain, draw, v_1|participant_id__factor_dim) float64 80kB ...\n    t                                          (chain, draw) float64 8kB 0.50...\n    v_C(trialtype)|participant_id_sigma        (chain, draw, v_C(trialtype)|participant_id__expr_dim) float64 16kB ...\n    p_outlier_1|C(participant_id)_sigma        (chain, draw) float64 8kB 0.60...\n    a                                          (chain, draw) float64 8kB 1.00...\n    ...                                         ...\n    v_C(trialtype)|participant_id_offset       (chain, draw, v_C(trialtype)|participant_id__expr_dim, v_C(trialtype)|participant_id__factor_dim) float64 160kB ...\n    p_outlier_Intercept                        (chain, draw) float64 8kB -2.1...\n    v_C(trialtype)|participant_id              (chain, draw, v_C(trialtype)|participant_id__expr_dim, v_C(trialtype)|participant_id__factor_dim) float64 160kB ...\n    v_1|participant_id_sigma                   (chain, draw) float64 8kB 0.39...\n    v                                          (chain, draw, __obs__) float64 60MB ...\n    p_outlier                                  (chain, draw, __obs__) float64 60MB ...\nAttributes:\n    created_at:                  2025-09-27T18:57:44.208796+00:00\n    arviz_version:               0.22.0\n    inference_library:           numpyro\n    inference_library_version:   0.19.0\n    sampling_time:               1494.128823\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li><li>v_1|participant_id__factor_dim: 10</li><li>v_C(trialtype)|participant_id__expr_dim: 2</li><li>C(participant_id)__factor_dim: 10</li><li>v_C(trialtype)|participant_id__factor_dim: 10</li><li>__obs__: 7500</li></ul></li><li>Coordinates: (7)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>v_1|participant_id__factor_dim(v_1|participant_id__factor_dim)&lt;U1'0' '1' '2' '3' ... '6' '7' '8' '9'<pre>array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='&lt;U1')</pre></li><li>v_C(trialtype)|participant_id__expr_dim(v_C(trialtype)|participant_id__expr_dim)&lt;U10'someswitch' 'switch'<pre>array(['someswitch', 'switch'], dtype='&lt;U10')</pre></li><li>v_C(trialtype)|participant_id__factor_dim(v_C(trialtype)|participant_id__factor_dim)&lt;U1'0' '1' '2' '3' ... '6' '7' '8' '9'<pre>array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='&lt;U1')</pre></li><li>C(participant_id)__factor_dim(C(participant_id)__factor_dim)&lt;U1'0' '1' '2' '3' ... '6' '7' '8' '9'<pre>array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='&lt;U1')</pre></li><li>__obs__(__obs__)int640 1 2 3 4 ... 7496 7497 7498 7499<pre>array([   0,    1,    2, ..., 7497, 7498, 7499], shape=(7500,))</pre></li></ul></li><li>Data variables: (17)<ul><li>v_1|participant_id_mu(chain, draw)float640.06558 -0.199 ... -0.2883 -0.3764<pre>array([[ 6.55784082e-02, -1.99030260e-01, -1.68565054e-02,\n         1.72697639e-02, -2.21228331e-01, -1.86584550e-01,\n         1.78533579e-01,  5.60247492e-03,  2.28745092e-03,\n        -2.48191935e-01, -1.39172746e-01,  8.35486794e-02,\n         1.16125904e-01,  1.65374589e-01,  2.69841298e-01,\n         4.92429734e-01,  1.77543240e-01, -1.55580388e-01,\n         1.42904502e-01,  1.25477829e-01,  1.64463123e-01,\n        -3.35402405e-01,  1.63402149e-01,  4.14611275e-01,\n         1.65667202e-01, -7.92554530e-02, -4.29386581e-01,\n        -4.89906344e-01,  1.54197762e-01,  3.92154607e-02,\n         4.06450359e-01,  2.43711071e-01,  2.33680663e-01,\n         2.16814935e-01, -2.26412773e-01, -1.63822067e-02,\n         8.86349636e-02, -1.28302493e-01, -1.49410659e-01,\n        -2.08805172e-01, -1.41226925e-01,  2.86120157e-01,\n        -2.06626585e-01,  7.20010766e-02,  4.42417083e-01,\n        -2.09092065e-01, -1.29784640e-01,  2.17617621e-01,\n         1.09724109e-01,  3.55419855e-02, -4.60694975e-02,\n         6.54209739e-02,  1.57440511e-01,  9.32499659e-03,\n        -2.21603045e-02, -3.14658892e-01, -2.93388890e-01,\n         4.16791235e-02,  7.62605724e-02,  2.42896499e-01,\n...\n        -1.02490191e-01,  2.22554079e-02, -4.27783034e-02,\n        -1.13811801e-01, -9.77262115e-02,  1.56959470e-01,\n         2.65073660e-01, -6.11950497e-02, -2.51636089e-01,\n        -5.39073533e-01,  5.68830143e-01,  5.50974236e-02,\n         2.93636235e-01, -2.56399823e-01,  3.41974684e-01,\n        -9.96655026e-02, -2.88615515e-01,  2.13416151e-02,\n         8.04487085e-02, -7.22582378e-02,  1.00382861e-01,\n        -1.49622837e-02,  6.36173890e-02,  4.36329217e-01,\n         3.38131858e-01, -3.03404727e-01, -1.29983297e-01,\n        -3.82741918e-02, -2.48688312e-02, -1.09832225e-01,\n        -9.56744620e-02, -4.16365490e-01, -3.28403659e-01,\n         1.52910884e-01, -1.90089433e-01, -1.61785455e-01,\n        -3.24106005e-01, -8.91384580e-02, -7.84074218e-02,\n        -5.16543837e-02,  2.74304799e-02,  2.98410804e-01,\n         1.14407652e-01, -4.42392564e-01, -5.65564931e-01,\n         4.20000626e-01, -3.62169137e-01, -1.03997485e-01,\n         9.39491141e-02,  1.34595540e-01,  3.22078287e-01,\n         3.55127548e-01,  1.52247539e-01, -1.94319893e-01,\n         1.92517117e-01, -2.55463714e-01,  1.33069456e-01,\n        -2.88311309e-01, -3.76410401e-01]])</pre></li><li>v_1|participant_id(chain, draw, v_1|participant_id__factor_dim)float64-0.4468 -0.5708 ... 0.1264 0.09778<pre>array([[[-0.44683439, -0.57083488,  0.1446001 , ...,  0.28418131,\n          0.20163916,  0.07537987],\n        [-0.50041868, -0.5275415 ,  0.12705525, ...,  0.13361673,\n          0.11227681,  0.00305158],\n        [-0.32445369, -0.65388511,  0.08203591, ...,  0.26201331,\n          0.32518152,  0.21220081],\n        ...,\n        [-0.47317081, -0.52162647,  0.15598418, ...,  0.22540557,\n          0.12339786, -0.05596364],\n        [-0.28931   , -0.61231685,  0.14006568, ...,  0.28448317,\n          0.32193491,  0.31005244],\n        [-0.38691842, -0.52432882,  0.14666315, ...,  0.16884874,\n          0.14371246,  0.07978017]],\n\n       [[-0.40319298, -0.56826678,  0.11732421, ...,  0.2590544 ,\n          0.07146863,  0.16457154],\n        [-0.40062772, -0.61260959,  0.08855053, ...,  0.27476259,\n          0.12704059,  0.06356891],\n        [-0.45334737, -0.51199759,  0.12100992, ...,  0.09152681,\n          0.21314386,  0.17166266],\n        ...,\n        [-0.49701574, -0.53279198,  0.26386136, ...,  0.22899595,\n          0.16864552,  0.07290829],\n        [-0.3491537 , -0.54139657,  0.22965866, ...,  0.18341219,\n          0.2950146 ,  0.25144245],\n        [-0.50522406, -0.57882999,  0.10913765, ...,  0.18807718,\n          0.12644678,  0.09777709]]], shape=(2, 500, 10))</pre></li><li>t(chain, draw)float640.5032 0.5033 ... 0.5035 0.5036<pre>array([[0.50321387, 0.50327104, 0.50288065, 0.50810087, 0.50176372,\n        0.50480002, 0.51363263, 0.50683614, 0.49944515, 0.50926112,\n        0.50809259, 0.50063276, 0.50659954, 0.50802615, 0.50727894,\n        0.50673415, 0.50046378, 0.51517185, 0.50626673, 0.50996391,\n        0.50605799, 0.50633569, 0.50648109, 0.50339287, 0.5061993 ,\n        0.51263574, 0.50099101, 0.50815887, 0.50329619, 0.50262441,\n        0.50846642, 0.50811592, 0.5082643 , 0.50804953, 0.50352528,\n        0.5041455 , 0.50394286, 0.50075057, 0.51000702, 0.50994774,\n        0.50730573, 0.50593554, 0.50558422, 0.5018496 , 0.51171548,\n        0.50348262, 0.50082601, 0.51360683, 0.50529601, 0.50625895,\n        0.50009432, 0.5017552 , 0.50788432, 0.50828595, 0.5062623 ,\n        0.51178981, 0.51197102, 0.50566293, 0.50856348, 0.50818338,\n        0.50592476, 0.51038325, 0.50329087, 0.51035025, 0.50346515,\n        0.50036023, 0.50662168, 0.50872473, 0.50830109, 0.50805301,\n        0.50200095, 0.50466232, 0.50202029, 0.50629279, 0.50862571,\n        0.50817799, 0.51006466, 0.50645853, 0.50489679, 0.50292326,\n        0.51165489, 0.50651749, 0.50403015, 0.50616011, 0.50678814,\n        0.5053887 , 0.50445502, 0.49493295, 0.51325913, 0.50383988,\n        0.51164042, 0.50382046, 0.50983549, 0.50364712, 0.50363022,\n        0.51062218, 0.50450508, 0.51196886, 0.50764629, 0.50581662,\n...\n        0.49890948, 0.50439412, 0.50595728, 0.50891015, 0.51042768,\n        0.51114155, 0.50313476, 0.51186743, 0.5092632 , 0.51093235,\n        0.50508396, 0.50980407, 0.5015563 , 0.51000145, 0.50740268,\n        0.50368688, 0.50697959, 0.49832263, 0.51067648, 0.51286337,\n        0.49796336, 0.51249316, 0.50816054, 0.50566984, 0.5071684 ,\n        0.50823109, 0.50806847, 0.50489419, 0.50501601, 0.50505194,\n        0.50998594, 0.51138557, 0.50377538, 0.5043528 , 0.50551457,\n        0.50512258, 0.51024639, 0.50740386, 0.50455257, 0.50762316,\n        0.50647602, 0.51144674, 0.50434678, 0.50737785, 0.50924311,\n        0.5028223 , 0.50819817, 0.50516058, 0.50467686, 0.50914579,\n        0.50549465, 0.50427065, 0.50241705, 0.50492627, 0.51336726,\n        0.51005971, 0.5062819 , 0.50552909, 0.50605446, 0.51004885,\n        0.50471216, 0.51218712, 0.49747177, 0.51198315, 0.50786543,\n        0.51215602, 0.50960887, 0.51020235, 0.50349873, 0.5100099 ,\n        0.50099648, 0.5106742 , 0.50202907, 0.50100852, 0.50338723,\n        0.50452714, 0.50822825, 0.50367025, 0.5054304 , 0.50421753,\n        0.51262549, 0.498036  , 0.51061147, 0.50118429, 0.5038447 ,\n        0.50536197, 0.5081946 , 0.50683473, 0.50960996, 0.50815918,\n        0.50540879, 0.51161644, 0.51243156, 0.50586508, 0.51308177,\n        0.50264949, 0.51144339, 0.50051511, 0.50354915, 0.50364851]])</pre></li><li>v_C(trialtype)|participant_id_sigma(chain, draw, v_C(trialtype)|participant_id__expr_dim)float641.045 1.067 0.8241 ... 0.676 0.9779<pre>array([[[1.0450334 , 1.06661608],\n        [0.82406838, 1.19493741],\n        [0.77185652, 0.91355771],\n        ...,\n        [0.91640751, 0.82854729],\n        [0.9394795 , 0.88142148],\n        [0.8135634 , 0.78928493]],\n\n       [[0.9731705 , 0.97610052],\n        [1.02112629, 0.9526402 ],\n        [0.89013679, 0.93364199],\n        ...,\n        [0.59997962, 0.88265727],\n        [0.6275665 , 0.89830425],\n        [0.67596294, 0.97788216]]], shape=(2, 500, 2))</pre></li><li>p_outlier_1|C(participant_id)_sigma(chain, draw)float640.6033 0.8842 ... 0.7842 0.8726<pre>array([[0.60328537, 0.88415882, 0.81950246, 0.80223234, 0.94866962,\n        1.00242785, 0.78493696, 0.83444933, 0.71098072, 0.91172849,\n        0.73042531, 0.98145518, 0.95332724, 1.02595039, 1.0790353 ,\n        0.88172119, 0.81429883, 0.81265644, 1.16816126, 0.94540583,\n        1.12072665, 0.88097405, 0.81629079, 0.85696296, 0.97891325,\n        0.86625466, 0.70360561, 0.76509909, 0.75628571, 0.71415749,\n        0.7255272 , 0.77689685, 0.51270902, 0.75691369, 0.78537758,\n        0.97644941, 0.77563001, 0.97731291, 0.85320794, 0.94240257,\n        0.90180528, 0.98168105, 0.88045856, 0.85705614, 0.79719193,\n        0.8009389 , 1.05852116, 0.78880243, 0.7108462 , 0.81314243,\n        0.79183187, 0.82146289, 0.93848787, 1.19755118, 0.84312726,\n        0.72762322, 0.78274358, 0.68609485, 0.94893613, 0.8710282 ,\n        1.34934359, 1.26602253, 0.78743635, 0.78250821, 0.80173069,\n        0.96345028, 1.03303754, 1.06730504, 0.84569216, 0.98070025,\n        0.94272854, 0.88628941, 1.21113208, 0.81380357, 0.75760717,\n        0.79932195, 0.65357092, 0.68032235, 0.94429508, 0.84277513,\n        0.69107116, 0.7179778 , 0.84190509, 0.93312017, 1.02848351,\n        0.85027524, 0.89404398, 0.95136391, 0.69513712, 0.78086927,\n        0.72819271, 0.64842008, 0.75323654, 0.73072341, 0.75175046,\n        0.86251802, 1.09315682, 0.80179527, 1.09873263, 0.81968029,\n...\n        1.01437036, 0.97123474, 0.81242571, 0.7467827 , 0.79189925,\n        0.70617498, 0.80525623, 0.80915854, 0.95185046, 1.10755023,\n        0.80854258, 0.87330032, 0.97308366, 0.93335381, 0.81494079,\n        0.99176782, 1.24821205, 0.91325513, 0.90840785, 0.98488424,\n        0.86561768, 0.88437644, 1.03446246, 0.79283434, 1.00034092,\n        1.12020919, 0.93150668, 1.028101  , 1.01355795, 1.01977114,\n        0.71362403, 1.02129998, 0.86721225, 0.89161328, 1.19995477,\n        0.88786355, 0.87478268, 1.2275888 , 0.93118791, 0.69041071,\n        0.74647597, 0.78560173, 0.64070706, 0.60057007, 0.77190412,\n        0.89339961, 1.02575398, 0.76677164, 0.89931751, 0.86166664,\n        0.85978041, 0.69679628, 0.74224745, 0.73523869, 0.78600718,\n        0.87399865, 0.86190632, 0.8824343 , 1.03041135, 0.70169031,\n        0.86887921, 0.70118892, 0.98117486, 0.76987873, 0.72015193,\n        0.86964709, 0.8073351 , 0.95214688, 1.24295031, 1.24019236,\n        1.06539971, 1.02460592, 1.00848546, 1.00326974, 0.76358679,\n        0.74298593, 0.99101934, 1.30322998, 1.06239591, 0.96025135,\n        1.08876717, 1.34598373, 1.10852278, 0.99637011, 1.07211701,\n        1.04843151, 0.81810536, 0.73534109, 0.68555452, 0.78198716,\n        0.76459174, 0.9284753 , 0.92795957, 0.89127238, 0.76349428,\n        0.91842568, 0.71264543, 0.83630064, 0.78424942, 0.87260813]])</pre></li><li>a(chain, draw)float641.008 0.9912 0.9964 ... 1.001 1.001<pre>array([[1.00827428, 0.99119199, 0.9964241 , 0.98814098, 0.99663601,\n        1.01052479, 0.9868873 , 0.99116146, 1.00718639, 1.00196763,\n        0.98918464, 1.00327662, 0.99696991, 0.98722142, 0.99415651,\n        0.99437571, 1.00930465, 0.98242336, 0.98988525, 0.99267055,\n        0.99804202, 0.99208769, 0.99417303, 1.00088552, 1.00054765,\n        0.9782566 , 1.00170003, 0.99789072, 0.99104793, 1.00395108,\n        0.99155441, 0.99361757, 0.98841774, 0.99655677, 1.00602785,\n        1.00402568, 0.9978994 , 1.00350649, 0.98460303, 0.9842837 ,\n        0.99951796, 0.98957889, 1.00543143, 0.99529355, 0.99620619,\n        0.99291075, 0.99289456, 0.99642845, 0.97934294, 0.98466953,\n        1.00132991, 1.00557558, 0.99781012, 0.9879857 , 1.0004192 ,\n        0.98408398, 0.98625685, 0.99333929, 0.99257357, 0.99037381,\n        0.99703   , 0.98819694, 0.99952047, 0.98442033, 1.00584051,\n        0.99966988, 0.9853759 , 0.99448642, 0.99329404, 0.98676325,\n        1.00898123, 1.00507912, 0.99395695, 0.99739749, 0.99583656,\n        0.98790021, 0.98763808, 1.00084228, 0.99115735, 0.99789543,\n        0.99401325, 0.98733975, 0.99676922, 0.99819554, 0.99549718,\n        0.99610906, 1.00361195, 1.02138902, 0.98916605, 0.9893648 ,\n        0.99616329, 0.99251125, 0.99856575, 0.99895161, 0.99797565,\n        0.99052109, 1.00328254, 0.98659062, 0.98916525, 0.99685526,\n...\n        1.00514426, 1.00244256, 0.99837911, 0.99744551, 0.98610604,\n        0.99000882, 1.00204673, 0.98709397, 0.9826581 , 0.98391138,\n        0.9888155 , 0.98432323, 1.00266756, 0.99508575, 0.99003534,\n        1.00191854, 0.99501472, 1.01154888, 0.98795694, 0.98515901,\n        1.0116787 , 0.98096339, 0.99618323, 0.99093324, 0.99699369,\n        0.98876929, 0.99425468, 0.9932754 , 0.99260528, 0.99665057,\n        0.9849775 , 0.98595919, 0.99818785, 1.00337287, 0.99732555,\n        0.99193013, 0.99570375, 0.98418224, 0.99216446, 0.99968046,\n        0.99317392, 0.98886888, 0.99674541, 0.98920277, 0.99475715,\n        1.00024938, 0.99433467, 0.99233931, 1.00124466, 0.98981393,\n        0.99103297, 1.00020573, 1.00112535, 1.00652707, 0.98453321,\n        0.98646226, 0.99666818, 0.99494946, 0.99350015, 0.99637391,\n        1.00759676, 0.97893936, 1.01189309, 0.9820839 , 0.99319971,\n        0.99424943, 0.980493  , 0.98952454, 0.99275744, 1.00045335,\n        0.9958843 , 0.99693745, 0.99187514, 0.9867891 , 1.01207862,\n        0.98922575, 1.00265909, 0.99337099, 1.00428052, 1.00710848,\n        0.98138896, 1.00723646, 0.99263102, 1.00526386, 1.00182591,\n        0.99500336, 0.99139127, 0.99849441, 0.98646374, 0.99627181,\n        0.99615445, 0.99160986, 0.99482607, 0.99065389, 0.98383782,\n        1.00734688, 0.97920829, 1.00846464, 1.00119476, 1.00109947]])</pre></li><li>p_outlier_1|C(participant_id)_offset(chain, draw, C(participant_id)__factor_dim)float64-1.529 -3.001 ... -0.658 0.507<pre>array([[[-1.52859431, -3.00081989,  0.70325016, ...,  1.08062995,\n         -1.87384877,  0.58114809],\n        [-1.77401161, -2.93869292,  0.20190234, ...,  0.18553215,\n         -1.19995749,  0.42434624],\n        [-1.59168242, -2.7350334 ,  0.63841226, ...,  0.68112651,\n         -1.0362473 ,  0.77435563],\n        ...,\n        [-1.06624563, -3.33843309,  0.87428162, ...,  0.61173962,\n         -0.28607807,  1.33989953],\n        [-0.70297839, -1.48869727,  0.79595795, ...,  0.86295319,\n         -1.00007392,  1.60084131],\n        [-1.62981869, -3.49803014,  0.47499008, ...,  0.46428441,\n         -0.62791088,  1.02757986]],\n\n       [[-0.85701367, -2.57603509,  0.64074554, ...,  0.63432375,\n         -0.72413192,  0.60974549],\n        [-0.99237403, -2.46574765,  0.40633584, ...,  0.71279967,\n         -0.84911977,  0.62782312],\n        [-1.07274098, -1.57550893,  0.15417928, ...,  0.68293071,\n         -0.81589448,  0.46879669],\n        ...,\n        [-1.40410904, -3.18928007,  0.57365769, ...,  0.63555135,\n         -1.06704398,  0.42669425],\n        [-1.60563612, -2.43186003,  0.57800998, ...,  0.78769743,\n         -1.52352201,  0.65738051],\n        [-1.12251326, -2.90411268,  0.6781353 , ...,  0.7293375 ,\n         -0.65800372,  0.507023  ]]], shape=(2, 500, 10))</pre></li><li>z(chain, draw)float640.5091 0.5052 ... 0.5038 0.5007<pre>array([[0.50907526, 0.50522722, 0.49843665, 0.50841066, 0.49784957,\n        0.49781986, 0.50310421, 0.49656011, 0.50219061, 0.49744417,\n        0.50386029, 0.50062671, 0.50273652, 0.51503662, 0.50329009,\n        0.49959442, 0.49897391, 0.4982927 , 0.5022283 , 0.50261572,\n        0.49403338, 0.49979802, 0.49746112, 0.50530158, 0.5007692 ,\n        0.49921897, 0.50267621, 0.49759364, 0.50610308, 0.50154174,\n        0.49893825, 0.49806402, 0.49891693, 0.49595798, 0.50643025,\n        0.50924681, 0.50821779, 0.509791  , 0.50050075, 0.50035244,\n        0.49945756, 0.50221782, 0.49973114, 0.49394707, 0.49897174,\n        0.50126007, 0.50379818, 0.50021377, 0.50382638, 0.49838452,\n        0.50615391, 0.50328426, 0.50192866, 0.50412023, 0.49646806,\n        0.50454773, 0.50478748, 0.50887255, 0.50010327, 0.50235451,\n        0.49831575, 0.50075653, 0.50177781, 0.50202368, 0.50187022,\n        0.49844503, 0.50328905, 0.50168768, 0.50674001, 0.50325552,\n        0.50187683, 0.49667116, 0.50604168, 0.51015734, 0.50363498,\n        0.50429179, 0.50887932, 0.49998776, 0.50610516, 0.49808017,\n        0.49191007, 0.496711  , 0.50148696, 0.4967017 , 0.50560829,\n        0.49798566, 0.49161596, 0.49743396, 0.51100367, 0.50293495,\n        0.5015905 , 0.50079727, 0.49945918, 0.50114836, 0.50263445,\n        0.50119808, 0.49569923, 0.50110428, 0.50342585, 0.50787965,\n...\n        0.50295392, 0.50284746, 0.50260425, 0.50425429, 0.50069327,\n        0.49342694, 0.51276839, 0.50373323, 0.49736068, 0.49599901,\n        0.50369913, 0.49341577, 0.50160524, 0.49111839, 0.51026016,\n        0.50500197, 0.50239204, 0.50529017, 0.49726665, 0.49448106,\n        0.50474329, 0.49841594, 0.50412002, 0.5080594 , 0.49767469,\n        0.50162658, 0.50285453, 0.50796939, 0.50805982, 0.49638362,\n        0.49985551, 0.49829408, 0.50393916, 0.5059218 , 0.5004735 ,\n        0.50284989, 0.50132884, 0.50201552, 0.50323524, 0.49739984,\n        0.50539512, 0.4989157 , 0.50078144, 0.50608571, 0.50961563,\n        0.50805756, 0.49897027, 0.50359845, 0.49902168, 0.50039856,\n        0.50145686, 0.49895601, 0.50278613, 0.4994125 , 0.50227443,\n        0.49685693, 0.50531528, 0.50536968, 0.49845245, 0.50088159,\n        0.50008184, 0.50083991, 0.50272711, 0.49841462, 0.50130171,\n        0.49913365, 0.49869388, 0.4984984 , 0.50077961, 0.49308891,\n        0.50523475, 0.4951056 , 0.50119276, 0.502516  , 0.50262765,\n        0.5040908 , 0.49525348, 0.50550588, 0.50206523, 0.50412873,\n        0.49792583, 0.5016798 , 0.49896366, 0.49937486, 0.50357401,\n        0.5015416 , 0.49753403, 0.50731068, 0.50403558, 0.49159652,\n        0.49543872, 0.4935486 , 0.49503655, 0.49698387, 0.49851264,\n        0.50295859, 0.50042883, 0.50341546, 0.50377805, 0.50071806]])</pre></li><li>p_outlier_1|C(participant_id)(chain, draw, C(participant_id)__factor_dim)float64-0.9222 -1.81 ... -0.5742 0.4424<pre>array([[[-0.92217858, -1.81035072,  0.42426053, ...,  0.65192824,\n         -1.13046554,  0.35059814],\n        [-1.56850801, -2.59827126,  0.17851373, ...,  0.16403989,\n         -1.060953  ,  0.37518947],\n        [-1.30438766, -2.24136661,  0.52318042, ...,  0.55818486,\n         -0.84920721,  0.63458635],\n        ...,\n        [-0.71634343, -2.24288338,  0.58737488, ...,  0.4109894 ,\n         -0.19219788,  0.90019429],\n        [-0.51337492, -1.08717403,  0.58127655, ...,  0.6302022 ,\n         -0.73033948,  1.16907119],\n        [-1.50077052, -3.22105799,  0.43738062, ...,  0.42752262,\n         -0.57819323,  0.94621664]],\n\n       [[-0.80795248, -2.42856565,  0.60406499, ...,  0.59801083,\n         -0.68267778,  0.57483959],\n        [-0.84916083, -2.10990641,  0.347696  , ...,  0.60993289,\n         -0.72658013,  0.53721962],\n        [-1.19261414, -1.75156377,  0.171408  , ...,  0.75924463,\n         -0.9070664 ,  0.52118225],\n        ...,\n        [-1.1742573 , -2.66719698,  0.4797503 , ...,  0.531512  ,\n         -0.89236957,  0.35684468],\n        [-1.25921919, -1.90718482,  0.45330399, ...,  0.61775125,\n         -1.19482125,  0.51555028],\n        [-0.9795142 , -2.53415234,  0.59174638, ...,  0.63642583,\n         -0.5741794 ,  0.44243239]]], shape=(2, 500, 10))</pre></li><li>v_1|participant_id_offset(chain, draw, v_1|participant_id__factor_dim)float64-1.123 -1.434 ... 0.2632 0.2035<pre>array([[[-1.12273672, -1.43430608,  0.36332888, ...,  0.71404708,\n          0.50664786,  0.18940295],\n        [-1.51947789, -1.60183398,  0.38579224, ...,  0.4057156 ,\n          0.34091879,  0.00926587],\n        [-0.62359633, -1.25675979,  0.1576721 , ...,  0.50358662,\n          0.62499521,  0.40784756],\n        ...,\n        [-0.78578795, -0.86625755,  0.25904068, ...,  0.37432778,\n          0.20492504, -0.09293801],\n        [-0.50457703, -1.06792376,  0.24428442, ...,  0.49615871,\n          0.56147718,  0.54075331],\n        [-0.9735064 , -1.31923795,  0.36901194, ...,  0.42483201,\n          0.36158784,  0.20073096]],\n\n       [[-0.85695968, -1.20781299,  0.24936475, ...,  0.55060278,\n          0.15190178,  0.34978578],\n        [-0.84674743, -1.2947821 ,  0.18715613, ...,  0.58072498,\n          0.26850687,  0.13435618],\n        [-0.94728901, -1.06984119,  0.25285548, ...,  0.19124924,\n          0.44537335,  0.35869658],\n        ...,\n        [-1.03337962, -1.10776446,  0.5486123 , ...,  0.47612124,\n          0.35064251,  0.15158864],\n        [-0.61044412, -0.94655263,  0.40152453, ...,  0.32066936,\n          0.51578984,  0.43961032],\n        [-1.05167853, -1.20489723,  0.22718183, ...,  0.39150299,\n          0.26321265,  0.20353359]]], shape=(2, 500, 10))</pre></li><li>v_C(trialtype)|participant_id_mu(chain, draw, v_C(trialtype)|participant_id__expr_dim)float64-0.1672 0.2117 ... 0.2513 -0.06343<pre>array([[[-0.16722805,  0.21171641],\n        [ 0.17712932, -0.16131414],\n        [ 0.0902443 ,  0.06914141],\n        ...,\n        [-0.08679806, -0.4356106 ],\n        [ 0.03069077,  0.44901746],\n        [ 0.08924784, -0.17810316]],\n\n       [[ 0.1553901 ,  0.72351926],\n        [ 0.14381198,  0.70442786],\n        [-0.54057402, -0.01462775],\n        ...,\n        [-0.43923828,  0.06372193],\n        [ 0.05009442,  0.13599765],\n        [ 0.25130221, -0.06343392]]], shape=(2, 500, 2))</pre></li><li>v_C(trialtype)|participant_id_offset(chain, draw, v_C(trialtype)|participant_id__expr_dim, v_C(trialtype)|participant_id__factor_dim)float64-0.9113 -1.057 ... 1.103 0.99<pre>array([[[[-0.91130033, -1.05717064, -1.04800779, ..., -1.07260431,\n          -0.97627255, -0.96432321],\n         [ 1.26954662,  1.03036908,  0.79226616, ...,  0.89148533,\n           1.04811205,  1.08893746]],\n\n        [[-0.87429229, -1.44494337, -1.23759654, ..., -1.1166454 ,\n          -1.28456444, -1.01805139],\n         [ 1.04716821,  0.86110089,  0.83139054, ...,  0.80765815,\n           0.95333882,  0.9163853 ]],\n\n        [[-1.11184983, -1.20539259, -1.30853152, ..., -1.52842958,\n          -1.70182013, -1.42416241],\n         [ 1.07634713,  1.31390959,  1.05579951, ...,  0.90429765,\n           1.06313547,  0.95457411]],\n\n        ...,\n\n        [[-0.9060125 , -1.29080099, -1.2241762 , ..., -1.07308668,\n          -1.08435427, -0.93016097],\n         [ 1.29254982,  1.10855004,  1.03161788, ...,  1.03197277,\n...\n          -1.33231937, -1.17768398],\n         [ 1.32641782,  0.91978447,  1.0301866 , ...,  0.90022125,\n           1.20846395,  1.0247765 ]],\n\n        ...,\n\n        [[-1.3055549 , -1.71867396, -2.19090753, ..., -1.56599903,\n          -1.71882003, -1.58108431],\n         [ 1.43049873,  1.09638624,  0.95211565, ...,  1.05964563,\n           1.25680421,  1.094131  ]],\n\n        [[-1.49395789, -2.15991557, -1.63703587, ..., -1.68185127,\n          -1.82508493, -1.78207404],\n         [ 1.18948307,  1.1629844 ,  0.91451451, ...,  1.01171704,\n           1.19720296,  1.17987297]],\n\n        [[-0.9631702 , -1.29852159, -1.70813096, ..., -1.49770218,\n          -1.48606837, -1.33603582],\n         [ 1.34595393,  1.05648807,  0.93349989, ...,  0.84482399,\n           1.103158  ,  0.9900352 ]]]], shape=(2, 500, 2, 10))</pre></li><li>p_outlier_Intercept(chain, draw)float64-2.156 -1.929 ... -2.248 -2.344<pre>array([[-2.15597522, -1.92914419, -2.27660139, -2.19662247, -2.24279709,\n        -2.12877494, -2.45530068, -2.52506045, -2.41290711, -2.24233487,\n        -2.27521835, -1.97996626, -2.13633484, -2.32405638, -2.07740467,\n        -2.2580938 , -2.23951473, -2.40933463, -2.38898678, -2.29265387,\n        -1.95498963, -1.91223503, -2.06001222, -2.49597739, -2.64858212,\n        -2.86590514, -2.668219  , -2.74689617, -2.82709948, -2.68087397,\n        -2.67859832, -2.75329325, -2.31803095, -2.7886679 , -2.61812833,\n        -2.88393768, -2.65295811, -2.85127901, -2.45409866, -2.44249389,\n        -2.25751605, -2.6620752 , -2.36444889, -2.35908773, -2.35430039,\n        -2.39001923, -2.49488049, -2.36437032, -2.5806005 , -2.33478408,\n        -2.37146552, -2.32603293, -2.6851422 , -2.56068287, -2.56870884,\n        -2.42031008, -2.35738892, -2.35441811, -2.32239013, -2.43290077,\n        -2.64582338, -2.858386  , -2.59150933, -2.60477232, -2.19811424,\n        -2.38920409, -2.30166236, -2.24695205, -1.98870814, -2.92282616,\n        -2.32255246, -2.13110492, -1.92285495, -2.59049615, -2.85248452,\n        -2.90888143, -2.88266268, -2.58977481, -2.56678965, -2.42580141,\n        -2.07874746, -2.29377307, -2.41123767, -2.28846214, -2.25254164,\n        -2.60380623, -2.71333525, -2.68452758, -2.36629424, -2.57903036,\n        -2.61623244, -2.38540406, -2.22753038, -2.11849326, -2.19115461,\n        -2.1172459 , -2.04056521, -2.62362171, -2.75022279, -2.166842  ,\n...\n        -1.99294245, -2.15548444, -2.69011758, -2.48180365, -2.70262431,\n        -2.81941594, -2.0185356 , -2.05592597, -1.99374305, -1.96265413,\n        -1.95517329, -1.9849407 , -1.98117569, -2.39340538, -1.93735318,\n        -2.04752569, -2.26141297, -2.12109816, -1.72498579, -1.67484   ,\n        -2.55834229, -2.17504813, -2.25774125, -2.22361153, -2.36595403,\n        -2.47449031, -2.21797011, -2.49478823, -2.48784212, -2.39205435,\n        -2.20273749, -2.01036236, -2.11520742, -2.59270231, -2.25773818,\n        -2.64980238, -2.74665431, -1.88998209, -2.37486431, -2.31785307,\n        -2.45779833, -2.53213496, -2.61224274, -2.61217028, -1.89047487,\n        -2.09681349, -2.21580982, -2.36771527, -2.82478157, -2.53043088,\n        -2.6229486 , -2.39447705, -2.34840609, -2.32896004, -2.56279283,\n        -2.44316963, -2.64345401, -2.49439355, -2.59146417, -2.69174521,\n        -2.64788383, -2.68278612, -2.27917723, -2.63199496, -2.50568803,\n        -2.51468235, -2.30285083, -2.33470613, -2.62515492, -2.56889359,\n        -2.40840443, -2.4985178 , -2.57481475, -2.55010583, -2.53540185,\n        -2.61118961, -2.80815908, -2.87661999, -2.8221422 , -2.81287861,\n        -2.28051385, -2.25485479, -2.25165836, -2.17534328, -1.88227509,\n        -1.92640634, -2.00569303, -2.14012969, -2.13899013, -2.12516613,\n        -2.00367711, -1.99055892, -2.04919197, -2.49586718, -2.47381423,\n        -2.68020116, -2.85436354, -2.09277636, -2.24801634, -2.34393571]])</pre></li><li>v_C(trialtype)|participant_id(chain, draw, v_C(trialtype)|participant_id__expr_dim, v_C(trialtype)|participant_id__factor_dim)float64-0.9523 -1.105 ... 1.079 0.9681<pre>array([[[[-0.95233928, -1.10477862, -1.09520314, ..., -1.12090733,\n          -1.02023742, -1.00774996],\n         [ 1.35411884,  1.09900822,  0.84504382, ...,  0.95087259,\n           1.11793317,  1.16147821]],\n\n        [[-0.72047663, -1.19073214, -1.01986417, ..., -0.92019217,\n          -1.05856894, -0.83894396],\n         [ 1.25130047,  1.02896167,  0.99345966, ...,  0.96510093,\n           1.13918022,  1.09502307]],\n\n        [[-0.85818854, -0.93039013, -1.00999859, ..., -1.17972834,\n          -1.31356097, -1.09924905],\n         [ 0.98330522,  1.20033224,  0.96453378, ...,  0.82612809,\n           0.97123561,  0.87205854]],\n\n        ...,\n\n        [[-0.83027665, -1.18289972, -1.12184426, ..., -0.98338469,\n          -0.99371039, -0.8524065 ],\n         [ 1.07093866,  0.91848613,  0.8547442 , ...,  0.85503824,\n...\n          -1.18594649, -1.04829985],\n         [ 1.23839937,  0.8587494 ,  0.96182547, ...,  0.84048436,\n           1.12827269,  0.95677437]],\n\n        ...,\n\n        [[-0.78330633, -1.03116935, -1.31449987, ..., -0.9395675 ,\n          -1.03125699, -0.94861837],\n         [ 1.2626401 ,  0.96773328,  0.84039179, ...,  0.93530392,\n           1.10932737,  0.96574268]],\n\n        [[-0.93755791, -1.35549064, -1.02734886, ..., -1.05547351,\n          -1.14536215, -1.11836996],\n         [ 1.06851769,  1.04471383,  0.82151226, ...,  0.90882972,\n           1.0754525 ,  1.0598849 ]],\n\n        [[-0.65106736, -0.87775247, -1.15463322, ..., -1.01239116,\n          -1.00452714, -0.9031107 ],\n         [ 1.31618433,  1.03312083,  0.91285288, ...,  0.8261383 ,\n           1.07875852,  0.96813776]]]], shape=(2, 500, 2, 10))</pre></li><li>v_1|participant_id_sigma(chain, draw)float640.398 0.3293 ... 0.572 0.4804<pre>array([[0.3979868 , 0.32933594, 0.52029442, 0.42819384, 0.42729182,\n        0.3643724 , 0.56182136, 0.45912385, 0.42153848, 0.32569417,\n        0.45317658, 0.49011546, 0.45061999, 0.45397927, 0.40939949,\n        0.40573562, 0.3314505 , 0.33098722, 0.32850394, 0.34852727,\n        0.4161696 , 0.39982608, 0.38096634, 0.30765596, 0.34467893,\n        0.46751969, 0.47415263, 0.42140784, 0.44638106, 0.42817908,\n        0.42933829, 0.45543281, 0.36800215, 0.39914948, 0.44904628,\n        0.37450812, 0.48934611, 0.50928904, 0.37922091, 0.40765049,\n        0.4104457 , 0.49704508, 0.61404095, 0.55840902, 0.56050524,\n        0.61176122, 0.48762216, 0.33518767, 0.36727743, 0.29637743,\n        0.54467728, 0.5231622 , 0.50729212, 0.53765312, 0.65558449,\n        0.66633581, 0.70448123, 0.92957223, 0.84029306, 0.82642217,\n        0.67489531, 0.80645983, 0.68944771, 0.5483914 , 0.53474121,\n        0.5811727 , 0.44334624, 0.47180089, 0.42971432, 0.39926776,\n        0.60347956, 0.37904013, 0.34034675, 0.45718583, 0.52852941,\n        0.578508  , 0.55155675, 0.6152597 , 0.50861047, 0.47834241,\n        0.59197849, 0.65315302, 0.56721444, 0.56630229, 0.51423798,\n        0.45695911, 0.40606294, 0.49694156, 0.39313365, 0.41985871,\n        0.49288797, 0.49214982, 0.36271614, 0.34570292, 0.37643242,\n        0.37114374, 0.50281711, 0.37930592, 0.41470644, 0.50707687,\n...\n        0.3939573 , 0.41249924, 0.3544989 , 0.33287899, 0.34449758,\n        0.32237592, 0.43571218, 0.47560047, 0.51540517, 0.51433227,\n        0.43359615, 0.40360078, 0.27445566, 0.62217104, 0.60695308,\n        0.61319197, 0.53275449, 0.5819923 , 0.35599382, 0.37060172,\n        0.3735807 , 0.36815088, 0.33880878, 0.3825467 , 0.34910413,\n        0.31096672, 0.32605089, 0.32249104, 0.32718263, 0.27615842,\n        0.61042364, 0.5465735 , 0.39608367, 0.32195674, 0.37881344,\n        0.37514877, 0.38913489, 0.52632365, 0.59402137, 0.50932707,\n        0.44906556, 0.49922105, 0.52096307, 0.52717668, 0.4303631 ,\n        0.46231282, 0.41819654, 0.41861819, 0.41691553, 0.43756577,\n        0.41458066, 0.43588102, 0.32943014, 0.39509044, 0.4938051 ,\n        0.5214652 , 0.59676008, 0.52158961, 0.52987845, 0.58868598,\n        0.51113836, 0.57071532, 0.41544596, 0.36168942, 0.30692826,\n        0.32128409, 0.35879239, 0.41043493, 0.42716313, 0.44615748,\n        0.45273156, 0.36427702, 0.45279313, 0.45994514, 0.62512567,\n        0.47415751, 0.449387  , 0.49261915, 0.49704331, 0.45989254,\n        0.41384225, 0.51186444, 0.49878239, 0.38938962, 0.3568318 ,\n        0.56227522, 0.55634914, 0.43510076, 0.43243072, 0.38228025,\n        0.37297985, 0.41456323, 0.42553742, 0.45424908, 0.40956618,\n        0.33426429, 0.4066109 , 0.48096143, 0.57196668, 0.48039781]])</pre></li><li>v(chain, draw, __obs__)float640.9073 0.9073 ... -0.8053 -0.8053<pre>array([[[ 0.90728445,  0.90728445,  0.90728445, ..., -0.93237009,\n         -0.93237009, -0.93237009],\n        [ 0.75088179,  0.75088179,  0.75088179, ..., -0.83589238,\n         -0.83589238, -0.83589238],\n        [ 0.65885153,  0.65885153,  0.65885153, ..., -0.88704823,\n         -0.88704823, -0.88704823],\n        ...,\n        [ 0.59776784,  0.59776784,  0.59776784, ..., -0.90837014,\n         -0.90837014, -0.90837014],\n        [ 0.77726106,  0.77726106,  0.77726106, ..., -0.83233051,\n         -0.83233051, -0.83233051],\n        [ 0.86099707,  0.86099707,  0.86099707, ..., -0.83531458,\n         -0.83531458, -0.83531458]],\n\n       [[ 0.74457451,  0.74457451,  0.74457451, ..., -0.82244105,\n         -0.82244105, -0.82244105],\n        [ 0.66435964,  0.66435964,  0.66435964, ..., -0.97915672,\n         -0.97915672, -0.97915672],\n        [ 0.785052  ,  0.785052  ,  0.785052  , ..., -0.87663718,\n         -0.87663718, -0.87663718],\n        ...,\n        [ 0.76562436,  0.76562436,  0.76562436, ..., -0.87571008,\n         -0.87571008, -0.87571008],\n        [ 0.71936399,  0.71936399,  0.71936399, ..., -0.86692751,\n         -0.86692751, -0.86692751],\n        [ 0.81096027,  0.81096027,  0.81096027, ..., -0.8053336 ,\n         -0.8053336 , -0.8053336 ]]], shape=(2, 500, 7500))</pre></li><li>p_outlier(chain, draw, __obs__)float640.04402 0.04402 ... 0.1299 0.1299<pre>array([[[0.04401744, 0.04401744, 0.04401744, ..., 0.14119778,\n         0.14119778, 0.14119778],\n        [0.02937911, 0.02937911, 0.02937911, ..., 0.17451582,\n         0.17451582, 0.17451582],\n        [0.02709363, 0.02709363, 0.02709363, ..., 0.16219106,\n         0.16219106, 0.16219106],\n        ...,\n        [0.04059269, 0.04059269, 0.04059269, ..., 0.17563724,\n         0.17563724, 0.17563724],\n        [0.04470405, 0.04470405, 0.04470405, ..., 0.20108753,\n         0.20108753, 0.20108753],\n        [0.02240671, 0.02240671, 0.02240671, ..., 0.20936684,\n         0.20936684, 0.20936684]],\n\n       [[0.04453503, 0.04453503, 0.04453503, ..., 0.15668147,\n         0.15668147, 0.15668147],\n        [0.04606175, 0.04606175, 0.04606175, ..., 0.16188954,\n         0.16188954, 0.16188954],\n        [0.02898049, 0.02898049, 0.02898049, ..., 0.14210354,\n         0.14210354, 0.14210354],\n        ...,\n        [0.03671961, 0.03671961, 0.03671961, ..., 0.14983042,\n         0.14983042, 0.14983042],\n        [0.02910706, 0.02910706, 0.02910706, ..., 0.15027242,\n         0.15027242, 0.15027242],\n        [0.03477542, 0.03477542, 0.03477542, ..., 0.12993842,\n         0.12993842, 0.12993842]]], shape=(2, 500, 7500))</pre></li></ul></li><li>Indexes: (7)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>v_1|participant_id__factor_dimPandasIndex<pre>PandasIndex(Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='object', name='v_1|participant_id__factor_dim'))</pre></li><li>v_C(trialtype)|participant_id__expr_dimPandasIndex<pre>PandasIndex(Index(['someswitch', 'switch'], dtype='object', name='v_C(trialtype)|participant_id__expr_dim'))</pre></li><li>v_C(trialtype)|participant_id__factor_dimPandasIndex<pre>PandasIndex(Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='object', name='v_C(trialtype)|participant_id__factor_dim'))</pre></li><li>C(participant_id)__factor_dimPandasIndex<pre>PandasIndex(Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='object', name='C(participant_id)__factor_dim'))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499],\n      dtype='int64', name='__obs__', length=7500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T18:57:44.208796+00:00arviz_version :0.22.0inference_library :numpyroinference_library_version :0.19.0sampling_time :1494.128823tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 60MB\nDimensions:      (chain: 2, draw: 500, __obs__: 7500)\nCoordinates:\n  * chain        (chain) int64 16B 0 1\n  * draw         (draw) int64 4kB 0 1 2 3 4 5 6 ... 493 494 495 496 497 498 499\n  * __obs__      (__obs__) int64 60kB 0 1 2 3 4 5 ... 7495 7496 7497 7498 7499\nData variables:\n    rt,response  (chain, draw, __obs__) float64 60MB -1.594 -0.09011 ... -1.824\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li><li>__obs__: 7500</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 ... 7496 7497 7498 7499<pre>array([   0,    1,    2, ..., 7497, 7498, 7499], shape=(7500,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-1.594 -0.09011 ... -5.036 -1.824<pre>array([[[-1.59389903, -0.09010676, -3.94874062, ..., -0.56139782,\n         -4.95331381, -1.83645019],\n        [-1.58820515, -0.17442989, -3.85242505, ..., -0.64749003,\n         -4.7414627 , -1.88198676],\n        [-1.56863109, -0.25721716, -3.72632893, ..., -0.61476299,\n         -4.81470218, -1.88507726],\n        ...,\n        [-1.60705242, -0.28562487, -3.70318811, ..., -0.61102643,\n         -4.73505745, -1.89593436],\n        [-1.57638521, -0.19747739, -3.85031825, ..., -0.68431784,\n         -4.59973929, -1.93218779],\n        [-1.58934621, -0.08477123, -3.98375003, ..., -0.67696902,\n         -4.55939207, -1.88967752]],\n\n       [[-1.56904085, -0.21271813, -3.76521573, ..., -0.63202709,\n         -4.8492619 , -1.84789398],\n        [-1.57748303, -0.27089844, -3.69838915, ..., -0.57454288,\n         -4.81656297, -1.90921989],\n        [-1.57151823, -0.15645715, -3.85346298, ..., -0.58955141,\n         -4.94691952, -1.84862079],\n        ...,\n        [-1.56078339, -0.1921747 , -3.77283193, ..., -0.60202203,\n         -4.89397204, -1.83930294],\n        [-1.56424837, -0.20845067, -3.76442664, ..., -0.60433556,\n         -4.89102648, -1.84868478],\n        [-1.5688645 , -0.15459483, -3.85797104, ..., -0.61246445,\n         -5.03641346, -1.82357344]]], shape=(2, 500, 7500))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499],\n      dtype='int64', name='__obs__', length=7500))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 53kB\nDimensions:          (chain: 2, draw: 500)\nCoordinates:\n  * chain            (chain) int64 16B 0 1\n  * draw             (draw) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\nData variables:\n    acceptance_rate  (chain, draw) float64 8kB 0.8577 1.0 ... 0.7922 0.9614\n    step_size        (chain, draw) float64 8kB 0.1055 0.1055 ... 0.1125 0.1125\n    diverging        (chain, draw) bool 1kB False False False ... False False\n    energy           (chain, draw) float64 8kB 1.107e+04 1.107e+04 ... 1.107e+04\n    n_steps          (chain, draw) int64 8kB 31 63 63 95 31 ... 31 63 95 31 63\n    tree_depth       (chain, draw) int64 8kB 5 6 6 7 5 5 5 5 ... 5 6 5 5 6 7 5 6\n    lp               (chain, draw) float64 8kB 1.104e+04 1.104e+04 ... 1.104e+04\nAttributes:\n    created_at:                  2025-09-27T18:57:44.266359+00:00\n    arviz_version:               0.22.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (7)<ul><li>acceptance_rate(chain, draw)float640.8577 1.0 0.9794 ... 0.7922 0.9614<pre>array([[0.85766441, 1.        , 0.97941993, 0.87799001, 0.66057002,\n        0.96100212, 0.71212282, 0.91431139, 0.98439777, 0.96850974,\n        0.67003686, 0.99942566, 0.93397179, 0.83739976, 0.98292886,\n        0.95618858, 0.9737181 , 0.95568065, 0.96711451, 0.73367069,\n        0.81202877, 0.85139189, 0.85470858, 0.98129661, 0.99939167,\n        0.94122111, 0.98460493, 0.92795437, 0.97627153, 0.63145554,\n        0.99776376, 0.76188792, 0.83012651, 0.98870474, 0.94795379,\n        0.91120105, 0.99146732, 0.75723983, 0.96834659, 0.83428521,\n        0.98305923, 1.        , 0.91144521, 0.84150386, 0.94586234,\n        0.93480691, 0.91764447, 0.93576869, 0.98997731, 0.92650619,\n        0.98540715, 0.99402726, 1.        , 0.94417014, 0.96781696,\n        0.92167515, 0.84653678, 0.89304995, 0.94269115, 0.97811468,\n        0.97463711, 0.85695632, 0.97027902, 0.70439679, 0.9208118 ,\n        0.81854874, 0.99321429, 0.92096722, 0.8821967 , 0.94713224,\n        0.98948606, 0.90526953, 0.83147268, 0.81381233, 0.95526749,\n        1.        , 0.97370864, 0.99452445, 0.87707376, 0.78378871,\n        0.98229991, 0.95460942, 0.9399773 , 0.83738198, 0.9637077 ,\n        0.82187325, 0.92633183, 0.9689411 , 0.87407857, 0.8065382 ,\n        0.90297387, 0.88231709, 0.699969  , 0.789111  , 0.93002367,\n        0.98230589, 0.85305498, 0.9185434 , 0.8914941 , 0.980289  ,\n...\n        0.99958641, 0.72937337, 0.79931363, 0.71824665, 0.99251741,\n        0.66177884, 1.        , 0.94106299, 0.9826299 , 0.9369173 ,\n        0.76671347, 0.91891863, 0.98955086, 0.87154907, 0.9096578 ,\n        0.9656679 , 0.97739219, 0.64182075, 0.9398039 , 0.80741272,\n        0.62891792, 0.77051745, 0.95991532, 0.90597797, 0.88380578,\n        0.86426301, 0.82445844, 0.99990054, 0.98938658, 0.94807151,\n        0.9654385 , 0.96639257, 0.89736782, 0.77355535, 0.88490399,\n        0.99812299, 0.81904711, 0.75014537, 0.99519004, 0.709598  ,\n        0.98868663, 0.99667575, 0.84491292, 0.94330746, 0.94983765,\n        0.70238463, 0.85075742, 0.92651437, 0.68493659, 0.99967254,\n        0.7794337 , 0.94266607, 0.79431317, 0.92181297, 0.98966449,\n        0.97523705, 0.8372493 , 0.9857852 , 0.9959557 , 0.97991825,\n        0.9575515 , 0.84896225, 0.74527422, 0.79474866, 0.98998571,\n        0.98027199, 0.91580517, 0.9109444 , 0.98062794, 0.73282395,\n        0.94511211, 0.86209983, 0.91972691, 0.94575144, 0.99596229,\n        0.97525017, 0.65380117, 1.        , 0.88945114, 0.99024914,\n        0.9182207 , 0.86306543, 0.98239812, 0.96034461, 0.93171671,\n        0.7918796 , 0.7904338 , 0.44392622, 0.97218707, 0.96578567,\n        0.82476109, 0.94087043, 0.99449092, 0.94136052, 0.97550418,\n        0.96690207, 0.94125167, 0.82559156, 0.79220335, 0.96141244]])</pre></li><li>step_size(chain, draw)float640.1055 0.1055 ... 0.1125 0.1125<pre>array([[0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n        0.10554138, 0.10554138, 0.10554138, 0.10554138, 0.10554138,\n...\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486,\n        0.11245486, 0.11245486, 0.11245486, 0.11245486, 0.11245486]])</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False]])</pre></li><li>energy(chain, draw)float641.107e+04 1.107e+04 ... 1.107e+04<pre>array([[11069.72368448, 11069.99055974, 11060.17912799, 11065.87275656,\n        11065.17725383, 11073.00822958, 11072.56282181, 11065.90036668,\n        11074.01471057, 11074.02759712, 11074.60090829, 11061.77974583,\n        11061.74333982, 11069.71852431, 11061.18763172, 11056.57484786,\n        11068.17057838, 11060.81349472, 11062.03683338, 11067.95569775,\n        11069.43382097, 11067.19292502, 11067.5952454 , 11078.27721652,\n        11075.18363953, 11068.93254623, 11072.43964206, 11069.59574996,\n        11076.11063922, 11080.33578243, 11069.44015719, 11076.9984568 ,\n        11075.00459531, 11079.77475974, 11078.59462959, 11077.03081183,\n        11060.51829603, 11070.7707861 , 11063.36552368, 11062.71554979,\n        11065.6421888 , 11059.62561047, 11066.8219877 , 11061.89081748,\n        11064.22972911, 11069.40999675, 11080.59457726, 11069.57932765,\n        11073.31115437, 11098.81450071, 11080.08089277, 11076.95429519,\n        11062.81965197, 11057.53431948, 11054.41647894, 11054.24543696,\n        11068.66657791, 11069.58132486, 11060.13733109, 11061.14100781,\n        11060.96790864, 11060.24072386, 11064.41010506, 11060.87465802,\n        11061.58341607, 11062.46548002, 11057.0647316 , 11063.33697028,\n        11061.67229866, 11063.83034433, 11060.52964354, 11065.78624276,\n        11063.49087112, 11075.61255857, 11079.10743628, 11070.86635609,\n        11063.81262879, 11065.48736106, 11061.85819016, 11056.91322151,\n...\n        11080.16710126, 11076.41339419, 11064.64901733, 11059.68434001,\n        11072.32934028, 11067.85975949, 11064.83847131, 11066.56615989,\n        11072.87062282, 11073.87043322, 11084.9073876 , 11067.86534434,\n        11072.5685314 , 11077.86493268, 11067.1497906 , 11067.24853909,\n        11065.35417743, 11078.1156764 , 11069.26640797, 11065.36493917,\n        11065.54909509, 11058.20240321, 11062.06436604, 11066.07649883,\n        11077.28446721, 11076.222642  , 11065.04960116, 11069.44267398,\n        11076.24288043, 11070.94872914, 11071.02651576, 11070.1502479 ,\n        11071.60153729, 11064.22943501, 11069.82743172, 11058.17205561,\n        11067.33926351, 11067.38138979, 11055.66427831, 11056.19343645,\n        11059.26618897, 11056.59561578, 11065.02031205, 11075.78907549,\n        11069.98480353, 11079.92337667, 11060.86091256, 11060.24511823,\n        11064.88571298, 11063.90367491, 11061.7600543 , 11071.97545387,\n        11072.89976367, 11068.94900723, 11070.20047203, 11068.08580274,\n        11067.65932332, 11068.19882487, 11061.53361396, 11052.75080098,\n        11052.98455683, 11056.75994931, 11058.10025072, 11059.94778709,\n        11066.71300564, 11071.88587957, 11077.776286  , 11082.28370729,\n        11077.25876279, 11064.02236418, 11065.8127158 , 11064.44136202,\n        11057.02840522, 11061.86670645, 11071.21653971, 11072.31778009,\n        11071.70418289, 11081.12757073, 11075.57132848, 11071.83439814]])</pre></li><li>n_steps(chain, draw)int6431 63 63 95 31 ... 31 63 95 31 63<pre>array([[ 31,  63,  63,  95,  31,  31,  31,  31,  31, 111,  63,  95, 127,\n         63,  31,  31, 127,  31,  63,  31, 127,  63,  31,  63,  63,  63,\n         63,  31,  31,  31,  63,  15,  63,  31, 127,  31,  31,  31,  95,\n         63,  31,  31,  31,  31,  63,  47, 127, 127,  15, 111,  79,  63,\n         63,  31,  31,  31, 127,  31,  31,  15,  31,  31,  31,  31,  63,\n         31,  31,  63,  63, 127, 127,  95,  63,  95,  63,  31,  31,  63,\n         63,  95,  63,  31,  31,  31, 127,  63,  31,  31,  63,  63,  31,\n         63,  63,  31,  31, 127,  63, 127,  63,  95,  63,  95,  63,  15,\n         95,  31,  63,  31,  31,  63,  31,  63,  63, 127,  31,  31,  31,\n         31,  63,  63,  31,  95,  63,  63, 127,  31,  31,  31,  31,  63,\n         31,  63,  31,  31,  63,  31,  31,  63,  47,  63,  95,  31,  31,\n         63,  63, 127,  63,  63,  79,  63,  31,  79,  31,  63,  63,  31,\n         31,  31, 127,  31,  31,  63,  31,  31,  31,  95,  63,  31,  63,\n         31,  63,  31,  63,  63,  31,  15,  63,  63,  31,  63,  63,  95,\n         63,  63,  63,  63,  95,  63,  63,  63,  63,  63,  95,  63,  31,\n         31,  31,  31, 127,  95,  15,  95,  31,  63,  31,  31,  31,  31,\n         63, 127,  31,  63,  63,  63,  31,  63,  63,  63,  63,  63,  63,\n         63,  31,  31,  31,  31,  31,  31,  31, 127,  63,  63,  63,  31,\n         31,  31,  31,  63,  95,  31,  31, 127,  63,  31,  31, 127,  63,\n         63,  31,  31,  63,  63,  63,  63, 127,  63,  63,  63,  63,  63,\n...\n         63,  95,  31,  31,  31,  63,  31,  31, 127,  95,  63,  95,  31,\n         63,  63,  95,  63, 127,  31,  31,  31,  31,  31,  31,  63,  31,\n         31,  31,  31,  63,  63,  31,  31,  15,  63, 127,  95, 127,  63,\n         31,  47,  31,  31,  63,  31,  31,  31,  31,  31,  31,  31,  31,\n         31,  31,  63,  63,  31,  63,  63,  63,  63,  31,  63,  31,  63,\n         31,  31, 127,  63,  31,  63,  63,  31,  63,  47,  31,  15,  31,\n         31,  15, 127,  63,  63,  31,  63,  31,  63,  31,  63,  31,  31,\n         63,  95,  31,  63,  63,  63,  47,  95,  31,  95,  95,  63,  31,\n         63,  63,  31,  63,  31,  31,  31,  95,  31,  31,  31,  31,  31,\n         31, 127, 127,  31,  31,  31,  31,  31,  31,  31, 127,  47,  63,\n         63,  63,  63,  31,  95,  63,  31,  63,  63,  31,  31,  31,  63,\n         31,  31,  63,  31,  63,  31,  63,  31,  31,  63,  31, 127, 127,\n         15,  31,  15,  63,  63,  63,  63,  31,  31,  63,  63,  31,  31,\n         63,  95,  95,  31,  63,  63,  63,  63,  63,  63,  31,  31,  31,\n         63,  95,  31,  31, 127,  63,  95,  31, 127,  63,  31,  31,  63,\n         95,  31,  79,  63,  31,  63,  31,  31,  31,  31,  31,  31,  63,\n         63,  31,  31,  31,  31, 127,  31,  95,  79,  63,  95,  31,  95,\n         31,  31,  63,  31,  31,  63,  31,  31,  31,  31,  31,  15,  63,\n         31,  31,  31,  63,  63,  31, 111,  63,  31,  31,  63,  31,  63,\n         31,  31,  63,  95,  31,  63]])</pre></li><li>tree_depth(chain, draw)int645 6 6 7 5 5 5 5 ... 5 6 5 5 6 7 5 6<pre>array([[5, 6, 6, 7, 5, 5, 5, 5, 5, 7, 6, 7, 7, 6, 5, 5, 7, 5, 6, 5, 7, 6,\n        5, 6, 6, 6, 6, 5, 5, 5, 6, 4, 6, 5, 7, 5, 5, 5, 7, 6, 5, 5, 5, 5,\n        6, 6, 7, 7, 4, 7, 7, 6, 6, 5, 5, 5, 7, 5, 5, 4, 5, 5, 5, 5, 6, 5,\n        5, 6, 6, 7, 7, 7, 6, 7, 6, 5, 5, 6, 6, 7, 6, 5, 5, 5, 7, 6, 5, 5,\n        6, 6, 5, 6, 6, 5, 5, 7, 6, 7, 6, 7, 6, 7, 6, 4, 7, 5, 6, 5, 5, 6,\n        5, 6, 6, 7, 5, 5, 5, 5, 6, 6, 5, 7, 6, 6, 7, 5, 5, 5, 5, 6, 5, 6,\n        5, 5, 6, 5, 5, 6, 6, 6, 7, 5, 5, 6, 6, 7, 6, 6, 7, 6, 5, 7, 5, 6,\n        6, 5, 5, 5, 7, 5, 5, 6, 5, 5, 5, 7, 6, 5, 6, 5, 6, 5, 6, 6, 5, 4,\n        6, 6, 5, 6, 6, 7, 6, 6, 6, 6, 7, 6, 6, 6, 6, 6, 7, 6, 5, 5, 5, 5,\n        7, 7, 4, 7, 5, 6, 5, 5, 5, 5, 6, 7, 5, 6, 6, 6, 5, 6, 6, 6, 6, 6,\n        6, 6, 5, 5, 5, 5, 5, 5, 5, 7, 6, 6, 6, 5, 5, 5, 5, 6, 7, 5, 5, 7,\n        6, 5, 5, 7, 6, 6, 5, 5, 6, 6, 6, 6, 7, 6, 6, 6, 6, 6, 7, 7, 6, 5,\n        7, 5, 6, 5, 5, 6, 6, 6, 6, 5, 7, 5, 6, 6, 6, 6, 5, 5, 6, 6, 6, 6,\n        7, 6, 5, 4, 6, 5, 7, 5, 7, 6, 6, 6, 6, 6, 6, 6, 6, 5, 7, 7, 5, 5,\n        5, 6, 5, 5, 6, 7, 6, 7, 7, 7, 7, 5, 6, 5, 6, 5, 5, 5, 6, 4, 5, 6,\n        6, 5, 5, 6, 7, 6, 6, 7, 7, 6, 5, 6, 5, 7, 6, 6, 7, 7, 7, 6, 7, 4,\n        5, 7, 6, 5, 6, 6, 6, 6, 6, 7, 7, 8, 6, 6, 6, 6, 6, 6, 5, 7, 5, 6,\n        7, 6, 6, 6, 7, 7, 7, 5, 5, 5, 5, 6, 7, 6, 4, 5, 6, 5, 6, 6, 6, 6,\n        7, 5, 6, 6, 5, 7, 7, 6, 5, 6, 7, 7, 6, 5, 5, 6, 6, 5, 5, 6, 6, 5,\n        6, 6, 5, 6, 5, 7, 6, 5, 5, 6, 7, 5, 7, 5, 7, 5, 7, 5, 5, 5, 6, 6,\n...\n        6, 5, 6, 5, 5, 6, 6, 6, 6, 4, 6, 6, 5, 7, 6, 5, 5, 5, 7, 6, 5, 5,\n        5, 5, 5, 5, 6, 6, 7, 5, 6, 5, 6, 6, 6, 6, 5, 6, 6, 6, 7, 6, 6, 6,\n        5, 5, 7, 6, 6, 6, 6, 6, 5, 5, 6, 6, 4, 4, 6, 5, 7, 5, 5, 7, 5, 5,\n        5, 7, 6, 5, 5, 5, 5, 6, 6, 4, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 6, 6,\n        5, 6, 6, 6, 5, 7, 6, 5, 6, 6, 6, 6, 6, 7, 7, 5, 6, 6, 5, 4, 6, 5,\n        5, 6, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 6, 6,\n        7, 6, 6, 4, 5, 5, 6, 5, 5, 5, 5, 6, 5, 7, 7, 4, 5, 5, 5, 5, 5, 6,\n        5, 5, 5, 6, 5, 5, 6, 5, 7, 5, 5, 6, 4, 5, 6, 5, 7, 7, 6, 5, 6, 5,\n        5, 6, 5, 6, 6, 6, 7, 5, 5, 5, 6, 5, 5, 7, 7, 6, 7, 5, 6, 6, 7, 6,\n        7, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5, 5, 6, 6, 5, 5, 4, 6, 7, 7, 7, 6,\n        5, 6, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 5, 6, 6, 6, 6,\n        5, 6, 5, 6, 5, 5, 7, 6, 5, 6, 6, 5, 6, 6, 5, 4, 5, 5, 4, 7, 6, 6,\n        5, 6, 5, 6, 5, 6, 5, 5, 6, 7, 5, 6, 6, 6, 6, 7, 5, 7, 7, 6, 5, 6,\n        6, 5, 6, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5,\n        7, 6, 6, 6, 6, 6, 5, 7, 6, 5, 6, 6, 5, 5, 5, 6, 5, 5, 6, 5, 6, 5,\n        6, 5, 5, 6, 5, 7, 7, 4, 5, 4, 6, 6, 6, 6, 5, 5, 6, 6, 5, 5, 6, 7,\n        7, 5, 6, 6, 6, 6, 6, 6, 5, 5, 5, 6, 7, 5, 5, 7, 6, 7, 5, 7, 6, 5,\n        5, 6, 7, 5, 7, 6, 5, 6, 5, 5, 5, 5, 5, 5, 6, 6, 5, 5, 5, 5, 7, 5,\n        7, 7, 6, 7, 5, 7, 5, 5, 6, 5, 5, 6, 5, 5, 5, 5, 5, 4, 6, 5, 5, 5,\n        6, 6, 5, 7, 6, 5, 5, 6, 5, 6, 5, 5, 6, 7, 5, 6]])</pre></li><li>lp(chain, draw)float641.104e+04 1.104e+04 ... 1.104e+04<pre>array([[11043.75091703, 11038.04082754, 11033.9545578 , 11041.87939255,\n        11045.73461249, 11047.05410485, 11045.33990302, 11044.0073329 ,\n        11052.0290493 , 11044.58790774, 11041.37927347, 11037.67487282,\n        11038.37024009, 11040.9452714 , 11035.46721947, 11040.34168629,\n        11039.49952183, 11043.97739234, 11038.48970354, 11037.57968093,\n        11039.57874162, 11044.12946084, 11047.55710254, 11052.29509116,\n        11047.11758798, 11045.06191271, 11044.93805548, 11046.21379878,\n        11048.32511792, 11049.67519975, 11042.78512508, 11047.50531618,\n        11050.66513856, 11051.75784599, 11049.95385276, 11040.60814191,\n        11043.55596015, 11044.55956848, 11037.15923449, 11042.42620746,\n        11043.03259436, 11036.00737182, 11034.6100264 , 11038.37826116,\n        11042.97061366, 11049.71353191, 11046.47417525, 11046.58535652,\n        11057.32773653, 11059.88542381, 11048.57490189, 11044.18809254,\n        11035.96370249, 11032.75882679, 11033.84527444, 11040.18476306,\n        11042.68005616, 11041.957095  , 11031.34480915, 11039.17001701,\n        11032.63076101, 11036.87393015, 11035.99239817, 11035.78124637,\n        11037.72432663, 11037.39691693, 11039.57569287, 11037.51346614,\n        11041.08850546, 11035.57712867, 11041.54939298, 11035.10833101,\n        11041.83279464, 11049.55641267, 11052.52328901, 11044.69878946,\n        11044.83615907, 11039.48775983, 11033.04196834, 11038.49410108,\n...\n        11052.1611176 , 11040.0463718 , 11037.62798793, 11043.99045246,\n        11041.25460413, 11041.42762067, 11045.4072395 , 11048.19034023,\n        11047.78704742, 11052.70621449, 11048.87145931, 11041.39429215,\n        11045.46079871, 11041.70925607, 11043.34538809, 11040.33664939,\n        11045.739785  , 11046.70236078, 11031.92944578, 11044.83691761,\n        11041.00797205, 11041.5599584 , 11039.33606726, 11044.81927219,\n        11050.437636  , 11046.65759051, 11038.98324032, 11045.62882096,\n        11053.32803878, 11039.22951473, 11047.51155874, 11048.01638489,\n        11042.1078171 , 11043.35562874, 11038.14563341, 11039.40634209,\n        11046.0031235 , 11039.25993293, 11033.6511059 , 11038.79597086,\n        11037.52135572, 11037.1672036 , 11047.04304251, 11047.41793273,\n        11048.22076491, 11044.27789251, 11035.14105416, 11034.19328095,\n        11036.72275779, 11038.72400584, 11038.87347748, 11045.01948008,\n        11046.46222181, 11051.06214316, 11043.58232809, 11039.75466147,\n        11041.2316548 , 11040.70573729, 11035.43556711, 11033.47659632,\n        11031.83358736, 11036.96811062, 11039.25980109, 11035.58927897,\n        11044.8213847 , 11049.65612656, 11044.21458746, 11050.57690866,\n        11045.13082867, 11044.12002617, 11041.15588776, 11040.48977865,\n        11041.15494145, 11040.99462139, 11046.96504157, 11044.01060853,\n        11052.29145506, 11046.47183921, 11049.51719689, 11039.51064768]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (4)created_at :2025-09-27T18:57:44.266359+00:00arviz_version :0.22.0modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 180kB\nDimensions:                  (__obs__: 7500, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 60kB 0 1 2 3 ... 7497 7498 7499\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 120kB ...\nAttributes:\n    created_at:                  2025-09-27T18:57:44.267620+00:00\n    arviz_version:               0.22.0\n    inference_library:           numpyro\n    inference_library_version:   0.19.0\n    sampling_time:               1494.128823\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 7500</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 ... 7496 7497 7498 7499<pre>array([   0,    1,    2, ..., 7497, 7498, 7499], shape=(7500,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float641.887 1.0 0.8828 ... 1.0 1.997 -1.0<pre>array([[ 1.88742435,  1.        ],\n       [ 0.88283288,  1.        ],\n       [ 3.43690109,  1.        ],\n       ...,\n       [ 1.18231833, -1.        ],\n       [14.26356594,  1.        ],\n       [ 1.99704194, -1.        ]], shape=(7500, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499],\n      dtype='int64', name='__obs__', length=7500))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T18:57:44.267620+00:00arviz_version :0.22.0inference_library :numpyroinference_library_version :0.19.0sampling_time :1494.128823tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[7]: Copied! <pre># Define output dataframe for plotting\ndata_df['p_outlier'] = test_model_1_C.traces.posterior.p_outlier.mean(dim = ['chain', 'draw'])\ndata_df[['hdi_higher', 'hdi_lower']] = az.hdi(test_model_1_C.traces.posterior.p_outlier, hdi_prob = 0.95).to_dataframe()\\\n                                        .reset_index().pivot(index = '__obs__', columns = 'hdi', values = 'p_outlier')\n</pre> # Define output dataframe for plotting data_df['p_outlier'] = test_model_1_C.traces.posterior.p_outlier.mean(dim = ['chain', 'draw']) data_df[['hdi_higher', 'hdi_lower']] = az.hdi(test_model_1_C.traces.posterior.p_outlier, hdi_prob = 0.95).to_dataframe()\\                                         .reset_index().pivot(index = '__obs__', columns = 'hdi', values = 'p_outlier') In\u00a0[8]: Copied! <pre>grouped_df = data_df.groupby('participant_id')[['p_outlier', 'hdi_higher', 'hdi_lower']].mean()\ngrouped_df['p_outlier_gt'] = p_outlier_noise\n</pre> grouped_df = data_df.groupby('participant_id')[['p_outlier', 'hdi_higher', 'hdi_lower']].mean() grouped_df['p_outlier_gt'] = p_outlier_noise In\u00a0[9]: Copied! <pre>grouped_df\n</pre> grouped_df Out[9]: p_outlier hdi_higher hdi_lower p_outlier_gt participant_id 0 0.032498 0.050727 0.019399 0.022874 1 0.011941 0.022223 0.002686 0.004256 2 0.151013 0.181665 0.121329 0.128981 3 0.023265 0.035896 0.010958 0.016532 4 0.153351 0.183267 0.127115 0.136520 5 0.104289 0.131790 0.077138 0.080146 6 0.165424 0.196696 0.133180 0.129931 7 0.162412 0.193465 0.128042 0.146699 8 0.044307 0.063794 0.026451 0.034605 9 0.151349 0.182518 0.116987 0.133559 In\u00a0[10]: Copied! <pre># Create forest plot\nplt.figure(figsize=(10, 6))\n\n# Plot HDI bands for each participant\nfor idx, row in grouped_df.iterrows():\n    plt.plot([row['hdi_lower'], row['hdi_higher']], [idx, idx], 'b-', linewidth=2)\n    plt.plot(row['p_outlier'], idx, 'b|')  # Plot mean point\n\n# Plot ground truth values as red crosses\nplt.plot(grouped_df['p_outlier_gt'], grouped_df.index, 'rx', label='Ground Truth', markersize=10)\n\nplt.xlabel('p_outlier)')\nplt.ylabel('participant_id')\nplt.title('Forest Plot of p_outlier Estimates with 95% HDI')\nplt.legend()\nplt.grid(True, alpha=0.3)\n</pre> # Create forest plot plt.figure(figsize=(10, 6))  # Plot HDI bands for each participant for idx, row in grouped_df.iterrows():     plt.plot([row['hdi_lower'], row['hdi_higher']], [idx, idx], 'b-', linewidth=2)     plt.plot(row['p_outlier'], idx, 'b|')  # Plot mean point  # Plot ground truth values as red crosses plt.plot(grouped_df['p_outlier_gt'], grouped_df.index, 'rx', label='Ground Truth', markersize=10)  plt.xlabel('p_outlier)') plt.ylabel('participant_id') plt.title('Forest Plot of p_outlier Estimates with 95% HDI') plt.legend() plt.grid(True, alpha=0.3)"},{"location":"tutorials/tutorial_p_outlier_regression/#regression-on-p_outlier","title":"Regression on \"p_outlier\"\u00b6","text":"<p>This small tutorial illustrates how we can define a regression directly on the <code>p_outlier</code> parameter. The way we define the regression follows the basic pattern we establish in the <code>include</code> argument.</p>"},{"location":"tutorials/tutorial_p_outlier_regression/#load-modules","title":"Load Modules\u00b6","text":""},{"location":"tutorials/tutorial_p_outlier_regression/#simulate-some-data","title":"Simulate some data\u00b6","text":""},{"location":"tutorials/tutorial_p_outlier_regression/#inject-uniform-noise-into-rts-to-simulate-outliers","title":"Inject Uniform noise into <code>rts</code> to simulate outliers\u00b6","text":""},{"location":"tutorials/tutorial_p_outlier_regression/#define-and-sample-from-hssm-model","title":"Define and sample from HSSM model\u00b6","text":""},{"location":"tutorials/tutorial_p_outlier_regression/#plot-results","title":"Plot Results\u00b6","text":""},{"location":"tutorials/tutorial_stim_coding/","title":"Stimulus Coding Example","text":"In\u00a0[1]: Copied! <pre>from copy import deepcopy\nimport arviz as az\nimport pandas as pd\nimport hssm\n</pre> from copy import deepcopy import arviz as az import pandas as pd import hssm In\u00a0[2]: Copied! <pre># Condition 1\nstim_1 = hssm.simulate_data(\n    model=\"ddm\", theta=dict(v=-0.5, a=1.5, z=0.5, t=0.1), size=500\n)\n\nstim_1[\"stim\"] = \"C-left\"\nstim_1[\"direction\"] = -1\nstim_1[\"response_acc\"] = (-1) * stim_1[\"response\"]\n\n# Condition 2\nstim_2 = hssm.simulate_data(\n    model=\"ddm\", theta=dict(v=0.5, a=1.5, z=0.5, t=0.1), size=500\n)\n\nstim_2[\"stim\"] = \"C-right\"\nstim_2[\"direction\"] = 1\nstim_2[\"response_acc\"] = stim_2[\"response\"]\n\ndata_stim = pd.concat([stim_1, stim_2]).reset_index(drop=True)\n\ndata_acc = deepcopy(data_stim)\ndata_acc[\"response\"] = data_acc[\"response_acc\"]\n\nprint(data_acc.head())\nprint(data_stim.head())\n</pre> # Condition 1 stim_1 = hssm.simulate_data(     model=\"ddm\", theta=dict(v=-0.5, a=1.5, z=0.5, t=0.1), size=500 )  stim_1[\"stim\"] = \"C-left\" stim_1[\"direction\"] = -1 stim_1[\"response_acc\"] = (-1) * stim_1[\"response\"]  # Condition 2 stim_2 = hssm.simulate_data(     model=\"ddm\", theta=dict(v=0.5, a=1.5, z=0.5, t=0.1), size=500 )  stim_2[\"stim\"] = \"C-right\" stim_2[\"direction\"] = 1 stim_2[\"response_acc\"] = stim_2[\"response\"]  data_stim = pd.concat([stim_1, stim_2]).reset_index(drop=True)  data_acc = deepcopy(data_stim) data_acc[\"response\"] = data_acc[\"response_acc\"]  print(data_acc.head()) print(data_stim.head()) <pre>         rt  response    stim  direction  response_acc\n0  0.765660       1.0  C-left         -1           1.0\n1  2.073602       1.0  C-left         -1           1.0\n2  2.756536      -1.0  C-left         -1          -1.0\n3  0.421932       1.0  C-left         -1           1.0\n4  3.323014       1.0  C-left         -1           1.0\n         rt  response    stim  direction  response_acc\n0  0.765660      -1.0  C-left         -1           1.0\n1  2.073602      -1.0  C-left         -1           1.0\n2  2.756536       1.0  C-left         -1          -1.0\n3  0.421932      -1.0  C-left         -1           1.0\n4  3.323014      -1.0  C-left         -1           1.0\n</pre> In\u00a0[3]: Copied! <pre>m_acc_stim_dummy = hssm.HSSM(\n    data=data_acc,\n    model=\"ddm\",\n    include=[{\"name\": \"v\", \"formula\": \"v ~ 1 + stim\"}],\n    z=0.5,\n)\n\nm_acc_stim_dummy.sample(sampler=\"mcmc\", tune=500, draws=500)\n\nm_acc_stim_dummy.summary()\n</pre> m_acc_stim_dummy = hssm.HSSM(     data=data_acc,     model=\"ddm\",     include=[{\"name\": \"v\", \"formula\": \"v ~ 1 + stim\"}],     z=0.5, )  m_acc_stim_dummy.sample(sampler=\"mcmc\", tune=500, draws=500)  m_acc_stim_dummy.summary() <pre>Model initialized successfully.\nUsing default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [a, t, v_Intercept, v_stim]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 500 tune and 500 draw iterations (2_000 + 2_000 draws total) took 7 seconds.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:01&lt;00:00, 1139.43it/s]\n</pre> Out[3]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat a 1.418 0.026 1.372 1.467 0.001 0.001 1649.0 1498.0 1.0 t 0.146 0.017 0.115 0.175 0.000 0.000 1378.0 1071.0 1.0 v_stim[C-right] 0.061 0.050 -0.034 0.153 0.001 0.001 1830.0 1570.0 1.0 v_Intercept 0.503 0.036 0.435 0.570 0.001 0.001 1877.0 1680.0 1.0 In\u00a0[4]: Copied! <pre>m_acc_simple = hssm.HSSM(\n    data=data_acc,\n    model=\"ddm\",\n    include=[\n        {\n            \"name\": \"v\",\n            \"formula\": \"v ~ 1\",\n            \"prior\": {\"Intercept\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 3.0}},\n        }\n    ],\n    z=0.5,\n)\n\nm_acc_simple.sample(sampler=\"mcmc\", tune=500, draws=500)\n\nm_acc_simple.summary()\n</pre> m_acc_simple = hssm.HSSM(     data=data_acc,     model=\"ddm\",     include=[         {             \"name\": \"v\",             \"formula\": \"v ~ 1\",             \"prior\": {\"Intercept\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 3.0}},         }     ],     z=0.5, )  m_acc_simple.sample(sampler=\"mcmc\", tune=500, draws=500)  m_acc_simple.summary() <pre>Model initialized successfully.\nUsing default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [a, t, v_Intercept]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 500 tune and 500 draw iterations (2_000 + 2_000 draws total) took 8 seconds.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:01&lt;00:00, 1776.00it/s]\n</pre> Out[4]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat a 1.417 0.026 1.366 1.463 0.001 0.001 1286.0 1040.0 1.0 t 0.146 0.017 0.114 0.177 0.000 0.000 1353.0 1210.0 1.0 v_Intercept 0.533 0.027 0.480 0.581 0.001 0.001 1377.0 1260.0 1.0 In\u00a0[5]: Copied! <pre>az.compare({\"m_acc_simple\": m_acc_simple.traces, \n            \"m_acc_stim_dummy\": m_acc_stim_dummy.traces})\n</pre> az.compare({\"m_acc_simple\": m_acc_simple.traces,              \"m_acc_stim_dummy\": m_acc_stim_dummy.traces}) Out[5]: rank elpd_loo p_loo elpd_diff weight se dse warning scale m_acc_simple 0 -1934.137814 2.890788 0.000000 0.68423 33.537324 0.000000 False log m_acc_stim_dummy 1 -1934.384055 3.830290 0.246242 0.31577 33.546728 1.155514 False log In\u00a0[6]: Copied! <pre>m_stim = hssm.HSSM(\n    data=data_stim,\n    model=\"ddm\",\n    include=[\n        {\n            \"name\": \"v\",\n            \"formula\": \"v ~ 0 + direction\",\n            \"prior\": {\"direction\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 3.0}},\n        }\n    ],\n    z=0.5,\n)\n\nm_stim.sample(sampler=\"mcmc\", tune=500, draws=500)\n\nm_stim.summary()\n</pre> m_stim = hssm.HSSM(     data=data_stim,     model=\"ddm\",     include=[         {             \"name\": \"v\",             \"formula\": \"v ~ 0 + direction\",             \"prior\": {\"direction\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 3.0}},         }     ],     z=0.5, )  m_stim.sample(sampler=\"mcmc\", tune=500, draws=500)  m_stim.summary() <pre>Model initialized successfully.\nUsing default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [a, t, v_direction]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 500 tune and 500 draw iterations (2_000 + 2_000 draws total) took 6 seconds.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:00&lt;00:00, 2677.81it/s]\n</pre> Out[6]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat a 1.416 0.027 1.368 1.468 0.001 0.001 1313.0 1262.0 1.0 t 0.147 0.017 0.115 0.177 0.001 0.001 1152.0 925.0 1.0 v_direction 0.532 0.027 0.485 0.583 0.001 0.001 1216.0 1194.0 1.0 In\u00a0[7]: Copied! <pre>az.compare(\n    {\n        \"m_acc_simple\": m_acc_simple.traces,\n        \"m_acc_stim_dummy\": m_acc_stim_dummy.traces,\n        \"m_stim\": m_stim.traces,\n    }\n)\n</pre> az.compare(     {         \"m_acc_simple\": m_acc_simple.traces,         \"m_acc_stim_dummy\": m_acc_stim_dummy.traces,         \"m_stim\": m_stim.traces,     } ) Out[7]: rank elpd_loo p_loo elpd_diff weight se dse warning scale m_acc_simple 0 -1934.137814 2.890788 0.000000 6.841569e-01 33.537324 0.000000 False log m_stim 1 -1934.223498 2.977101 0.085684 1.479114e-31 33.507170 0.045821 False log m_acc_stim_dummy 2 -1934.384055 3.830290 0.246242 3.158431e-01 33.546728 1.155514 False log <p>So far we focused on the <code>v</code> parameter. The are two relevant concepts concerning <code>bias</code> that we need to account for in the stimulus coding approach:</p> In\u00a0[8]: Copied! <pre># Folling the math above, we can define the new covariates as follows:\ndata_stim[\"left.stimcoding\"] = (2 * (data_stim[\"stim\"] == \"C-left\").astype(int)) - 1\ndata_stim[\"right.stimcoding\"] = (data_stim[\"stim\"] == \"C-right\").astype(int)\n</pre> # Folling the math above, we can define the new covariates as follows: data_stim[\"left.stimcoding\"] = (2 * (data_stim[\"stim\"] == \"C-left\").astype(int)) - 1 data_stim[\"right.stimcoding\"] = (data_stim[\"stim\"] == \"C-right\").astype(int) In\u00a0[9]: Copied! <pre>m_stim_inc_z = hssm.HSSM(\n    data=data_stim,\n    model=\"ddm\",\n    include=[\n        {\n            \"name\": \"v\",\n            \"formula\": \"v ~ 0 + direction\",\n            \"prior\": {\"direction\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 3.0}},\n        },\n        {\n            \"name\": \"z\",\n            \"formula\": \"z ~ 0 + left.stimcoding + offset(right.stimcoding)\",\n            \"prior\": {\n                \"left.stimcoding\": {\"name\": \"Uniform\", \"lower\": 0.0, \"upper\": 1.0},\n            },\n        },\n    ],\n)\n\nm_stim_inc_z.sample(sampler=\"mcmc\", tune=500, draws=500)\n\nm_stim_inc_z.summary()\n</pre> m_stim_inc_z = hssm.HSSM(     data=data_stim,     model=\"ddm\",     include=[         {             \"name\": \"v\",             \"formula\": \"v ~ 0 + direction\",             \"prior\": {\"direction\": {\"name\": \"Normal\", \"mu\": 0.0, \"sigma\": 3.0}},         },         {             \"name\": \"z\",             \"formula\": \"z ~ 0 + left.stimcoding + offset(right.stimcoding)\",             \"prior\": {                 \"left.stimcoding\": {\"name\": \"Uniform\", \"lower\": 0.0, \"upper\": 1.0},             },         },     ], )  m_stim_inc_z.sample(sampler=\"mcmc\", tune=500, draws=500)  m_stim_inc_z.summary() <pre>Model initialized successfully.\nUsing default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [a, t, v_direction, z_left.stimcoding]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 500 tune and 500 draw iterations (2_000 + 2_000 draws total) took 11 seconds.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:00&lt;00:00, 2560.91it/s]\n</pre> Out[9]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat a 1.417 0.026 1.373 1.467 0.001 0.001 1303.0 1402.0 1.0 t 0.146 0.019 0.109 0.181 0.001 0.000 1263.0 1065.0 1.0 z_left.stimcoding 0.500 0.013 0.475 0.523 0.000 0.000 1225.0 1130.0 1.0 v_direction 0.532 0.034 0.469 0.595 0.001 0.001 1205.0 1349.0 1.0 In\u00a0[10]: Copied! <pre>az.compare(\n    {\n        \"m_acc_simple\": m_acc_simple.traces,\n        \"m_acc_stim_dummy\": m_acc_stim_dummy.traces,\n        \"m_stim\": m_stim.traces,\n        \"m_stim_inc_z\": m_stim_inc_z.traces,\n    }\n)\n</pre> az.compare(     {         \"m_acc_simple\": m_acc_simple.traces,         \"m_acc_stim_dummy\": m_acc_stim_dummy.traces,         \"m_stim\": m_stim.traces,         \"m_stim_inc_z\": m_stim_inc_z.traces,     } ) Out[10]: rank elpd_loo p_loo elpd_diff weight se dse warning scale m_acc_simple 0 -1934.137814 2.890788 0.000000 0.684171 33.537324 0.000000 False log m_stim 1 -1934.223498 2.977101 0.085684 0.000000 33.507170 0.045821 False log m_acc_stim_dummy 2 -1934.384055 3.830290 0.246242 0.315829 33.546728 1.155514 False log m_stim_inc_z 3 -1935.079805 3.749734 0.941991 0.000000 33.535745 0.096578 False log"},{"location":"tutorials/tutorial_stim_coding/#stimulus-coding-example","title":"Stimulus Coding Example\u00b6","text":"<p>In this tutorial we illustrate how to use the regression approach to model the effect of stimulus coding on the drift rate parameter of the DDM.</p>"},{"location":"tutorials/tutorial_stim_coding/#import-modules","title":"Import Modules\u00b6","text":""},{"location":"tutorials/tutorial_stim_coding/#what-is-stimulus-coding","title":"What is Stimulus Coding?\u00b6","text":"<p>There are two core approaches to coding the stimuli when fitting paramters of 2-choice SSMs (the discussion here is simplified, to bring across the core ideas, de facto ideas from both approaches can be mixed):</p> <ol> <li>Accuracy coding: Responses are treated as correct or incorrect</li> <li>Stimulus coding: Responses are treated as stimulus_1 or stimulus_2</li> </ol> <p>Take as a running example a simple random dot motion task with two conditions, <code>left</code> and <code>right</code>. Both conditions are equally difficult, but for half of the experiments the correct motion direction is left, and for the other half it is right.</p> <p>So it will be reasonable to assume that, ceteris paribus, nothing should really change in terms of participant behavior, apart from symmetrically preferring right to left when it is correct and vice versa.</p> <p>Now, when applying Accuracy coding, we would expect the drift rate to be the same for both conditions, any condition effect to vanish by the time we code responses as correct or incorrect.</p> <p>When we apply Stimulus coding on the other hand, we actively need to account for the direction change (since we now attach our response values, e.g. <code>-1</code>, <code>1</code>, permanently to specific choice-options, regardless correctness).</p> <p>To formulate a model that is equivalent to the one described above in terms of accuracy coding, we again want to estimate only a single <code>v</code> parameter, but we have to respect the direction change in response when respectively completing experiment conditions <code>left</code> and <code>right</code>.</p> <p>Note that an important aspect of what we describe above is that we want to estimate a single <code>v</code> parameter in each of the two coding approaches.</p> <p>For Accuracy coding we simply estimate a single <code>v</code> parameter, and no extra work is necessary.</p> <p>For Stimulus coding we need to account for symmetric shift in direction from the two experiment conditions. One way to do this, is the following:</p> <p>We can simply assign a covariate, <code>direction</code>, which codes <code>-1</code> for <code>left</code> and <code>1</code> for <code>right</code>. Then we use the following regression formula for the <code>v</code> parameter: <code>v ~ 0 + direction</code>.</p> <p>Note that we are not using an intercept here.</p> <p>Let's how this works in practice.</p>"},{"location":"tutorials/tutorial_stim_coding/#simulate-data","title":"Simulate Data\u00b6","text":""},{"location":"tutorials/tutorial_stim_coding/#set-up-models","title":"Set up Models\u00b6","text":""},{"location":"tutorials/tutorial_stim_coding/#accuracy-coding","title":"Accuracy Coding\u00b6","text":""},{"location":"tutorials/tutorial_stim_coding/#stim-coding","title":"Stim Coding\u00b6","text":""},{"location":"tutorials/tutorial_stim_coding/#stim-coding-advanced","title":"Stim coding advanced\u00b6","text":""},{"location":"tutorials/tutorial_stim_coding/#1-bias-in-v","title":"1. Bias in <code>v</code>:\u00b6","text":"<p>What is drift bias? Imagine our experimental design is such that the correct motion direction is left for half of the experiments and right for the other half. However, the sensory stimuli are such that the participant will nevertheless be accumulating excess evidence toward the left direction, even when the correct motion direction is right for a given trial. To account for drift bias, we simply include an <code>Intercept</code> term, which will capture the drift bias, so that the <code>direction</code> term will capture the direction effect, a symmetric shift around the <code>Intercept</code> (previously this <code>Intercept</code> was set to 0, or appeared in the model that operated on a dummy <code>stim</code> variable, which remember, creates a models that is too complex / has unnecessary extra parameters).</p>"},{"location":"tutorials/tutorial_stim_coding/#2-bias-in-z","title":"2. Bias in <code>z</code>:\u00b6","text":"<p>Bias in the <code>z</code> parameter gets a bit more tricky. What's the idea here? The <code>z</code> parameter represents the starting point bias. This notion is to some extend more intuitive when using stimulus coding than accuracy coding. A starting point bias under the stimulus coding approach is a bias toward a specific choice option (direction). A starting point bias under the accuracy coding approach is a ... bias toward a correct or incorrect response ... (?)</p> <p>By itself not a problem, but to create the often desired symmetry in the <code>z</code> parameter across <code>stim</code> conditions, keeping in mind that bias takes values in the interval <code>[0, 1]</code>, we need to account for the direction effect in the <code>z</code> parameter. So if in the <code>left</code> condition $z_i = z$, then in the <code>right</code> condition $z_i = 1 - z$.</p> <p>How might we incoporate this into our regression framework?</p> <p>Consider the following varible $\\mathbb{1}_{C_i = c}, \\text{for} \\ c \\in \\{left, right\\}$ which is 1 if the condition is <code>left</code> and 0 otherwise for a given trial. Now we can write the following function for $z_i$,</p> <p>$$  z_i = \\mathbb{1}_{(C_i = left)} \\cdot z + (1 - \\mathbb{1}_{(C_i = left)}) \\cdot (1 - z) $$</p> <p>which after a bit of algebra can be rewritten as,</p> <p>$$ z_i = \\left((2 \\cdot \\mathbb{1}_{(C_i = left)}) - 1\\right) \\cdot z + (1 - \\mathbb{1}_{(C_i = left)}) $$</p> <p>or,</p> <p>$$ z_i = \\left((2 \\cdot \\mathbb{1}_{(C_i = left)}) - 1\\right) \\cdot z + \\mathbb{1}_{(C_i = right)} $$</p> <p>This is a linear function of the <code>z</code> parameter, so we will be able to include it in our model, with a little bit of care.</p> <p>You will see the use of the <code>offset</code> function, to account for the <code>right</code> condition, and we will a priori massage our data a little to define the <code>left.stimcoding</code> and <code>right.stimcoding</code> covariates (dummy variables that identify the <code>left</code> and <code>right</code> conditions).</p>"},{"location":"tutorials/tutorial_stim_coding/#defining-the-new-covariates","title":"Defining the new covariates\u00b6","text":""},{"location":"tutorials/tutorial_stim_coding/#defining-the-model","title":"Defining the model\u00b6","text":"<p>Below an example of a model that take into account both the bias in <code>v</code> and in <code>z</code>.</p>"},{"location":"tutorials/tutorial_trial_wise_parameters/","title":"Attach trial wise parameters to idata post-hoc","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport hssm\n</pre> import pandas as pd import hssm In\u00a0[2]: Copied! <pre># Condition 1\ncondition_1 = hssm.simulate_data(\n    model=\"ddm\", theta=dict(v=0.5, a=1.5, z=0.5, t=0.1), size=500\n)\n\n# Condition 2\ncondition_2 = hssm.simulate_data(\n    model=\"ddm\", theta=dict(v=1.0, a=1.5, z=0.5, t=0.1), size=500\n)\n\ncondition_1[\"condition\"] = \"C1\"\ncondition_2[\"condition\"] = \"C2\"\n\ndata = pd.concat([condition_1, condition_2]).reset_index(drop=True)\n</pre> # Condition 1 condition_1 = hssm.simulate_data(     model=\"ddm\", theta=dict(v=0.5, a=1.5, z=0.5, t=0.1), size=500 )  # Condition 2 condition_2 = hssm.simulate_data(     model=\"ddm\", theta=dict(v=1.0, a=1.5, z=0.5, t=0.1), size=500 )  condition_1[\"condition\"] = \"C1\" condition_2[\"condition\"] = \"C2\"  data = pd.concat([condition_1, condition_2]).reset_index(drop=True) In\u00a0[3]: Copied! <pre>model = hssm.HSSM(\n    model=\"ddm\",\n    data=data,\n    include=[\n        {\n            \"name\": \"v\",\n            \"formula\": \"v ~ 1 + condition\",\n        }\n    ],\n)\n\nidata = model.sample(sampler=\"mcmc\", tune=500, draws=500)\n</pre> model = hssm.HSSM(     model=\"ddm\",     data=data,     include=[         {             \"name\": \"v\",             \"formula\": \"v ~ 1 + condition\",         }     ], )  idata = model.sample(sampler=\"mcmc\", tune=500, draws=500) <pre>Model initialized successfully.\nUsing default initvals. \n\n</pre> <pre>Initializing NUTS using adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [t, z, a, v_Intercept, v_condition]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 500 tune and 500 draw iterations (2_000 + 2_000 draws total) took 17 seconds.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:01&lt;00:00, 1468.13it/s]\n</pre> In\u00a0[4]: Copied! <pre>idata\n</pre> idata Out[4]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 84kB\nDimensions:          (chain: 4, draw: 500, v_condition_dim: 1)\nCoordinates:\n  * chain            (chain) int64 32B 0 1 2 3\n  * draw             (draw) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\n  * v_condition_dim  (v_condition_dim) &lt;U2 8B 'C2'\nData variables:\n    a                (chain, draw) float64 16kB 1.451 1.525 1.459 ... 1.496 1.5\n    v_condition      (chain, draw, v_condition_dim) float64 16kB 0.5384 ... 0...\n    t                (chain, draw) float64 16kB 0.13 0.07329 ... 0.0961 0.08384\n    v_Intercept      (chain, draw) float64 16kB 0.5206 0.6879 ... 0.6451 0.6132\n    z                (chain, draw) float64 16kB 0.4694 0.4642 ... 0.4799 0.4654\nAttributes:\n    created_at:                  2025-09-27T18:39:31.023458+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               16.67565417289734\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 500</li><li>v_condition_dim: 1</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>v_condition_dim(v_condition_dim)&lt;U2'C2'<pre>array(['C2'], dtype='&lt;U2')</pre></li></ul></li><li>Data variables: (5)<ul><li>a(chain, draw)float641.451 1.525 1.459 ... 1.496 1.5<pre>array([[1.45106251, 1.52540826, 1.4585095 , ..., 1.52873593, 1.45183711,\n        1.47778987],\n       [1.46361445, 1.44220471, 1.51224383, ..., 1.47909987, 1.50950166,\n        1.50950166],\n       [1.45868234, 1.54219879, 1.44265243, ..., 1.51037699, 1.53777174,\n        1.46544368],\n       [1.51788694, 1.49004495, 1.47905407, ..., 1.51462907, 1.49611195,\n        1.50037066]], shape=(4, 500))</pre></li><li>v_condition(chain, draw, v_condition_dim)float640.5384 0.4482 ... 0.5208 0.5374<pre>array([[[0.53844103],\n        [0.44816191],\n        [0.53327817],\n        ...,\n        [0.43633621],\n        [0.60838475],\n        [0.52596921]],\n\n       [[0.5006698 ],\n        [0.50254597],\n        [0.43987986],\n        ...,\n        [0.54335352],\n        [0.48653608],\n        [0.48653608]],\n\n       [[0.52135926],\n        [0.56453401],\n        [0.42446749],\n        ...,\n        [0.50524344],\n        [0.55846193],\n        [0.5269144 ]],\n\n       [[0.52615289],\n        [0.48857295],\n        [0.51738512],\n        ...,\n        [0.5976951 ],\n        [0.52080949],\n        [0.53736947]]], shape=(4, 500, 1))</pre></li><li>t(chain, draw)float640.13 0.07329 ... 0.0961 0.08384<pre>array([[0.13002467, 0.07328917, 0.11544478, ..., 0.10161289, 0.11814377,\n        0.11359321],\n       [0.12548785, 0.12196935, 0.12650266, ..., 0.11752049, 0.09821315,\n        0.09821315],\n       [0.10997856, 0.09242201, 0.12533099, ..., 0.08185775, 0.09374628,\n        0.10354658],\n       [0.08482131, 0.09963758, 0.09045551, ..., 0.10063254, 0.09609583,\n        0.08383561]], shape=(4, 500))</pre></li><li>v_Intercept(chain, draw)float640.5206 0.6879 ... 0.6451 0.6132<pre>array([[0.52057116, 0.68786649, 0.58917466, ..., 0.6717081 , 0.58515998,\n        0.66611895],\n       [0.60325827, 0.58101715, 0.62427288, ..., 0.57570264, 0.56480157,\n        0.56480157],\n       [0.61653292, 0.58269045, 0.6912627 , ..., 0.64704138, 0.55962568,\n        0.62759394],\n       [0.66574029, 0.63600607, 0.63584347, ..., 0.56674825, 0.64509774,\n        0.61320631]], shape=(4, 500))</pre></li><li>z(chain, draw)float640.4694 0.4642 ... 0.4799 0.4654<pre>array([[0.46939311, 0.46419084, 0.47891836, ..., 0.46820404, 0.45072108,\n        0.46547088],\n       [0.48861865, 0.46088043, 0.49085177, ..., 0.48556738, 0.48940291,\n        0.48940291],\n       [0.46582484, 0.49091371, 0.45417345, ..., 0.45180981, 0.48274017,\n        0.46659174],\n       [0.44614093, 0.48027135, 0.4595854 , ..., 0.45017127, 0.479884  ,\n        0.46544844]], shape=(4, 500))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>v_condition_dimPandasIndex<pre>PandasIndex(Index(['C2'], dtype='object', name='v_condition_dim'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T18:39:31.023458+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :16.67565417289734tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 16MB\nDimensions:      (chain: 4, draw: 500, __obs__: 1000)\nCoordinates:\n  * chain        (chain) int64 32B 0 1 2 3\n  * draw         (draw) int64 4kB 0 1 2 3 4 5 6 ... 493 494 495 496 497 498 499\n  * __obs__      (__obs__) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    rt,response  (chain, draw, __obs__) float64 16MB -3.858 -1.374 ... -0.9489\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 500</li><li>__obs__: 1000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-3.858 -1.374 ... -0.4278 -0.9489<pre>array([[[-3.85848945, -1.37393815, -2.57483123, ..., -0.45588408,\n         -0.47506209, -0.94711629],\n        [-3.90929099, -1.2063424 , -2.5479039 , ..., -0.44970905,\n         -0.46508231, -0.94284191],\n        [-3.9386467 , -1.18801417, -2.60364964, ..., -0.37925217,\n         -0.38816327, -0.96706647],\n        ...,\n        [-3.86028023, -1.296508  , -2.51833799, ..., -0.47890596,\n         -0.50222945, -0.92133335],\n        [-3.91809806, -1.39129426, -2.57636139, ..., -0.37968016,\n         -0.39929244, -0.91816928],\n        [-3.98012917, -1.23006596, -2.58911706, ..., -0.36675365,\n         -0.38235068, -0.93015378]],\n\n       [[-3.94997198, -1.14420594, -2.6078308 , ..., -0.37809112,\n         -0.38665203, -0.9702782 ],\n        [-3.95064076, -1.30635588, -2.60195119, ..., -0.44447482,\n         -0.46285426, -0.94508972],\n        [-3.84812582, -1.2272447 , -2.53900288, ..., -0.44993658,\n         -0.46782362, -0.94648071],\n...\n        [-3.86061607, -1.33697386, -2.52548294, ..., -0.45577065,\n         -0.47627026, -0.92238583],\n        [-3.70277969, -1.33342965, -2.48515747, ..., -0.43562071,\n         -0.45036724, -0.94591531],\n        [-3.96435766, -1.20764743, -2.59969025, ..., -0.38372953,\n         -0.39519894, -0.95296914]],\n\n       [[-3.85974309, -1.38421905, -2.51214968, ..., -0.44281834,\n         -0.46671254, -0.90237079],\n        [-3.92807279, -1.15154592, -2.58393491, ..., -0.39766475,\n         -0.40694348, -0.96529985],\n        [-3.93658614, -1.24139837, -2.57904697, ..., -0.409857  ,\n         -0.42348133, -0.94614434],\n        ...,\n        [-3.72702227, -1.53900928, -2.47583538, ..., -0.4625643 ,\n         -0.49005786, -0.8964341 ],\n        [-3.92713124, -1.14735518, -2.5804977 , ..., -0.36779772,\n         -0.37560744, -0.96349689],\n        [-3.85505369, -1.2644351 , -2.54458644, ..., -0.41497751,\n         -0.42777218, -0.9488609 ]]], shape=(4, 500, 1000))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 264kB\nDimensions:                (chain: 4, draw: 500)\nCoordinates:\n  * chain                  (chain) int64 32B 0 1 2 3\n  * draw                   (draw) int64 4kB 0 1 2 3 4 5 ... 495 496 497 498 499\nData variables: (12/18)\n    largest_eigval         (chain, draw) float64 16kB nan nan nan ... nan nan\n    step_size              (chain, draw) float64 16kB 0.5168 0.5168 ... 0.7723\n    process_time_diff      (chain, draw) float64 16kB 0.01719 ... 0.009474\n    n_steps                (chain, draw) float64 16kB 15.0 7.0 7.0 ... 7.0 7.0\n    divergences            (chain, draw) int64 16kB 0 0 0 0 0 0 ... 0 0 0 0 0 0\n    reached_max_treedepth  (chain, draw) bool 2kB False False ... False False\n    ...                     ...\n    energy_error           (chain, draw) float64 16kB 0.3607 -0.593 ... -0.7798\n    perf_counter_diff      (chain, draw) float64 16kB 0.02747 0.01597 ... 0.0147\n    step_size_bar          (chain, draw) float64 16kB 0.4898 0.4898 ... 0.4493\n    perf_counter_start     (chain, draw) float64 16kB 1.499e+06 ... 1.499e+06\n    tree_depth             (chain, draw) int64 16kB 4 3 3 4 3 3 ... 2 3 3 3 3 3\n    smallest_eigval        (chain, draw) float64 16kB nan nan nan ... nan nan\nAttributes:\n    created_at:                  2025-09-27T18:39:31.034727+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               16.67565417289734\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (18)<ul><li>largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], shape=(4, 500))</pre></li><li>step_size(chain, draw)float640.5168 0.5168 ... 0.7723 0.7723<pre>array([[0.51675147, 0.51675147, 0.51675147, ..., 0.51675147, 0.51675147,\n        0.51675147],\n       [0.55420921, 0.55420921, 0.55420921, ..., 0.55420921, 0.55420921,\n        0.55420921],\n       [0.53203696, 0.53203696, 0.53203696, ..., 0.53203696, 0.53203696,\n        0.53203696],\n       [0.77234289, 0.77234289, 0.77234289, ..., 0.77234289, 0.77234289,\n        0.77234289]], shape=(4, 500))</pre></li><li>process_time_diff(chain, draw)float640.01719 0.01044 ... 0.009474<pre>array([[0.01719 , 0.010435, 0.008502, ..., 0.007504, 0.007367, 0.003594],\n       [0.017574, 0.00698 , 0.008895, ..., 0.004574, 0.007443, 0.004378],\n       [0.007771, 0.007692, 0.008376, ..., 0.007151, 0.007274, 0.007837],\n       [0.009783, 0.009288, 0.009394, ..., 0.007894, 0.007694, 0.009474]],\n      shape=(4, 500))</pre></li><li>n_steps(chain, draw)float6415.0 7.0 7.0 15.0 ... 7.0 7.0 7.0<pre>array([[15.,  7.,  7., ...,  7.,  7.,  3.],\n       [15.,  7.,  7., ...,  3.,  7.,  3.],\n       [ 7.,  7.,  7., ...,  7.,  7.,  7.],\n       [ 7.,  7.,  7., ...,  7.,  7.,  7.]], shape=(4, 500))</pre></li><li>divergences(chain, draw)int640 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0<pre>array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], shape=(4, 500))</pre></li><li>reached_max_treedepth(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]], shape=(4, 500))</pre></li><li>energy(chain, draw)float641.623e+03 1.621e+03 ... 1.621e+03<pre>array([[1622.55753976, 1621.4752906 , 1617.83769046, ..., 1618.89207187,\n        1622.2453911 , 1621.02435687],\n       [1619.85581743, 1619.24036813, 1620.30010332, ..., 1619.66374189,\n        1618.36403591, 1620.58369708],\n       [1618.62393975, 1619.41508679, 1619.87868432, ..., 1620.83553985,\n        1623.34006535, 1619.36081427],\n       [1618.05695539, 1619.22692425, 1617.45198434, ..., 1621.9103877 ,\n        1622.49794265, 1620.635279  ]], shape=(4, 500))</pre></li><li>max_energy_error(chain, draw)float641.064 -1.087 ... -1.033 -1.04<pre>array([[ 1.06365648, -1.08669436, -0.2592765 , ..., -0.27414452,\n         0.85031687,  0.25236655],\n       [-0.20014867, -0.16155421,  0.61043222, ..., -0.26262394,\n         0.12648184,  0.47195494],\n       [ 0.12098678,  0.16736508, -0.16541261, ..., -1.51278415,\n         1.07130559,  0.39162307],\n       [ 0.10180616,  0.40247401, -0.48494692, ...,  1.67033313,\n        -1.03348825, -1.03957123]], shape=(4, 500))</pre></li><li>lp(chain, draw)float64-1.621e+03 ... -1.616e+03<pre>array([[-1620.90291372, -1617.45456518, -1616.63852423, ...,\n        -1617.30435871, -1619.81645372, -1617.36216118],\n       [-1616.78713398, -1617.9929163 , -1617.98586336, ...,\n        -1616.45919   , -1617.36262687, -1617.36262687],\n       [-1616.22871125, -1618.76464982, -1618.81421381, ...,\n        -1616.31029488, -1618.03942512, -1616.62017112],\n       [-1617.15041356, -1616.68226689, -1616.37548286, ...,\n        -1621.46744027, -1618.24789559, -1616.3530092 ]], shape=(4, 500))</pre></li><li>acceptance_rate(chain, draw)float640.6481 1.0 0.9938 ... 0.9967 0.8867<pre>array([[0.64809912, 1.        , 0.99382154, ..., 0.99420815, 0.71218516,\n        0.92134003],\n       [0.98046137, 0.96383016, 0.77709434, ..., 1.        , 0.95804722,\n        0.73047262],\n       [0.95341966, 0.94517137, 0.99974494, ..., 0.94760509, 0.61317598,\n        0.9003985 ],\n       [0.95807264, 0.85073795, 0.98497829, ..., 0.42711303, 0.99674609,\n        0.8867474 ]], shape=(4, 500))</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]], shape=(4, 500))</pre></li><li>index_in_trajectory(chain, draw)int64-1 6 5 -5 6 -5 1 ... 3 7 2 -4 2 -1<pre>array([[-1,  6,  5, ...,  2,  4, -2],\n       [ 1, -2,  5, ..., -3, -3,  0],\n       [-5,  4,  6, ..., -3, -2,  3],\n       [ 4, -6,  5, ..., -4,  2, -1]], shape=(4, 500))</pre></li><li>energy_error(chain, draw)float640.3607 -0.593 ... -0.5461 -0.7798<pre>array([[ 0.36071587, -0.59302466,  0.02225975, ..., -0.05641847,\n        -0.12101546, -0.09211537],\n       [-0.04110109,  0.13004571,  0.08141575, ..., -0.20037088,\n         0.0103881 ,  0.        ],\n       [ 0.02060882,  0.16736508,  0.00178702, ..., -1.21627901,\n         0.13934482,  0.12449368],\n       [ 0.06952252,  0.27998519, -0.20413714, ...,  1.67033313,\n        -0.54613277, -0.779776  ]], shape=(4, 500))</pre></li><li>perf_counter_diff(chain, draw)float640.02747 0.01597 ... 0.009273 0.0147<pre>array([[0.02747221, 0.01596596, 0.01357229, ..., 0.00915821, 0.00919796,\n        0.00438154],\n       [0.02059396, 0.00763917, 0.01051242, ..., 0.00525967, 0.00765942,\n        0.00562883],\n       [0.01049604, 0.00923392, 0.01029546, ..., 0.00736713, 0.00870612,\n        0.00984667],\n       [0.01467388, 0.01012329, 0.01065242, ..., 0.00864583, 0.00927275,\n        0.01470104]], shape=(4, 500))</pre></li><li>step_size_bar(chain, draw)float640.4898 0.4898 ... 0.4493 0.4493<pre>array([[0.48981696, 0.48981696, 0.48981696, ..., 0.48981696, 0.48981696,\n        0.48981696],\n       [0.4695521 , 0.4695521 , 0.4695521 , ..., 0.4695521 , 0.4695521 ,\n        0.4695521 ],\n       [0.51710055, 0.51710055, 0.51710055, ..., 0.51710055, 0.51710055,\n        0.51710055],\n       [0.44930076, 0.44930076, 0.44930076, ..., 0.44930076, 0.44930076,\n        0.44930076]], shape=(4, 500))</pre></li><li>perf_counter_start(chain, draw)float641.499e+06 1.499e+06 ... 1.499e+06<pre>array([[1499376.95434575, 1499376.981935  , 1499376.99869779, ...,\n        1499382.57976129, 1499382.58900408, 1499382.59829046],\n       [1499376.35775242, 1499376.3784425 , 1499376.38620508, ...,\n        1499382.07366817, 1499382.07909338, 1499382.08684254],\n       [1499377.18749921, 1499377.19809488, 1499377.20743017, ...,\n        1499382.58103742, 1499382.58849342, 1499382.59731133],\n       [1499376.436144  , 1499376.45097575, 1499376.46117792, ...,\n        1499382.24516879, 1499382.25424417, 1499382.26360346]],\n      shape=(4, 500))</pre></li><li>tree_depth(chain, draw)int644 3 3 4 3 3 2 3 ... 3 3 2 3 3 3 3 3<pre>array([[4, 3, 3, ..., 3, 3, 2],\n       [4, 3, 3, ..., 2, 3, 2],\n       [3, 3, 3, ..., 3, 3, 3],\n       [3, 3, 3, ..., 3, 3, 3]], shape=(4, 500))</pre></li><li>smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], shape=(4, 500))</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T18:39:31.034727+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :16.67565417289734tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 24kB\nDimensions:                  (__obs__: 1000, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 8kB 0 1 2 3 4 ... 996 997 998 999\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 16kB ...\nAttributes:\n    created_at:                  2025-09-27T18:39:31.038482+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 1000</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float645.32 1.0 0.5291 ... 1.0 1.435 1.0<pre>array([[5.31995773, 1.        ],\n       [0.5291388 , 1.        ],\n       [3.41043663, 1.        ],\n       ...,\n       [0.71072024, 1.        ],\n       [0.67398906, 1.        ],\n       [1.4347496 , 1.        ]], shape=(1000, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (6)created_at :2025-09-27T18:39:31.038482+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> <p>Note that none of the parameters attached in the posterior have the observation coordinate (<code>__obs__</code>). However, looking at the model graph, we clearly see that for variables that are represented as regression targets, we in fact compute intermediate deterministics which do have dimensionality <code>__obs__</code>.</p> In\u00a0[5]: Copied! <pre>model.graph()\n</pre> model.graph() Out[5]: In\u00a0[6]: Copied! <pre>### Attach Trial Wise Parameters\nidata_trialwise = model.add_likelihood_parameters_to_idata(idata)\n\n### Checking the `idata` object\nidata_trialwise\n</pre> ### Attach Trial Wise Parameters idata_trialwise = model.add_likelihood_parameters_to_idata(idata)  ### Checking the `idata` object idata_trialwise Out[6]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 16MB\nDimensions:          (chain: 4, draw: 500, v_condition_dim: 1, __obs__: 1000)\nCoordinates:\n  * chain            (chain) int64 32B 0 1 2 3\n  * draw             (draw) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\n  * v_condition_dim  (v_condition_dim) &lt;U2 8B 'C2'\n  * __obs__          (__obs__) int64 8kB 0 1 2 3 4 5 ... 994 995 996 997 998 999\nData variables:\n    a                (chain, draw) float64 16kB 1.451 1.525 1.459 ... 1.496 1.5\n    v_condition      (chain, draw, v_condition_dim) float64 16kB 0.5384 ... 0...\n    t                (chain, draw) float64 16kB 0.13 0.07329 ... 0.0961 0.08384\n    v_Intercept      (chain, draw) float64 16kB 0.5206 0.6879 ... 0.6451 0.6132\n    z                (chain, draw) float64 16kB 0.4694 0.4642 ... 0.4799 0.4654\n    v                (chain, draw, __obs__) float64 16MB 0.5206 0.5206 ... 1.151\nAttributes:\n    created_at:                  2025-09-27T18:39:31.023458+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               16.67565417289734\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 500</li><li>v_condition_dim: 1</li><li>__obs__: 1000</li></ul></li><li>Coordinates: (4)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>v_condition_dim(v_condition_dim)&lt;U2'C2'<pre>array(['C2'], dtype='&lt;U2')</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li></ul></li><li>Data variables: (6)<ul><li>a(chain, draw)float641.451 1.525 1.459 ... 1.496 1.5<pre>array([[1.45106251, 1.52540826, 1.4585095 , ..., 1.52873593, 1.45183711,\n        1.47778987],\n       [1.46361445, 1.44220471, 1.51224383, ..., 1.47909987, 1.50950166,\n        1.50950166],\n       [1.45868234, 1.54219879, 1.44265243, ..., 1.51037699, 1.53777174,\n        1.46544368],\n       [1.51788694, 1.49004495, 1.47905407, ..., 1.51462907, 1.49611195,\n        1.50037066]], shape=(4, 500))</pre></li><li>v_condition(chain, draw, v_condition_dim)float640.5384 0.4482 ... 0.5208 0.5374<pre>array([[[0.53844103],\n        [0.44816191],\n        [0.53327817],\n        ...,\n        [0.43633621],\n        [0.60838475],\n        [0.52596921]],\n\n       [[0.5006698 ],\n        [0.50254597],\n        [0.43987986],\n        ...,\n        [0.54335352],\n        [0.48653608],\n        [0.48653608]],\n\n       [[0.52135926],\n        [0.56453401],\n        [0.42446749],\n        ...,\n        [0.50524344],\n        [0.55846193],\n        [0.5269144 ]],\n\n       [[0.52615289],\n        [0.48857295],\n        [0.51738512],\n        ...,\n        [0.5976951 ],\n        [0.52080949],\n        [0.53736947]]], shape=(4, 500, 1))</pre></li><li>t(chain, draw)float640.13 0.07329 ... 0.0961 0.08384<pre>array([[0.13002467, 0.07328917, 0.11544478, ..., 0.10161289, 0.11814377,\n        0.11359321],\n       [0.12548785, 0.12196935, 0.12650266, ..., 0.11752049, 0.09821315,\n        0.09821315],\n       [0.10997856, 0.09242201, 0.12533099, ..., 0.08185775, 0.09374628,\n        0.10354658],\n       [0.08482131, 0.09963758, 0.09045551, ..., 0.10063254, 0.09609583,\n        0.08383561]], shape=(4, 500))</pre></li><li>v_Intercept(chain, draw)float640.5206 0.6879 ... 0.6451 0.6132<pre>array([[0.52057116, 0.68786649, 0.58917466, ..., 0.6717081 , 0.58515998,\n        0.66611895],\n       [0.60325827, 0.58101715, 0.62427288, ..., 0.57570264, 0.56480157,\n        0.56480157],\n       [0.61653292, 0.58269045, 0.6912627 , ..., 0.64704138, 0.55962568,\n        0.62759394],\n       [0.66574029, 0.63600607, 0.63584347, ..., 0.56674825, 0.64509774,\n        0.61320631]], shape=(4, 500))</pre></li><li>z(chain, draw)float640.4694 0.4642 ... 0.4799 0.4654<pre>array([[0.46939311, 0.46419084, 0.47891836, ..., 0.46820404, 0.45072108,\n        0.46547088],\n       [0.48861865, 0.46088043, 0.49085177, ..., 0.48556738, 0.48940291,\n        0.48940291],\n       [0.46582484, 0.49091371, 0.45417345, ..., 0.45180981, 0.48274017,\n        0.46659174],\n       [0.44614093, 0.48027135, 0.4595854 , ..., 0.45017127, 0.479884  ,\n        0.46544844]], shape=(4, 500))</pre></li><li>v(chain, draw, __obs__)float640.5206 0.5206 ... 1.151 1.151<pre>array([[[0.52057116, 0.52057116, 0.52057116, ..., 1.05901219,\n         1.05901219, 1.05901219],\n        [0.68786649, 0.68786649, 0.68786649, ..., 1.1360284 ,\n         1.1360284 , 1.1360284 ],\n        [0.58917466, 0.58917466, 0.58917466, ..., 1.12245284,\n         1.12245284, 1.12245284],\n        ...,\n        [0.6717081 , 0.6717081 , 0.6717081 , ..., 1.1080443 ,\n         1.1080443 , 1.1080443 ],\n        [0.58515998, 0.58515998, 0.58515998, ..., 1.19354473,\n         1.19354473, 1.19354473],\n        [0.66611895, 0.66611895, 0.66611895, ..., 1.19208816,\n         1.19208816, 1.19208816]],\n\n       [[0.60325827, 0.60325827, 0.60325827, ..., 1.10392808,\n         1.10392808, 1.10392808],\n        [0.58101715, 0.58101715, 0.58101715, ..., 1.08356311,\n         1.08356311, 1.08356311],\n        [0.62427288, 0.62427288, 0.62427288, ..., 1.06415273,\n         1.06415273, 1.06415273],\n...\n        [0.64704138, 0.64704138, 0.64704138, ..., 1.15228481,\n         1.15228481, 1.15228481],\n        [0.55962568, 0.55962568, 0.55962568, ..., 1.11808761,\n         1.11808761, 1.11808761],\n        [0.62759394, 0.62759394, 0.62759394, ..., 1.15450835,\n         1.15450835, 1.15450835]],\n\n       [[0.66574029, 0.66574029, 0.66574029, ..., 1.19189318,\n         1.19189318, 1.19189318],\n        [0.63600607, 0.63600607, 0.63600607, ..., 1.12457902,\n         1.12457902, 1.12457902],\n        [0.63584347, 0.63584347, 0.63584347, ..., 1.15322859,\n         1.15322859, 1.15322859],\n        ...,\n        [0.56674825, 0.56674825, 0.56674825, ..., 1.16444335,\n         1.16444335, 1.16444335],\n        [0.64509774, 0.64509774, 0.64509774, ..., 1.16590723,\n         1.16590723, 1.16590723],\n        [0.61320631, 0.61320631, 0.61320631, ..., 1.15057578,\n         1.15057578, 1.15057578]]], shape=(4, 500, 1000))</pre></li></ul></li><li>Indexes: (4)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>v_condition_dimPandasIndex<pre>PandasIndex(Index(['C2'], dtype='object', name='v_condition_dim'))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T18:39:31.023458+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :16.67565417289734tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 16MB\nDimensions:      (chain: 4, draw: 500, __obs__: 1000)\nCoordinates:\n  * chain        (chain) int64 32B 0 1 2 3\n  * draw         (draw) int64 4kB 0 1 2 3 4 5 6 ... 493 494 495 496 497 498 499\n  * __obs__      (__obs__) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    rt,response  (chain, draw, __obs__) float64 16MB -3.858 -1.374 ... -0.9489\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 500</li><li>__obs__: 1000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-3.858 -1.374 ... -0.4278 -0.9489<pre>array([[[-3.85848945, -1.37393815, -2.57483123, ..., -0.45588408,\n         -0.47506209, -0.94711629],\n        [-3.90929099, -1.2063424 , -2.5479039 , ..., -0.44970905,\n         -0.46508231, -0.94284191],\n        [-3.9386467 , -1.18801417, -2.60364964, ..., -0.37925217,\n         -0.38816327, -0.96706647],\n        ...,\n        [-3.86028023, -1.296508  , -2.51833799, ..., -0.47890596,\n         -0.50222945, -0.92133335],\n        [-3.91809806, -1.39129426, -2.57636139, ..., -0.37968016,\n         -0.39929244, -0.91816928],\n        [-3.98012917, -1.23006596, -2.58911706, ..., -0.36675365,\n         -0.38235068, -0.93015378]],\n\n       [[-3.94997198, -1.14420594, -2.6078308 , ..., -0.37809112,\n         -0.38665203, -0.9702782 ],\n        [-3.95064076, -1.30635588, -2.60195119, ..., -0.44447482,\n         -0.46285426, -0.94508972],\n        [-3.84812582, -1.2272447 , -2.53900288, ..., -0.44993658,\n         -0.46782362, -0.94648071],\n...\n        [-3.86061607, -1.33697386, -2.52548294, ..., -0.45577065,\n         -0.47627026, -0.92238583],\n        [-3.70277969, -1.33342965, -2.48515747, ..., -0.43562071,\n         -0.45036724, -0.94591531],\n        [-3.96435766, -1.20764743, -2.59969025, ..., -0.38372953,\n         -0.39519894, -0.95296914]],\n\n       [[-3.85974309, -1.38421905, -2.51214968, ..., -0.44281834,\n         -0.46671254, -0.90237079],\n        [-3.92807279, -1.15154592, -2.58393491, ..., -0.39766475,\n         -0.40694348, -0.96529985],\n        [-3.93658614, -1.24139837, -2.57904697, ..., -0.409857  ,\n         -0.42348133, -0.94614434],\n        ...,\n        [-3.72702227, -1.53900928, -2.47583538, ..., -0.4625643 ,\n         -0.49005786, -0.8964341 ],\n        [-3.92713124, -1.14735518, -2.5804977 , ..., -0.36779772,\n         -0.37560744, -0.96349689],\n        [-3.85505369, -1.2644351 , -2.54458644, ..., -0.41497751,\n         -0.42777218, -0.9488609 ]]], shape=(4, 500, 1000))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 264kB\nDimensions:                (chain: 4, draw: 500)\nCoordinates:\n  * chain                  (chain) int64 32B 0 1 2 3\n  * draw                   (draw) int64 4kB 0 1 2 3 4 5 ... 495 496 497 498 499\nData variables: (12/18)\n    largest_eigval         (chain, draw) float64 16kB nan nan nan ... nan nan\n    step_size              (chain, draw) float64 16kB 0.5168 0.5168 ... 0.7723\n    process_time_diff      (chain, draw) float64 16kB 0.01719 ... 0.009474\n    n_steps                (chain, draw) float64 16kB 15.0 7.0 7.0 ... 7.0 7.0\n    divergences            (chain, draw) int64 16kB 0 0 0 0 0 0 ... 0 0 0 0 0 0\n    reached_max_treedepth  (chain, draw) bool 2kB False False ... False False\n    ...                     ...\n    energy_error           (chain, draw) float64 16kB 0.3607 -0.593 ... -0.7798\n    perf_counter_diff      (chain, draw) float64 16kB 0.02747 0.01597 ... 0.0147\n    step_size_bar          (chain, draw) float64 16kB 0.4898 0.4898 ... 0.4493\n    perf_counter_start     (chain, draw) float64 16kB 1.499e+06 ... 1.499e+06\n    tree_depth             (chain, draw) int64 16kB 4 3 3 4 3 3 ... 2 3 3 3 3 3\n    smallest_eigval        (chain, draw) float64 16kB nan nan nan ... nan nan\nAttributes:\n    created_at:                  2025-09-27T18:39:31.034727+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    sampling_time:               16.67565417289734\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 4</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1 2 3<pre>array([0, 1, 2, 3])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (18)<ul><li>largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], shape=(4, 500))</pre></li><li>step_size(chain, draw)float640.5168 0.5168 ... 0.7723 0.7723<pre>array([[0.51675147, 0.51675147, 0.51675147, ..., 0.51675147, 0.51675147,\n        0.51675147],\n       [0.55420921, 0.55420921, 0.55420921, ..., 0.55420921, 0.55420921,\n        0.55420921],\n       [0.53203696, 0.53203696, 0.53203696, ..., 0.53203696, 0.53203696,\n        0.53203696],\n       [0.77234289, 0.77234289, 0.77234289, ..., 0.77234289, 0.77234289,\n        0.77234289]], shape=(4, 500))</pre></li><li>process_time_diff(chain, draw)float640.01719 0.01044 ... 0.009474<pre>array([[0.01719 , 0.010435, 0.008502, ..., 0.007504, 0.007367, 0.003594],\n       [0.017574, 0.00698 , 0.008895, ..., 0.004574, 0.007443, 0.004378],\n       [0.007771, 0.007692, 0.008376, ..., 0.007151, 0.007274, 0.007837],\n       [0.009783, 0.009288, 0.009394, ..., 0.007894, 0.007694, 0.009474]],\n      shape=(4, 500))</pre></li><li>n_steps(chain, draw)float6415.0 7.0 7.0 15.0 ... 7.0 7.0 7.0<pre>array([[15.,  7.,  7., ...,  7.,  7.,  3.],\n       [15.,  7.,  7., ...,  3.,  7.,  3.],\n       [ 7.,  7.,  7., ...,  7.,  7.,  7.],\n       [ 7.,  7.,  7., ...,  7.,  7.,  7.]], shape=(4, 500))</pre></li><li>divergences(chain, draw)int640 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0<pre>array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], shape=(4, 500))</pre></li><li>reached_max_treedepth(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]], shape=(4, 500))</pre></li><li>energy(chain, draw)float641.623e+03 1.621e+03 ... 1.621e+03<pre>array([[1622.55753976, 1621.4752906 , 1617.83769046, ..., 1618.89207187,\n        1622.2453911 , 1621.02435687],\n       [1619.85581743, 1619.24036813, 1620.30010332, ..., 1619.66374189,\n        1618.36403591, 1620.58369708],\n       [1618.62393975, 1619.41508679, 1619.87868432, ..., 1620.83553985,\n        1623.34006535, 1619.36081427],\n       [1618.05695539, 1619.22692425, 1617.45198434, ..., 1621.9103877 ,\n        1622.49794265, 1620.635279  ]], shape=(4, 500))</pre></li><li>max_energy_error(chain, draw)float641.064 -1.087 ... -1.033 -1.04<pre>array([[ 1.06365648, -1.08669436, -0.2592765 , ..., -0.27414452,\n         0.85031687,  0.25236655],\n       [-0.20014867, -0.16155421,  0.61043222, ..., -0.26262394,\n         0.12648184,  0.47195494],\n       [ 0.12098678,  0.16736508, -0.16541261, ..., -1.51278415,\n         1.07130559,  0.39162307],\n       [ 0.10180616,  0.40247401, -0.48494692, ...,  1.67033313,\n        -1.03348825, -1.03957123]], shape=(4, 500))</pre></li><li>lp(chain, draw)float64-1.621e+03 ... -1.616e+03<pre>array([[-1620.90291372, -1617.45456518, -1616.63852423, ...,\n        -1617.30435871, -1619.81645372, -1617.36216118],\n       [-1616.78713398, -1617.9929163 , -1617.98586336, ...,\n        -1616.45919   , -1617.36262687, -1617.36262687],\n       [-1616.22871125, -1618.76464982, -1618.81421381, ...,\n        -1616.31029488, -1618.03942512, -1616.62017112],\n       [-1617.15041356, -1616.68226689, -1616.37548286, ...,\n        -1621.46744027, -1618.24789559, -1616.3530092 ]], shape=(4, 500))</pre></li><li>acceptance_rate(chain, draw)float640.6481 1.0 0.9938 ... 0.9967 0.8867<pre>array([[0.64809912, 1.        , 0.99382154, ..., 0.99420815, 0.71218516,\n        0.92134003],\n       [0.98046137, 0.96383016, 0.77709434, ..., 1.        , 0.95804722,\n        0.73047262],\n       [0.95341966, 0.94517137, 0.99974494, ..., 0.94760509, 0.61317598,\n        0.9003985 ],\n       [0.95807264, 0.85073795, 0.98497829, ..., 0.42711303, 0.99674609,\n        0.8867474 ]], shape=(4, 500))</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]], shape=(4, 500))</pre></li><li>index_in_trajectory(chain, draw)int64-1 6 5 -5 6 -5 1 ... 3 7 2 -4 2 -1<pre>array([[-1,  6,  5, ...,  2,  4, -2],\n       [ 1, -2,  5, ..., -3, -3,  0],\n       [-5,  4,  6, ..., -3, -2,  3],\n       [ 4, -6,  5, ..., -4,  2, -1]], shape=(4, 500))</pre></li><li>energy_error(chain, draw)float640.3607 -0.593 ... -0.5461 -0.7798<pre>array([[ 0.36071587, -0.59302466,  0.02225975, ..., -0.05641847,\n        -0.12101546, -0.09211537],\n       [-0.04110109,  0.13004571,  0.08141575, ..., -0.20037088,\n         0.0103881 ,  0.        ],\n       [ 0.02060882,  0.16736508,  0.00178702, ..., -1.21627901,\n         0.13934482,  0.12449368],\n       [ 0.06952252,  0.27998519, -0.20413714, ...,  1.67033313,\n        -0.54613277, -0.779776  ]], shape=(4, 500))</pre></li><li>perf_counter_diff(chain, draw)float640.02747 0.01597 ... 0.009273 0.0147<pre>array([[0.02747221, 0.01596596, 0.01357229, ..., 0.00915821, 0.00919796,\n        0.00438154],\n       [0.02059396, 0.00763917, 0.01051242, ..., 0.00525967, 0.00765942,\n        0.00562883],\n       [0.01049604, 0.00923392, 0.01029546, ..., 0.00736713, 0.00870612,\n        0.00984667],\n       [0.01467388, 0.01012329, 0.01065242, ..., 0.00864583, 0.00927275,\n        0.01470104]], shape=(4, 500))</pre></li><li>step_size_bar(chain, draw)float640.4898 0.4898 ... 0.4493 0.4493<pre>array([[0.48981696, 0.48981696, 0.48981696, ..., 0.48981696, 0.48981696,\n        0.48981696],\n       [0.4695521 , 0.4695521 , 0.4695521 , ..., 0.4695521 , 0.4695521 ,\n        0.4695521 ],\n       [0.51710055, 0.51710055, 0.51710055, ..., 0.51710055, 0.51710055,\n        0.51710055],\n       [0.44930076, 0.44930076, 0.44930076, ..., 0.44930076, 0.44930076,\n        0.44930076]], shape=(4, 500))</pre></li><li>perf_counter_start(chain, draw)float641.499e+06 1.499e+06 ... 1.499e+06<pre>array([[1499376.95434575, 1499376.981935  , 1499376.99869779, ...,\n        1499382.57976129, 1499382.58900408, 1499382.59829046],\n       [1499376.35775242, 1499376.3784425 , 1499376.38620508, ...,\n        1499382.07366817, 1499382.07909338, 1499382.08684254],\n       [1499377.18749921, 1499377.19809488, 1499377.20743017, ...,\n        1499382.58103742, 1499382.58849342, 1499382.59731133],\n       [1499376.436144  , 1499376.45097575, 1499376.46117792, ...,\n        1499382.24516879, 1499382.25424417, 1499382.26360346]],\n      shape=(4, 500))</pre></li><li>tree_depth(chain, draw)int644 3 3 4 3 3 2 3 ... 3 3 2 3 3 3 3 3<pre>array([[4, 3, 3, ..., 3, 3, 2],\n       [4, 3, 3, ..., 2, 3, 2],\n       [3, 3, 3, ..., 3, 3, 3],\n       [3, 3, 3, ..., 3, 3, 3]], shape=(4, 500))</pre></li><li>smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], shape=(4, 500))</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T18:39:31.034727+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1sampling_time :16.67565417289734tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 24kB\nDimensions:                  (__obs__: 1000, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 8kB 0 1 2 3 4 ... 996 997 998 999\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 16kB ...\nAttributes:\n    created_at:                  2025-09-27T18:39:31.038482+00:00\n    arviz_version:               0.22.0\n    inference_library:           pymc\n    inference_library_version:   5.25.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 1000</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float645.32 1.0 0.5291 ... 1.0 1.435 1.0<pre>array([[5.31995773, 1.        ],\n       [0.5291388 , 1.        ],\n       [3.41043663, 1.        ],\n       ...,\n       [0.71072024, 1.        ],\n       [0.67398906, 1.        ],\n       [1.4347496 , 1.        ]], shape=(1000, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='__obs__', length=1000))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (6)created_at :2025-09-27T18:39:31.038482+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> <p>Note how the idata include the <code>v</code> parameter which has the <code>__obs__</code> coordinate. Computing these trial-wise parameters, can serve us well for plotting and other post-hoc analyses, here we would otherwise struggle to be effective without incurring some pre-computation pain (which in addition would be error prone). See e.g. the tutorial on hierarchical Variational Inference for an example.</p>"},{"location":"tutorials/tutorial_trial_wise_parameters/#attach-trial-wise-parameters-to-idata-post-hoc","title":"Attach trial wise parameters to idata post-hoc\u00b6","text":"<p>HSSM automatically cleans up the idata object we return from samplers, to avoid unnecessarily huge objects from being passed around (the decision was taken after having observed many inadvertent out of memory errors).</p> <p>Instead, if so desired, we can attach the trial wise parameters to the idata object ourselves, using the <code>add_likelihood_parameters_to_idata</code> function.</p> <p>This quick tutorial shows you how to do this.</p>"},{"location":"tutorials/tutorial_trial_wise_parameters/#load-modules","title":"Load Modules\u00b6","text":""},{"location":"tutorials/tutorial_trial_wise_parameters/#simulate-data","title":"Simulate Data\u00b6","text":""},{"location":"tutorials/tutorial_trial_wise_parameters/#define-model-and-sample","title":"Define Model and Sample\u00b6","text":""},{"location":"tutorials/tutorial_trial_wise_parameters/#checking-the-idata-object","title":"Checking the <code>idata</code> object\u00b6","text":""},{"location":"tutorials/tutorial_trial_wise_parameters/#computing-trial-wise-parameters","title":"Computing Trial Wise Parameters\u00b6","text":"<p>We can use the <code>compute_likelihood_parameters_to_idata</code> function to recompute and include in our <code>idata</code> the trial wise deterministics, which are part of our model graph.</p>"},{"location":"tutorials/variational_inference/","title":"Variational Inference with HSSM","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport hssm\n\nimport arviz as az\nimport pymc as pm\n</pre> import matplotlib.pyplot as plt import numpy as np import hssm  import arviz as az import pymc as pm In\u00a0[2]: Copied! <pre>cav_data = hssm.load_data(\"cavanagh_theta\")\ncav_model = hssm.HSSM(data=cav_data,\n                      model=\"angle\")\n</pre> cav_data = hssm.load_data(\"cavanagh_theta\") cav_model = hssm.HSSM(data=cav_data,                       model=\"angle\") <pre>Model initialized successfully.\n</pre> In\u00a0[3]: Copied! <pre># Slight adjustment to initial values\ninitvals_tmp = cav_model.initvals\ninitvals_tmp['theta'] = 0.1\n\n\nmcmc_idata = cav_model.sample(chains = 2,\n                              tune = 500,\n                              draws = 500,\n                              sampler = \"nuts_numpyro\",\n                              initvals = initvals_tmp\n                              )\n</pre> # Slight adjustment to initial values initvals_tmp = cav_model.initvals initvals_tmp['theta'] = 0.1   mcmc_idata = cav_model.sample(chains = 2,                               tune = 500,                               draws = 500,                               sampler = \"nuts_numpyro\",                               initvals = initvals_tmp                               ) <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/proj_hssm/HSSM/.venv/lib/python3.11/site-packages/pymc/sampling/jax.py:475: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  pmap_numpyro = MCMC(\nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:39&lt;00:00, 10.01it/s, 15 steps of size 3.32e-01. acc. prob=0.91]\nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:39&lt;00:00, 10.04it/s, 3 steps of size 3.21e-01. acc. prob=0.90] \nWe recommend running at least 4 chains for robust computation of convergence diagnostics\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:07&lt;00:00, 137.19it/s]\n</pre> In\u00a0[4]: Copied! <pre>az.summary(mcmc_idata)\n</pre> az.summary(mcmc_idata) Out[4]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat z 0.505 0.006 0.494 0.516 0.000 0.000 581.0 556.0 1.00 theta 0.227 0.013 0.205 0.255 0.001 0.000 292.0 448.0 1.01 t 0.285 0.009 0.265 0.300 0.001 0.000 346.0 444.0 1.01 v 0.360 0.021 0.317 0.394 0.001 0.001 492.0 450.0 1.00 a 1.304 0.022 1.265 1.347 0.001 0.001 225.0 417.0 1.02 In\u00a0[5]: Copied! <pre>vi_idata = cav_model.vi(niter=100000,\n                        method=\"fullrank_advi\")\n</pre> vi_idata = cav_model.vi(niter=100000,                         method=\"fullrank_advi\") <pre>Using MCMC starting point defaults.\n</pre> <pre>Output()</pre> <pre></pre> <pre>Finished [100%]: Average Loss = 6,035.9\n</pre> In\u00a0[6]: Copied! <pre>cav_model.vi_idata\n</pre> cav_model.vi_idata Out[6]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 48kB\nDimensions:  (chain: 1, draw: 1000)\nCoordinates:\n  * chain    (chain) int64 8B 0\n  * draw     (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\nData variables:\n    z        (chain, draw) float64 8kB 0.5057 0.5051 0.4891 ... 0.4977 0.49\n    theta    (chain, draw) float64 8kB 0.2281 0.2178 0.2149 ... 0.2146 0.235\n    t        (chain, draw) float64 8kB 0.2751 0.2885 0.2751 ... 0.2953 0.2684\n    v        (chain, draw) float64 8kB 0.3644 0.3528 0.4111 ... 0.3945 0.386\n    a        (chain, draw) float64 8kB 1.308 1.293 1.296 ... 1.305 1.281 1.312\nAttributes:\n    created_at:                 2025-09-27T19:01:23.168370+00:00\n    arviz_version:              0.22.0\n    inference_library:          pymc\n    inference_library_version:  5.25.1</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 1000</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li></ul></li><li>Data variables: (5)<ul><li>z(chain, draw)float640.5057 0.5051 ... 0.4977 0.49<pre>array([[0.50566955, 0.50508991, 0.48906624, 0.5068282 , 0.50868351,\n        0.51560065, 0.49670771, 0.51881069, 0.50673363, 0.5084266 ,\n        0.49863826, 0.49621917, 0.49571588, 0.50071778, 0.50228908,\n        0.514211  , 0.49299532, 0.50642226, 0.50226157, 0.50460444,\n        0.50720818, 0.50381299, 0.50037372, 0.50632981, 0.51344419,\n        0.50591572, 0.50500266, 0.51997764, 0.50091707, 0.50440882,\n        0.50052259, 0.51577316, 0.49351038, 0.50047814, 0.5042472 ,\n        0.49937374, 0.51206756, 0.50003935, 0.50742407, 0.49587107,\n        0.5128617 , 0.50399453, 0.51043487, 0.50105149, 0.50187487,\n        0.50895896, 0.51258787, 0.49650678, 0.50168977, 0.50462839,\n        0.51043636, 0.49831937, 0.48679889, 0.50252197, 0.50044189,\n        0.5039064 , 0.49639281, 0.49796783, 0.49357939, 0.50919708,\n        0.50316298, 0.4902274 , 0.4998971 , 0.50831288, 0.50536291,\n        0.49076142, 0.49677303, 0.51527376, 0.4986705 , 0.49407293,\n        0.4940318 , 0.50679934, 0.50691879, 0.51553597, 0.48912838,\n        0.49418498, 0.50889643, 0.50619972, 0.50717903, 0.5074223 ,\n        0.5128082 , 0.50820228, 0.50377989, 0.51109241, 0.51864141,\n        0.50896677, 0.50203833, 0.50150409, 0.50432379, 0.51259387,\n        0.5105833 , 0.50283449, 0.50506458, 0.51348693, 0.50740877,\n        0.50690554, 0.50536284, 0.50381333, 0.49824878, 0.50441766,\n...\n        0.49894983, 0.50298202, 0.50522451, 0.50489557, 0.50063507,\n        0.49001175, 0.5022016 , 0.49903396, 0.49925572, 0.50794495,\n        0.50175735, 0.50987525, 0.4947769 , 0.49553305, 0.50161258,\n        0.48993537, 0.50260323, 0.50399126, 0.49877092, 0.5067034 ,\n        0.49183026, 0.51356585, 0.50095212, 0.49717726, 0.50743001,\n        0.50465951, 0.50238021, 0.49700991, 0.5013694 , 0.50047241,\n        0.50547481, 0.51046546, 0.50032449, 0.49907644, 0.49466791,\n        0.50617669, 0.50274524, 0.50392407, 0.51741099, 0.50631076,\n        0.50114849, 0.51112019, 0.49533294, 0.48311896, 0.4999575 ,\n        0.50155221, 0.50254856, 0.50714594, 0.50389633, 0.4989751 ,\n        0.49332418, 0.50533434, 0.5017995 , 0.51109106, 0.51734875,\n        0.50426886, 0.49825563, 0.50921658, 0.49280618, 0.50856661,\n        0.50397154, 0.50756187, 0.49104305, 0.49800162, 0.49898405,\n        0.50195345, 0.49691986, 0.50555448, 0.50882003, 0.50019357,\n        0.50832945, 0.51739815, 0.5150213 , 0.50102723, 0.49579528,\n        0.49816861, 0.5090572 , 0.51087303, 0.5159736 , 0.50718635,\n        0.51148627, 0.50079393, 0.51351042, 0.51132572, 0.50359573,\n        0.491079  , 0.51139125, 0.50491324, 0.50588689, 0.50119681,\n        0.50557572, 0.50742371, 0.50857329, 0.50183656, 0.50491182,\n        0.50493432, 0.50495636, 0.50041758, 0.49765176, 0.48996939]])</pre></li><li>theta(chain, draw)float640.2281 0.2178 ... 0.2146 0.235<pre>array([[0.22814425, 0.21784194, 0.21493473, 0.23407719, 0.21631111,\n        0.20924198, 0.22401044, 0.23380562, 0.19607359, 0.24031626,\n        0.21960892, 0.22892589, 0.23635576, 0.22039804, 0.24160504,\n        0.23824281, 0.22783008, 0.22598158, 0.24833535, 0.22486367,\n        0.22396461, 0.24475164, 0.20735544, 0.24595469, 0.2564191 ,\n        0.23623768, 0.22696927, 0.23758035, 0.21798882, 0.23755814,\n        0.25204021, 0.22204261, 0.23021214, 0.24497944, 0.2255912 ,\n        0.22422122, 0.20934813, 0.21682982, 0.22911449, 0.2246174 ,\n        0.23572599, 0.2508987 , 0.22659627, 0.20080658, 0.20439566,\n        0.2369444 , 0.21634596, 0.22244779, 0.22340155, 0.2414689 ,\n        0.21769732, 0.2144388 , 0.220086  , 0.23806577, 0.23605649,\n        0.23906515, 0.21821772, 0.21495394, 0.22124855, 0.23019132,\n        0.24411559, 0.2271124 , 0.24923002, 0.23986897, 0.22034095,\n        0.22053968, 0.23403613, 0.24190171, 0.23396312, 0.23062314,\n        0.21276707, 0.22373452, 0.21399051, 0.24880128, 0.22793482,\n        0.22782497, 0.23185921, 0.21825509, 0.19149434, 0.23235756,\n        0.23427888, 0.25078002, 0.20958682, 0.2120604 , 0.21765335,\n        0.22113418, 0.19957339, 0.23876026, 0.22021959, 0.22448979,\n        0.24411535, 0.26451159, 0.24156888, 0.21544755, 0.24041417,\n        0.23009321, 0.22401688, 0.23376793, 0.22368363, 0.23049046,\n...\n        0.24555679, 0.2234334 , 0.22331292, 0.24188783, 0.22759474,\n        0.21406575, 0.21205952, 0.23423325, 0.21361715, 0.26766772,\n        0.21613556, 0.24484814, 0.24943551, 0.21888889, 0.23475118,\n        0.23201415, 0.23972211, 0.23885956, 0.21572465, 0.23375826,\n        0.23865736, 0.23997834, 0.22729262, 0.22292408, 0.22045738,\n        0.22862514, 0.2359088 , 0.23105418, 0.25588886, 0.22532592,\n        0.24193909, 0.21863544, 0.22601972, 0.23339364, 0.24007166,\n        0.21595157, 0.24362628, 0.23194059, 0.25838979, 0.24096364,\n        0.26312622, 0.20384372, 0.25076249, 0.21880976, 0.21477262,\n        0.21230207, 0.23129579, 0.24237124, 0.21611726, 0.22251322,\n        0.23007885, 0.21449159, 0.22772293, 0.21251599, 0.23815844,\n        0.2359013 , 0.23265312, 0.20329458, 0.21907666, 0.22358453,\n        0.2511139 , 0.21996926, 0.22289469, 0.22387753, 0.23467104,\n        0.26822763, 0.24447861, 0.22913916, 0.21487564, 0.20539526,\n        0.22394424, 0.25301654, 0.21589739, 0.22459413, 0.21121412,\n        0.22049166, 0.21515556, 0.20587392, 0.21142034, 0.22778832,\n        0.25125881, 0.22783469, 0.26608296, 0.21949332, 0.22791289,\n        0.23936499, 0.2416575 , 0.22371376, 0.23594665, 0.22471155,\n        0.23578819, 0.21469442, 0.22869016, 0.21358757, 0.21338779,\n        0.23667978, 0.24581851, 0.22241598, 0.2145882 , 0.23502417]])</pre></li><li>t(chain, draw)float640.2751 0.2885 ... 0.2953 0.2684<pre>array([[0.27511475, 0.28848359, 0.27510362, 0.27574774, 0.29332303,\n        0.29506884, 0.28311852, 0.28081642, 0.31670921, 0.27617199,\n        0.27952684, 0.27466476, 0.28127386, 0.29281426, 0.29239033,\n        0.27870149, 0.28838173, 0.28859196, 0.26384324, 0.29315118,\n        0.29393514, 0.27154437, 0.29186001, 0.27809671, 0.27732481,\n        0.27809609, 0.2812931 , 0.29443264, 0.29061051, 0.27663189,\n        0.26827069, 0.28818491, 0.30983976, 0.26704625, 0.2853691 ,\n        0.27877322, 0.30417795, 0.28975713, 0.28192522, 0.27733643,\n        0.28347642, 0.27454798, 0.27730743, 0.30357605, 0.29654529,\n        0.27703374, 0.30063423, 0.28817406, 0.28568771, 0.26991088,\n        0.29601154, 0.30785468, 0.2692505 , 0.28809977, 0.26984496,\n        0.28376111, 0.2852669 , 0.29348826, 0.29032888, 0.28821158,\n        0.28032402, 0.27783775, 0.26759041, 0.28075453, 0.28286093,\n        0.27695754, 0.27947416, 0.28162519, 0.28276458, 0.28275096,\n        0.28277806, 0.28824113, 0.29788029, 0.30009033, 0.27965644,\n        0.28414142, 0.2800196 , 0.28951831, 0.30245561, 0.28970964,\n        0.28558677, 0.26093142, 0.29058686, 0.30439029, 0.29866584,\n        0.28913639, 0.29332311, 0.26716601, 0.28219994, 0.28304132,\n        0.28922665, 0.26662621, 0.27480596, 0.29893045, 0.28821   ,\n        0.27715908, 0.2879346 , 0.27692366, 0.28452416, 0.27937369,\n...\n        0.27545803, 0.28220337, 0.28955451, 0.28300426, 0.27579147,\n        0.28134546, 0.29135342, 0.2787294 , 0.29322593, 0.2727941 ,\n        0.29566295, 0.28776205, 0.28110216, 0.28832213, 0.26290836,\n        0.27809738, 0.27592116, 0.27884509, 0.29046685, 0.2660067 ,\n        0.27933959, 0.28204365, 0.29105968, 0.29865211, 0.27739396,\n        0.28259463, 0.27028663, 0.27571927, 0.2569336 , 0.28246292,\n        0.2797112 , 0.28854877, 0.2845056 , 0.27815056, 0.27059827,\n        0.2946747 , 0.28249122, 0.29001942, 0.26507655, 0.2807543 ,\n        0.26679462, 0.29695313, 0.26154573, 0.2771741 , 0.29455179,\n        0.29222585, 0.27275916, 0.27396207, 0.28764128, 0.28393506,\n        0.26790407, 0.28530931, 0.28288266, 0.30123436, 0.28441155,\n        0.28286955, 0.27654622, 0.29277934, 0.28452622, 0.2791627 ,\n        0.27176406, 0.29627608, 0.29161288, 0.277538  , 0.29480032,\n        0.25919194, 0.27734531, 0.29120754, 0.29350208, 0.31083143,\n        0.29328292, 0.27030494, 0.29128943, 0.28170848, 0.29154099,\n        0.29135741, 0.2930277 , 0.29700938, 0.30783653, 0.30045821,\n        0.27099113, 0.27966182, 0.28120134, 0.30017273, 0.27674228,\n        0.28171479, 0.27404999, 0.27206499, 0.27502568, 0.27252822,\n        0.27370893, 0.30346651, 0.29186432, 0.29251865, 0.28838068,\n        0.28008227, 0.28902539, 0.28624178, 0.29530755, 0.26840259]])</pre></li><li>v(chain, draw)float640.3644 0.3528 ... 0.3945 0.386<pre>array([[0.36437944, 0.35278813, 0.41111859, 0.34414163, 0.33027799,\n        0.30948848, 0.38671757, 0.32065081, 0.37005884, 0.36549108,\n        0.40007888, 0.41540933, 0.40081526, 0.40175667, 0.38080027,\n        0.28917968, 0.38944991, 0.34173007, 0.38156301, 0.37010198,\n        0.33913684, 0.36532373, 0.37226944, 0.34964171, 0.30548307,\n        0.35317291, 0.30979218, 0.30838672, 0.39339332, 0.35264136,\n        0.37882466, 0.3376159 , 0.38964784, 0.38356212, 0.3713469 ,\n        0.38025566, 0.3558334 , 0.35812888, 0.32680553, 0.38535088,\n        0.37381427, 0.34454139, 0.33880454, 0.38774733, 0.34895433,\n        0.31525171, 0.33760631, 0.4016307 , 0.35407727, 0.372282  ,\n        0.33255793, 0.40942139, 0.41624352, 0.35398447, 0.35969716,\n        0.36049824, 0.37351044, 0.38708278, 0.37624558, 0.32888304,\n        0.37036297, 0.37120223, 0.36272094, 0.34425809, 0.37113275,\n        0.37669572, 0.40177516, 0.32986707, 0.36693991, 0.37096347,\n        0.37120126, 0.3810528 , 0.31657614, 0.30746257, 0.40829979,\n        0.40216388, 0.33804998, 0.36837313, 0.34144142, 0.34887783,\n        0.31872391, 0.3298701 , 0.35908224, 0.32230393, 0.31459061,\n        0.35095613, 0.37192703, 0.35863654, 0.30214531, 0.35044885,\n        0.33931981, 0.35601319, 0.35424668, 0.33595712, 0.34856287,\n        0.37932173, 0.34677592, 0.32920531, 0.37957286, 0.33881484,\n...\n        0.38766414, 0.38584386, 0.33278691, 0.36139277, 0.38041551,\n        0.40004561, 0.319741  , 0.37948687, 0.37825095, 0.34349832,\n        0.34181473, 0.33911418, 0.36403496, 0.37525631, 0.33932384,\n        0.38255207, 0.37318667, 0.35864026, 0.38023163, 0.35087006,\n        0.37676057, 0.3397871 , 0.36780686, 0.3842621 , 0.3600082 ,\n        0.36426267, 0.38732291, 0.40793947, 0.35254324, 0.34328996,\n        0.34993697, 0.36588324, 0.36471417, 0.37855241, 0.36876625,\n        0.36011434, 0.36840481, 0.38419458, 0.29829323, 0.34286408,\n        0.35966627, 0.34047162, 0.38171323, 0.4326272 , 0.35933445,\n        0.36744695, 0.34956292, 0.30494244, 0.37695754, 0.38301374,\n        0.37038634, 0.36236531, 0.34647223, 0.34877845, 0.29210147,\n        0.39071741, 0.4007905 , 0.33454965, 0.41087074, 0.36329292,\n        0.37730265, 0.35005497, 0.41642035, 0.38883203, 0.38821067,\n        0.3671953 , 0.40473535, 0.32683794, 0.34803967, 0.37393883,\n        0.36064442, 0.32524706, 0.33215398, 0.3841506 , 0.35018962,\n        0.37955131, 0.33905254, 0.34845364, 0.33352653, 0.34080326,\n        0.33312057, 0.38471184, 0.32384105, 0.34087545, 0.38152839,\n        0.36124399, 0.33290348, 0.34964274, 0.34753301, 0.37047635,\n        0.32870261, 0.30587481, 0.34941087, 0.39780702, 0.37523974,\n        0.31848712, 0.3293403 , 0.34673799, 0.3945221 , 0.38596267]])</pre></li><li>a(chain, draw)float641.308 1.293 1.296 ... 1.281 1.312<pre>array([[1.30800896, 1.29269904, 1.29573663, 1.32605997, 1.27485184,\n        1.2854569 , 1.30562631, 1.29536132, 1.25033222, 1.32960885,\n        1.30412822, 1.30651638, 1.30402053, 1.29611881, 1.3016855 ,\n        1.32480612, 1.29456357, 1.30695057, 1.34331009, 1.30730635,\n        1.30734629, 1.34675949, 1.28317554, 1.3246786 , 1.32760577,\n        1.32128346, 1.29870713, 1.30231836, 1.28671131, 1.33503226,\n        1.33589801, 1.30650118, 1.28854294, 1.32995533, 1.2981706 ,\n        1.30754273, 1.26914337, 1.2983575 , 1.29358315, 1.30947615,\n        1.31362771, 1.33358884, 1.31274517, 1.26542877, 1.26736786,\n        1.31780488, 1.2777619 , 1.29107112, 1.29929675, 1.34501064,\n        1.2777382 , 1.2669137 , 1.33156545, 1.30678146, 1.32478282,\n        1.31193233, 1.29467618, 1.286284  , 1.29770719, 1.30527365,\n        1.34634262, 1.30192911, 1.33241016, 1.3149492 , 1.30104393,\n        1.30203334, 1.33506485, 1.32479814, 1.28880485, 1.29986532,\n        1.29825768, 1.29602659, 1.29305005, 1.30665524, 1.31443473,\n        1.29984952, 1.3073661 , 1.29650688, 1.23771645, 1.29852456,\n        1.30795019, 1.3502403 , 1.28657871, 1.28481513, 1.2783211 ,\n        1.30133288, 1.27128356, 1.32518766, 1.28592404, 1.32066783,\n        1.30350606, 1.34591115, 1.32864889, 1.28806552, 1.29524261,\n        1.31582789, 1.28686265, 1.31300388, 1.31216036, 1.33016211,\n...\n        1.33074013, 1.31436723, 1.31096325, 1.32106211, 1.31453909,\n        1.2980876 , 1.27335817, 1.30980444, 1.26912918, 1.34474533,\n        1.28384419, 1.32355552, 1.33727768, 1.27707972, 1.3320301 ,\n        1.30188378, 1.32790343, 1.32353209, 1.28994503, 1.31311277,\n        1.32265798, 1.33388205, 1.31716436, 1.27381451, 1.30952479,\n        1.32173507, 1.32418105, 1.31362523, 1.34882024, 1.31239557,\n        1.33540517, 1.30755704, 1.30625056, 1.31414837, 1.33553744,\n        1.28017444, 1.31719139, 1.31440148, 1.33518282, 1.31849039,\n        1.33967538, 1.26190092, 1.34078007, 1.30787728, 1.27342097,\n        1.29152142, 1.31555807, 1.32384577, 1.29205485, 1.30983634,\n        1.31057374, 1.29767405, 1.30452685, 1.27646858, 1.30864291,\n        1.31592144, 1.31116059, 1.28536233, 1.28185541, 1.30661341,\n        1.34745828, 1.28496601, 1.28824299, 1.30126496, 1.32365174,\n        1.38116652, 1.3358749 , 1.29831081, 1.28036329, 1.26497327,\n        1.30246631, 1.33840959, 1.28589199, 1.30417187, 1.28503001,\n        1.29408362, 1.28611186, 1.28453191, 1.2615447 , 1.29078875,\n        1.34516261, 1.30173929, 1.3522001 , 1.28697542, 1.310245  ,\n        1.32016458, 1.33157845, 1.3161946 , 1.31509454, 1.30104928,\n        1.31137447, 1.289556  , 1.29021873, 1.29377683, 1.30189685,\n        1.29891685, 1.31618452, 1.30463263, 1.28127203, 1.31227253]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li></ul></li><li>Attributes: (4)created_at :2025-09-27T19:01:23.168370+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 96kB\nDimensions:                  (__obs__: 3988, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 32kB 0 1 2 3 ... 3985 3986 3987\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 64kB ...\nAttributes:\n    created_at:                 2025-09-27T19:01:23.181189+00:00\n    arviz_version:              0.22.0\n    inference_library:          pymc\n    inference_library_version:  5.25.1</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 3988</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 ... 3984 3985 3986 3987<pre>array([   0,    1,    2, ..., 3985, 3986, 3987], shape=(3988,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float641.21 1.0 1.63 1.0 ... -1.0 1.25 1.0<pre>array([[ 1.21 ,  1.   ],\n       [ 1.63 ,  1.   ],\n       [ 1.03 ,  1.   ],\n       ...,\n       [ 0.784,  1.   ],\n       [ 2.35 , -1.   ],\n       [ 1.25 ,  1.   ]], shape=(3988, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987],\n      dtype='int64', name='__obs__', length=3988))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (4)created_at :2025-09-27T19:01:23.181189+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1</li></ul> </ul> </li> </ul> In\u00a0[7]: Copied! <pre>cav_model.vi_approx.sample(draws=1000)\n</pre> cav_model.vi_approx.sample(draws=1000) Out[7]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 32MB\nDimensions:  (chain: 1, draw: 1000, __obs__: 3988)\nCoordinates:\n  * chain    (chain) int64 8B 0\n  * draw     (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\n  * __obs__  (__obs__) int64 32kB 0 1 2 3 4 5 ... 3982 3983 3984 3985 3986 3987\nData variables:\n    z        (chain, draw) float64 8kB 0.5082 0.5148 0.5128 ... 0.5058 0.5147\n    t        (chain, draw) float64 8kB 0.2932 0.2873 0.2849 ... 0.2774 0.283\n    theta    (chain, draw) float64 8kB 0.2193 0.2377 0.2463 ... 0.2262 0.2273\n    a        (chain, draw) float64 8kB 1.285 1.309 1.312 ... 1.308 1.295 1.298\n    v        (chain, draw) float64 8kB 0.3358 0.3224 0.3449 ... 0.3408 0.3359\n    v_mean   (chain, draw, __obs__) float64 32MB 0.3358 0.3358 ... 0.3359 0.3359\nAttributes:\n    created_at:                 2025-09-27T19:01:23.290578+00:00\n    arviz_version:              0.22.0\n    inference_library:          pymc\n    inference_library_version:  5.25.1</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 1000</li><li>__obs__: 3988</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 ... 3984 3985 3986 3987<pre>array([   0,    1,    2, ..., 3985, 3986, 3987], shape=(3988,))</pre></li></ul></li><li>Data variables: (6)<ul><li>z(chain, draw)float640.5082 0.5148 ... 0.5058 0.5147<pre>array([[0.50819082, 0.51484666, 0.51277213, 0.50546121, 0.50631524,\n        0.50831628, 0.51533525, 0.51442775, 0.5040062 , 0.50022396,\n        0.50704863, 0.49530301, 0.50734486, 0.50592128, 0.49330168,\n        0.50513509, 0.50827613, 0.50197968, 0.51478493, 0.50426165,\n        0.50867994, 0.49589955, 0.5048361 , 0.50873841, 0.49995863,\n        0.49388316, 0.49633516, 0.49946152, 0.51952564, 0.50982944,\n        0.49885522, 0.51755371, 0.49654515, 0.51285119, 0.52034497,\n        0.51036872, 0.50714058, 0.50742289, 0.51204962, 0.49936217,\n        0.50259834, 0.49972817, 0.48996137, 0.5031876 , 0.51006941,\n        0.5120302 , 0.50005056, 0.50491374, 0.50089307, 0.49173036,\n        0.49809243, 0.51366571, 0.49876966, 0.50372799, 0.50048645,\n        0.49866044, 0.50338908, 0.50686677, 0.49717275, 0.50524449,\n        0.50731365, 0.51380222, 0.49836251, 0.4919799 , 0.51179204,\n        0.51313089, 0.51866255, 0.50362191, 0.50336178, 0.50874276,\n        0.49630212, 0.50552393, 0.49328326, 0.49629322, 0.49073059,\n        0.50215178, 0.49528949, 0.51258982, 0.51041413, 0.50095   ,\n        0.4959123 , 0.50476751, 0.51176568, 0.50042044, 0.51099637,\n        0.51284066, 0.50662084, 0.50259318, 0.50502961, 0.49865701,\n        0.50177877, 0.49839037, 0.50529185, 0.50227729, 0.50381521,\n        0.50981378, 0.48817729, 0.50038132, 0.51342578, 0.50512085,\n...\n        0.51453933, 0.51798289, 0.50848606, 0.50079323, 0.509516  ,\n        0.49420125, 0.50597544, 0.50030442, 0.49512675, 0.50684254,\n        0.50730041, 0.49327476, 0.50199543, 0.48797839, 0.49507232,\n        0.50102337, 0.50644563, 0.50637925, 0.51042723, 0.49850523,\n        0.50280446, 0.51054608, 0.50266933, 0.50174034, 0.50298474,\n        0.50453773, 0.50575533, 0.49804512, 0.51406632, 0.49824271,\n        0.501124  , 0.50553308, 0.508557  , 0.51544093, 0.50007234,\n        0.49343913, 0.50284729, 0.50644725, 0.50335155, 0.51331088,\n        0.49548094, 0.48907436, 0.50764431, 0.51608501, 0.50846495,\n        0.50638992, 0.50505797, 0.5003635 , 0.49348025, 0.48468073,\n        0.50885897, 0.51209259, 0.49638262, 0.49976371, 0.5080424 ,\n        0.50676109, 0.50805134, 0.52064911, 0.50267388, 0.50296407,\n        0.5052477 , 0.51183075, 0.49806634, 0.50837662, 0.4887382 ,\n        0.51028239, 0.49816381, 0.4994105 , 0.51389589, 0.50530098,\n        0.51411946, 0.50092651, 0.49675929, 0.50090925, 0.50385657,\n        0.51168312, 0.51190798, 0.51061029, 0.50974127, 0.50797891,\n        0.50254763, 0.51581441, 0.50705121, 0.4925932 , 0.51450478,\n        0.51778669, 0.49858229, 0.49619692, 0.4862662 , 0.49966637,\n        0.49722564, 0.50758537, 0.51056141, 0.50717489, 0.51099507,\n        0.4969403 , 0.50794077, 0.51008644, 0.5057867 , 0.51469548]])</pre></li><li>t(chain, draw)float640.2932 0.2873 ... 0.2774 0.283<pre>array([[0.29319412, 0.28732834, 0.28488204, 0.2968602 , 0.29020805,\n        0.27883177, 0.29941241, 0.27209393, 0.30651461, 0.26871947,\n        0.28082408, 0.2891145 , 0.28799033, 0.28809855, 0.28073367,\n        0.29457762, 0.26796069, 0.28221339, 0.28577789, 0.27497696,\n        0.28150522, 0.28051397, 0.2952206 , 0.29011561, 0.26580012,\n        0.29861004, 0.28290531, 0.29308168, 0.29540524, 0.29133105,\n        0.26194208, 0.27453629, 0.28423632, 0.28754957, 0.29105858,\n        0.30371917, 0.29414405, 0.26434759, 0.27398082, 0.28836646,\n        0.27865106, 0.29033272, 0.26956972, 0.27145813, 0.29911516,\n        0.27794537, 0.27782338, 0.29954715, 0.28066076, 0.27972522,\n        0.27804922, 0.26821622, 0.28906917, 0.2632705 , 0.28286189,\n        0.29065819, 0.29193461, 0.28136507, 0.28026778, 0.29295554,\n        0.26517387, 0.29425549, 0.30804006, 0.28704281, 0.27740026,\n        0.2846133 , 0.28770851, 0.29012461, 0.27287708, 0.29972155,\n        0.27844286, 0.28768296, 0.27147132, 0.27616361, 0.28509646,\n        0.29117302, 0.29372196, 0.28586758, 0.2859284 , 0.30335367,\n        0.29085349, 0.29039441, 0.2726979 , 0.29491554, 0.27661858,\n        0.25943946, 0.26505466, 0.28597869, 0.29392625, 0.27005283,\n        0.3007907 , 0.28118015, 0.28983639, 0.2765901 , 0.28024438,\n        0.28915336, 0.27045131, 0.28473   , 0.26954618, 0.26821224,\n...\n        0.29068942, 0.26988927, 0.28109482, 0.2727146 , 0.27852304,\n        0.29607621, 0.28032608, 0.27395366, 0.27287236, 0.28504641,\n        0.28525569, 0.28583191, 0.2712375 , 0.26716243, 0.30230949,\n        0.28532536, 0.29423557, 0.29564959, 0.29745661, 0.26888218,\n        0.26838015, 0.29565356, 0.26967397, 0.27187912, 0.28135606,\n        0.28304505, 0.27158211, 0.2864493 , 0.27307191, 0.2847543 ,\n        0.25584078, 0.27832424, 0.2843386 , 0.29095929, 0.27687202,\n        0.28019217, 0.27081437, 0.2627205 , 0.28201011, 0.29490209,\n        0.28876678, 0.27503415, 0.2842989 , 0.27999148, 0.29424204,\n        0.28593155, 0.28583873, 0.29779127, 0.28225148, 0.28155015,\n        0.2765703 , 0.26505134, 0.28760766, 0.29557532, 0.27780627,\n        0.29144613, 0.28853826, 0.29743531, 0.27179258, 0.29709818,\n        0.28026963, 0.29804811, 0.29445129, 0.2743523 , 0.28993555,\n        0.27446623, 0.28617753, 0.29551676, 0.28833088, 0.29440384,\n        0.2728848 , 0.29908961, 0.30078822, 0.2707285 , 0.2921563 ,\n        0.28173769, 0.27039685, 0.28419392, 0.30701929, 0.27466561,\n        0.29472175, 0.29612711, 0.29507025, 0.28789346, 0.28496826,\n        0.28092347, 0.30135626, 0.27865633, 0.29522402, 0.28187341,\n        0.30057319, 0.29331467, 0.279472  , 0.26419982, 0.28482659,\n        0.28272494, 0.28532657, 0.28146858, 0.27743249, 0.28302309]])</pre></li><li>theta(chain, draw)float640.2193 0.2377 ... 0.2262 0.2273<pre>array([[0.21926395, 0.23773322, 0.24634127, 0.21623296, 0.23642652,\n        0.22111949, 0.22495697, 0.25628748, 0.2101466 , 0.25276287,\n        0.2248501 , 0.21608373, 0.22108407, 0.21188956, 0.22757382,\n        0.20355766, 0.25681857, 0.22190125, 0.23127363, 0.24678573,\n        0.2446126 , 0.24752589, 0.2313172 , 0.22477503, 0.26900641,\n        0.20995488, 0.21967078, 0.22203012, 0.22892195, 0.22769598,\n        0.25575986, 0.25595893, 0.21430266, 0.22365223, 0.23292054,\n        0.20526836, 0.21202613, 0.2546274 , 0.24183135, 0.22952325,\n        0.22292148, 0.20872177, 0.22185982, 0.23809911, 0.23752978,\n        0.22814755, 0.22079192, 0.21364439, 0.24816604, 0.23122433,\n        0.24655796, 0.23733056, 0.22518473, 0.25434664, 0.20824618,\n        0.21976595, 0.2108437 , 0.21045591, 0.22355502, 0.22577752,\n        0.24842082, 0.21834319, 0.21233718, 0.21915394, 0.23707888,\n        0.22525017, 0.25202085, 0.21517827, 0.2414136 , 0.21781122,\n        0.2365061 , 0.23860844, 0.27468043, 0.21973295, 0.20026981,\n        0.22067574, 0.18132838, 0.23645642, 0.23097993, 0.18447166,\n        0.19527009, 0.22169826, 0.23963433, 0.2002778 , 0.24650898,\n        0.25187738, 0.24773773, 0.25478376, 0.21796462, 0.24346695,\n        0.21071514, 0.22171505, 0.21281513, 0.21765848, 0.2284458 ,\n        0.22115093, 0.22770016, 0.22933262, 0.24337266, 0.23186298,\n...\n        0.21670135, 0.24861222, 0.22146249, 0.2506762 , 0.23231077,\n        0.19756246, 0.21777681, 0.23146048, 0.24204975, 0.2424217 ,\n        0.23771232, 0.22250254, 0.25197794, 0.24076696, 0.20795189,\n        0.23186916, 0.20964025, 0.20730416, 0.22433539, 0.24340138,\n        0.2508239 , 0.21712312, 0.2424278 , 0.22653518, 0.22090151,\n        0.22205752, 0.23802657, 0.23314603, 0.23950914, 0.2203865 ,\n        0.24260487, 0.20959579, 0.22465133, 0.22004824, 0.23880392,\n        0.23068052, 0.26252431, 0.23620393, 0.22530524, 0.21775211,\n        0.20980739, 0.23063954, 0.23212991, 0.23637161, 0.22364425,\n        0.23110318, 0.2392161 , 0.20954491, 0.21991859, 0.2087387 ,\n        0.23872653, 0.25995964, 0.22881131, 0.20958961, 0.24461115,\n        0.2146412 , 0.22597724, 0.2174929 , 0.24099965, 0.22547724,\n        0.23682659, 0.19493773, 0.21302602, 0.23015471, 0.2290815 ,\n        0.24505429, 0.20619053, 0.20623068, 0.19991083, 0.22382226,\n        0.24120312, 0.20654341, 0.22900255, 0.23754717, 0.22801739,\n        0.25161296, 0.22909458, 0.23738216, 0.2101982 , 0.23479435,\n        0.20825109, 0.21132243, 0.20052585, 0.2160261 , 0.22578189,\n        0.2296359 , 0.20252146, 0.22473617, 0.18527293, 0.22672382,\n        0.20743966, 0.21889597, 0.23897781, 0.26547891, 0.22751831,\n        0.23281818, 0.23623766, 0.2351832 , 0.2262477 , 0.22725364]])</pre></li><li>a(chain, draw)float641.285 1.309 1.312 ... 1.295 1.298<pre>array([[1.28475906, 1.30890026, 1.31235637, 1.28969082, 1.30863346,\n        1.29274738, 1.2922128 , 1.33838389, 1.2745223 , 1.34211467,\n        1.30510908, 1.29245893, 1.30359944, 1.29742554, 1.30658332,\n        1.27043371, 1.33472433, 1.30484586, 1.30852222, 1.33272623,\n        1.31879058, 1.3445422 , 1.29924595, 1.29956244, 1.36620433,\n        1.271762  , 1.29579591, 1.29193419, 1.29603785, 1.302541  ,\n        1.34615222, 1.33458025, 1.29317231, 1.30272786, 1.31115362,\n        1.25953418, 1.28938158, 1.33459182, 1.32384272, 1.3164572 ,\n        1.32102001, 1.28447442, 1.31414504, 1.32754903, 1.29119168,\n        1.30566598, 1.30169741, 1.28398308, 1.34172376, 1.30301794,\n        1.32603556, 1.32341137, 1.30244512, 1.3439934 , 1.29525096,\n        1.29812187, 1.2838868 , 1.3044025 , 1.30322685, 1.29926699,\n        1.33719564, 1.29928418, 1.27859585, 1.29495032, 1.30862729,\n        1.30261046, 1.32892213, 1.27561764, 1.3364463 , 1.28750201,\n        1.30670988, 1.32246793, 1.35070496, 1.31527557, 1.27697781,\n        1.2962227 , 1.25603687, 1.31761889, 1.30389364, 1.2470743 ,\n        1.26436481, 1.29408725, 1.3219767 , 1.26129136, 1.31983323,\n        1.34983902, 1.35591223, 1.3416063 , 1.29412076, 1.32898513,\n        1.26756738, 1.30809464, 1.27988889, 1.29301654, 1.30928361,\n        1.30067185, 1.31947064, 1.31304575, 1.33310203, 1.32014696,\n...\n        1.30148412, 1.3352124 , 1.30023184, 1.32062589, 1.30142197,\n        1.27015885, 1.29955974, 1.32780092, 1.32672372, 1.3141769 ,\n        1.30807247, 1.30547549, 1.34026774, 1.33704704, 1.27251072,\n        1.29675233, 1.26303706, 1.29264309, 1.28067621, 1.33808895,\n        1.33606247, 1.28869704, 1.33306963, 1.3173753 , 1.29498735,\n        1.29376449, 1.32116346, 1.30957099, 1.33014477, 1.28812351,\n        1.33619508, 1.29226081, 1.30273949, 1.29542772, 1.31372517,\n        1.29708468, 1.34639599, 1.34483926, 1.30236775, 1.30410657,\n        1.28323645, 1.31685479, 1.31139322, 1.31719385, 1.29796922,\n        1.30775926, 1.31963247, 1.29491533, 1.31031682, 1.28172028,\n        1.3064698 , 1.36559938, 1.30960613, 1.28426468, 1.33030221,\n        1.28872787, 1.30669535, 1.27823048, 1.33488688, 1.29060993,\n        1.3276135 , 1.26891032, 1.29614095, 1.28870262, 1.3109047 ,\n        1.34492831, 1.28205115, 1.26627827, 1.28126727, 1.29958853,\n        1.31898163, 1.28044143, 1.28250121, 1.3410728 , 1.29501248,\n        1.3283658 , 1.3250073 , 1.31218495, 1.28645895, 1.31187113,\n        1.27890625, 1.27833428, 1.27638026, 1.28051096, 1.30114453,\n        1.31602706, 1.26020944, 1.31818674, 1.25525851, 1.31164809,\n        1.27940103, 1.29708345, 1.30649921, 1.36353855, 1.29025566,\n        1.30397091, 1.31514884, 1.30813718, 1.29488701, 1.29777805]])</pre></li><li>v(chain, draw)float640.3358 0.3224 ... 0.3408 0.3359<pre>array([[0.33581982, 0.32235392, 0.3449133 , 0.36997377, 0.33441157,\n        0.34487149, 0.31479096, 0.31433332, 0.35234192, 0.36551983,\n        0.31675314, 0.4052858 , 0.38193523, 0.31313494, 0.37464769,\n        0.38876752, 0.33913929, 0.34445574, 0.29814269, 0.33455576,\n        0.38566882, 0.3735342 , 0.37703582, 0.33306208, 0.37272406,\n        0.39862415, 0.38708858, 0.38056832, 0.30615591, 0.3527573 ,\n        0.38188212, 0.32469811, 0.39097083, 0.32059082, 0.2801328 ,\n        0.31858489, 0.33467101, 0.3598167 , 0.35549586, 0.38858847,\n        0.37399628, 0.3186202 , 0.39540513, 0.37643299, 0.35555243,\n        0.32355694, 0.37792505, 0.34954663, 0.38350635, 0.3717197 ,\n        0.37082072, 0.30973589, 0.36943208, 0.36593273, 0.36984315,\n        0.37919166, 0.3755465 , 0.33693367, 0.38422707, 0.37332096,\n        0.35294142, 0.35109923, 0.37018656, 0.37412308, 0.33070493,\n        0.32812474, 0.3342909 , 0.35996028, 0.36359712, 0.32906623,\n        0.35523607, 0.36519636, 0.37823472, 0.40113852, 0.40451362,\n        0.36181164, 0.38424757, 0.33623958, 0.32940643, 0.36166431,\n        0.39147871, 0.38669682, 0.32998115, 0.37936191, 0.33744363,\n        0.3160134 , 0.36313434, 0.3925911 , 0.36616426, 0.37275724,\n        0.37087946, 0.37565887, 0.35368952, 0.34658674, 0.30942679,\n        0.35155978, 0.36115894, 0.38508574, 0.30897199, 0.36915236,\n...\n        0.35563066, 0.31942814, 0.35955315, 0.38162363, 0.32334013,\n        0.33452425, 0.34012941, 0.37147578, 0.40044943, 0.36291859,\n        0.34573525, 0.39243154, 0.37576601, 0.41932386, 0.37671828,\n        0.36631275, 0.34455489, 0.36656859, 0.30817195, 0.38460255,\n        0.35503296, 0.31088798, 0.3559875 , 0.37436455, 0.36979945,\n        0.33764716, 0.36748837, 0.40427364, 0.32130627, 0.3365596 ,\n        0.31454096, 0.36774801, 0.36497923, 0.32010142, 0.37731449,\n        0.36684456, 0.36432824, 0.34789206, 0.37599563, 0.3312929 ,\n        0.38695416, 0.42456233, 0.37612367, 0.304544  , 0.32845186,\n        0.33442958, 0.36838972, 0.39101608, 0.39296146, 0.40245293,\n        0.33523435, 0.33359946, 0.42295131, 0.36148556, 0.34344691,\n        0.3312494 , 0.33949441, 0.28639462, 0.36440653, 0.36950185,\n        0.36278345, 0.28840209, 0.36512219, 0.32417541, 0.41728764,\n        0.37426958, 0.38037466, 0.37564118, 0.33560262, 0.36396291,\n        0.29822499, 0.36626668, 0.35856656, 0.37786936, 0.36754017,\n        0.35098831, 0.34656115, 0.31589568, 0.37039288, 0.33042469,\n        0.35748841, 0.3089385 , 0.34982645, 0.3652055 , 0.33109751,\n        0.30607552, 0.35960712, 0.40397295, 0.41555831, 0.37508935,\n        0.35630397, 0.33701726, 0.34748495, 0.34084425, 0.3419569 ,\n        0.37486642, 0.34892624, 0.32453122, 0.34082057, 0.33589438]])</pre></li><li>v_mean(chain, draw, __obs__)float640.3358 0.3358 ... 0.3359 0.3359<pre>array([[[0.33581982, 0.33581982, 0.33581982, ..., 0.33581982,\n         0.33581982, 0.33581982],\n        [0.32235392, 0.32235392, 0.32235392, ..., 0.32235392,\n         0.32235392, 0.32235392],\n        [0.3449133 , 0.3449133 , 0.3449133 , ..., 0.3449133 ,\n         0.3449133 , 0.3449133 ],\n        ...,\n        [0.32453122, 0.32453122, 0.32453122, ..., 0.32453122,\n         0.32453122, 0.32453122],\n        [0.34082057, 0.34082057, 0.34082057, ..., 0.34082057,\n         0.34082057, 0.34082057],\n        [0.33589438, 0.33589438, 0.33589438, ..., 0.33589438,\n         0.33589438, 0.33589438]]], shape=(1, 1000, 3988))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987],\n      dtype='int64', name='__obs__', length=3988))</pre></li></ul></li><li>Attributes: (4)created_at :2025-09-27T19:01:23.290578+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 96kB\nDimensions:                  (__obs__: 3988, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 32kB 0 1 2 3 ... 3985 3986 3987\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 64kB ...\nAttributes:\n    created_at:                 2025-09-27T19:01:23.292987+00:00\n    arviz_version:              0.22.0\n    inference_library:          pymc\n    inference_library_version:  5.25.1</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 3988</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 ... 3984 3985 3986 3987<pre>array([   0,    1,    2, ..., 3985, 3986, 3987], shape=(3988,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float641.21 1.0 1.63 1.0 ... -1.0 1.25 1.0<pre>array([[ 1.21 ,  1.   ],\n       [ 1.63 ,  1.   ],\n       [ 1.03 ,  1.   ],\n       ...,\n       [ 0.784,  1.   ],\n       [ 2.35 , -1.   ],\n       [ 1.25 ,  1.   ]], shape=(3988, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987],\n      dtype='int64', name='__obs__', length=3988))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (4)created_at :2025-09-27T19:01:23.292987+00:00arviz_version :0.22.0inference_library :pymcinference_library_version :5.25.1</li></ul> </ul> </li> </ul> <p>The <code>.hist</code> attribute stores the loss history. We can plot this to see how the loss function converged.</p> In\u00a0[8]: Copied! <pre>plt.plot(cav_model.vi_approx.hist)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\n</pre> plt.plot(cav_model.vi_approx.hist) plt.xlabel(\"Iteration\") plt.ylabel(\"Loss\") Out[8]: <pre>Text(0, 0.5, 'Loss')</pre> In\u00a0[9]: Copied! <pre>__, axes = plt.subplots(4, 4, figsize=(10, 5))\naz.plot_pair(cav_model.traces, ax=axes, scatter_kwargs=dict(alpha=0.01, color=\"blue\"))\naz.plot_pair(cav_model.vi_idata, ax=axes, scatter_kwargs=dict(alpha=0.04, color=\"red\"))\n</pre> __, axes = plt.subplots(4, 4, figsize=(10, 5)) az.plot_pair(cav_model.traces, ax=axes, scatter_kwargs=dict(alpha=0.01, color=\"blue\")) az.plot_pair(cav_model.vi_idata, ax=axes, scatter_kwargs=dict(alpha=0.04, color=\"red\")) Out[9]: <pre>array([[&lt;Axes: ylabel='theta'&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;],\n       [&lt;Axes: ylabel='t'&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;],\n       [&lt;Axes: ylabel='v'&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;],\n       [&lt;Axes: xlabel='z', ylabel='a'&gt;, &lt;Axes: xlabel='theta'&gt;,\n        &lt;Axes: xlabel='t'&gt;, &lt;Axes: xlabel='v'&gt;]], dtype=object)</pre> In\u00a0[10]: Copied! <pre>cav_model.traces\n</pre> cav_model.traces Out[10]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt; Size: 44kB\nDimensions:  (chain: 2, draw: 500)\nCoordinates:\n  * chain    (chain) int64 16B 0 1\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    z        (chain, draw) float64 8kB 0.518 0.5027 0.4997 ... 0.5025 0.5035\n    theta    (chain, draw) float64 8kB 0.2354 0.2169 0.2264 ... 0.2089 0.2117\n    t        (chain, draw) float64 8kB 0.2907 0.2973 0.2919 ... 0.2948 0.2924\n    v        (chain, draw) float64 8kB 0.3416 0.4068 0.3936 ... 0.3441 0.3442\n    a        (chain, draw) float64 8kB 1.325 1.296 1.287 ... 1.244 1.283 1.276\nAttributes:\n    created_at:                  2025-09-27T18:43:49.668934+00:00\n    arviz_version:               0.22.0\n    inference_library:           numpyro\n    inference_library_version:   0.19.0\n    sampling_time:               202.619676\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (5)<ul><li>z(chain, draw)float640.518 0.5027 ... 0.5025 0.5035<pre>array([[0.51798045, 0.50266511, 0.49966555, 0.50489683, 0.50312745,\n        0.5020739 , 0.50743648, 0.50975316, 0.50932737, 0.51500558,\n        0.50889804, 0.50286078, 0.49765248, 0.50672763, 0.51213887,\n        0.51164559, 0.51255287, 0.50939326, 0.50974035, 0.51359787,\n        0.5147922 , 0.50719079, 0.50474743, 0.50414187, 0.50687158,\n        0.49852627, 0.49971928, 0.49971928, 0.49971928, 0.50951071,\n        0.51520761, 0.50010936, 0.49419227, 0.50407714, 0.5073961 ,\n        0.51053065, 0.51085661, 0.50898426, 0.49991354, 0.51130565,\n        0.51413634, 0.50294831, 0.50356286, 0.5108842 , 0.51229276,\n        0.50507682, 0.51346098, 0.50347168, 0.50268706, 0.50330135,\n        0.5035116 , 0.50955544, 0.50684159, 0.50510406, 0.50612866,\n        0.51044164, 0.50411028, 0.51241915, 0.51537727, 0.49839101,\n        0.49794174, 0.50449601, 0.50659524, 0.50335781, 0.4962278 ,\n        0.51043534, 0.51693207, 0.50413904, 0.50898212, 0.50947712,\n        0.50580158, 0.49899318, 0.51393462, 0.511976  , 0.51538009,\n        0.51826794, 0.5066692 , 0.50483803, 0.50817294, 0.50987094,\n        0.49735484, 0.49584948, 0.49882328, 0.51452912, 0.49794776,\n        0.50097013, 0.50263888, 0.50199889, 0.51094625, 0.49009086,\n        0.49147193, 0.50090399, 0.5183829 , 0.51019322, 0.5054049 ,\n        0.51235221, 0.50149869, 0.49918714, 0.49647816, 0.50120866,\n...\n        0.50411638, 0.50775443, 0.48843599, 0.48485266, 0.48989245,\n        0.50436717, 0.50757659, 0.49846341, 0.49712141, 0.49732529,\n        0.50504122, 0.50416344, 0.50342165, 0.51169113, 0.50599734,\n        0.5070966 , 0.51240548, 0.50504958, 0.50193168, 0.49742132,\n        0.50264166, 0.50803385, 0.49910192, 0.49549425, 0.49677006,\n        0.50564194, 0.51286268, 0.51281032, 0.51199779, 0.50974115,\n        0.50713348, 0.5050283 , 0.50822827, 0.5036496 , 0.51155079,\n        0.50507255, 0.50234396, 0.50776946, 0.50827039, 0.51486421,\n        0.51454626, 0.50028621, 0.50247879, 0.50717023, 0.50688365,\n        0.50729993, 0.49413071, 0.50315183, 0.50685821, 0.51123006,\n        0.49638534, 0.49866497, 0.49990893, 0.50244745, 0.50626103,\n        0.50788085, 0.50813434, 0.50747725, 0.50695331, 0.50126092,\n        0.50321683, 0.50357532, 0.50190505, 0.50696542, 0.50444915,\n        0.51366315, 0.50965581, 0.50192592, 0.50171876, 0.50145544,\n        0.49946422, 0.50550998, 0.51170294, 0.50629708, 0.50389656,\n        0.49993949, 0.49384796, 0.49953994, 0.51511436, 0.51463768,\n        0.51174141, 0.51206747, 0.50979435, 0.50333413, 0.49747034,\n        0.49794693, 0.49836936, 0.51237104, 0.51514951, 0.4942617 ,\n        0.50215679, 0.50498531, 0.49683681, 0.51319305, 0.49963952,\n        0.51206312, 0.50632939, 0.50933088, 0.5024959 , 0.50353521]])</pre></li><li>theta(chain, draw)float640.2354 0.2169 ... 0.2089 0.2117<pre>array([[0.23540898, 0.21693812, 0.22635795, 0.22558629, 0.23011574,\n        0.23656436, 0.22590466, 0.22233997, 0.22833745, 0.21996942,\n        0.22714471, 0.22700598, 0.22136428, 0.22161189, 0.23761118,\n        0.22908717, 0.22567381, 0.22692328, 0.21025771, 0.21045068,\n        0.21763276, 0.20042627, 0.2046727 , 0.2167833 , 0.23023238,\n        0.20741471, 0.21159766, 0.21159766, 0.21159766, 0.23384325,\n        0.22749502, 0.23451818, 0.21896525, 0.22972588, 0.227366  ,\n        0.23782416, 0.22391853, 0.23010872, 0.24095236, 0.2299433 ,\n        0.22397857, 0.23530112, 0.23909238, 0.22950693, 0.22913283,\n        0.20862993, 0.24504473, 0.22904394, 0.23064158, 0.21920744,\n        0.21941618, 0.2216866 , 0.22292941, 0.22920273, 0.23849432,\n        0.22790336, 0.22352528, 0.22836899, 0.22847831, 0.23835588,\n        0.23680178, 0.2269851 , 0.23746174, 0.26455596, 0.24251476,\n        0.2143982 , 0.23401064, 0.21422283, 0.23344674, 0.23743298,\n        0.2260642 , 0.21137152, 0.23814633, 0.23372171, 0.23026843,\n        0.23126126, 0.22608013, 0.22542055, 0.24355352, 0.22100954,\n        0.21893326, 0.21534902, 0.2076019 , 0.22543064, 0.20619974,\n        0.20376264, 0.20973314, 0.20909389, 0.23410555, 0.2303346 ,\n        0.22605199, 0.23926546, 0.24175554, 0.2425176 , 0.21295108,\n        0.2307045 , 0.21950657, 0.21931832, 0.22020415, 0.2464707 ,\n...\n        0.2262166 , 0.2135575 , 0.21525753, 0.24926573, 0.24580404,\n        0.2059084 , 0.23391552, 0.21835866, 0.22136853, 0.21781305,\n        0.22176296, 0.23148392, 0.23369717, 0.23612418, 0.23191592,\n        0.21279556, 0.21339536, 0.22464346, 0.21970534, 0.23501628,\n        0.24622252, 0.24079616, 0.22765143, 0.21950761, 0.21683857,\n        0.23287293, 0.23606266, 0.21059647, 0.20694018, 0.20719054,\n        0.21672385, 0.21059765, 0.21553682, 0.22194415, 0.22468216,\n        0.2133681 , 0.24559023, 0.21630752, 0.21436963, 0.21608735,\n        0.21937717, 0.24353113, 0.24332982, 0.23354454, 0.22604504,\n        0.23505772, 0.22727541, 0.24544311, 0.23749072, 0.22160279,\n        0.23056412, 0.22649434, 0.22314785, 0.24039143, 0.23430511,\n        0.24214248, 0.22557563, 0.224337  , 0.22210203, 0.20620939,\n        0.20902776, 0.20171284, 0.21581563, 0.21763087, 0.21878739,\n        0.22518151, 0.230854  , 0.21971674, 0.22083233, 0.2174607 ,\n        0.22469071, 0.25385658, 0.23694432, 0.23052795, 0.24741409,\n        0.22435856, 0.21639149, 0.22720441, 0.22376847, 0.21949567,\n        0.21616045, 0.21637691, 0.2104669 , 0.20710016, 0.26394139,\n        0.25763881, 0.25750588, 0.24317795, 0.23107264, 0.23410487,\n        0.23332746, 0.23181248, 0.22841208, 0.24408487, 0.23477704,\n        0.20504662, 0.19843104, 0.19112317, 0.20886419, 0.2116664 ]])</pre></li><li>t(chain, draw)float640.2907 0.2973 ... 0.2948 0.2924<pre>array([[0.29066673, 0.29732839, 0.2918803 , 0.28206727, 0.28301163,\n        0.2819077 , 0.28016093, 0.2776777 , 0.28033566, 0.2816084 ,\n        0.27957997, 0.28989819, 0.29275117, 0.27396132, 0.28249438,\n        0.28592738, 0.2844956 , 0.29313038, 0.30081924, 0.31124125,\n        0.30723209, 0.30398958, 0.29291168, 0.29265525, 0.28984461,\n        0.28594309, 0.28874729, 0.28874729, 0.28874729, 0.27931812,\n        0.28027044, 0.28479284, 0.27868869, 0.28547046, 0.27480315,\n        0.29506111, 0.28493946, 0.2879311 , 0.27398279, 0.28910319,\n        0.28651547, 0.27887805, 0.28151451, 0.28357716, 0.28466482,\n        0.29140468, 0.28516574, 0.2781693 , 0.27876048, 0.28520297,\n        0.28449853, 0.2932282 , 0.28231073, 0.28803226, 0.2895779 ,\n        0.28639805, 0.27368172, 0.28932918, 0.29532352, 0.27420406,\n        0.26985195, 0.2709297 , 0.2700005 , 0.26776802, 0.26613963,\n        0.29154636, 0.29466975, 0.28169163, 0.29490428, 0.29504815,\n        0.28848925, 0.29024819, 0.28301984, 0.28398589, 0.29205357,\n        0.29341779, 0.27350903, 0.27362709, 0.28064676, 0.27748654,\n        0.27249168, 0.28439096, 0.2936808 , 0.29152858, 0.29341747,\n        0.28924204, 0.2947731 , 0.28912462, 0.27437604, 0.28256722,\n        0.27942738, 0.28330872, 0.26765686, 0.28859678, 0.28575526,\n        0.28584799, 0.28704789, 0.28820805, 0.28898599, 0.27631979,\n...\n        0.28694665, 0.29380682, 0.2848301 , 0.26759905, 0.26736483,\n        0.29292369, 0.28271018, 0.28826716, 0.28301777, 0.28348628,\n        0.28232256, 0.28129016, 0.27538799, 0.28367193, 0.29205408,\n        0.28263663, 0.28411965, 0.28401922, 0.29325105, 0.27208966,\n        0.26895057, 0.27917833, 0.27833768, 0.2850537 , 0.28717811,\n        0.28630314, 0.27969722, 0.30333256, 0.30091543, 0.29348512,\n        0.3032844 , 0.29526772, 0.29457258, 0.28950594, 0.28926087,\n        0.29904259, 0.27984854, 0.27749653, 0.28074542, 0.29582328,\n        0.29572475, 0.26441399, 0.26669134, 0.2764194 , 0.29496523,\n        0.27940286, 0.27881931, 0.2793494 , 0.26692752, 0.27992826,\n        0.28409046, 0.28614242, 0.28161673, 0.27528053, 0.26938248,\n        0.27635211, 0.28477699, 0.2907622 , 0.29358143, 0.29929841,\n        0.29738294, 0.29226223, 0.30242891, 0.28659061, 0.28971351,\n        0.2845867 , 0.28484981, 0.28340833, 0.27916258, 0.2784293 ,\n        0.27977728, 0.27284913, 0.27804037, 0.27834289, 0.2663524 ,\n        0.2899974 , 0.28121124, 0.2736723 , 0.28581505, 0.28172216,\n        0.27631226, 0.27672076, 0.29794942, 0.29530409, 0.2617644 ,\n        0.25630982, 0.25847615, 0.29885696, 0.29903635, 0.27137859,\n        0.28428732, 0.28851994, 0.27003979, 0.28397951, 0.2653719 ,\n        0.29591655, 0.304073  , 0.29229635, 0.29479409, 0.2923953 ]])</pre></li><li>v(chain, draw)float640.3416 0.4068 ... 0.3441 0.3442<pre>array([[0.34159598, 0.40684306, 0.3935965 , 0.36303257, 0.35615625,\n        0.34612818, 0.33240301, 0.34186188, 0.34378314, 0.34789356,\n        0.33442959, 0.38704575, 0.38078045, 0.34505608, 0.32444458,\n        0.34406935, 0.34299148, 0.34462297, 0.35501462, 0.33089168,\n        0.34610263, 0.37351392, 0.39128703, 0.34696665, 0.36462366,\n        0.37372826, 0.37565148, 0.37565148, 0.37565148, 0.34100512,\n        0.32706842, 0.38717769, 0.38802369, 0.36548839, 0.3551233 ,\n        0.35090791, 0.36154214, 0.34860973, 0.35925097, 0.36522902,\n        0.35254557, 0.35536968, 0.35695332, 0.35873995, 0.35385469,\n        0.32818755, 0.35545441, 0.34919602, 0.35562988, 0.35169245,\n        0.34914007, 0.3669255 , 0.3391939 , 0.37156172, 0.35078819,\n        0.33007914, 0.34404856, 0.35512821, 0.35417704, 0.35880866,\n        0.3563793 , 0.34009606, 0.33711359, 0.37685546, 0.35798623,\n        0.30865021, 0.31964205, 0.34034162, 0.37730902, 0.38332526,\n        0.37691626, 0.35709466, 0.35506209, 0.33901285, 0.34584483,\n        0.34349976, 0.33372863, 0.33659086, 0.33374575, 0.35409125,\n        0.39383168, 0.38493581, 0.38330808, 0.33223004, 0.37844997,\n        0.3785653 , 0.38538424, 0.37529013, 0.35766284, 0.39166098,\n        0.38999048, 0.36903115, 0.31268075, 0.34857337, 0.34519484,\n        0.3298684 , 0.36396211, 0.37349471, 0.37573753, 0.37006984,\n...\n        0.37790548, 0.34022912, 0.3842575 , 0.42870663, 0.42564466,\n        0.37650554, 0.34921923, 0.38532543, 0.38748277, 0.38481364,\n        0.36312584, 0.34897674, 0.34475096, 0.36363894, 0.3598301 ,\n        0.35500103, 0.37227094, 0.36690931, 0.37292149, 0.38136336,\n        0.36121676, 0.35257307, 0.36823438, 0.37365636, 0.37660065,\n        0.37896889, 0.32982554, 0.3500105 , 0.35547507, 0.35823603,\n        0.36212575, 0.35059705, 0.35357751, 0.35412096, 0.34512644,\n        0.36176334, 0.36395028, 0.34251676, 0.33401627, 0.32056223,\n        0.35315514, 0.38481772, 0.35545796, 0.3425387 , 0.35564492,\n        0.32860845, 0.37597236, 0.34779942, 0.36093395, 0.3486092 ,\n        0.37350871, 0.37530074, 0.38425741, 0.37445558, 0.35999854,\n        0.34655295, 0.36383242, 0.35871377, 0.36083089, 0.34893688,\n        0.35438811, 0.34920261, 0.39269872, 0.34927488, 0.3394094 ,\n        0.3421977 , 0.35378812, 0.35989925, 0.38234995, 0.3866259 ,\n        0.36915542, 0.36321605, 0.33429356, 0.34509582, 0.36073679,\n        0.38953556, 0.37943083, 0.37846359, 0.3335723 , 0.34113405,\n        0.33842525, 0.33710596, 0.38254649, 0.35032305, 0.37978271,\n        0.38619277, 0.36605131, 0.34247184, 0.33942679, 0.38493692,\n        0.37574903, 0.37159439, 0.36091279, 0.36829862, 0.38628953,\n        0.34731598, 0.34184143, 0.35102922, 0.34412976, 0.34423263]])</pre></li><li>a(chain, draw)float641.325 1.296 1.287 ... 1.283 1.276<pre>array([[1.32454351, 1.29577497, 1.28662606, 1.30890059, 1.31582245,\n        1.30528893, 1.31705578, 1.29532489, 1.30640793, 1.30969495,\n        1.30070394, 1.28760062, 1.30030822, 1.32057172, 1.32730631,\n        1.30718916, 1.31390686, 1.28827171, 1.26731362, 1.27821495,\n        1.2648741 , 1.27041082, 1.26195802, 1.29702502, 1.28846571,\n        1.27064208, 1.29373796, 1.29373796, 1.29373796, 1.31625427,\n        1.31323644, 1.31480435, 1.29607064, 1.30215309, 1.3161783 ,\n        1.30355091, 1.30634202, 1.29860193, 1.32801762, 1.30726603,\n        1.30076282, 1.31339114, 1.30479311, 1.29421703, 1.29468502,\n        1.27694437, 1.32678446, 1.30606451, 1.31153771, 1.30035153,\n        1.29216609, 1.28957672, 1.30259033, 1.31047101, 1.30201157,\n        1.31076203, 1.29442287, 1.31090756, 1.31065136, 1.31039337,\n        1.30646175, 1.33274135, 1.32493926, 1.35665826, 1.34119691,\n        1.29714642, 1.290631  , 1.28315479, 1.29892406, 1.29596894,\n        1.30618773, 1.2833231 , 1.32007423, 1.30360985, 1.31057225,\n        1.31723424, 1.30810331, 1.31346197, 1.33278112, 1.31973179,\n        1.3031322 , 1.29452816, 1.27899098, 1.30128673, 1.26833062,\n        1.27987189, 1.26773319, 1.27910638, 1.33089389, 1.31406876,\n        1.31758162, 1.30657227, 1.33054291, 1.31925926, 1.29726008,\n        1.30728876, 1.29136051, 1.29347633, 1.29709361, 1.32387869,\n...\n        1.30950116, 1.27902184, 1.29657511, 1.34043153, 1.34445288,\n        1.2776715 , 1.32475535, 1.30026514, 1.28505873, 1.30085334,\n        1.28922084, 1.32088007, 1.30925014, 1.31540506, 1.3037651 ,\n        1.2939805 , 1.29353102, 1.29258447, 1.30124975, 1.31964273,\n        1.34284047, 1.32393198, 1.31787814, 1.28555761, 1.28942647,\n        1.31107719, 1.30816131, 1.25685589, 1.27930169, 1.26911402,\n        1.27414207, 1.28732435, 1.29394562, 1.29075691, 1.29772386,\n        1.27895314, 1.32799984, 1.28791356, 1.2999865 , 1.27189655,\n        1.300432  , 1.36145986, 1.32358144, 1.30294115, 1.30849355,\n        1.29825711, 1.29866083, 1.3351012 , 1.34202088, 1.30678631,\n        1.2965307 , 1.30867503, 1.28825982, 1.31638443, 1.33765505,\n        1.34227501, 1.30627414, 1.28801233, 1.29563761, 1.26030947,\n        1.25913705, 1.26937122, 1.26878314, 1.29713717, 1.29762855,\n        1.32113497, 1.29781988, 1.28579185, 1.29689313, 1.30366195,\n        1.30124755, 1.34957515, 1.32359584, 1.31880416, 1.34479126,\n        1.29593576, 1.28416397, 1.3296642 , 1.31826654, 1.31317732,\n        1.28934793, 1.29079486, 1.2914163 , 1.27609445, 1.36630493,\n        1.36865466, 1.36381288, 1.30495894, 1.31084896, 1.32303915,\n        1.30993625, 1.31709389, 1.31382995, 1.32516351, 1.33391225,\n        1.27754682, 1.26162721, 1.24382503, 1.28319414, 1.27633693]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T18:43:49.668934+00:00arviz_version :0.22.0inference_library :numpyroinference_library_version :0.19.0sampling_time :202.619676tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> log_likelihood <ul> <pre>&lt;xarray.Dataset&gt; Size: 32MB\nDimensions:      (chain: 2, draw: 500, __obs__: 3988)\nCoordinates:\n  * chain        (chain) int64 16B 0 1\n  * draw         (draw) int64 4kB 0 1 2 3 4 5 6 ... 493 494 495 496 497 498 499\n  * __obs__      (__obs__) int64 32kB 0 1 2 3 4 5 ... 3983 3984 3985 3986 3987\nData variables:\n    rt,response  (chain, draw, __obs__) float64 32MB -0.8889 -1.271 ... -0.9434\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li><li>__obs__: 3988</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li><li>__obs__(__obs__)int640 1 2 3 4 ... 3984 3985 3986 3987<pre>array([   0,    1,    2, ..., 3985, 3986, 3987], shape=(3988,))</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(chain, draw, __obs__)float64-0.8889 -1.271 ... -2.746 -0.9434<pre>array([[[-0.8888652 , -1.27110687, -0.76025298, ..., -0.66592139,\n         -2.69059231, -0.92341052],\n        [-0.85250468, -1.24615263, -0.72272928, ..., -0.63769826,\n         -2.80278409, -0.88780887],\n        [-0.86084841, -1.26173231, -0.7283877 , ..., -0.63740146,\n         -2.81284589, -0.8968854 ],\n        ...,\n        [-0.91602235, -1.29646187, -0.78810766, ..., -0.69063488,\n         -2.67540579, -0.95056154],\n        [-0.90283619, -1.27451579, -0.7811053 , ..., -0.69850796,\n         -2.6967001 , -0.93619635],\n        [-0.89254232, -1.27269274, -0.76772733, ..., -0.68330571,\n         -2.70629768, -0.92662407]],\n\n       [[-0.90969737, -1.27859145, -0.79093624, ..., -0.71210845,\n         -2.68344331, -0.94255117],\n        [-0.88710331, -1.27630288, -0.75719206, ..., -0.66387393,\n         -2.71502916, -0.92221254],\n        [-0.89990378, -1.28844579, -0.76800096, ..., -0.66619086,\n         -2.71132393, -0.93531206],\n        ...,\n        [-0.91924274, -1.32857308, -0.77010755, ..., -0.63495593,\n         -2.8051876 , -0.95800669],\n        [-0.90804227, -1.29141664, -0.78055508, ..., -0.69212427,\n         -2.72637468, -0.94278474],\n        [-0.90767017, -1.29877729, -0.77526259, ..., -0.67644543,\n         -2.7464633 , -0.94341367]]], shape=(2, 500, 3988))</pre></li></ul></li><li>Indexes: (3)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987],\n      dtype='int64', name='__obs__', length=3988))</pre></li></ul></li><li>Attributes: (2)modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> sample_stats <ul> <pre>&lt;xarray.Dataset&gt; Size: 53kB\nDimensions:          (chain: 2, draw: 500)\nCoordinates:\n  * chain            (chain) int64 16B 0 1\n  * draw             (draw) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\nData variables:\n    acceptance_rate  (chain, draw) float64 8kB 0.932 0.9337 1.0 ... 0.9322 1.0\n    step_size        (chain, draw) float64 8kB 0.3315 0.3315 ... 0.3215 0.3215\n    diverging        (chain, draw) bool 1kB False False False ... False False\n    energy           (chain, draw) float64 8kB 6.028e+03 6.029e+03 ... 6.024e+03\n    n_steps          (chain, draw) int64 8kB 7 15 3 15 11 3 ... 15 15 15 3 15 3\n    tree_depth       (chain, draw) int64 8kB 3 4 2 4 4 2 4 2 ... 3 3 4 4 4 2 4 2\n    lp               (chain, draw) float64 8kB 6.026e+03 6.027e+03 ... 6.023e+03\nAttributes:\n    created_at:                  2025-09-27T18:43:49.684970+00:00\n    arviz_version:               0.22.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 2</li><li>draw: 500</li></ul></li><li>Coordinates: (2)<ul><li>chain(chain)int640 1<pre>array([0, 1])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499<pre>array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))</pre></li></ul></li><li>Data variables: (7)<ul><li>acceptance_rate(chain, draw)float640.932 0.9337 1.0 ... 0.9322 1.0<pre>array([[0.93196714, 0.93372568, 1.        , 1.        , 0.9083529 ,\n        0.95539402, 0.96425132, 0.96241834, 0.99553361, 0.83699955,\n        1.        , 0.8481636 , 1.        , 0.95846871, 0.93454531,\n        1.        , 0.81341739, 0.99574965, 0.9722524 , 0.48879782,\n        1.        , 0.85323787, 0.96951008, 0.94792741, 0.97611014,\n        0.98969088, 0.75460287, 0.70509635, 0.66935836, 0.98137595,\n        0.94985859, 0.99542076, 0.94564122, 0.99257824, 0.88761083,\n        0.95607192, 0.99713775, 0.86532331, 0.99060066, 0.96080803,\n        0.97596541, 0.96169731, 0.76773539, 0.97568365, 0.9602348 ,\n        0.99096414, 0.94008483, 0.97574517, 0.99726117, 0.8805187 ,\n        0.98249345, 0.98488729, 0.99017445, 0.94537862, 0.92468064,\n        0.99776685, 0.78639447, 1.        , 0.89915056, 0.90252323,\n        0.82225603, 1.        , 0.9351795 , 0.90128879, 0.92473217,\n        0.76100798, 0.95086106, 0.93705911, 0.98995479, 0.86543566,\n        0.98621557, 0.99171334, 0.8572098 , 0.84618514, 0.89210402,\n        0.90748398, 1.        , 0.99232477, 0.92927664, 0.77979329,\n        0.93095009, 0.90315702, 0.94786811, 0.81405719, 0.98375231,\n        0.83851792, 0.7241212 , 0.95138058, 0.88357196, 0.95722717,\n        0.54301011, 0.99069797, 0.96406995, 0.93066746, 0.8477239 ,\n        1.        , 0.97853948, 0.96873368, 0.79129404, 0.90219873,\n...\n        0.90138522, 0.623164  , 0.63411309, 0.9881188 , 1.        ,\n        0.9308672 , 0.84693035, 0.99874574, 0.69119059, 0.86599888,\n        0.87453492, 0.99495956, 1.        , 0.85677168, 0.98701703,\n        0.98809908, 0.76432503, 0.83174628, 0.83925529, 0.99915616,\n        0.88462203, 0.99753757, 0.67429337, 0.97409665, 0.99370502,\n        0.87874214, 0.85501924, 0.98286417, 1.        , 0.94001809,\n        0.6708907 , 0.64629328, 0.93197999, 0.84737438, 0.97006412,\n        0.94769291, 0.98814377, 0.82283677, 0.90018249, 0.92620402,\n        0.85127444, 0.81603811, 0.98116605, 0.98074058, 0.99693488,\n        0.94828764, 0.81529678, 0.99060592, 0.98939471, 0.98955631,\n        0.93355703, 1.        , 0.72644182, 0.89225003, 0.98626034,\n        0.86296746, 0.99479367, 0.91785865, 1.        , 0.93519695,\n        0.65313815, 0.95707122, 0.922793  , 1.        , 0.61796387,\n        0.72135069, 0.9630709 , 0.99413713, 0.68365506, 0.95696161,\n        0.99834758, 0.93236752, 0.96728047, 0.76288632, 0.99573243,\n        0.97053801, 0.75372592, 0.7742931 , 0.97881333, 0.98739429,\n        0.97955735, 1.        , 0.87499443, 0.98783553, 0.90348242,\n        0.98612451, 0.95750502, 0.90590255, 0.77616646, 0.95831796,\n        0.99067986, 0.8118573 , 0.99306067, 0.95932564, 1.        ,\n        0.90181344, 0.92881191, 0.80894999, 0.9322017 , 1.        ]])</pre></li><li>step_size(chain, draw)float640.3315 0.3315 ... 0.3215 0.3215<pre>array([[0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n        0.33152062, 0.33152062, 0.33152062, 0.33152062, 0.33152062,\n...\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ,\n        0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 , 0.3214885 ]])</pre></li><li>diverging(chain, draw)boolFalse False False ... False False<pre>array([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False]])</pre></li><li>energy(chain, draw)float646.028e+03 6.029e+03 ... 6.024e+03<pre>array([[6027.71767382, 6029.32959643, 6027.86250011, 6024.72816804,\n        6023.88263734, 6023.54981833, 6025.75093089, 6025.61168678,\n        6025.84214636, 6025.02788253, 6025.30870339, 6025.54968735,\n        6024.84591403, 6026.98953702, 6027.21357015, 6024.01272943,\n        6023.52282684, 6024.19969126, 6025.56650662, 6033.28611371,\n        6029.46641487, 6030.18852725, 6028.02052108, 6028.77941554,\n        6025.11737323, 6025.57180614, 6026.46005899, 6027.70968537,\n        6027.48126646, 6023.59095306, 6024.66415087, 6024.46215147,\n        6024.6614819 , 6024.0656791 , 6023.78735244, 6024.84346939,\n        6024.8061418 , 6023.78888476, 6023.40969803, 6024.85575512,\n        6024.31544556, 6024.54658473, 6024.69897048, 6025.1985485 ,\n        6025.20917949, 6026.69952106, 6027.01930177, 6024.96157471,\n        6022.92055832, 6024.48757302, 6022.92489889, 6023.06636626,\n        6023.93687244, 6023.06788204, 6024.75098627, 6024.90290564,\n        6027.9445378 , 6026.26018645, 6027.32953406, 6028.48168666,\n        6027.89286128, 6029.16996105, 6027.75908488, 6026.5875523 ,\n        6029.20791885, 6030.51857639, 6029.605913  , 6029.50500676,\n        6026.09811855, 6027.43914189, 6027.32398486, 6023.52603728,\n        6027.00107195, 6025.50356873, 6025.04063546, 6026.90717117,\n        6027.13902762, 6024.81409169, 6027.53683577, 6026.52305638,\n...\n        6025.42706619, 6024.14505   , 6024.91344081, 6025.5279954 ,\n        6023.61412307, 6025.41129041, 6025.64249812, 6029.28622241,\n        6026.1192622 , 6025.42079327, 6026.52912538, 6026.85768062,\n        6028.25254447, 6028.79833516, 6023.36308704, 6024.0670848 ,\n        6025.04167754, 6027.28896346, 6026.60994311, 6028.5238924 ,\n        6026.98312273, 6029.17536277, 6028.65009277, 6025.96432455,\n        6025.75943738, 6027.30740716, 6028.1114207 , 6025.6226775 ,\n        6025.21450146, 6025.36455841, 6025.43426006, 6023.48837761,\n        6025.07811661, 6026.16428813, 6024.99264587, 6025.91258563,\n        6025.24178937, 6023.16879007, 6022.34223404, 6026.87793844,\n        6026.85979286, 6025.54439731, 6026.30062588, 6025.61665047,\n        6025.43266345, 6027.36515743, 6025.36699146, 6023.98238871,\n        6026.81501101, 6024.11290066, 6023.98245187, 6025.70143096,\n        6025.45285579, 6025.58197684, 6024.93800935, 6025.37587811,\n        6026.93105526, 6028.84981643, 6026.17751291, 6027.79164353,\n        6027.88387623, 6026.62501692, 6031.91402306, 6026.83444326,\n        6028.92475168, 6027.06280357, 6028.55948874, 6029.87489814,\n        6029.98392565, 6030.60350988, 6023.91524929, 6024.99315883,\n        6025.11428914, 6026.86725323, 6026.54276103, 6026.78917791,\n        6027.15348929, 6030.171777  , 6031.45760128, 6024.31491035]])</pre></li><li>n_steps(chain, draw)int647 15 3 15 11 3 ... 15 15 15 3 15 3<pre>array([[ 7, 15,  3, 15, 11,  3, 15,  3, 15,  3,  7, 15,  7, 15,  7, 15,\n         3, 31, 15,  7,  3, 15,  7, 15,  7, 15,  7,  3,  7, 15,  7, 15,\n        15,  7, 15,  7,  7,  3, 31, 15,  7, 15, 15, 15,  7,  7, 15,  7,\n         7, 15, 11,  7, 15, 15, 11, 15,  7,  7,  7, 15,  7, 11,  7, 15,\n         7, 15,  7, 15,  7,  3,  7,  7, 15,  7,  3,  7, 15, 15,  7, 15,\n        23, 15,  7, 15, 11,  7,  7,  7, 15, 11,  7, 11,  7,  7, 23, 15,\n        15, 15,  7, 23,  3, 11, 23, 15,  7,  7,  7, 15, 11,  7, 15, 15,\n         7,  7,  7,  7,  7, 15, 15, 31,  7, 15, 15,  7, 15, 31, 15, 23,\n         7, 23, 15, 15, 15,  7, 15,  7, 15,  7,  7, 15, 15, 23,  7,  7,\n         7, 15,  3,  7,  7,  7,  7,  7, 11, 15, 11,  7,  7,  7,  7,  7,\n         7, 11, 11,  7,  7,  7,  7,  7,  7,  7,  7,  7,  3,  7, 15, 23,\n         3, 15,  7,  7, 15,  7, 15,  7,  7,  7, 15,  7,  7,  3, 11,  3,\n        15, 15, 15,  7,  7,  7, 15, 15, 15, 15,  7, 23, 15, 15, 15, 15,\n         7,  7, 15,  7, 15,  7, 11, 15, 15,  7, 15, 15, 15,  7,  7,  7,\n        15,  3, 15,  7,  7, 15, 15,  7, 15, 15,  7, 15,  7,  7,  7,  7,\n        15, 23, 31, 15,  7, 15, 15, 15, 15, 15,  7, 31, 15,  7, 15, 11,\n         3,  7, 23, 15,  7, 15,  3,  7,  7, 15,  7,  7,  3, 15, 15, 15,\n         7, 15,  3,  7, 15, 11, 15, 15, 15, 15, 15, 15, 15,  7, 15, 19,\n         7, 23, 15, 15, 11, 11,  7,  7, 11, 15,  7, 23, 15,  7,  3, 23,\n        31, 15,  7, 15, 15,  7, 15,  7,  7, 15, 15,  7,  3,  7,  7,  7,\n...\n        15, 15,  7,  7, 15,  7,  7, 17, 15, 31, 15,  7, 15,  7,  7,  7,\n        15, 15, 11, 11, 15, 15, 15,  7, 15, 15,  7,  7, 15,  3,  7, 11,\n        39, 15, 15,  3, 15,  7,  7, 15, 11, 15,  7,  7,  7, 15,  3, 15,\n        15, 11, 11,  7,  7,  7,  7,  7, 15, 15, 15, 15, 15, 15, 15, 15,\n         7,  7,  7, 15, 15,  7,  7,  7,  7,  7, 15,  7, 15, 15,  7,  3,\n        15, 15, 15, 15, 15, 15,  7,  7, 15,  7,  7,  3,  7, 11, 15, 23,\n         3,  7, 15, 15,  7,  7,  3, 15, 15,  7,  3, 11, 15,  7, 15, 15,\n         7, 15,  7,  3, 39, 15, 15, 15,  7,  7, 15,  7,  7,  3, 15, 15,\n        11,  7, 15, 15,  7, 15, 15, 15, 15, 15, 15, 15, 15, 11, 15, 15,\n        11, 11, 15, 15,  7, 15,  7,  7, 15, 15, 15,  3,  7,  7,  3, 15,\n        15,  7, 15, 15,  7, 15,  7,  7, 23, 15, 15,  7,  7,  7,  7, 15,\n        15,  7, 15, 15, 15, 15,  7,  7,  7, 19,  7,  7, 19, 11, 15,  7,\n        15, 15,  7, 23, 15,  3, 11,  7,  7, 15, 15, 15,  3,  7, 15,  3,\n         7, 11, 15, 15,  7, 15, 23, 15,  7,  7, 15, 15,  3,  7, 15,  7,\n         3,  7, 11, 15, 15, 15, 15, 15,  3, 23, 11, 23,  3,  7,  7,  7,\n         7, 15,  7, 15, 31, 15, 15, 15,  7, 15,  7, 15, 15, 15, 15, 15,\n         7, 15, 15,  3,  3, 19, 15,  7, 15,  7,  3, 15,  7,  7,  7, 31,\n         3,  7,  7, 15, 15,  3,  7, 31, 15,  7, 11, 15,  7, 15, 15,  7,\n        15,  1, 15,  7, 15,  7, 15, 15,  3, 11,  7, 11,  7,  7, 15, 15,\n        15,  3, 15,  3]])</pre></li><li>tree_depth(chain, draw)int643 4 2 4 4 2 4 2 ... 3 3 4 4 4 2 4 2<pre>array([[3, 4, 2, 4, 4, 2, 4, 2, 4, 2, 3, 4, 3, 4, 3, 4, 2, 5, 4, 3, 2, 4,\n        3, 4, 3, 4, 3, 2, 3, 4, 3, 4, 4, 3, 4, 3, 3, 2, 5, 4, 3, 4, 4, 4,\n        3, 3, 4, 3, 3, 4, 4, 3, 4, 4, 4, 4, 3, 3, 3, 4, 3, 4, 3, 4, 3, 4,\n        3, 4, 3, 2, 3, 3, 4, 3, 2, 3, 4, 4, 3, 4, 5, 4, 3, 4, 4, 3, 3, 3,\n        4, 4, 3, 4, 3, 3, 5, 4, 4, 4, 3, 5, 2, 4, 5, 4, 3, 3, 3, 4, 4, 3,\n        4, 4, 3, 3, 3, 3, 3, 4, 4, 5, 3, 4, 4, 3, 4, 5, 4, 5, 3, 5, 4, 4,\n        4, 3, 4, 3, 4, 3, 3, 4, 4, 5, 3, 3, 3, 4, 2, 3, 3, 3, 3, 3, 4, 4,\n        4, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 4, 5,\n        2, 4, 3, 3, 4, 3, 4, 3, 3, 3, 4, 3, 3, 2, 4, 2, 4, 4, 4, 3, 3, 3,\n        4, 4, 4, 4, 3, 5, 4, 4, 4, 4, 3, 3, 4, 3, 4, 3, 4, 4, 4, 3, 4, 4,\n        4, 3, 3, 3, 4, 2, 4, 3, 3, 4, 4, 3, 4, 4, 3, 4, 3, 3, 3, 3, 4, 5,\n        5, 4, 3, 4, 4, 4, 4, 4, 3, 5, 4, 3, 4, 4, 2, 3, 5, 4, 3, 4, 2, 3,\n        3, 4, 3, 3, 2, 4, 4, 4, 3, 4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3,\n        4, 5, 3, 5, 4, 4, 4, 4, 3, 3, 4, 4, 3, 5, 4, 3, 2, 5, 5, 4, 3, 4,\n        4, 3, 4, 3, 3, 4, 4, 3, 2, 3, 3, 3, 5, 4, 2, 4, 4, 5, 2, 3, 4, 4,\n        3, 3, 5, 4, 4, 4, 4, 4, 3, 3, 2, 3, 3, 4, 4, 4, 3, 3, 5, 4, 2, 4,\n        4, 3, 3, 4, 3, 4, 2, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4, 4, 3, 4, 4,\n        3, 4, 4, 3, 4, 4, 4, 3, 3, 2, 4, 4, 5, 4, 2, 3, 3, 4, 3, 4, 4, 4,\n        4, 3, 4, 3, 4, 3, 2, 4, 4, 3, 3, 4, 3, 4, 4, 3, 5, 5, 3, 3, 3, 2,\n        3, 3, 4, 3, 3, 4, 4, 3, 3, 4, 3, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 3,\n...\n        4, 3, 3, 3, 3, 5, 3, 4, 4, 4, 2, 4, 4, 4, 4, 3, 3, 3, 4, 3, 4, 4,\n        3, 4, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 5, 2, 4, 4, 5, 4, 4, 4, 3, 3,\n        4, 3, 3, 4, 3, 4, 3, 3, 3, 4, 4, 4, 5, 4, 4, 3, 3, 2, 4, 3, 3, 2,\n        2, 3, 3, 4, 3, 4, 2, 3, 2, 3, 4, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 4,\n        2, 4, 4, 3, 3, 4, 3, 4, 2, 3, 4, 4, 3, 4, 4, 3, 4, 5, 4, 4, 4, 4,\n        4, 3, 2, 4, 4, 2, 6, 4, 4, 4, 4, 3, 3, 4, 4, 3, 4, 4, 3, 3, 4, 3,\n        3, 5, 4, 5, 4, 3, 4, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 3, 3,\n        4, 2, 3, 4, 6, 4, 4, 2, 4, 3, 3, 4, 4, 4, 3, 3, 3, 4, 2, 4, 4, 4,\n        4, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 4, 4, 3, 3, 3,\n        3, 3, 4, 3, 4, 4, 3, 2, 4, 4, 4, 4, 4, 4, 3, 3, 4, 3, 3, 2, 3, 4,\n        4, 5, 2, 3, 4, 4, 3, 3, 2, 4, 4, 3, 2, 4, 4, 3, 4, 4, 3, 4, 3, 2,\n        6, 4, 4, 4, 3, 3, 4, 3, 3, 2, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 4, 4,\n        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 4, 2, 3, 3, 2, 4,\n        4, 3, 4, 4, 3, 4, 3, 3, 5, 4, 4, 3, 3, 3, 3, 4, 4, 3, 4, 4, 4, 4,\n        3, 3, 3, 5, 3, 3, 5, 4, 4, 3, 4, 4, 3, 5, 4, 2, 4, 3, 3, 4, 4, 4,\n        2, 3, 4, 2, 3, 4, 4, 4, 3, 4, 5, 4, 3, 3, 4, 4, 2, 3, 4, 3, 2, 3,\n        4, 4, 4, 4, 4, 4, 2, 5, 4, 5, 2, 3, 3, 3, 3, 4, 3, 4, 5, 4, 4, 4,\n        3, 4, 3, 4, 4, 4, 4, 4, 3, 4, 4, 2, 2, 5, 4, 3, 4, 3, 2, 4, 3, 3,\n        3, 5, 2, 3, 3, 4, 4, 2, 3, 5, 4, 3, 4, 4, 3, 4, 4, 3, 4, 1, 4, 3,\n        4, 3, 4, 4, 2, 4, 3, 4, 3, 3, 4, 4, 4, 2, 4, 2]])</pre></li><li>lp(chain, draw)float646.026e+03 6.027e+03 ... 6.023e+03<pre>array([[6025.94965338, 6026.99767892, 6024.23553907, 6021.6313988 ,\n        6022.16546294, 6023.4405738 , 6023.84476196, 6024.06252076,\n        6022.07725083, 6024.59898695, 6023.06187371, 6024.02677754,\n        6024.01886751, 6025.03146545, 6023.88004758, 6021.97351524,\n        6023.05127744, 6022.78119107, 6023.31926999, 6028.83068925,\n        6026.52351542, 6025.44229833, 6026.02901379, 6023.77978331,\n        6023.87787318, 6024.2163242 , 6023.39750728, 6023.39750728,\n        6023.39750728, 6022.0283603 , 6023.62969437, 6022.75537234,\n        6023.59156904, 6021.60432214, 6022.61967192, 6024.17786884,\n        6022.52255185, 6022.05630402, 6022.68656313, 6022.82153135,\n        6022.9683315 , 6021.93651064, 6023.72523243, 6024.24799482,\n        6023.96478563, 6024.60216454, 6024.04996045, 6022.27571044,\n        6021.79591581, 6022.34019588, 6022.16461465, 6022.5467762 ,\n        6022.31280483, 6022.22392619, 6023.62264133, 6022.91769548,\n        6025.46460196, 6022.95047562, 6025.25180456, 6024.40630411,\n        6026.78591326, 6026.63293736, 6023.47860196, 6026.07875455,\n        6025.114431  , 6027.40304588, 6026.11339135, 6024.24144615,\n        6024.66866101, 6027.33831672, 6022.39769169, 6023.25563102,\n        6023.2554078 , 6022.89983753, 6023.76073904, 6026.02009224,\n        6024.03712532, 6023.94563307, 6023.60064166, 6024.89700245,\n...\n        6023.01021436, 6022.0153253 , 6022.7778465 , 6023.33329826,\n        6022.54920771, 6022.47521193, 6023.80690284, 6025.77290833,\n        6024.46036923, 6023.49680156, 6023.77052171, 6023.6145698 ,\n        6023.05657109, 6021.78851545, 6021.99178684, 6022.70145833,\n        6022.76664128, 6024.98444824, 6024.31181131, 6024.92939879,\n        6024.6107964 , 6027.54791676, 6024.81981599, 6024.20363088,\n        6024.34795042, 6025.75611028, 6023.74848091, 6023.69822074,\n        6024.27339167, 6022.99417467, 6023.43468818, 6022.61537913,\n        6023.94007137, 6023.21584467, 6024.06442612, 6023.89543141,\n        6021.82346284, 6022.22581864, 6022.10500982, 6024.45497633,\n        6024.3383071 , 6023.84442364, 6025.47650018, 6022.10119413,\n        6023.34347789, 6025.01648936, 6022.82047792, 6022.8359241 ,\n        6022.95402541, 6023.54173799, 6022.10156368, 6023.70928471,\n        6022.6123551 , 6022.2484372 , 6023.25159562, 6022.6560245 ,\n        6024.33074789, 6024.8399478 , 6025.38844993, 6025.0648724 ,\n        6026.26891721, 6025.89971343, 6026.37396723, 6023.20932482,\n        6026.26572828, 6026.43964385, 6025.69735752, 6026.95320082,\n        6026.27862571, 6023.57138304, 6021.91115452, 6023.13422104,\n        6024.21075942, 6024.78403435, 6024.45005429, 6023.79903927,\n        6025.21394724, 6029.14551065, 6024.41545162, 6022.97746019]])</pre></li></ul></li><li>Indexes: (2)<ul><li>chainPandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='chain'))</pre></li><li>drawPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))</pre></li></ul></li><li>Attributes: (4)created_at :2025-09-27T18:43:49.684970+00:00arviz_version :0.22.0modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt; Size: 96kB\nDimensions:                  (__obs__: 3988, rt,response_extra_dim_0: 2)\nCoordinates:\n  * __obs__                  (__obs__) int64 32kB 0 1 2 3 ... 3985 3986 3987\n  * rt,response_extra_dim_0  (rt,response_extra_dim_0) int64 16B 0 1\nData variables:\n    rt,response              (__obs__, rt,response_extra_dim_0) float64 64kB ...\nAttributes:\n    created_at:                  2025-09-27T18:43:49.686531+00:00\n    arviz_version:               0.22.0\n    inference_library:           numpyro\n    inference_library_version:   0.19.0\n    sampling_time:               202.619676\n    tuning_steps:                500\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__obs__: 3988</li><li>rt,response_extra_dim_0: 2</li></ul></li><li>Coordinates: (2)<ul><li>__obs__(__obs__)int640 1 2 3 4 ... 3984 3985 3986 3987<pre>array([   0,    1,    2, ..., 3985, 3986, 3987], shape=(3988,))</pre></li><li>rt,response_extra_dim_0(rt,response_extra_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (1)<ul><li>rt,response(__obs__, rt,response_extra_dim_0)float641.21 1.0 1.63 1.0 ... -1.0 1.25 1.0<pre>array([[ 1.21 ,  1.   ],\n       [ 1.63 ,  1.   ],\n       [ 1.03 ,  1.   ],\n       ...,\n       [ 0.784,  1.   ],\n       [ 2.35 , -1.   ],\n       [ 1.25 ,  1.   ]], shape=(3988, 2))</pre></li></ul></li><li>Indexes: (2)<ul><li>__obs__PandasIndex<pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987],\n      dtype='int64', name='__obs__', length=3988))</pre></li><li>rt,response_extra_dim_0PandasIndex<pre>PandasIndex(Index([0, 1], dtype='int64', name='rt,response_extra_dim_0'))</pre></li></ul></li><li>Attributes: (8)created_at :2025-09-27T18:43:49.686531+00:00arviz_version :0.22.0inference_library :numpyroinference_library_version :0.19.0sampling_time :202.619676tuning_steps :500modeling_interface :bambimodeling_interface_version :0.15.0</li></ul> </ul> </li> </ul> In\u00a0[11]: Copied! <pre>import warnings\n\nimport xarray as xr\nfrom matplotlib import gridspec\nfrom pymc.blocking import DictToArrayBijection, RaveledVars\n\n\ndef tracker_to_idata(tracker, model):\n    \"\"\"Turn a tracker object into an InferenceData object.\"\"\"\n    tracker_groups = list(tracker.whatchdict.keys())\n    # n_steps = len(tracker[tracker_groups[0]])\n    stacked_results = {\n        tracker_group: {\n            key: np.stack([d[key] for d in tracker[tracker_group]])\n            for key in tracker[tracker_group][0]\n        }\n        for tracker_group in tracker_groups\n    }\n\n    # coords = {\"vi_step\": np.arange(n_steps)} | {\n    #     k: np.array(v) for k, v in model.coords.items()\n    # }\n    var_to_dims = {\n        var.name: (\"vi_step\", *(model.named_vars_to_dims.get(var.name, ())))\n        for var in model.continuous_value_vars\n    }\n    datasets = {\n        key: xr.Dataset(\n            {\n                var: (var_to_dims[var], stacked_results[key][var])\n                for var in stacked_results[key].keys()\n            }\n        )\n        for key in tracker_groups\n    }\n\n    with warnings.catch_warnings(action=\"ignore\"):\n        return az.InferenceData(**datasets)\n\n\ndef untransform_params(idata, model):\n    \"\"\"Bring transformed parmater back to their original scale.\"\"\"\n    suffixes = [\"_interval__\", \"_log__\"]\n\n    def remove_suffixes(word, suffixes):\n        for suffix in suffixes:\n            if word.endswith(suffix):\n                return word[: -len(suffix)]\n        return word\n\n    free_rv_names = [rv_.name for rv_ in model.free_RVs]\n    transformed_vars = list(idata.mean.data_vars.keys())\n    collect_untransformed_vars = []\n    collect_untransformed_xarray_datasets = []\n\n    for var_ in transformed_vars:\n        var_untrans = remove_suffixes(var_, suffixes=suffixes)\n        if var_untrans in free_rv_names:\n            rv = model.free_RVs[free_rv_names.index(var_untrans)]\n            if model.rvs_to_transforms[rv] is not None:\n                untransformed_var = (\n                    model.rvs_to_transforms[rv]\n                    .backward(idata.mean[var_].values, *rv.owner.inputs)\n                    .eval()\n                )\n                collect_untransformed_vars.append(var_)\n                collect_untransformed_xarray_datasets.append(\n                    xr.Dataset(\n                        data_vars={var_untrans: ((\"vi_step\"), untransformed_var)}\n                    )\n                )\n\n    return xr.merge([idata.mean] + collect_untransformed_xarray_datasets).drop_vars(\n        collect_untransformed_vars\n    )\n\n\ndef plot_vi_traces(idata):\n    \"\"\"Plot parameter history of the optimization alogrithm.\"\"\"\n    if not isinstance(idata, az.InferenceData):\n        raise ValueError(\"idata must be an InferenceData object\")\n    if \"loss\" not in idata.groups():\n        raise ValueError(\"InferenceData object must contain a 'loss' group\")\n    if \"mean_untransformed\" not in idata.groups():\n        print(\n            \"Using transformed variables because 'mean_untransformed' group not found\"\n        )\n        data_vars = list(idata[\"mean\"].data_vars.keys())\n    else:\n        data_vars = list(idata[\"mean_untransformed\"].data_vars.keys())\n\n    fig = plt.figure(figsize=(8, 1.5 * len(data_vars)))\n    gs = gridspec.GridSpec(\n        len(data_vars) // 2 + 2\n        if (len(data_vars) % 2) == 0\n        else (len(data_vars) // 2) + 3,\n        2,\n    )\n\n    for i, var_ in enumerate(data_vars):\n        ax_tmp = fig.add_subplot(gs[i // 2, i % 2])\n        idata[\"mean_untransformed\"][var_].plot(ax=ax_tmp)\n        ax_tmp.set_title(var_)\n\n    last_ax = fig.add_subplot(gs[-2:, :])\n    idata[\"loss\"].loss.plot(ax=last_ax)\n    gs.tight_layout(fig)\n    return fig\n</pre> import warnings  import xarray as xr from matplotlib import gridspec from pymc.blocking import DictToArrayBijection, RaveledVars   def tracker_to_idata(tracker, model):     \"\"\"Turn a tracker object into an InferenceData object.\"\"\"     tracker_groups = list(tracker.whatchdict.keys())     # n_steps = len(tracker[tracker_groups[0]])     stacked_results = {         tracker_group: {             key: np.stack([d[key] for d in tracker[tracker_group]])             for key in tracker[tracker_group][0]         }         for tracker_group in tracker_groups     }      # coords = {\"vi_step\": np.arange(n_steps)} | {     #     k: np.array(v) for k, v in model.coords.items()     # }     var_to_dims = {         var.name: (\"vi_step\", *(model.named_vars_to_dims.get(var.name, ())))         for var in model.continuous_value_vars     }     datasets = {         key: xr.Dataset(             {                 var: (var_to_dims[var], stacked_results[key][var])                 for var in stacked_results[key].keys()             }         )         for key in tracker_groups     }      with warnings.catch_warnings(action=\"ignore\"):         return az.InferenceData(**datasets)   def untransform_params(idata, model):     \"\"\"Bring transformed parmater back to their original scale.\"\"\"     suffixes = [\"_interval__\", \"_log__\"]      def remove_suffixes(word, suffixes):         for suffix in suffixes:             if word.endswith(suffix):                 return word[: -len(suffix)]         return word      free_rv_names = [rv_.name for rv_ in model.free_RVs]     transformed_vars = list(idata.mean.data_vars.keys())     collect_untransformed_vars = []     collect_untransformed_xarray_datasets = []      for var_ in transformed_vars:         var_untrans = remove_suffixes(var_, suffixes=suffixes)         if var_untrans in free_rv_names:             rv = model.free_RVs[free_rv_names.index(var_untrans)]             if model.rvs_to_transforms[rv] is not None:                 untransformed_var = (                     model.rvs_to_transforms[rv]                     .backward(idata.mean[var_].values, *rv.owner.inputs)                     .eval()                 )                 collect_untransformed_vars.append(var_)                 collect_untransformed_xarray_datasets.append(                     xr.Dataset(                         data_vars={var_untrans: ((\"vi_step\"), untransformed_var)}                     )                 )      return xr.merge([idata.mean] + collect_untransformed_xarray_datasets).drop_vars(         collect_untransformed_vars     )   def plot_vi_traces(idata):     \"\"\"Plot parameter history of the optimization alogrithm.\"\"\"     if not isinstance(idata, az.InferenceData):         raise ValueError(\"idata must be an InferenceData object\")     if \"loss\" not in idata.groups():         raise ValueError(\"InferenceData object must contain a 'loss' group\")     if \"mean_untransformed\" not in idata.groups():         print(             \"Using transformed variables because 'mean_untransformed' group not found\"         )         data_vars = list(idata[\"mean\"].data_vars.keys())     else:         data_vars = list(idata[\"mean_untransformed\"].data_vars.keys())      fig = plt.figure(figsize=(8, 1.5 * len(data_vars)))     gs = gridspec.GridSpec(         len(data_vars) // 2 + 2         if (len(data_vars) % 2) == 0         else (len(data_vars) // 2) + 3,         2,     )      for i, var_ in enumerate(data_vars):         ax_tmp = fig.add_subplot(gs[i // 2, i % 2])         idata[\"mean_untransformed\"][var_].plot(ax=ax_tmp)         ax_tmp.set_title(var_)      last_ax = fig.add_subplot(gs[-2:, :])     idata[\"loss\"].loss.plot(ax=last_ax)     gs.tight_layout(fig)     return fig In\u00a0[12]: Copied! <pre># Define the ADVI runner\nwith cav_model.pymc_model:\n    advi = pm.ADVI()\n</pre> # Define the ADVI runner with cav_model.pymc_model:     advi = pm.ADVI() In\u00a0[13]: Copied! <pre># Set up starting point\nstart = cav_model.pymc_model.initial_point()\nvars_dict = {var.name: var for var in cav_model.pymc_model.continuous_value_vars}\nx0 = DictToArrayBijection.map(\n    {var_name: value for var_name, value in start.items() if var_name in vars_dict}\n)\n\n# Define quantities to track\ntracker = pm.callbacks.Tracker(\n    mean=lambda: DictToArrayBijection.rmap(\n        RaveledVars(advi.approx.mean.eval(), x0.point_map_info), start\n    ),  # callable that returns mean\n    std=lambda: DictToArrayBijection.rmap(\n        RaveledVars(advi.approx.std.eval(), x0.point_map_info), start\n    ),  # callable that returns std\n)\n</pre> # Set up starting point start = cav_model.pymc_model.initial_point() vars_dict = {var.name: var for var in cav_model.pymc_model.continuous_value_vars} x0 = DictToArrayBijection.map(     {var_name: value for var_name, value in start.items() if var_name in vars_dict} )  # Define quantities to track tracker = pm.callbacks.Tracker(     mean=lambda: DictToArrayBijection.rmap(         RaveledVars(advi.approx.mean.eval(), x0.point_map_info), start     ),  # callable that returns mean     std=lambda: DictToArrayBijection.rmap(         RaveledVars(advi.approx.std.eval(), x0.point_map_info), start     ),  # callable that returns std ) In\u00a0[14]: Copied! <pre># Run VI fit\napprox = advi.fit(n=30000, callbacks=[tracker])\nvi_posterior_samples = approx.sample(1000)\nvi_posterior_samples.posterior = vi_posterior_samples.posterior.drop_vars(\"v_mean\")\n</pre> # Run VI fit approx = advi.fit(n=30000, callbacks=[tracker]) vi_posterior_samples = approx.sample(1000) vi_posterior_samples.posterior = vi_posterior_samples.posterior.drop_vars(\"v_mean\") <pre>Output()</pre> <pre></pre> <pre>Finished [100%]: Average Loss = 6,037.2\n</pre> In\u00a0[15]: Copied! <pre>from copy import deepcopy\n\n# Convert tracked quantities to idata\nresult = tracker_to_idata(tracker, cav_model.pymc_model)\n\n# Add untransformed parameters\nresult.add_groups(\n    {\"mean_untransformed\": untransform_params(deepcopy(result), cav_model.pymc_model)}\n)\n# Add loss group\nresult.add_groups(\n    {\"loss\": xr.Dataset(data_vars={\"loss\": (\"vi_step\", np.array(approx.hist))})}\n)\n</pre> from copy import deepcopy  # Convert tracked quantities to idata result = tracker_to_idata(tracker, cav_model.pymc_model)  # Add untransformed parameters result.add_groups(     {\"mean_untransformed\": untransform_params(deepcopy(result), cav_model.pymc_model)} ) # Add loss group result.add_groups(     {\"loss\": xr.Dataset(data_vars={\"loss\": (\"vi_step\", np.array(approx.hist))})} ) <p>A quick look at our <code>result</code> <code>InferenceData</code> object, to understand what happened here.</p> <p>We now have two additional groups:</p> <ol> <li><code>mean_untransformed</code> which holds parameter values in the orignal space (instead of the parameters over which the optimization operates, which will always live in an unconstrained space)</li> <li><code>loss</code> which holds our training history</li> </ol> In\u00a0[16]: Copied! <pre>result\n</pre> result Out[16]: arviz.InferenceData <ul> <li> mean <ul> <pre>&lt;xarray.Dataset&gt; Size: 1MB\nDimensions:           (vi_step: 30000)\nDimensions without coordinates: vi_step\nData variables:\n    z_interval__      (vi_step) float64 240kB 0.001 0.001546 ... 0.02299 0.02269\n    t_interval__      (vi_step) float64 240kB -0.001 -0.001678 ... -1.801 -1.801\n    theta_interval__  (vi_step) float64 240kB -0.001 -0.001875 ... -1.187 -1.187\n    a_interval__      (vi_step) float64 240kB -0.001 -0.001364 ... -0.5246\n    v_interval__      (vi_step) float64 240kB 0.001 0.0001871 ... 0.2398 0.2395</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>vi_step: 30000</li></ul></li><li>Coordinates: (0)<ul></ul></li><li>Data variables: (5)<ul><li>z_interval__(vi_step)float640.001 0.001546 ... 0.02299 0.02269<pre>array([0.001     , 0.00154577, 0.00122624, ..., 0.02296367, 0.02298615,\n       0.02268703], shape=(30000,))</pre></li><li>t_interval__(vi_step)float64-0.001 -0.001678 ... -1.801 -1.801<pre>array([-9.99999996e-04, -1.67842811e-03, -2.20955741e-03, ...,\n       -1.80062184e+00, -1.80085303e+00, -1.80063229e+00], shape=(30000,))</pre></li><li>theta_interval__(vi_step)float64-0.001 -0.001875 ... -1.187 -1.187<pre>array([-9.99999456e-04, -1.87538907e-03, -2.85086611e-03, ...,\n       -1.18757816e+00, -1.18707740e+00, -1.18729679e+00], shape=(30000,))</pre></li><li>a_interval__(vi_step)float64-0.001 -0.001364 ... -0.5246<pre>array([-0.001     , -0.00136369, -0.00057203, ..., -0.52437142,\n       -0.52481097, -0.5245951 ], shape=(30000,))</pre></li><li>v_interval__(vi_step)float640.001 0.0001871 ... 0.2398 0.2395<pre>array([ 9.99999245e-04,  1.87066816e-04, -7.48552236e-04, ...,\n        2.39852514e-01,  2.39762967e-01,  2.39463850e-01], shape=(30000,))</pre></li></ul></li><li>Indexes: (0)<ul></ul></li><li>Attributes: (0)</li></ul> </ul> </li> <li> std <ul> <pre>&lt;xarray.Dataset&gt; Size: 1MB\nDimensions:           (vi_step: 30000)\nDimensions without coordinates: vi_step\nData variables:\n    z_interval__      (vi_step) float64 240kB 0.6926 0.6931 ... 0.02424 0.02424\n    t_interval__      (vi_step) float64 240kB 0.6926 0.6923 ... 0.02481 0.02481\n    theta_interval__  (vi_step) float64 240kB 0.6926 0.6921 ... 0.02624 0.02624\n    a_interval__      (vi_step) float64 240kB 0.6936 0.6932 ... 0.01369 0.0137\n    v_interval__      (vi_step) float64 240kB 0.6936 0.6932 ... 0.01156 0.01156</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>vi_step: 30000</li></ul></li><li>Coordinates: (0)<ul></ul></li><li>Data variables: (5)<ul><li>z_interval__(vi_step)float640.6926 0.6931 ... 0.02424 0.02424<pre>array([0.69264739, 0.69314108, 0.69276369, ..., 0.02423687, 0.02424093,\n       0.02424097], shape=(30000,))</pre></li><li>t_interval__(vi_step)float640.6926 0.6923 ... 0.02481 0.02481<pre>array([0.69264731, 0.692336  , 0.69261916, ..., 0.02480861, 0.02481052,\n       0.02481199], shape=(30000,))</pre></li><li>theta_interval__(vi_step)float640.6926 0.6921 ... 0.02624 0.02624<pre>array([0.69264733, 0.69214857, 0.69173706, ..., 0.02625421, 0.02624047,\n       0.0262396 ], shape=(30000,))</pre></li><li>a_interval__(vi_step)float640.6936 0.6932 ... 0.01369 0.0137<pre>array([0.6936473 , 0.69317198, 0.69270987, ..., 0.01370299, 0.01369314,\n       0.01369778], shape=(30000,))</pre></li><li>v_interval__(vi_step)float640.6936 0.6932 ... 0.01156 0.01156<pre>array([0.69364729, 0.6931645 , 0.69271898, ..., 0.01155817, 0.01155963,\n       0.0115599 ], shape=(30000,))</pre></li></ul></li><li>Indexes: (0)<ul></ul></li><li>Attributes: (0)</li></ul> </ul> </li> <li> mean_untransformed <ul> <pre>&lt;xarray.Dataset&gt; Size: 1MB\nDimensions:  (vi_step: 30000)\nDimensions without coordinates: vi_step\nData variables:\n    z        (vi_step) float64 240kB 0.5002 0.5003 0.5002 ... 0.5046 0.5045\n    t        (vi_step) float64 240kB 1.0 0.9997 0.9994 ... 0.2844 0.2844 0.2844\n    theta    (vi_step) float64 240kB 0.5997 0.5993 0.599 ... 0.2273 0.2272\n    a        (vi_step) float64 240kB 1.649 1.649 1.65 ... 1.304 1.304 1.304\n    v        (vi_step) float64 240kB 0.0015 0.0002806 ... 0.3579 0.3575</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>vi_step: 30000</li></ul></li><li>Coordinates: (0)<ul></ul></li><li>Data variables: (5)<ul><li>z(vi_step)float640.5002 0.5003 ... 0.5046 0.5045<pre>array([0.5002    , 0.50030915, 0.50024525, ..., 0.50459253, 0.50459703,\n       0.50453721], shape=(30000,))</pre></li><li>t(vi_step)float641.0 0.9997 0.9994 ... 0.2844 0.2844<pre>array([1.00000025, 0.99966121, 0.99939577, ..., 0.284409  , 0.28435277,\n       0.28440645], shape=(30000,))</pre></li><li>theta(vi_step)float640.5997 0.5993 ... 0.2273 0.2272<pre>array([0.59965   , 0.59934361, 0.5990022 , ..., 0.2271693 , 0.22729487,\n       0.22723985], shape=(30000,))</pre></li><li>a(vi_step)float641.649 1.649 1.65 ... 1.304 1.304<pre>array([1.649325  , 1.64907951, 1.64961388, ..., 1.30394267, 1.30366549,\n       1.30380161], shape=(30000,))</pre></li><li>v(vi_step)float640.0015 0.0002806 ... 0.3579 0.3575<pre>array([ 1.49999874e-03,  2.80600223e-04, -1.12282830e-03, ...,\n        3.58063820e-01,  3.57931412e-01,  3.57489115e-01], shape=(30000,))</pre></li></ul></li><li>Indexes: (0)<ul></ul></li><li>Attributes: (0)</li></ul> </ul> </li> <li> loss <ul> <pre>&lt;xarray.Dataset&gt; Size: 240kB\nDimensions:  (vi_step: 30000)\nDimensions without coordinates: vi_step\nData variables:\n    loss     (vi_step) float64 240kB 1.76e+04 1.829e+04 ... 6.039e+03 6.037e+03</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>vi_step: 30000</li></ul></li><li>Coordinates: (0)<ul></ul></li><li>Data variables: (1)<ul><li>loss(vi_step)float641.76e+04 1.829e+04 ... 6.037e+03<pre>array([17597.26519313, 18288.45609872, 10325.56967041, ...,\n        6037.23556064,  6038.81266654,  6036.75160277], shape=(30000,))</pre></li></ul></li><li>Indexes: (0)<ul></ul></li><li>Attributes: (0)</li></ul> </ul> </li> </ul> In\u00a0[17]: Copied! <pre>fig = plot_vi_traces(result)\n</pre> fig = plot_vi_traces(result) In\u00a0[18]: Copied! <pre>__, axes = plt.subplots(4, 4, figsize=(10, 5))\n\n# Plot MCMC [nuts]\naz.plot_pair(cav_model.traces, ax=axes, scatter_kwargs=dict(alpha=0.01, color=\"blue\"))\n# Plot VI via .vi() [fullrank_advi]\naz.plot_pair(cav_model.vi_idata, ax=axes, scatter_kwargs=dict(alpha=0.04, color=\"red\"))\n# Plot VI via pymc interface [advi]\n# (We need to make sure the variables are in correct order)\naz.plot_pair(\n    vi_posterior_samples.posterior[list(cav_model.traces.posterior.data_vars)],\n    ax=axes,\n    scatter_kwargs=dict(alpha=0.04, color=\"green\"),\n)\n</pre> __, axes = plt.subplots(4, 4, figsize=(10, 5))  # Plot MCMC [nuts] az.plot_pair(cav_model.traces, ax=axes, scatter_kwargs=dict(alpha=0.01, color=\"blue\")) # Plot VI via .vi() [fullrank_advi] az.plot_pair(cav_model.vi_idata, ax=axes, scatter_kwargs=dict(alpha=0.04, color=\"red\")) # Plot VI via pymc interface [advi] # (We need to make sure the variables are in correct order) az.plot_pair(     vi_posterior_samples.posterior[list(cav_model.traces.posterior.data_vars)],     ax=axes,     scatter_kwargs=dict(alpha=0.04, color=\"green\"), ) Out[18]: <pre>array([[&lt;Axes: ylabel='theta'&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;],\n       [&lt;Axes: ylabel='t'&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;],\n       [&lt;Axes: ylabel='v'&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;],\n       [&lt;Axes: xlabel='z', ylabel='a'&gt;, &lt;Axes: xlabel='theta'&gt;,\n        &lt;Axes: xlabel='t'&gt;, &lt;Axes: xlabel='v'&gt;]], dtype=object)</pre> <p>NOTE:</p> <p>It is expected that the posterior of our last run looks a little worse. We chose to run the <code>advi</code> algorithm, which implies only an isotropic Gaussian approximation to the posterior, so we expect to miss the posterior covariances which we pick up via <code>fullrank_advi</code> as well as MCMC.</p>"},{"location":"tutorials/variational_inference/#variational-inference-with-hssm","title":"Variational Inference with HSSM\u00b6","text":"<p>Through our PyMC interface, we also gain access to approximate posteriors via Variational Inference. This tutorial illustrates how to perform variational inference via HSSM.</p>"},{"location":"tutorials/variational_inference/#load-data-and-specify-model","title":"Load Data and Specify model\u00b6","text":""},{"location":"tutorials/variational_inference/#inference","title":"Inference\u00b6","text":"<p>We will run MCMC and VI here to contrast results.</p>"},{"location":"tutorials/variational_inference/#run-mcmc","title":"Run MCMC\u00b6","text":""},{"location":"tutorials/variational_inference/#run-vi","title":"Run VI\u00b6","text":""},{"location":"tutorials/variational_inference/#inspect-outputs","title":"Inspect Outputs\u00b6","text":"<p>From our variational inference runs, we extract two objects.</p> <ol> <li>An <code>az.InferenceData</code> object stored under <code>cav_model.vi_idata</code>. This stores a slightly cleaned up posterior sample, constructed by sampling from the variational posterior.</li> <li>An <code>pm.Approximator</code> object stored under <code>cav_model.vi_approx</code> that holds the variational posterior object itself. This is a rich structure and it is beyond the purpose of this tutorial to illustrate all it's details. Amongst other things you will be able to inspect the loss history and take samples such as those stored under <code>cam_model.vi_data</code>.</li> </ol>"},{"location":"tutorials/variational_inference/#vi_idata","title":"<code>.vi_idata</code>\u00b6","text":"<p>The approximate variational posterior <code>InferenceData</code>.</p>"},{"location":"tutorials/variational_inference/#vi_approx","title":"<code>.vi_approx</code>\u00b6","text":"<p>The approximate variational posterior <code>pm.Approximator</code> object.</p> <p>We can take draws from the posterior with the <code>.sample()</code> method.</p>"},{"location":"tutorials/variational_inference/#contrast-outputs-between-mcmc-and-vi","title":"Contrast outputs between MCMC and VI\u00b6","text":""},{"location":"tutorials/variational_inference/#further-reading","title":"Further Reading\u00b6","text":"<p>We suggest to check out the documentation on the VI api in Pymc for the full glory details of the capabilities we have access to.</p>"},{"location":"tutorials/variational_inference/#working-directly-through-pymc","title":"Working directly through PyMC\u00b6","text":"<p>Here we illustrate how to use our attached <code>pymc_model</code> to make use of the object oriented API for variational inference. This allows us a few extra affordances.</p> <p>Let's define a few helper functions first.</p>"},{"location":"tutorials/variational_inference/#plot-results","title":"Plot Results\u00b6","text":"<p>We can plot the parameter trajectories (histories) over optimization steps, with our little helper function <code>plot_vi_traces()</code>.</p> <p>NOTE:</p> <p>This is a random run, and we did not thoroughly check if the number of steps we allowed the optimizer were indeed enough to converge.</p>"},{"location":"tutorials/variational_inference/#caveats","title":"Caveats\u00b6","text":"<p>Variational Inference is powerful, however it comes with it's own set of sharp edges.</p> <ol> <li>You will not always be in a position to compare VI with MCMC runs (after all if you can run full MCMC, there isn't much benefit to using VI at all) and it can be hard to a priori estimate how many steps you may need to run the algorithm for.</li> <li>The posteriors will be approximate, if the true posterior includes complex parameter trade-offs, VI might result in inaccurate posterior estimates.</li> <li>We recommend VI for <code>loglik_kind=\"approx_differentiable\"</code>, since the gradients of the <code>analytical</code> log-likelihoods still prove somewhat brittle at this point in time.</li> </ol>"},{"location":"tutorials/variational_inference/#read","title":"Read\u00b6","text":"<p>To learn a bit more about the VI-API in PyMC, we recommend you to read the excellent short tutorial in the main documentation.</p>"},{"location":"tutorials/variational_inference_hierarchical/","title":"Hierarchical Variational Inference","text":"In\u00a0[1]: Copied! <pre>import matplotlib\nimport numpy as np\nimport pandas as pd\n\nmatplotlib.use(\"Agg\")\nimport warnings\n\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport pymc as pm\nimport pytensor\n\nimport hssm\n\nwarnings.filterwarnings(\"ignore\")\n# pytensor.config.floatX = \"float64\"\n\n%matplotlib inline\n</pre> import matplotlib import numpy as np import pandas as pd  matplotlib.use(\"Agg\") import warnings  import arviz as az import matplotlib.pyplot as plt import pymc as pm import pytensor  import hssm  warnings.filterwarnings(\"ignore\") # pytensor.config.floatX = \"float64\"  %matplotlib inline In\u00a0[2]: Copied! <pre>def process_idata_for_plotting(\n    idata: az.InferenceData, parameter_matrix: pd.DataFrame, model: str\n) -&gt; pd.DataFrame:\n    \"\"\"Process inference data and parameter matrix into a dataframe for plotting.\n\n    Parameters\n    ----------\n    idata : az.InferenceData\n        Inference data containing posterior samples\n    parameter_matrix : pd.DataFrame\n        DataFrame containing true parameter values\n    model : str\n        Name of model to get parameters for from defaults\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing processed posterior means, HDIs and true parameters\n        ready for plotting\n    \"\"\"\n    # Get Posterior Means VI\n    params_df_mean = pd.DataFrame(\n        {\n            param: idata.posterior[param].mean(dim=[\"chain\", \"draw\"]).values\n            for param in hssm.defaults.default_model_config[model][\"list_params\"]\n        }\n    )\n    params_df_mean.columns = [f\"{param}_mean\" for param in params_df_mean.columns]\n\n    # Get Posterior HDIs VI\n    params_df_hdi = (\n        az.hdi(\n            idata.posterior[hssm.defaults.default_model_config[model][\"list_params\"]],\n            hdi_prob=0.95,\n        )\n        .to_dataframe()\n        .reset_index()\n        .pivot(\n            index=\"__obs__\",\n            columns=\"hdi\",\n            values=hssm.defaults.default_model_config[model][\"list_params\"],\n        )\n    )\n\n    # Get rid of multiindex\n    params_df_hdi.columns = [\"_\".join(col + (\"hdi\",)) for col in params_df_hdi.columns]\n    params_df_hdi = params_df_hdi.reset_index(drop=True)\n\n    # Combine data\n    data_processed = pd.concat([simDataDDM, params_df_mean, params_df_hdi], axis=1)\n\n    # Make plotting ready\n    plot_df = pd.concat(\n        [\n            data_processed.drop(columns=[\"rt\", \"response\"])\n            .drop_duplicates()\n            .reset_index(drop=True),\n            parameter_matrix.drop(columns=[\"participant_id\"]),\n        ],\n        axis=1,\n    )\n    return plot_df\n</pre> def process_idata_for_plotting(     idata: az.InferenceData, parameter_matrix: pd.DataFrame, model: str ) -&gt; pd.DataFrame:     \"\"\"Process inference data and parameter matrix into a dataframe for plotting.      Parameters     ----------     idata : az.InferenceData         Inference data containing posterior samples     parameter_matrix : pd.DataFrame         DataFrame containing true parameter values     model : str         Name of model to get parameters for from defaults      Returns     -------     pd.DataFrame         DataFrame containing processed posterior means, HDIs and true parameters         ready for plotting     \"\"\"     # Get Posterior Means VI     params_df_mean = pd.DataFrame(         {             param: idata.posterior[param].mean(dim=[\"chain\", \"draw\"]).values             for param in hssm.defaults.default_model_config[model][\"list_params\"]         }     )     params_df_mean.columns = [f\"{param}_mean\" for param in params_df_mean.columns]      # Get Posterior HDIs VI     params_df_hdi = (         az.hdi(             idata.posterior[hssm.defaults.default_model_config[model][\"list_params\"]],             hdi_prob=0.95,         )         .to_dataframe()         .reset_index()         .pivot(             index=\"__obs__\",             columns=\"hdi\",             values=hssm.defaults.default_model_config[model][\"list_params\"],         )     )      # Get rid of multiindex     params_df_hdi.columns = [\"_\".join(col + (\"hdi\",)) for col in params_df_hdi.columns]     params_df_hdi = params_df_hdi.reset_index(drop=True)      # Combine data     data_processed = pd.concat([simDataDDM, params_df_mean, params_df_hdi], axis=1)      # Make plotting ready     plot_df = pd.concat(         [             data_processed.drop(columns=[\"rt\", \"response\"])             .drop_duplicates()             .reset_index(drop=True),             parameter_matrix.drop(columns=[\"participant_id\"]),         ],         axis=1,     )     return plot_df <p>We simulate a dataset of 20 participants, each with 120 trials.</p> In\u00a0[3]: Copied! <pre># Group level parameters\nv_mu = 0.2\nv_sigma = 0.3\na_mu = 1\na_sigma = 0.2\nz_mu = 0.5\nz_sigma = 0.01\nt_mu = 0.4\nt_sigma = 0.00\n\n# Make hierarchical dataset\nparameter_matrix = pd.DataFrame(\n    {\n        \"participant_id\": [f\"subj_{str(i).zfill(2)}\" for i in range(1, 21)],\n        \"v_true\": np.sort(np.random.normal(loc=v_mu, scale=v_sigma, size=20)).round(1),\n        \"a_true\": np.sort(np.random.normal(loc=a_mu, scale=a_sigma, size=20)).round(1),\n        \"z_true\": np.sort(np.random.normal(loc=z_mu, scale=z_sigma, size=20)).round(1),\n        \"t_true\": np.sort(np.random.normal(loc=t_mu, scale=t_sigma, size=20)).round(1),\n        \"nTrials\": [120] * 20,\n    }\n)\n\n\ndfs = []\nfor _, row in parameter_matrix.iterrows():\n    df = hssm.simulate_data(\n        model=\"ddm\",\n        theta=dict(v=row[\"v_true\"], a=row[\"a_true\"], z=row[\"z_true\"], t=row[\"t_true\"]),\n        size=row[\"nTrials\"],\n    )\n    df[\"participant_id\"] = row[\"participant_id\"]\n    dfs.append(df)\n    simDataDDM = pd.concat(dfs, ignore_index=True)\n</pre> # Group level parameters v_mu = 0.2 v_sigma = 0.3 a_mu = 1 a_sigma = 0.2 z_mu = 0.5 z_sigma = 0.01 t_mu = 0.4 t_sigma = 0.00  # Make hierarchical dataset parameter_matrix = pd.DataFrame(     {         \"participant_id\": [f\"subj_{str(i).zfill(2)}\" for i in range(1, 21)],         \"v_true\": np.sort(np.random.normal(loc=v_mu, scale=v_sigma, size=20)).round(1),         \"a_true\": np.sort(np.random.normal(loc=a_mu, scale=a_sigma, size=20)).round(1),         \"z_true\": np.sort(np.random.normal(loc=z_mu, scale=z_sigma, size=20)).round(1),         \"t_true\": np.sort(np.random.normal(loc=t_mu, scale=t_sigma, size=20)).round(1),         \"nTrials\": [120] * 20,     } )   dfs = [] for _, row in parameter_matrix.iterrows():     df = hssm.simulate_data(         model=\"ddm\",         theta=dict(v=row[\"v_true\"], a=row[\"a_true\"], z=row[\"z_true\"], t=row[\"t_true\"]),         size=row[\"nTrials\"],     )     df[\"participant_id\"] = row[\"participant_id\"]     dfs.append(df)     simDataDDM = pd.concat(dfs, ignore_index=True) <p>We first instantiate the generative model using HSSM syntax. We will fit this model to the synthetic data using two approaches 1. Variational inference 2. MCMC</p> In\u00a0[4]: Copied! <pre># Generative model\nmSimCentered = hssm.HSSM(\n    data=simDataDDM,\n    p_outlier=0.01,\n    prior_settings=\"safe\",\n    noncentered=False,\n    model=\"ddm\",\n    loglik_kind=\"approx_differentiable\",\n    include=[\n        {\n            \"name\": \"v\",\n            \"formula\": \"v ~ 0 +(1|participant_id)\",\n            \"prior\": {\n                \"1|id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": {\n                        \"name\": \"Normal\",\n                        \"mu\": 1.3,\n                        \"sigma\": 0.3,\n                    },\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 0.2},\n                },\n            },\n        },\n        {\n            \"name\": \"a\",\n            \"formula\": \"a ~ 0 + (1|participant_id)\",\n            \"prior\": {\n                \"1|id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": {\"name\": \"Gamma\", \"mu\": 1, \"sigma\": 0.2},\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 0.2},\n                },\n            },\n        },\n        {\n            \"name\": \"z\",\n            \"formula\": \"z ~ 0 + (1|participant_id)\",\n            \"prior\": {\n                \"1|id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": {\"name\": \"Beta\", \"alpha\": 10, \"beta\": 10},\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 0.01},\n                },\n            },\n        },\n        {\n            \"name\": \"t\",\n            \"formula\": \"t ~ 0 + (1|participant_id)\",\n            \"prior\": {\n                \"1|id\": {\n                    \"name\": \"Normal\",\n                    \"mu\": {\n                        \"name\": \"Normal\",\n                        \"mu\": 0.4,\n                        \"sigma\": 0.1,\n                    },\n                    \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 0.01},\n                },\n            },\n        },\n    ],\n)\n</pre> # Generative model mSimCentered = hssm.HSSM(     data=simDataDDM,     p_outlier=0.01,     prior_settings=\"safe\",     noncentered=False,     model=\"ddm\",     loglik_kind=\"approx_differentiable\",     include=[         {             \"name\": \"v\",             \"formula\": \"v ~ 0 +(1|participant_id)\",             \"prior\": {                 \"1|id\": {                     \"name\": \"Normal\",                     \"mu\": {                         \"name\": \"Normal\",                         \"mu\": 1.3,                         \"sigma\": 0.3,                     },                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 0.2},                 },             },         },         {             \"name\": \"a\",             \"formula\": \"a ~ 0 + (1|participant_id)\",             \"prior\": {                 \"1|id\": {                     \"name\": \"Normal\",                     \"mu\": {\"name\": \"Gamma\", \"mu\": 1, \"sigma\": 0.2},                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 0.2},                 },             },         },         {             \"name\": \"z\",             \"formula\": \"z ~ 0 + (1|participant_id)\",             \"prior\": {                 \"1|id\": {                     \"name\": \"Normal\",                     \"mu\": {\"name\": \"Beta\", \"alpha\": 10, \"beta\": 10},                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 0.01},                 },             },         },         {             \"name\": \"t\",             \"formula\": \"t ~ 0 + (1|participant_id)\",             \"prior\": {                 \"1|id\": {                     \"name\": \"Normal\",                     \"mu\": {                         \"name\": \"Normal\",                         \"mu\": 0.4,                         \"sigma\": 0.1,                     },                     \"sigma\": {\"name\": \"HalfNormal\", \"sigma\": 0.01},                 },             },         },     ], ) <pre>No common intercept. Bounds for parameter v is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter a is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter z is not applied due to a current limitation of Bambi. This will change in the future.\nNo common intercept. Bounds for parameter t is not applied due to a current limitation of Bambi. This will change in the future.\nModel initialized successfully.\n</pre> In\u00a0[5]: Copied! <pre>mSimCentered.graph()\n</pre> mSimCentered.graph() Out[5]: <p>Approach 1: Using Variational Inference (VI) to estimate posteriors. VI is deterministic, it treats inference as an optimization problem. While typically faster then MCMC, it can underestimate true posterior variance and may not correctly consider the tradeoffs that may exist between parameters. Be sure to always use method \"fullrank_advi\".</p> In\u00a0[6]: Copied! <pre># VI\n# obj_optimizer=pm.adamax(learning_rate=0.1)\nvi_idata = mSimCentered.vi(\n    niter=20000,\n    method=\"advi\",  # mention full_rank_advi\n    obj_optimizer=pm.adamax(learning_rate=0.01),\n)\nmSimCenteredVIObject = mSimCentered.vi_approx.sample(draws=1000)\n</pre> # VI # obj_optimizer=pm.adamax(learning_rate=0.1) vi_idata = mSimCentered.vi(     niter=20000,     method=\"advi\",  # mention full_rank_advi     obj_optimizer=pm.adamax(learning_rate=0.01), ) mSimCenteredVIObject = mSimCentered.vi_approx.sample(draws=1000) <pre>Using MCMC starting point defaults.\n</pre> <pre>Output()</pre> <pre></pre> <pre>Finished [100%]: Average Loss = 4,202.5\n</pre> <p>Before looking at the posteriors we must ensure that the model successfully \"converged\".</p> In\u00a0[7]: Copied! <pre># Loss plot\nplt.plot(mSimCentered.vi_approx.hist)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"VI iteration loss\")\nplt.show()\n</pre> # Loss plot plt.plot(mSimCentered.vi_approx.hist) plt.xlabel(\"Iteration\") plt.ylabel(\"Loss\") plt.title(\"VI iteration loss\") plt.show() <p>Once we are satisfied the loss is acceptable, we can take a look at the mean of the group parameters. These should match our generative parameters above.</p> In\u00a0[8]: Copied! <pre>summary_vi = az.summary(\n    mSimCenteredVIObject.posterior,\n    var_names=[r\".*_mu\", r\".*_sigma\"],\n    filter_vars=\"regex\",\n).sort_index()\nsummary_vi\n</pre> summary_vi = az.summary(     mSimCenteredVIObject.posterior,     var_names=[r\".*_mu\", r\".*_sigma\"],     filter_vars=\"regex\", ).sort_index() summary_vi <pre>arviz - WARNING - Shape validation failed: input_shape: (1, 1000), minimum_shape: (chains=2, draws=4)\n</pre> Out[8]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat a_1|participant_id_mu 1.054 0.043 0.974 1.138 0.001 0.001 940.0 1023.0 NaN a_1|participant_id_sigma 0.203 0.033 0.141 0.263 0.001 0.001 1077.0 875.0 NaN t_1|participant_id_mu 0.405 0.004 0.398 0.411 0.000 0.000 1048.0 952.0 NaN t_1|participant_id_sigma 0.017 0.003 0.012 0.022 0.000 0.000 907.0 747.0 NaN v_1|participant_id_mu 0.142 0.047 0.055 0.227 0.002 0.001 905.0 817.0 NaN v_1|participant_id_sigma 0.213 0.037 0.151 0.283 0.001 0.001 935.0 873.0 NaN z_1|participant_id_mu 0.501 0.006 0.491 0.513 0.000 0.000 953.0 911.0 NaN z_1|participant_id_sigma 0.027 0.004 0.019 0.035 0.000 0.000 921.0 960.0 NaN <p>Approach 2: Using MCMC to estimate posteriors. MCMC is typically slower than VI, but can more accurately capture the true posterior variance in some cases.</p> In\u00a0[9]: Copied! <pre># MCMC\nmSimCenteredSampled = mSimCentered.sample(\n    sampler=\"nuts_numpyro\", cores=4, chains=4, draws=250, tune=750, mp_ctx=\"forkserver\"\n)\n\nmSimCentered.sample_posterior_predictive(idata=mSimCenteredSampled)\n</pre> # MCMC mSimCenteredSampled = mSimCentered.sample(     sampler=\"nuts_numpyro\", cores=4, chains=4, draws=250, tune=750, mp_ctx=\"forkserver\" )  mSimCentered.sample_posterior_predictive(idata=mSimCenteredSampled) <pre>Using default initvals. \n\n</pre> <pre>sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [02:56&lt;00:00,  5.67it/s, 31 steps of size 1.55e-01. acc. prob=0.90] \nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [02:12&lt;00:00,  7.53it/s, 31 steps of size 1.54e-01. acc. prob=0.83] \nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:49&lt;00:00,  9.09it/s, 31 steps of size 1.66e-01. acc. prob=0.85] \nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [03:55&lt;00:00,  4.25it/s, 4 steps of size 9.90e-02. acc. prob=0.89]  \nThere were 16 divergences after tuning. Increase `target_accept` or reparameterize.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:02&lt;00:00, 475.47it/s]\n</pre> <p>Let's make sure MCMC converged and can successfully recapitulate the raw data</p> <p>At an individual level, our model can capture the real data relatively well. How do the summary statistics of the posteriors compare to when we fit using VI?</p> In\u00a0[10]: Copied! <pre>summary_mcmc = az.summary(\n    mSimCenteredSampled, var_names=[r\".*_mu\", r\".*_sigma\"], filter_vars=\"regex\"\n).sort_index()\nsummary_mcmc\n</pre> summary_mcmc = az.summary(     mSimCenteredSampled, var_names=[r\".*_mu\", r\".*_sigma\"], filter_vars=\"regex\" ).sort_index() summary_mcmc Out[10]: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat a_1|participant_id_mu 1.056 0.046 0.975 1.149 0.002 0.001 814.0 757.0 1.01 a_1|participant_id_sigma 0.202 0.037 0.134 0.268 0.001 0.001 1087.0 732.0 1.00 t_1|participant_id_mu 0.407 0.008 0.392 0.420 0.000 0.000 356.0 571.0 1.00 t_1|participant_id_sigma 0.011 0.007 0.002 0.023 0.001 0.000 31.0 41.0 1.10 v_1|participant_id_mu 0.130 0.054 0.031 0.234 0.002 0.002 1000.0 520.0 1.00 v_1|participant_id_sigma 0.207 0.044 0.128 0.283 0.002 0.002 777.0 738.0 1.00 z_1|participant_id_mu 0.495 0.010 0.478 0.513 0.001 0.000 366.0 575.0 1.00 z_1|participant_id_sigma 0.023 0.011 0.004 0.042 0.001 0.001 112.0 154.0 1.04 In\u00a0[11]: Copied! <pre># Add trialwise parameters to idata\nmSimCenteredSampled = mSimCentered.add_likelihood_parameters_to_idata(\n    mSimCenteredSampled\n)\n\nplot_df_mcmc = process_idata_for_plotting(\n    idata=mSimCenteredSampled, parameter_matrix=parameter_matrix, model=\"ddm\"\n)\n\nplot_df_vi = process_idata_for_plotting(\n    idata=mSimCenteredVIObject, parameter_matrix=parameter_matrix, model=\"ddm\"\n)\n</pre> # Add trialwise parameters to idata mSimCenteredSampled = mSimCentered.add_likelihood_parameters_to_idata(     mSimCenteredSampled )  plot_df_mcmc = process_idata_for_plotting(     idata=mSimCenteredSampled, parameter_matrix=parameter_matrix, model=\"ddm\" )  plot_df_vi = process_idata_for_plotting(     idata=mSimCenteredVIObject, parameter_matrix=parameter_matrix, model=\"ddm\" ) In\u00a0[12]: Copied! <pre>hssm.plotting.plot_predictive(mSimCentered, col=\"participant_id\", col_wrap=5)\n</pre> hssm.plotting.plot_predictive(mSimCentered, col=\"participant_id\", col_wrap=5) Out[12]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x2f0ef4210&gt;</pre> In\u00a0[13]: Copied! <pre># Suppose your dataframe is called `df`\nparams = hssm.defaults.default_model_config[\"ddm\"][\"list_params\"]\nfig, axes = plt.subplots(nrows=1, ncols=len(params), figsize=(14, 6), sharey=True)\n\nfor ax, par in zip(axes, params):\n    # Identify the relevant columns for this parameter\n    mean_col = f\"{par}_mean\"\n    lower_col = f\"{par}_lower_hdi\"\n    upper_col = f\"{par}_higher_hdi\"\n    true_col = f\"{par}_true\"  # if you also want to plot 'true' values\n\n    # Sort if you want the participants in order on the y\u2010axis\n    df_sorted_vi = plot_df_vi.sort_values(\"participant_id\")\n    df_sorted_mcmc = plot_df_mcmc.sort_values(\"participant_id\")\n\n    # We'll use the row index (0..N-1) for plotting against x=mean\n    yvals = np.array(range(len(df_sorted_vi)))\n    yvals_vi = yvals + 0.1\n    yvals_mcmc = yvals - 0.1\n\n    # Draw horizontal lines from lower_HDI to upper_HDI\n    ax.hlines(\n        yvals_vi,\n        df_sorted_vi[lower_col],\n        df_sorted_vi[upper_col],\n        color=\"blue\",\n        alpha=0.5,\n    )\n\n    ax.hlines(\n        yvals_mcmc,\n        df_sorted_mcmc[lower_col],\n        df_sorted_mcmc[upper_col],\n        color=\"green\",\n        alpha=0.5,\n    )\n\n    # Mark the posterior mean in blue\n    ax.plot(\n        df_sorted_vi[mean_col],\n        yvals_vi,\n        \"|\",\n        color=\"blue\",\n        label=\"Mean_vi\" if par == \"v\" else None,\n    )\n    ax.plot(\n        df_sorted_mcmc[mean_col],\n        yvals_mcmc,\n        \"|\",\n        color=\"green\",\n        label=\"Mean_mcmc\" if par == \"v\" else None,\n    )\n\n    # Optionally, if you have true values, plot them as red 'x'\n    if true_col in df_sorted_vi.columns:\n        ax.plot(\n            df_sorted_vi[true_col],\n            yvals,\n            \"x\",\n            color=\"red\",\n            label=\"True_vi\" if par == \"v\" else None,\n        )\n\n    ax.set_title(par)\n    ax.set_yticks(yvals)\n    ax.set_yticklabels(df_sorted_vi[\"participant_id\"])\n    ax.invert_yaxis()  # optional, if you prefer subject_01 at the top\n    if par == \"v\":\n        ax.legend()\n\nplt.tight_layout()\nplt.show()\n</pre> # Suppose your dataframe is called `df` params = hssm.defaults.default_model_config[\"ddm\"][\"list_params\"] fig, axes = plt.subplots(nrows=1, ncols=len(params), figsize=(14, 6), sharey=True)  for ax, par in zip(axes, params):     # Identify the relevant columns for this parameter     mean_col = f\"{par}_mean\"     lower_col = f\"{par}_lower_hdi\"     upper_col = f\"{par}_higher_hdi\"     true_col = f\"{par}_true\"  # if you also want to plot 'true' values      # Sort if you want the participants in order on the y\u2010axis     df_sorted_vi = plot_df_vi.sort_values(\"participant_id\")     df_sorted_mcmc = plot_df_mcmc.sort_values(\"participant_id\")      # We'll use the row index (0..N-1) for plotting against x=mean     yvals = np.array(range(len(df_sorted_vi)))     yvals_vi = yvals + 0.1     yvals_mcmc = yvals - 0.1      # Draw horizontal lines from lower_HDI to upper_HDI     ax.hlines(         yvals_vi,         df_sorted_vi[lower_col],         df_sorted_vi[upper_col],         color=\"blue\",         alpha=0.5,     )      ax.hlines(         yvals_mcmc,         df_sorted_mcmc[lower_col],         df_sorted_mcmc[upper_col],         color=\"green\",         alpha=0.5,     )      # Mark the posterior mean in blue     ax.plot(         df_sorted_vi[mean_col],         yvals_vi,         \"|\",         color=\"blue\",         label=\"Mean_vi\" if par == \"v\" else None,     )     ax.plot(         df_sorted_mcmc[mean_col],         yvals_mcmc,         \"|\",         color=\"green\",         label=\"Mean_mcmc\" if par == \"v\" else None,     )      # Optionally, if you have true values, plot them as red 'x'     if true_col in df_sorted_vi.columns:         ax.plot(             df_sorted_vi[true_col],             yvals,             \"x\",             color=\"red\",             label=\"True_vi\" if par == \"v\" else None,         )      ax.set_title(par)     ax.set_yticks(yvals)     ax.set_yticklabels(df_sorted_vi[\"participant_id\"])     ax.invert_yaxis()  # optional, if you prefer subject_01 at the top     if par == \"v\":         ax.legend()  plt.tight_layout() plt.show() <p>Here, results match up quite nicely between the two approaches. Overall the posteriors are very similar, and we don't see a strong tendency for VI posteriors to be more peaked than MCMC posteriors. However this is just an example, and we can't deduce a general rule from this observation.</p> <p>To highlight that once we have the trial wise parameters, we can easily generate subject (and/or condition) wise plots of the posteriors, we also show an example using the non-centered parameterization.</p> <p>The non-centered parameterization is slightly more complex, so manually recomposing parameters can sometimes be a little bit more confusing.</p> <p>We can let HSSM handle this via Bambi under the hood, and not worry about this complexity.</p> In\u00a0[14]: Copied! <pre># Generative model\nmSimNonCentered = hssm.HSSM(\n    data=simDataDDM,\n    p_outlier=0.01,\n    prior_settings=\"safe\",\n    noncentered=True,\n    model=\"ddm\",\n    loglik_kind=\"approx_differentiable\",\n    include=[\n        {\n            \"name\": \"v\",\n            \"formula\": \"v ~ 1 + (1|participant_id)\",\n        },\n        {\n            \"name\": \"a\",\n            \"formula\": \"a ~ 1 + (1|participant_id)\",\n        },\n        {\n            \"name\": \"z\",\n            \"formula\": \"z ~ 1 + (1|participant_id)\",\n        },\n        {\n            \"name\": \"t\",\n            \"formula\": \"t ~ 1 + (1|participant_id)\",\n        },\n    ],\n)\n</pre> # Generative model mSimNonCentered = hssm.HSSM(     data=simDataDDM,     p_outlier=0.01,     prior_settings=\"safe\",     noncentered=True,     model=\"ddm\",     loglik_kind=\"approx_differentiable\",     include=[         {             \"name\": \"v\",             \"formula\": \"v ~ 1 + (1|participant_id)\",         },         {             \"name\": \"a\",             \"formula\": \"a ~ 1 + (1|participant_id)\",         },         {             \"name\": \"z\",             \"formula\": \"z ~ 1 + (1|participant_id)\",         },         {             \"name\": \"t\",             \"formula\": \"t ~ 1 + (1|participant_id)\",         },     ], ) <pre>Model initialized successfully.\n</pre> In\u00a0[15]: Copied! <pre># VI\n# obj_optimizer=pm.adamax(learning_rate=0.1)\nvi_idata_NC = mSimNonCentered.vi(\n    niter=30000,\n    method=\"advi\",  # mention full_rank_advi\n    obj_optimizer=pm.adamax(learning_rate=0.01),\n)\nmSimNonCenteredVIObject = mSimNonCentered.vi_approx.sample(draws=1000)\n</pre> # VI # obj_optimizer=pm.adamax(learning_rate=0.1) vi_idata_NC = mSimNonCentered.vi(     niter=30000,     method=\"advi\",  # mention full_rank_advi     obj_optimizer=pm.adamax(learning_rate=0.01), ) mSimNonCenteredVIObject = mSimNonCentered.vi_approx.sample(draws=1000) <pre>Using MCMC starting point defaults.\n</pre> <pre>Output()</pre> <pre></pre> <pre>Finished [100%]: Average Loss = 4,191.3\n</pre> In\u00a0[16]: Copied! <pre># Loss plot\nplt.plot(mSimNonCentered.vi_approx.hist)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"VI iteration loss\")\nplt.show()\n</pre> # Loss plot plt.plot(mSimNonCentered.vi_approx.hist) plt.xlabel(\"Iteration\") plt.ylabel(\"Loss\") plt.title(\"VI iteration loss\") plt.show() In\u00a0[17]: Copied! <pre># MCMC\nmSimNonCenteredSampled = mSimNonCentered.sample(\n    sampler=\"nuts_numpyro\",\n    cores=4,\n    chains=4,\n    draws=250,\n    tune=750,\n    mp_ctx=\"forkserver\",\n    nuts_kwargs={\"max_tree_depth\": 5},\n)\n\nmSimNonCentered.sample_posterior_predictive(idata=mSimNonCenteredSampled)\n</pre> # MCMC mSimNonCenteredSampled = mSimNonCentered.sample(     sampler=\"nuts_numpyro\",     cores=4,     chains=4,     draws=250,     tune=750,     mp_ctx=\"forkserver\",     nuts_kwargs={\"max_tree_depth\": 5}, )  mSimNonCentered.sample_posterior_predictive(idata=mSimNonCenteredSampled) <pre>Using default initvals. \n\n</pre> <pre>sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [02:49&lt;00:00,  5.91it/s, 31 steps of size 1.47e-01. acc. prob=0.94]\nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [02:49&lt;00:00,  5.91it/s, 31 steps of size 1.51e-01. acc. prob=0.90]\nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [02:40&lt;00:00,  6.24it/s, 31 steps of size 1.78e-01. acc. prob=0.90]\nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [02:50&lt;00:00,  5.85it/s, 31 steps of size 1.66e-01. acc. prob=0.91]\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:02&lt;00:00, 421.69it/s]\n</pre> In\u00a0[18]: Copied! <pre>mSimNonCenteredSampled = mSimNonCentered.add_likelihood_parameters_to_idata(\n    mSimNonCenteredSampled\n)\n\nplot_df_vi_nc = process_idata_for_plotting(\n    idata=mSimNonCenteredVIObject, parameter_matrix=parameter_matrix, model=\"ddm\"\n)\n\nplot_df_mcmc_nc = process_idata_for_plotting(\n    idata=mSimNonCenteredSampled, parameter_matrix=parameter_matrix, model=\"ddm\"\n)\n</pre> mSimNonCenteredSampled = mSimNonCentered.add_likelihood_parameters_to_idata(     mSimNonCenteredSampled )  plot_df_vi_nc = process_idata_for_plotting(     idata=mSimNonCenteredVIObject, parameter_matrix=parameter_matrix, model=\"ddm\" )  plot_df_mcmc_nc = process_idata_for_plotting(     idata=mSimNonCenteredSampled, parameter_matrix=parameter_matrix, model=\"ddm\" ) In\u00a0[19]: Copied! <pre>hssm.plotting.plot_predictive(\n    mSimNonCentered, col=\"participant_id\", col_wrap=5\n)\n</pre> hssm.plotting.plot_predictive(     mSimNonCentered, col=\"participant_id\", col_wrap=5 ) Out[19]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x31cda4610&gt;</pre> In\u00a0[20]: Copied! <pre># Suppose your dataframe is called `df`\nparams = hssm.defaults.default_model_config[\"ddm\"][\"list_params\"]\nfig, axes = plt.subplots(nrows=1, ncols=len(params), figsize=(14, 6), sharey=True)\n\nfor ax, par in zip(axes, params):\n    # Identify the relevant columns for this parameter\n    mean_col = f\"{par}_mean\"\n    lower_col = f\"{par}_lower_hdi\"\n    upper_col = f\"{par}_higher_hdi\"\n    true_col = f\"{par}_true\"  # if you also want to plot 'true' values\n\n    # Sort if you want the participants in order on the y\u2010axis\n    df_sorted_vi_nc = plot_df_vi_nc.sort_values(\"participant_id\")\n    df_sorted_mcmc_nc = plot_df_mcmc_nc.sort_values(\"participant_id\")\n\n    # We'll use the row index (0..N-1) for plotting against x=mean\n    yvals = np.array(range(len(df_sorted_vi_nc)))\n    yvals_vi = yvals + 0.1\n    yvals_mcmc = yvals - 0.1\n\n    # Draw horizontal lines from lower_HDI to upper_HDI\n    ax.hlines(\n        yvals_vi,\n        df_sorted_vi_nc[lower_col],\n        df_sorted_vi_nc[upper_col],\n        color=\"blue\",\n        alpha=0.5,\n    )\n\n    ax.hlines(\n        yvals_mcmc,\n        df_sorted_mcmc_nc[lower_col],\n        df_sorted_mcmc_nc[upper_col],\n        color=\"green\",\n        alpha=0.5,\n    )\n\n    # Mark the posterior mean in blue\n    ax.plot(\n        df_sorted_vi_nc[mean_col],\n        yvals,\n        \"|\",\n        color=\"blue\",\n        label=\"Mean_vi\" if par == \"v\" else None,\n    )\n    ax.plot(\n        df_sorted_mcmc_nc[mean_col],\n        yvals_mcmc,\n        \"|\",\n        color=\"green\",\n        label=\"Mean_mcmc\" if par == \"v\" else None,\n    )\n\n    # Optionally, if you have true values, plot them as red 'x'\n    if true_col in df_sorted_vi_nc.columns:\n        ax.plot(\n            df_sorted_vi_nc[true_col],\n            yvals,\n            \"x\",\n            color=\"red\",\n            label=\"True_vi\" if par == \"v\" else None,\n        )\n\n    ax.set_title(par)\n    ax.set_yticks(yvals)\n    ax.set_yticklabels(df_sorted_vi_nc[\"participant_id\"])\n    ax.invert_yaxis()  # optional, if you prefer subject_01 at the top\n    if par == \"v\":\n        ax.legend()\n\nplt.tight_layout()\nplt.show()\n</pre> # Suppose your dataframe is called `df` params = hssm.defaults.default_model_config[\"ddm\"][\"list_params\"] fig, axes = plt.subplots(nrows=1, ncols=len(params), figsize=(14, 6), sharey=True)  for ax, par in zip(axes, params):     # Identify the relevant columns for this parameter     mean_col = f\"{par}_mean\"     lower_col = f\"{par}_lower_hdi\"     upper_col = f\"{par}_higher_hdi\"     true_col = f\"{par}_true\"  # if you also want to plot 'true' values      # Sort if you want the participants in order on the y\u2010axis     df_sorted_vi_nc = plot_df_vi_nc.sort_values(\"participant_id\")     df_sorted_mcmc_nc = plot_df_mcmc_nc.sort_values(\"participant_id\")      # We'll use the row index (0..N-1) for plotting against x=mean     yvals = np.array(range(len(df_sorted_vi_nc)))     yvals_vi = yvals + 0.1     yvals_mcmc = yvals - 0.1      # Draw horizontal lines from lower_HDI to upper_HDI     ax.hlines(         yvals_vi,         df_sorted_vi_nc[lower_col],         df_sorted_vi_nc[upper_col],         color=\"blue\",         alpha=0.5,     )      ax.hlines(         yvals_mcmc,         df_sorted_mcmc_nc[lower_col],         df_sorted_mcmc_nc[upper_col],         color=\"green\",         alpha=0.5,     )      # Mark the posterior mean in blue     ax.plot(         df_sorted_vi_nc[mean_col],         yvals,         \"|\",         color=\"blue\",         label=\"Mean_vi\" if par == \"v\" else None,     )     ax.plot(         df_sorted_mcmc_nc[mean_col],         yvals_mcmc,         \"|\",         color=\"green\",         label=\"Mean_mcmc\" if par == \"v\" else None,     )      # Optionally, if you have true values, plot them as red 'x'     if true_col in df_sorted_vi_nc.columns:         ax.plot(             df_sorted_vi_nc[true_col],             yvals,             \"x\",             color=\"red\",             label=\"True_vi\" if par == \"v\" else None,         )      ax.set_title(par)     ax.set_yticks(yvals)     ax.set_yticklabels(df_sorted_vi_nc[\"participant_id\"])     ax.invert_yaxis()  # optional, if you prefer subject_01 at the top     if par == \"v\":         ax.legend()  plt.tight_layout() plt.show() <p>Overall posteriors look very similar between the two approaches again. However, we can see that the MCMC posteriors are more variable than the VI posteriors in this case. This illustrates a case where VI posteriors are too confident, and bayesian t-tests on parameter differences may yield different results when using the MCMC vs. the VI posteriors. From a practical perspective, it is sometimes simply infeasible to run MCMC, while VI is still computationally tractable. In such cases we have to take what we can get...</p>"},{"location":"tutorials/variational_inference_hierarchical/#hierarchical-vi-example","title":"Hierarchical VI Example\u00b6","text":"<p>In this example we will fit a hierarchical model to a simulated dataset using both MCMC and VI, and perform a simple comparison of the results.</p> <p>Thanks for Guillaume Pagnier PhD for the initial version of this example.</p>"},{"location":"tutorials/variational_inference_hierarchical/#utilities","title":"Utilities\u00b6","text":""},{"location":"tutorials/variational_inference_hierarchical/#simulate-dataset","title":"Simulate Dataset\u00b6","text":""},{"location":"tutorials/variational_inference_hierarchical/#hierarchical-model-centered-parameterization","title":"Hierarchical Model: Centered Parameterization\u00b6","text":""},{"location":"tutorials/variational_inference_hierarchical/#fit-vi","title":"Fit VI\u00b6","text":""},{"location":"tutorials/variational_inference_hierarchical/#fit-mcmc","title":"Fit MCMC\u00b6","text":""},{"location":"tutorials/variational_inference_hierarchical/#process-results-for-plotting","title":"Process results for plotting\u00b6","text":""},{"location":"tutorials/variational_inference_hierarchical/#plotting","title":"Plotting\u00b6","text":""},{"location":"tutorials/variational_inference_hierarchical/#hierarchical-model-non-centered-parameterization","title":"Hierarchical Model: Non-Centered Parameterization\u00b6","text":""},{"location":"tutorials/variational_inference_hierarchical/#fit-vi","title":"Fit VI\u00b6","text":""},{"location":"tutorials/variational_inference_hierarchical/#fit-mcmc","title":"Fit MCMC\u00b6","text":""},{"location":"tutorials/variational_inference_hierarchical/#post-process-vi-mcmc-results","title":"Post-process VI / MCMC results\u00b6","text":""},{"location":"tutorials/variational_inference_hierarchical/#plotting","title":"Plotting\u00b6","text":""}]}